{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-06T09:33:52.023395800Z",
     "start_time": "2023-09-06T09:33:49.149504300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ML\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LIST = []\n",
    "torch.manual_seed(42)\n",
    "for rand in tqdm(range(10)):\n",
    "    # print(f\"we are going to {rand + 1} itter #########################################################################\")\n",
    "    all_r2 = []\n",
    "    for iterr in tqdm(range(9)):\n",
    "        # print(f\"we are going to {iterr + 1} itter ********************************************************************\")\n",
    "        temp = 0\n",
    "        folder = f\"prepared_data/prepared_data-{rand + 1}/Scaled/Anaheim/{int((iterr + 1) * 10)}\"\n",
    "        miss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        data = ML.DatLoader_Me(folder, miss[iterr], batch_size=32, device=\"cpu\")\n",
    "        train, val, test = data.train, data.val, data.test\n",
    "\n",
    "        each_model_feature_and_r2 = []\n",
    "        each_miss_rate_r2 = []\n",
    "        repeat = 5\n",
    "        for r in range(repeat):\n",
    "            # print(\n",
    "            # f\"we are going to {r + 1} repeat _____________________________________________________________________\")\n",
    "            input_size = 3\n",
    "            hide1 = 64\n",
    "            hide2 = 64\n",
    "            output_size = 1\n",
    "            epochs = 200\n",
    "            batch_size = 32\n",
    "            learning_rate = np.random.choice([0.01, 0.02, 0.05, 0.005])\n",
    "\n",
    "            model = ML.FlowPredict(input_size, hide1, hide2, output_size)\n",
    "\n",
    "            model, [count_epoch, loss_values, val_loss_values] = ML.train_model(model=model, train=train,\n",
    "                                                                                val=val, epochs=epochs,\n",
    "                                                                                learning_rate=learning_rate)\n",
    "            [val_loss, val_acc] = ML.evaluate_model(model, val, rand, iterr, r)\n",
    "            r2 = val_acc\n",
    "            each_model_feature_and_r2.append([val_acc, hide1, hide2, learning_rate, epochs, miss[iterr]])\n",
    "\n",
    "\n",
    "            # plt.clf()\n",
    "            # plt.plot(count_epoch, loss_values, c=\"b\", label=\"Train\")\n",
    "            # plt.plot(count_epoch, val_loss_values, c=\"r\", label=\"val\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "            def f(x):\n",
    "                return x[0]\n",
    "\n",
    "\n",
    "            best = max(each_model_feature_and_r2, key=f)\n",
    "\n",
    "            LIST.append(best)\n",
    "            # print(best)\n",
    "            if not os.path.exists(\"Models/Anaheim\"):\n",
    "                os.makedirs(\"Models/Anaheim\")\n",
    "            if temp < best[0]:\n",
    "                temp = best[0]\n",
    "                torch.save(model, f\"Models/Anaheim/Anaheim{rand}_{iterr}_{r}_model_nn.pth\")\n",
    "            each_miss_rate_r2.append(r2)\n",
    "        each_miss_rate_r2.append(temp)\n",
    "        all_r2.append(each_miss_rate_r2)\n",
    "    data = pd.DataFrame(all_r2)\n",
    "    col_name = list(range(1, repeat + 1))\n",
    "    col_name.append(\"max\")\n",
    "    data.columns = col_name\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "    data.to_csv(f\"data/Anaheim{rand + 1}_model_nn.csv\")\n",
    "for i in LIST:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70b616cc6a8cbb5d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6c9ff6fdb2c43c78e14d33114feb1a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44c4acf2290343c79a583619988d51be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9bed6364776447c91974b1f7734b48f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "653df51c0a33494fa2a7c95435943d6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43739fc1d442446084589012e248b52d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 29\u001B[0m\n\u001B[0;32m     25\u001B[0m learning_rate \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.02\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.005\u001B[39m])\n\u001B[0;32m     27\u001B[0m model \u001B[38;5;241m=\u001B[39m ML\u001B[38;5;241m.\u001B[39mFlowPredict(input_size, hide1, hide2, output_size)\n\u001B[1;32m---> 29\u001B[0m model, [count_epoch, loss_values, val_loss_values] \u001B[38;5;241m=\u001B[39m \u001B[43mML\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m                                                                    \u001B[49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m [val_loss, val_acc] \u001B[38;5;241m=\u001B[39m ML\u001B[38;5;241m.\u001B[39mevaluate_model(model, val, rand, iterr, r)\n\u001B[0;32m     32\u001B[0m r2 \u001B[38;5;241m=\u001B[39m val_acc\n",
      "File \u001B[1;32mC:\\All Python Projects\\Machine Learning Projects\\Project 2 - Transportation\\Nueral Network\\ML.py:132\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train, val, epochs, learning_rate)\u001B[0m\n\u001B[0;32m    130\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[0;32m    131\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 132\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    135\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "LIST = []\n",
    "torch.manual_seed(42)\n",
    "for rand in tqdm(range(10)):\n",
    "    # print(f\"we are going to {rand + 1} itter #########################################################################\")\n",
    "    all_r2 = []\n",
    "    for iterr in tqdm(range(9)):\n",
    "        # print(f\"we are going to {iterr + 1} itter ********************************************************************\")\n",
    "        temp = 0\n",
    "        folder = f\"prepared_data/prepared_data-{rand + 1}/Scaled/SiouxFall/{int((iterr + 1) * 10)}\"\n",
    "        miss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        data = ML.DatLoader_Me(folder, miss[iterr], batch_size=32, device=\"cpu\")\n",
    "        train, val, test = data.train, data.val, data.test\n",
    "\n",
    "        each_model_feature_and_r2 = []\n",
    "        each_miss_rate_r2 = []\n",
    "        repeat = 5\n",
    "        for r in range(repeat):\n",
    "            # print(\n",
    "            #     f\"we are going to {r + 1} repeat _____________________________________________________________________\")\n",
    "            input_size = 3\n",
    "            hide1 = 64\n",
    "            hide2 = 64\n",
    "            output_size = 1\n",
    "            epochs = 300\n",
    "            learning_rate = np.random.choice([0.01, 0.02, 0.05, 0.005])\n",
    "\n",
    "            model = ML.FlowPredict(input_size, hide1, hide2, output_size)\n",
    "\n",
    "            model, [count_epoch, loss_values, val_loss_values] = ML.train_model(model, train,\n",
    "                                                                                val, epochs, learning_rate)\n",
    "            [val_loss, val_acc] = ML.evaluate_model(model, val, rand, iterr, r)\n",
    "            r2 = val_acc\n",
    "            each_model_feature_and_r2.append([val_acc, hide1, hide2, learning_rate, epochs, miss[iterr]])\n",
    "\n",
    "\n",
    "            # plt.clf()\n",
    "            # plt.plot(count_epoch, loss_values, c=\"b\", label=\"Train\")\n",
    "            # plt.plot(count_epoch, val_loss_values, c=\"r\", label=\"val\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "            def f(x):\n",
    "                return x[0]\n",
    "\n",
    "\n",
    "            best = max(each_model_feature_and_r2, key=f)\n",
    "\n",
    "            LIST.append(best)\n",
    "            if not os.path.exists(\"Models/SiouxFall\"):\n",
    "                os.makedirs(\"Models/SiouxFall\")\n",
    "            if temp < best[0]:\n",
    "                temp = best[0]\n",
    "                torch.save(model, f\"Models/SiouxFall/SiouxFall{rand}_{iterr}_{r}_model_nn.pth\")\n",
    "            each_miss_rate_r2.append(r2)\n",
    "        each_miss_rate_r2.append(temp)\n",
    "        all_r2.append(each_miss_rate_r2)\n",
    "    data = pd.DataFrame(all_r2)\n",
    "    col_name = list(range(1, repeat + 1))\n",
    "    col_name.append(\"max\")\n",
    "    data.columns = col_name\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "    data.to_csv(f\"data/SiouxFall{rand + 1}_model_nn.csv\")\n",
    "for i in LIST:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T09:35:14.644649200Z",
     "start_time": "2023-09-06T09:34:54.427611500Z"
    }
   },
   "id": "aabb89c57aa676ce"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95aa6e91c3d545cf8b44571ea0ba246c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b57af37ad3434d09b3e31d39060901d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c705200a9f604c85a5c0b55adf547193"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 30\u001B[0m\n\u001B[0;32m     26\u001B[0m learning_rate \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m0.02\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;241m0.005\u001B[39m])\n\u001B[0;32m     28\u001B[0m model \u001B[38;5;241m=\u001B[39m ML\u001B[38;5;241m.\u001B[39mFlowPredict(input_size, hide1, hide2, output_size)\n\u001B[1;32m---> 30\u001B[0m model, [count_epoch, loss_values, val_loss_values] \u001B[38;5;241m=\u001B[39m \u001B[43mML\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m                                                                    \u001B[49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m [val_loss, val_acc] \u001B[38;5;241m=\u001B[39m ML\u001B[38;5;241m.\u001B[39mevaluate_model(model, val, rand, iterr, r)\n\u001B[0;32m     33\u001B[0m r2 \u001B[38;5;241m=\u001B[39m val_acc\n",
      "File \u001B[1;32mC:\\All Python Projects\\Machine Learning Projects\\Project 2 - Transportation\\Nueral Network\\ML.py:133\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train, val, epochs, learning_rate)\u001B[0m\n\u001B[0;32m    131\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    132\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m--> 133\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    130\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    133\u001B[0m         group,\n\u001B[0;32m    134\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    138\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    139\u001B[0m         state_steps)\n\u001B[1;32m--> 141\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    279\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 281\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\All Python Projects\\Interpreters\\in_Machine_Learning\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    389\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 391\u001B[0m     denom \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    393\u001B[0m param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "LIST = []\n",
    "torch.manual_seed(42)\n",
    "for rand in tqdm(range(10)):\n",
    "    # print(f\"we are going to {rand + 1} itter #########################################################################\")\n",
    "    all_r2 = []\n",
    "    for iterr in tqdm(range(9)):\n",
    "        # print(f\"we are going to {iterr + 1} itter ********************************************************************\")\n",
    "        temp = 0\n",
    "        folder = f\"prepared_data/prepared_data-{rand + 1}/Scaled/Chicago_Sketch/{int((iterr + 1) * 10)}\"\n",
    "        miss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        data = ML.DatLoader_Me(folder, miss[iterr], batch_size=32, device=\"cpu\")\n",
    "        train, val, test = data.train, data.val, data.test\n",
    "\n",
    "        each_model_feature_and_r2 = []\n",
    "        each_miss_rate_r2 = []\n",
    "        repeat = 1\n",
    "        for r in range(repeat):\n",
    "            # print(\n",
    "            #     f\"we are going to {r + 1} repeat _____________________________________________________________________\")\n",
    "            input_size = 3\n",
    "            hide1 = 32\n",
    "            hide2 = 32\n",
    "            output_size = 1\n",
    "            epochs = 300\n",
    "            batch_size = 32\n",
    "            learning_rate = np.random.choice([0.01, 0.02, 0.05, 0.005])\n",
    "\n",
    "            model = ML.FlowPredict(input_size, hide1, hide2, output_size)\n",
    "\n",
    "            model, [count_epoch, loss_values, val_loss_values] = ML.train_model(model, train,\n",
    "                                                                                val, epochs, learning_rate)\n",
    "            [val_loss, val_acc] = ML.evaluate_model(model, val, rand, iterr, r)\n",
    "            r2 = val_acc\n",
    "            each_model_feature_and_r2.append([val_acc, hide1, hide2, learning_rate, epochs, miss[iterr]])\n",
    "\n",
    "            plt.clf()\n",
    "            plt.plot(count_epoch, loss_values, c=\"b\", label=\"Train\")\n",
    "            plt.plot(count_epoch, val_loss_values, c=\"r\", label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            def f(x):\n",
    "                return x[0]\n",
    "\n",
    "\n",
    "            best = max(each_model_feature_and_r2, key=f)\n",
    "\n",
    "            print(best)\n",
    "            if not os.path.exists(\"Models/Chicago_Sketch\"):\n",
    "                os.makedirs(\"Models/Chicago_Sketch\")\n",
    "            if temp < best[0]:\n",
    "                temp = best[0]\n",
    "                torch.save(model, f\"Models/Chicago_Sketch/Chicago_Sketch{rand}_{iterr}_{r}_model_nn.pth\")\n",
    "            each_miss_rate_r2.append(r2)\n",
    "        each_miss_rate_r2.append(temp)\n",
    "        all_r2.append(each_miss_rate_r2)\n",
    "    data = pd.DataFrame(all_r2)\n",
    "    col_name = list(range(1, repeat + 1))\n",
    "    col_name.append(\"max\")\n",
    "    data.columns = col_name\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "    data.to_csv(f\"data/Chicago_Sketch{rand + 1}_model_nn.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T09:38:11.813533500Z",
     "start_time": "2023-09-06T09:37:56.250025500Z"
    }
   },
   "id": "e1a00e77c1624384"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4915535d266e41c68580f9a372d0a837"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "006096860102401381055adadba9ef5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprepared_data/prepared_data-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrand\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/Scaled/Chicago_Regional/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m((iterr\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m10\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     13\u001B[0m miss \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.3\u001B[39m, \u001B[38;5;241m0.4\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.6\u001B[39m, \u001B[38;5;241m0.7\u001B[39m, \u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0.9\u001B[39m]\n\u001B[1;32m---> 14\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mML\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDatLoader_Me\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfolder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmiss\u001B[49m\u001B[43m[\u001B[49m\u001B[43miterr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m train, val, test \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mtrain, data\u001B[38;5;241m.\u001B[39mval, data\u001B[38;5;241m.\u001B[39mtest\n\u001B[0;32m     17\u001B[0m each_model_feature_and_r2 \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mC:\\All Python Projects\\Machine Learning Projects\\Project 2 - Transportation\\Nueral Network\\ML.py:51\u001B[0m, in \u001B[0;36mDatLoader_Me.__init__\u001B[1;34m(self, folder_name, miss_rate, batch_size, device)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train \u001B[38;5;241m=\u001B[39m y_train[mask_train]\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train\u001B[38;5;241m.\u001B[39mdropna(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 51\u001B[0m y_val \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_data\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mreshape(index_len), index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mindex_len\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     52\u001B[0m mask_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39my_val\u001B[38;5;241m.\u001B[39misin([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFalse\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo_connection\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_val \u001B[38;5;241m=\u001B[39m y_val[mask_val]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "LIST = []\n",
    "torch.manual_seed(42)\n",
    "for rand in tqdm(range(10)):\n",
    "    # print(f\"we are going to {rand + 1} itter #########################################################################\")\n",
    "    all_r2 = []\n",
    "    for iterr in tqdm(range(9)):\n",
    "        start_time = timer()\n",
    "        # print(f\"we are going to {iterr + 1} itter ********************************************************************\")\n",
    "        temp = 0\n",
    "        folder = f\"prepared_data/prepared_data-{rand + 1}/Scaled/Chicago_Regional/{int((iterr + 1) * 10)}\"\n",
    "        miss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        data = ML.DatLoader_Me(folder, miss[iterr], batch_size=32, device=\"cpu\")\n",
    "        train, val, test = data.train, data.val, data.test\n",
    "\n",
    "        each_model_feature_and_r2 = []\n",
    "        each_miss_rate_r2 = []\n",
    "        repeat = 1\n",
    "        end_time = timer()\n",
    "        print(f\"time of this is {end_time - start_time}\")\n",
    "        for r in range(repeat):\n",
    "            # print(\n",
    "            #     f\"we are going to {r + 1} repeat _____________________________________________________________________\")\n",
    "            input_size = 3\n",
    "            hide1 = 32\n",
    "            hide2 = 32\n",
    "            output_size = 1\n",
    "            epochs = 300\n",
    "            batch_size = 32\n",
    "            learning_rate = np.random.choice([0.01, 0.02, 0.05, 0.005])\n",
    "\n",
    "            model = ML.FlowPredict(input_size, hide1, hide2, output_size)\n",
    "            model = model\n",
    "            model, [count_epoch, loss_values, val_loss_values] = ML.train_model(model, train,\n",
    "                                                                                val, epochs, learning_rate)\n",
    "            [val_loss, val_acc] = ML.evaluate_model(model, val, rand, iterr, r)\n",
    "            r2 = val_acc\n",
    "            each_model_feature_and_r2.append([val_acc, hide1, hide2, learning_rate, epochs, miss[iterr]])\n",
    "\n",
    "\n",
    "            # plt.clf()\n",
    "            # plt.plot(count_epoch, loss_values, c=\"b\", label=\"Train\")\n",
    "            # plt.plot(count_epoch, val_loss_values, c=\"r\", label=\"val\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "            def f(x):\n",
    "                return x[0]\n",
    "\n",
    "\n",
    "            best = max(each_model_feature_and_r2, key=f)\n",
    "\n",
    "            print(best)\n",
    "            if not os.path.exists(\"Models/Chicago_Regional\"):\n",
    "                os.makedirs(\"Models/Chicago_Regional\")\n",
    "            if temp < best[0]:\n",
    "                temp = best[0]\n",
    "                torch.save(model, f\"Models/Chicago_Regional/Chicago_Regional{rand}_{iterr}_{r}_model_nn.pth\")\n",
    "            each_miss_rate_r2.append(r2)\n",
    "        each_miss_rate_r2.append(temp)\n",
    "        all_r2.append(each_miss_rate_r2)\n",
    "    data = pd.DataFrame(all_r2)\n",
    "    col_name = list(range(1, repeat + 1))\n",
    "    col_name.append(\"max\")\n",
    "    data.columns = col_name\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "    data.to_csv(f\"data/Chicago_Regional{rand + 1}_model_nn.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T09:20:27.146519200Z",
     "start_time": "2023-09-06T09:20:23.026563800Z"
    }
   },
   "id": "ab6c9c57de5e1c84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed5f29ca2f10f104"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
