{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-19 23:54:13,079] A new study created in memory with name: no-name-219e95e4-484c-4477-a065-02a40267be10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a25d21ddcb843d8b58915f755557b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train: Loss 0.616300 Accuracy -21.6018 | validation: Loss 2.008386 Accuracy -125.1727\n",
      "Epoch 2 | train: Loss 0.292755 Accuracy -9.7363 | validation: Loss 0.118164 Accuracy -6.4234\n",
      "Epoch 3 | train: Loss 0.049796 Accuracy -0.8262 | validation: Loss 0.073399 Accuracy -3.6112\n",
      "Epoch 4 | train: Loss 0.064498 Accuracy -1.3654 | validation: Loss 0.061511 Accuracy -2.8643\n",
      "Epoch 5 | train: Loss 0.042314 Accuracy -0.5518 | validation: Loss 0.064972 Accuracy -3.0817\n",
      "Epoch 6 | train: Loss 0.023661 Accuracy 0.1323 | validation: Loss 0.090944 Accuracy -4.7134\n",
      "Epoch 7 | train: Loss 0.026356 Accuracy 0.0334 | validation: Loss 0.089937 Accuracy -4.6501\n",
      "Epoch 8 | train: Loss 0.027590 Accuracy -0.0118 | validation: Loss 0.061180 Accuracy -2.8435\n",
      "Epoch 9 | train: Loss 0.021193 Accuracy 0.2228 | validation: Loss 0.041817 Accuracy -1.6271\n",
      "Epoch 10 | train: Loss 0.019919 Accuracy 0.2695 | validation: Loss 0.037318 Accuracy -1.3444\n",
      "Epoch 11 | train: Loss 0.022379 Accuracy 0.1793 | validation: Loss 0.033441 Accuracy -1.1009\n",
      "Epoch 12 | train: Loss 0.023375 Accuracy 0.1427 | validation: Loss 0.061858 Accuracy -2.8861\n",
      "Epoch 13 | train: Loss 0.022491 Accuracy 0.1752 | validation: Loss 0.039735 Accuracy -1.4963\n",
      "Epoch 14 | train: Loss 0.019747 Accuracy 0.2758 | validation: Loss 0.030746 Accuracy -0.9316\n",
      "Epoch 15 | train: Loss 0.019793 Accuracy 0.2741 | validation: Loss 0.030563 Accuracy -0.9200\n",
      "Epoch 16 | train: Loss 0.019242 Accuracy 0.2943 | validation: Loss 0.031170 Accuracy -0.9582\n",
      "Epoch 17 | train: Loss 0.017601 Accuracy 0.3545 | validation: Loss 0.059725 Accuracy -2.7521\n",
      "Epoch 18 | train: Loss 0.017804 Accuracy 0.3471 | validation: Loss 0.061761 Accuracy -2.8800\n",
      "Epoch 19 | train: Loss 0.015869 Accuracy 0.4180 | validation: Loss 0.043243 Accuracy -1.7167\n",
      "Epoch 20 | train: Loss 0.015627 Accuracy 0.4269 | validation: Loss 0.078772 Accuracy -3.9487\n",
      "Epoch 21 | train: Loss 0.015560 Accuracy 0.4294 | validation: Loss 0.067706 Accuracy -3.2535\n",
      "Epoch 22 | train: Loss 0.014887 Accuracy 0.4540 | validation: Loss 0.025680 Accuracy -0.6133\n",
      "Epoch 23 | train: Loss 0.013568 Accuracy 0.5024 | validation: Loss 0.029057 Accuracy -0.8255\n",
      "Epoch 24 | train: Loss 0.012794 Accuracy 0.5308 | validation: Loss 0.030217 Accuracy -0.8983\n",
      "Epoch 25 | train: Loss 0.012106 Accuracy 0.5560 | validation: Loss 0.032132 Accuracy -1.0186\n",
      "Epoch 26 | train: Loss 0.011912 Accuracy 0.5631 | validation: Loss 0.027336 Accuracy -0.7173\n",
      "Epoch 27 | train: Loss 0.012686 Accuracy 0.5347 | validation: Loss 0.054867 Accuracy -2.4469\n",
      "Epoch 28 | train: Loss 0.017222 Accuracy 0.3684 | validation: Loss 0.026854 Accuracy -0.6871\n",
      "Epoch 29 | train: Loss 0.013143 Accuracy 0.5180 | validation: Loss 0.025758 Accuracy -0.6182\n",
      "Epoch 30 | train: Loss 0.014612 Accuracy 0.4641 | validation: Loss 0.059133 Accuracy -2.7149\n",
      "Epoch 31 | train: Loss 0.013946 Accuracy 0.4886 | validation: Loss 0.059724 Accuracy -2.7521\n",
      "Epoch 32 | train: Loss 0.013608 Accuracy 0.5010 | validation: Loss 0.023776 Accuracy -0.4937\n",
      "Epoch 33 | train: Loss 0.012459 Accuracy 0.5431 | validation: Loss 0.021186 Accuracy -0.3309\n",
      "Epoch 34 | train: Loss 0.012657 Accuracy 0.5358 | validation: Loss 0.046734 Accuracy -1.9359\n",
      "Epoch 35 | train: Loss 0.013181 Accuracy 0.5166 | validation: Loss 0.040214 Accuracy -1.5263\n",
      "Epoch 36 | train: Loss 0.012820 Accuracy 0.5298 | validation: Loss 0.018609 Accuracy -0.1691\n",
      "Epoch 37 | train: Loss 0.013493 Accuracy 0.5052 | validation: Loss 0.024331 Accuracy -0.5285\n",
      "Epoch 38 | train: Loss 0.010965 Accuracy 0.5979 | validation: Loss 0.033923 Accuracy -1.1311\n",
      "Epoch 39 | train: Loss 0.011900 Accuracy 0.5636 | validation: Loss 0.023116 Accuracy -0.4522\n",
      "Epoch 40 | train: Loss 0.010470 Accuracy 0.6160 | validation: Loss 0.031214 Accuracy -0.9609\n",
      "Epoch 41 | train: Loss 0.009778 Accuracy 0.6414 | validation: Loss 0.041087 Accuracy -1.5812\n",
      "Epoch 42 | train: Loss 0.011065 Accuracy 0.5942 | validation: Loss 0.231574 Accuracy -13.5481\n",
      "Epoch 43 | train: Loss 0.045629 Accuracy -0.6734 | validation: Loss 0.406658 Accuracy -24.5474\n",
      "Epoch 44 | train: Loss 0.131767 Accuracy -3.8323 | validation: Loss 0.096169 Accuracy -5.0416\n",
      "Epoch 45 | train: Loss 0.032844 Accuracy -0.2045 | validation: Loss 0.442457 Accuracy -26.7964\n",
      "Epoch 46 | train: Loss 0.098435 Accuracy -2.6100 | validation: Loss 0.057870 Accuracy -2.6355\n",
      "Epoch 47 | train: Loss 0.014497 Accuracy 0.4683 | validation: Loss 0.052590 Accuracy -2.3039\n",
      "Epoch 48 | train: Loss 0.023418 Accuracy 0.1412 | validation: Loss 0.062289 Accuracy -2.9132\n",
      "Epoch 49 | train: Loss 0.030865 Accuracy -0.1319 | validation: Loss 0.049874 Accuracy -2.1332\n",
      "Epoch 50 | train: Loss 0.027098 Accuracy 0.0062 | validation: Loss 0.036764 Accuracy -1.3096\n",
      "Epoch 51 | train: Loss 0.019014 Accuracy 0.3027 | validation: Loss 0.049145 Accuracy -2.0874\n",
      "Epoch 52 | train: Loss 0.016421 Accuracy 0.3978 | validation: Loss 0.071735 Accuracy -3.5066\n",
      "Epoch 53 | train: Loss 0.021155 Accuracy 0.2242 | validation: Loss 0.054546 Accuracy -2.4268\n",
      "Epoch 54 | train: Loss 0.021133 Accuracy 0.2250 | validation: Loss 0.029771 Accuracy -0.8703\n",
      "Epoch 55 | train: Loss 0.018044 Accuracy 0.3383 | validation: Loss 0.029949 Accuracy -0.8815\n",
      "Epoch 56 | train: Loss 0.015628 Accuracy 0.4269 | validation: Loss 0.052154 Accuracy -2.2765\n",
      "Epoch 57 | train: Loss 0.017491 Accuracy 0.3586 | validation: Loss 0.048089 Accuracy -2.0211\n",
      "Epoch 58 | train: Loss 0.017303 Accuracy 0.3654 | validation: Loss 0.039441 Accuracy -1.4778\n",
      "Epoch 59 | train: Loss 0.017032 Accuracy 0.3754 | validation: Loss 0.031082 Accuracy -0.9526\n",
      "Epoch 60 | train: Loss 0.015606 Accuracy 0.4277 | validation: Loss 0.023564 Accuracy -0.4804\n",
      "Epoch 61 | train: Loss 0.014628 Accuracy 0.4635 | validation: Loss 0.034321 Accuracy -1.1561\n",
      "Epoch 62 | train: Loss 0.014117 Accuracy 0.4823 | validation: Loss 0.021702 Accuracy -0.3634\n",
      "Epoch 63 | train: Loss 0.013929 Accuracy 0.4892 | validation: Loss 0.035562 Accuracy -1.2341\n",
      "Epoch 64 | train: Loss 0.013469 Accuracy 0.5060 | validation: Loss 0.025444 Accuracy -0.5985\n",
      "Epoch 65 | train: Loss 0.012410 Accuracy 0.5449 | validation: Loss 0.024835 Accuracy -0.5602\n",
      "Epoch 66 | train: Loss 0.010683 Accuracy 0.6082 | validation: Loss 0.026919 Accuracy -0.6911\n",
      "Epoch 67 | train: Loss 0.010671 Accuracy 0.6086 | validation: Loss 0.040914 Accuracy -1.5703\n",
      "Epoch 68 | train: Loss 0.010491 Accuracy 0.6153 | validation: Loss 0.037824 Accuracy -1.3762\n",
      "Epoch 69 | train: Loss 0.009551 Accuracy 0.6497 | validation: Loss 0.034084 Accuracy -1.1413\n",
      "Epoch 70 | train: Loss 0.009886 Accuracy 0.6374 | validation: Loss 0.046451 Accuracy -1.9182\n",
      "Epoch 71 | train: Loss 0.009476 Accuracy 0.6525 | validation: Loss 0.045905 Accuracy -1.8839\n",
      "Epoch 72 | train: Loss 0.009542 Accuracy 0.6501 | validation: Loss 0.034468 Accuracy -1.1654\n",
      "Epoch 73 | train: Loss 0.009626 Accuracy 0.6470 | validation: Loss 0.037952 Accuracy -1.3843\n",
      "Epoch 74 | train: Loss 0.009305 Accuracy 0.6588 | validation: Loss 0.042708 Accuracy -1.6831\n",
      "Epoch 75 | train: Loss 0.009718 Accuracy 0.6436 | validation: Loss 0.031027 Accuracy -0.9492\n",
      "Epoch 76 | train: Loss 0.009562 Accuracy 0.6493 | validation: Loss 0.035297 Accuracy -1.2175\n",
      "Epoch 77 | train: Loss 0.009528 Accuracy 0.6506 | validation: Loss 0.031320 Accuracy -0.9676\n",
      "Epoch 78 | train: Loss 0.009412 Accuracy 0.6548 | validation: Loss 0.039917 Accuracy -1.5077\n",
      "Epoch 79 | train: Loss 0.009260 Accuracy 0.6604 | validation: Loss 0.048592 Accuracy -2.0527\n",
      "Epoch 80 | train: Loss 0.009189 Accuracy 0.6630 | validation: Loss 0.044235 Accuracy -1.7790\n",
      "Epoch 81 | train: Loss 0.009215 Accuracy 0.6621 | validation: Loss 0.059441 Accuracy -2.7342\n",
      "Epoch 82 | train: Loss 0.009514 Accuracy 0.6511 | validation: Loss 0.029638 Accuracy -0.8619\n",
      "Epoch 83 | train: Loss 0.011361 Accuracy 0.5833 | validation: Loss 0.093920 Accuracy -4.9003\n",
      "Epoch 84 | train: Loss 0.021584 Accuracy 0.2084 | validation: Loss 0.087933 Accuracy -4.5242\n",
      "Epoch 85 | train: Loss 0.036536 Accuracy -0.3399 | validation: Loss 0.062128 Accuracy -2.9031\n",
      "Epoch 86 | train: Loss 0.017014 Accuracy 0.3760 | validation: Loss 0.043570 Accuracy -1.7372\n",
      "Epoch 87 | train: Loss 0.016580 Accuracy 0.3920 | validation: Loss 0.038233 Accuracy -1.4019\n",
      "Epoch 88 | train: Loss 0.015468 Accuracy 0.4327 | validation: Loss 0.046225 Accuracy -1.9040\n",
      "Epoch 89 | train: Loss 0.017514 Accuracy 0.3577 | validation: Loss 0.026813 Accuracy -0.6845\n",
      "Epoch 90 | train: Loss 0.011306 Accuracy 0.5854 | validation: Loss 0.049689 Accuracy -2.1216\n",
      "Epoch 91 | train: Loss 0.017786 Accuracy 0.3477 | validation: Loss 0.028578 Accuracy -0.7953\n",
      "Epoch 92 | train: Loss 0.012893 Accuracy 0.5272 | validation: Loss 0.041814 Accuracy -1.6269\n",
      "Epoch 93 | train: Loss 0.015062 Accuracy 0.4476 | validation: Loss 0.051083 Accuracy -2.2092\n",
      "Epoch 94 | train: Loss 0.016598 Accuracy 0.3913 | validation: Loss 0.030737 Accuracy -0.9310\n",
      "Epoch 95 | train: Loss 0.012643 Accuracy 0.5363 | validation: Loss 0.027596 Accuracy -0.7337\n",
      "Epoch 96 | train: Loss 0.014847 Accuracy 0.4555 | validation: Loss 0.026452 Accuracy -0.6618\n",
      "Epoch 97 | train: Loss 0.014704 Accuracy 0.4608 | validation: Loss 0.028011 Accuracy -0.7597\n",
      "Epoch 98 | train: Loss 0.011943 Accuracy 0.5620 | validation: Loss 0.048100 Accuracy -2.0218\n",
      "Epoch 99 | train: Loss 0.014177 Accuracy 0.4801 | validation: Loss 0.045838 Accuracy -1.8797\n",
      "Epoch 100 | train: Loss 0.013581 Accuracy 0.5019 | validation: Loss 0.026617 Accuracy -0.6722\n",
      "Epoch 101 | train: Loss 0.011609 Accuracy 0.5743 | validation: Loss 0.021710 Accuracy -0.3639\n",
      "Epoch 102 | train: Loss 0.013144 Accuracy 0.5180 | validation: Loss 0.021784 Accuracy -0.3686\n",
      "Epoch 103 | train: Loss 0.012255 Accuracy 0.5506 | validation: Loss 0.030315 Accuracy -0.9045\n",
      "Epoch 104 | train: Loss 0.011710 Accuracy 0.5706 | validation: Loss 0.037268 Accuracy -1.3413\n",
      "Epoch 105 | train: Loss 0.012782 Accuracy 0.5312 | validation: Loss 0.028052 Accuracy -0.7623\n",
      "Epoch 106 | train: Loss 0.011664 Accuracy 0.5722 | validation: Loss 0.028339 Accuracy -0.7803\n",
      "Epoch 107 | train: Loss 0.011896 Accuracy 0.5637 | validation: Loss 0.029829 Accuracy -0.8739\n",
      "Epoch 108 | train: Loss 0.012096 Accuracy 0.5564 | validation: Loss 0.026180 Accuracy -0.6447\n",
      "Epoch 109 | train: Loss 0.011192 Accuracy 0.5895 | validation: Loss 0.030304 Accuracy -0.9038\n",
      "Epoch 110 | train: Loss 0.012048 Accuracy 0.5582 | validation: Loss 0.026052 Accuracy -0.6366\n",
      "Epoch 111 | train: Loss 0.010879 Accuracy 0.6010 | validation: Loss 0.025243 Accuracy -0.5858\n",
      "Epoch 112 | train: Loss 0.011044 Accuracy 0.5950 | validation: Loss 0.025916 Accuracy -0.6281\n",
      "Epoch 113 | train: Loss 0.010424 Accuracy 0.6177 | validation: Loss 0.025421 Accuracy -0.5970\n",
      "Epoch 114 | train: Loss 0.010347 Accuracy 0.6205 | validation: Loss 0.025957 Accuracy -0.6307\n",
      "Epoch 115 | train: Loss 0.009659 Accuracy 0.6458 | validation: Loss 0.041752 Accuracy -1.6230\n",
      "Epoch 116 | train: Loss 0.010270 Accuracy 0.6234 | validation: Loss 0.031334 Accuracy -0.9685\n",
      "Epoch 117 | train: Loss 0.008757 Accuracy 0.6789 | validation: Loss 0.034884 Accuracy -1.1915\n",
      "Epoch 118 | train: Loss 0.008172 Accuracy 0.7003 | validation: Loss 0.057250 Accuracy -2.5966\n",
      "Epoch 119 | train: Loss 0.009301 Accuracy 0.6589 | validation: Loss 0.028652 Accuracy -0.8000\n",
      "Epoch 120 | train: Loss 0.012249 Accuracy 0.5508 | validation: Loss 0.094664 Accuracy -4.9471\n",
      "Epoch 121 | train: Loss 0.013611 Accuracy 0.5008 | validation: Loss 0.044691 Accuracy -1.8076\n",
      "Epoch 122 | train: Loss 0.008889 Accuracy 0.6740 | validation: Loss 0.039696 Accuracy -1.4938\n",
      "Epoch 123 | train: Loss 0.012197 Accuracy 0.5527 | validation: Loss 0.087112 Accuracy -4.4726\n",
      "Epoch 124 | train: Loss 0.008461 Accuracy 0.6897 | validation: Loss 0.115287 Accuracy -6.2427\n",
      "Epoch 125 | train: Loss 0.010004 Accuracy 0.6331 | validation: Loss 0.055538 Accuracy -2.4891\n",
      "Epoch 126 | train: Loss 0.009326 Accuracy 0.6580 | validation: Loss 0.057144 Accuracy -2.5899\n",
      "Epoch 127 | train: Loss 0.009318 Accuracy 0.6583 | validation: Loss 0.116918 Accuracy -6.3451\n",
      "Epoch 128 | train: Loss 0.008564 Accuracy 0.6859 | validation: Loss 0.100421 Accuracy -5.3088\n",
      "Epoch 129 | train: Loss 0.007408 Accuracy 0.7283 | validation: Loss 0.051621 Accuracy -2.2430\n",
      "Epoch 130 | train: Loss 0.009249 Accuracy 0.6608 | validation: Loss 0.105585 Accuracy -5.6331\n",
      "Epoch 131 | train: Loss 0.006992 Accuracy 0.7436 | validation: Loss 0.127335 Accuracy -6.9996\n",
      "Epoch 132 | train: Loss 0.007577 Accuracy 0.7221 | validation: Loss 0.072613 Accuracy -3.5618\n",
      "Epoch 133 | train: Loss 0.008459 Accuracy 0.6898 | validation: Loss 0.107734 Accuracy -5.7682\n",
      "Epoch 134 | train: Loss 0.006712 Accuracy 0.7538 | validation: Loss 0.155216 Accuracy -8.7511\n",
      "Epoch 135 | train: Loss 0.008095 Accuracy 0.7031 | validation: Loss 0.092913 Accuracy -4.8371\n",
      "Epoch 136 | train: Loss 0.006846 Accuracy 0.7489 | validation: Loss 0.079145 Accuracy -3.9721\n",
      "Epoch 137 | train: Loss 0.007172 Accuracy 0.7370 | validation: Loss 0.120451 Accuracy -6.5671\n",
      "Epoch 138 | train: Loss 0.006902 Accuracy 0.7469 | validation: Loss 0.099696 Accuracy -5.2632\n",
      "Epoch 139 | train: Loss 0.006301 Accuracy 0.7689 | validation: Loss 0.068284 Accuracy -3.2898\n",
      "Epoch 140 | train: Loss 0.007246 Accuracy 0.7343 | validation: Loss 0.100309 Accuracy -5.3017\n",
      "Epoch 141 | train: Loss 0.006606 Accuracy 0.7578 | validation: Loss 0.102252 Accuracy -5.4238\n",
      "Epoch 142 | train: Loss 0.006481 Accuracy 0.7623 | validation: Loss 0.074417 Accuracy -3.6751\n",
      "Epoch 143 | train: Loss 0.006818 Accuracy 0.7500 | validation: Loss 0.102763 Accuracy -5.4559\n",
      "Epoch 144 | train: Loss 0.006157 Accuracy 0.7742 | validation: Loss 0.107526 Accuracy -5.7551\n",
      "Epoch 145 | train: Loss 0.006211 Accuracy 0.7722 | validation: Loss 0.078223 Accuracy -3.9142\n",
      "Epoch 146 | train: Loss 0.006394 Accuracy 0.7655 | validation: Loss 0.094841 Accuracy -4.9582\n",
      "Epoch 147 | train: Loss 0.005771 Accuracy 0.7884 | validation: Loss 0.102717 Accuracy -5.4530\n",
      "Epoch 148 | train: Loss 0.005765 Accuracy 0.7886 | validation: Loss 0.074511 Accuracy -3.6810\n",
      "Epoch 149 | train: Loss 0.006130 Accuracy 0.7752 | validation: Loss 0.114136 Accuracy -6.1704\n",
      "Epoch 150 | train: Loss 0.006243 Accuracy 0.7711 | validation: Loss 0.075003 Accuracy -3.7119\n",
      "Epoch 151 | train: Loss 0.006127 Accuracy 0.7753 | validation: Loss 0.102684 Accuracy -5.4509\n",
      "Epoch 152 | train: Loss 0.005691 Accuracy 0.7913 | validation: Loss 0.085259 Accuracy -4.3562\n",
      "Epoch 153 | train: Loss 0.005484 Accuracy 0.7989 | validation: Loss 0.083163 Accuracy -4.2246\n",
      "Epoch 154 | train: Loss 0.005466 Accuracy 0.7996 | validation: Loss 0.102660 Accuracy -5.4494\n",
      "Epoch 155 | train: Loss 0.005557 Accuracy 0.7962 | validation: Loss 0.079461 Accuracy -3.9920\n",
      "Epoch 156 | train: Loss 0.005471 Accuracy 0.7993 | validation: Loss 0.105877 Accuracy -5.6515\n",
      "Epoch 157 | train: Loss 0.005062 Accuracy 0.8143 | validation: Loss 0.110110 Accuracy -5.9175\n",
      "Epoch 158 | train: Loss 0.004996 Accuracy 0.8168 | validation: Loss 0.107993 Accuracy -5.7845\n",
      "Epoch 159 | train: Loss 0.005184 Accuracy 0.8099 | validation: Loss 0.146593 Accuracy -8.2094\n",
      "Epoch 160 | train: Loss 0.006508 Accuracy 0.7613 | validation: Loss 0.052929 Accuracy -2.3251\n",
      "Epoch 161 | train: Loss 0.015673 Accuracy 0.4252 | validation: Loss 0.308988 Accuracy -18.4116\n",
      "Epoch 162 | train: Loss 0.035617 Accuracy -0.3062 | validation: Loss 0.043482 Accuracy -1.7317\n",
      "Epoch 163 | train: Loss 0.031710 Accuracy -0.1629 | validation: Loss 0.045781 Accuracy -1.8761\n",
      "Epoch 164 | train: Loss 0.020884 Accuracy 0.2341 | validation: Loss 0.249290 Accuracy -14.6611\n",
      "Epoch 165 | train: Loss 0.021823 Accuracy 0.1997 | validation: Loss 0.216231 Accuracy -12.5842\n",
      "Epoch 166 | train: Loss 0.015318 Accuracy 0.4382 | validation: Loss 0.065156 Accuracy -3.0933\n",
      "Epoch 167 | train: Loss 0.011239 Accuracy 0.5878 | validation: Loss 0.044933 Accuracy -1.8229\n",
      "Epoch 168 | train: Loss 0.017252 Accuracy 0.3673 | validation: Loss 0.076756 Accuracy -3.8221\n",
      "Epoch 169 | train: Loss 0.009532 Accuracy 0.6504 | validation: Loss 0.170381 Accuracy -9.7038\n",
      "Epoch 170 | train: Loss 0.014551 Accuracy 0.4664 | validation: Loss 0.140605 Accuracy -7.8332\n",
      "Epoch 171 | train: Loss 0.011221 Accuracy 0.5885 | validation: Loss 0.077177 Accuracy -3.8485\n",
      "Epoch 172 | train: Loss 0.009647 Accuracy 0.6462 | validation: Loss 0.064181 Accuracy -3.0320\n",
      "Epoch 173 | train: Loss 0.011788 Accuracy 0.5677 | validation: Loss 0.092678 Accuracy -4.8223\n",
      "Epoch 174 | train: Loss 0.008804 Accuracy 0.6771 | validation: Loss 0.166467 Accuracy -9.4579\n",
      "Epoch 175 | train: Loss 0.007915 Accuracy 0.7097 | validation: Loss 0.216134 Accuracy -12.5782\n",
      "Epoch 176 | train: Loss 0.009156 Accuracy 0.6642 | validation: Loss 0.171758 Accuracy -9.7904\n",
      "Epoch 177 | train: Loss 0.007103 Accuracy 0.7395 | validation: Loss 0.123259 Accuracy -6.7435\n",
      "Epoch 178 | train: Loss 0.009242 Accuracy 0.6611 | validation: Loss 0.153847 Accuracy -8.6651\n",
      "Epoch 179 | train: Loss 0.008055 Accuracy 0.7046 | validation: Loss 0.246406 Accuracy -14.4799\n",
      "Epoch 180 | train: Loss 0.008161 Accuracy 0.7007 | validation: Loss 0.262828 Accuracy -15.5116\n",
      "Epoch 181 | train: Loss 0.008849 Accuracy 0.6755 | validation: Loss 0.157530 Accuracy -8.8965\n",
      "Epoch 182 | train: Loss 0.007510 Accuracy 0.7246 | validation: Loss 0.111774 Accuracy -6.0220\n",
      "Epoch 183 | train: Loss 0.008700 Accuracy 0.6810 | validation: Loss 0.130850 Accuracy -7.2204\n",
      "Epoch 184 | train: Loss 0.006815 Accuracy 0.7501 | validation: Loss 0.128038 Accuracy -7.0437\n",
      "Epoch 185 | train: Loss 0.006729 Accuracy 0.7532 | validation: Loss 0.075887 Accuracy -3.7674\n",
      "Epoch 186 | train: Loss 0.006782 Accuracy 0.7513 | validation: Loss 0.131782 Accuracy -7.2789\n",
      "Epoch 187 | train: Loss 0.006815 Accuracy 0.7501 | validation: Loss 0.086063 Accuracy -4.4067\n",
      "Epoch 188 | train: Loss 0.006147 Accuracy 0.7746 | validation: Loss 0.105042 Accuracy -5.5991\n",
      "Epoch 189 | train: Loss 0.005890 Accuracy 0.7840 | validation: Loss 0.123179 Accuracy -6.7384\n",
      "Epoch 190 | train: Loss 0.006048 Accuracy 0.7782 | validation: Loss 0.088577 Accuracy -4.5647\n",
      "Epoch 191 | train: Loss 0.006036 Accuracy 0.7786 | validation: Loss 0.112716 Accuracy -6.0812\n",
      "Epoch 192 | train: Loss 0.005729 Accuracy 0.7899 | validation: Loss 0.100084 Accuracy -5.2876\n",
      "Epoch 193 | train: Loss 0.005590 Accuracy 0.7950 | validation: Loss 0.085914 Accuracy -4.3974\n",
      "Epoch 194 | train: Loss 0.005672 Accuracy 0.7920 | validation: Loss 0.087508 Accuracy -4.4975\n",
      "Epoch 195 | train: Loss 0.005689 Accuracy 0.7914 | validation: Loss 0.070515 Accuracy -3.4299\n",
      "Epoch 196 | train: Loss 0.005785 Accuracy 0.7878 | validation: Loss 0.066402 Accuracy -3.1716\n",
      "Epoch 197 | train: Loss 0.005851 Accuracy 0.7854 | validation: Loss 0.077723 Accuracy -3.8828\n",
      "Epoch 198 | train: Loss 0.005851 Accuracy 0.7854 | validation: Loss 0.071889 Accuracy -3.5163\n",
      "Epoch 199 | train: Loss 0.005782 Accuracy 0.7879 | validation: Loss 0.063264 Accuracy -2.9744\n",
      "Epoch 200 | train: Loss 0.005873 Accuracy 0.7846 | validation: Loss 0.072883 Accuracy -3.5787\n",
      "Epoch 201 | train: Loss 0.005713 Accuracy 0.7905 | validation: Loss 0.073665 Accuracy -3.6279\n",
      "Epoch 202 | train: Loss 0.005665 Accuracy 0.7922 | validation: Loss 0.060652 Accuracy -2.8103\n",
      "Epoch 203 | train: Loss 0.005657 Accuracy 0.7925 | validation: Loss 0.067133 Accuracy -3.2175\n",
      "Epoch 204 | train: Loss 0.005498 Accuracy 0.7984 | validation: Loss 0.074997 Accuracy -3.7115\n",
      "Epoch 205 | train: Loss 0.005454 Accuracy 0.8000 | validation: Loss 0.068280 Accuracy -3.2896\n",
      "Epoch 206 | train: Loss 0.005450 Accuracy 0.8001 | validation: Loss 0.078258 Accuracy -3.9164\n",
      "Epoch 207 | train: Loss 0.005324 Accuracy 0.8048 | validation: Loss 0.082284 Accuracy -4.1693\n",
      "Epoch 208 | train: Loss 0.005291 Accuracy 0.8060 | validation: Loss 0.069203 Accuracy -3.3475\n",
      "Epoch 209 | train: Loss 0.005418 Accuracy 0.8013 | validation: Loss 0.093401 Accuracy -4.8677\n",
      "Epoch 210 | train: Loss 0.005295 Accuracy 0.8058 | validation: Loss 0.074608 Accuracy -3.6871\n",
      "Epoch 211 | train: Loss 0.005140 Accuracy 0.8115 | validation: Loss 0.085714 Accuracy -4.3848\n",
      "Epoch 212 | train: Loss 0.004957 Accuracy 0.8182 | validation: Loss 0.092267 Accuracy -4.7965\n",
      "Epoch 213 | train: Loss 0.004941 Accuracy 0.8188 | validation: Loss 0.079003 Accuracy -3.9632\n",
      "Epoch 214 | train: Loss 0.005044 Accuracy 0.8150 | validation: Loss 0.104456 Accuracy -5.5623\n",
      "Epoch 215 | train: Loss 0.005043 Accuracy 0.8150 | validation: Loss 0.077789 Accuracy -3.8869\n",
      "Epoch 216 | train: Loss 0.005084 Accuracy 0.8135 | validation: Loss 0.103095 Accuracy -5.4767\n",
      "Epoch 217 | train: Loss 0.005041 Accuracy 0.8151 | validation: Loss 0.071037 Accuracy -3.4627\n",
      "Epoch 218 | train: Loss 0.005201 Accuracy 0.8093 | validation: Loss 0.103527 Accuracy -5.5038\n",
      "Epoch 219 | train: Loss 0.005372 Accuracy 0.8030 | validation: Loss 0.062679 Accuracy -2.9377\n",
      "Epoch 220 | train: Loss 0.005452 Accuracy 0.8001 | validation: Loss 0.098776 Accuracy -5.2054\n",
      "Epoch 221 | train: Loss 0.005355 Accuracy 0.8036 | validation: Loss 0.065838 Accuracy -3.1361\n",
      "Epoch 222 | train: Loss 0.005073 Accuracy 0.8139 | validation: Loss 0.078272 Accuracy -3.9173\n",
      "Epoch 223 | train: Loss 0.004744 Accuracy 0.8260 | validation: Loss 0.083170 Accuracy -4.2250\n",
      "Epoch 224 | train: Loss 0.004772 Accuracy 0.8250 | validation: Loss 0.064215 Accuracy -3.0342\n",
      "Epoch 225 | train: Loss 0.005125 Accuracy 0.8120 | validation: Loss 0.099140 Accuracy -5.2283\n",
      "Epoch 226 | train: Loss 0.005424 Accuracy 0.8011 | validation: Loss 0.058196 Accuracy -2.6561\n",
      "Epoch 227 | train: Loss 0.005463 Accuracy 0.7997 | validation: Loss 0.095176 Accuracy -4.9792\n",
      "Epoch 228 | train: Loss 0.005355 Accuracy 0.8036 | validation: Loss 0.063127 Accuracy -2.9658\n",
      "Epoch 229 | train: Loss 0.004824 Accuracy 0.8231 | validation: Loss 0.072441 Accuracy -3.5509\n",
      "Epoch 230 | train: Loss 0.004557 Accuracy 0.8329 | validation: Loss 0.078146 Accuracy -3.9094\n",
      "Epoch 231 | train: Loss 0.004572 Accuracy 0.8323 | validation: Loss 0.061475 Accuracy -2.8620\n",
      "Epoch 232 | train: Loss 0.004741 Accuracy 0.8261 | validation: Loss 0.087808 Accuracy -4.5163\n",
      "Epoch 233 | train: Loss 0.004847 Accuracy 0.8222 | validation: Loss 0.057453 Accuracy -2.6093\n",
      "Epoch 234 | train: Loss 0.005069 Accuracy 0.8141 | validation: Loss 0.092725 Accuracy -4.8253\n",
      "Epoch 235 | train: Loss 0.005269 Accuracy 0.8068 | validation: Loss 0.051569 Accuracy -2.2397\n",
      "Epoch 236 | train: Loss 0.005503 Accuracy 0.7982 | validation: Loss 0.087369 Accuracy -4.4888\n",
      "Epoch 237 | train: Loss 0.004914 Accuracy 0.8198 | validation: Loss 0.064055 Accuracy -3.0241\n",
      "Epoch 238 | train: Loss 0.004411 Accuracy 0.8382 | validation: Loss 0.067510 Accuracy -3.2412\n",
      "Epoch 239 | train: Loss 0.004308 Accuracy 0.8420 | validation: Loss 0.088747 Accuracy -4.5754\n",
      "Epoch 240 | train: Loss 0.004693 Accuracy 0.8279 | validation: Loss 0.057301 Accuracy -2.5998\n",
      "Epoch 241 | train: Loss 0.005079 Accuracy 0.8137 | validation: Loss 0.090953 Accuracy -4.7139\n",
      "Epoch 242 | train: Loss 0.004816 Accuracy 0.8234 | validation: Loss 0.067604 Accuracy -3.2471\n",
      "Epoch 243 | train: Loss 0.004329 Accuracy 0.8413 | validation: Loss 0.064660 Accuracy -3.0621\n",
      "Epoch 244 | train: Loss 0.004261 Accuracy 0.8437 | validation: Loss 0.083391 Accuracy -4.2389\n",
      "Epoch 245 | train: Loss 0.004530 Accuracy 0.8339 | validation: Loss 0.055304 Accuracy -2.4744\n",
      "Epoch 246 | train: Loss 0.004628 Accuracy 0.8303 | validation: Loss 0.081161 Accuracy -4.0988\n",
      "Epoch 247 | train: Loss 0.004317 Accuracy 0.8417 | validation: Loss 0.065126 Accuracy -3.0914\n",
      "Epoch 248 | train: Loss 0.004038 Accuracy 0.8519 | validation: Loss 0.070905 Accuracy -3.4545\n",
      "Epoch 249 | train: Loss 0.003914 Accuracy 0.8565 | validation: Loss 0.082480 Accuracy -4.1816\n",
      "Epoch 250 | train: Loss 0.004009 Accuracy 0.8530 | validation: Loss 0.064002 Accuracy -3.0208\n",
      "Epoch 251 | train: Loss 0.004290 Accuracy 0.8427 | validation: Loss 0.095458 Accuracy -4.9969\n",
      "Epoch 252 | train: Loss 0.004446 Accuracy 0.8369 | validation: Loss 0.063724 Accuracy -3.0033\n",
      "Epoch 253 | train: Loss 0.004375 Accuracy 0.8396 | validation: Loss 0.083885 Accuracy -4.2699\n",
      "Epoch 254 | train: Loss 0.003958 Accuracy 0.8549 | validation: Loss 0.073760 Accuracy -3.6338\n",
      "Epoch 255 | train: Loss 0.003733 Accuracy 0.8631 | validation: Loss 0.064838 Accuracy -3.0733\n",
      "Epoch 256 | train: Loss 0.003900 Accuracy 0.8570 | validation: Loss 0.089422 Accuracy -4.6178\n",
      "Epoch 257 | train: Loss 0.004041 Accuracy 0.8518 | validation: Loss 0.063041 Accuracy -2.9604\n",
      "Epoch 258 | train: Loss 0.004333 Accuracy 0.8411 | validation: Loss 0.102743 Accuracy -5.4546\n",
      "Epoch 259 | train: Loss 0.004695 Accuracy 0.8278 | validation: Loss 0.059488 Accuracy -2.7372\n",
      "Epoch 260 | train: Loss 0.004845 Accuracy 0.8223 | validation: Loss 0.102784 Accuracy -5.4572\n",
      "Epoch 261 | train: Loss 0.004843 Accuracy 0.8224 | validation: Loss 0.067523 Accuracy -3.2420\n",
      "Epoch 262 | train: Loss 0.004069 Accuracy 0.8508 | validation: Loss 0.070490 Accuracy -3.4284\n",
      "Epoch 263 | train: Loss 0.003943 Accuracy 0.8554 | validation: Loss 0.096543 Accuracy -5.0651\n",
      "Epoch 264 | train: Loss 0.004198 Accuracy 0.8460 | validation: Loss 0.064084 Accuracy -3.0260\n",
      "Epoch 265 | train: Loss 0.003989 Accuracy 0.8537 | validation: Loss 0.090859 Accuracy -4.7080\n",
      "Epoch 266 | train: Loss 0.003723 Accuracy 0.8634 | validation: Loss 0.079655 Accuracy -4.0041\n",
      "Epoch 267 | train: Loss 0.003466 Accuracy 0.8729 | validation: Loss 0.090893 Accuracy -4.7102\n",
      "Epoch 268 | train: Loss 0.003483 Accuracy 0.8723 | validation: Loss 0.095301 Accuracy -4.9871\n",
      "Epoch 269 | train: Loss 0.003416 Accuracy 0.8747 | validation: Loss 0.083427 Accuracy -4.2411\n",
      "Epoch 270 | train: Loss 0.003579 Accuracy 0.8688 | validation: Loss 0.102395 Accuracy -5.4328\n",
      "Epoch 271 | train: Loss 0.003622 Accuracy 0.8672 | validation: Loss 0.075590 Accuracy -3.7488\n",
      "Epoch 272 | train: Loss 0.003781 Accuracy 0.8613 | validation: Loss 0.104466 Accuracy -5.5628\n",
      "Epoch 273 | train: Loss 0.003862 Accuracy 0.8584 | validation: Loss 0.070796 Accuracy -3.4476\n",
      "Epoch 274 | train: Loss 0.003721 Accuracy 0.8635 | validation: Loss 0.093801 Accuracy -4.8929\n",
      "Epoch 275 | train: Loss 0.003594 Accuracy 0.8682 | validation: Loss 0.075464 Accuracy -3.7409\n",
      "Epoch 276 | train: Loss 0.003354 Accuracy 0.8770 | validation: Loss 0.088554 Accuracy -4.5632\n",
      "Epoch 277 | train: Loss 0.003267 Accuracy 0.8802 | validation: Loss 0.087057 Accuracy -4.4692\n",
      "Epoch 278 | train: Loss 0.003176 Accuracy 0.8835 | validation: Loss 0.086792 Accuracy -4.4525\n",
      "Epoch 279 | train: Loss 0.003177 Accuracy 0.8835 | validation: Loss 0.102517 Accuracy -5.4404\n",
      "Epoch 280 | train: Loss 0.003423 Accuracy 0.8745 | validation: Loss 0.075364 Accuracy -3.7346\n",
      "Epoch 281 | train: Loss 0.004068 Accuracy 0.8508 | validation: Loss 0.131450 Accuracy -7.2581\n",
      "Epoch 282 | train: Loss 0.006601 Accuracy 0.7579 | validation: Loss 0.052765 Accuracy -2.3148\n",
      "Epoch 283 | train: Loss 0.008042 Accuracy 0.7051 | validation: Loss 0.096891 Accuracy -5.0870\n",
      "Epoch 284 | train: Loss 0.004290 Accuracy 0.8427 | validation: Loss 0.113518 Accuracy -6.1316\n",
      "Epoch 285 | train: Loss 0.005878 Accuracy 0.7844 | validation: Loss 0.052034 Accuracy -2.2689\n",
      "Epoch 286 | train: Loss 0.006807 Accuracy 0.7504 | validation: Loss 0.071692 Accuracy -3.5039\n",
      "Epoch 287 | train: Loss 0.004102 Accuracy 0.8496 | validation: Loss 0.105053 Accuracy -5.5998\n",
      "Epoch 288 | train: Loss 0.006274 Accuracy 0.7699 | validation: Loss 0.048742 Accuracy -2.0621\n",
      "Epoch 289 | train: Loss 0.005637 Accuracy 0.7933 | validation: Loss 0.058273 Accuracy -2.6609\n",
      "Epoch 290 | train: Loss 0.004188 Accuracy 0.8464 | validation: Loss 0.097764 Accuracy -5.1418\n",
      "Epoch 291 | train: Loss 0.005833 Accuracy 0.7861 | validation: Loss 0.059094 Accuracy -2.7125\n",
      "Epoch 292 | train: Loss 0.004448 Accuracy 0.8369 | validation: Loss 0.060954 Accuracy -2.8293\n",
      "Epoch 293 | train: Loss 0.004666 Accuracy 0.8289 | validation: Loss 0.107959 Accuracy -5.7823\n",
      "Epoch 294 | train: Loss 0.004876 Accuracy 0.8212 | validation: Loss 0.089955 Accuracy -4.6512\n",
      "Epoch 295 | train: Loss 0.003881 Accuracy 0.8577 | validation: Loss 0.073929 Accuracy -3.6445\n",
      "Epoch 296 | train: Loss 0.004882 Accuracy 0.8210 | validation: Loss 0.107890 Accuracy -5.7780\n",
      "Epoch 297 | train: Loss 0.004102 Accuracy 0.8496 | validation: Loss 0.105035 Accuracy -5.5986\n",
      "Epoch 298 | train: Loss 0.003999 Accuracy 0.8534 | validation: Loss 0.073264 Accuracy -3.6027\n",
      "Epoch 299 | train: Loss 0.004457 Accuracy 0.8365 | validation: Loss 0.085842 Accuracy -4.3928\n",
      "Epoch 300 | train: Loss 0.003699 Accuracy 0.8644 | validation: Loss 0.101814 Accuracy -5.3963\n",
      "Epoch 301 | train: Loss 0.004070 Accuracy 0.8507 | validation: Loss 0.068905 Accuracy -3.3288\n",
      "Epoch 302 | train: Loss 0.003917 Accuracy 0.8563 | validation: Loss 0.075404 Accuracy -3.7371\n",
      "Epoch 303 | train: Loss 0.003491 Accuracy 0.8720 | validation: Loss 0.099948 Accuracy -5.2790\n",
      "Epoch 304 | train: Loss 0.003894 Accuracy 0.8572 | validation: Loss 0.071591 Accuracy -3.4975\n",
      "Epoch 305 | train: Loss 0.003999 Accuracy 0.8534 | validation: Loss 0.112088 Accuracy -6.0417\n",
      "Epoch 306 | train: Loss 0.003616 Accuracy 0.8674 | validation: Loss 0.098098 Accuracy -5.1628\n",
      "Epoch 307 | train: Loss 0.003202 Accuracy 0.8826 | validation: Loss 0.094582 Accuracy -4.9419\n",
      "Epoch 308 | train: Loss 0.003314 Accuracy 0.8785 | validation: Loss 0.123342 Accuracy -6.7487\n",
      "Epoch 309 | train: Loss 0.003353 Accuracy 0.8770 | validation: Loss 0.089401 Accuracy -4.6164\n",
      "Epoch 310 | train: Loss 0.003339 Accuracy 0.8776 | validation: Loss 0.117240 Accuracy -6.3654\n",
      "Epoch 311 | train: Loss 0.002876 Accuracy 0.8945 | validation: Loss 0.104389 Accuracy -5.5580\n",
      "Epoch 312 | train: Loss 0.002606 Accuracy 0.9044 | validation: Loss 0.096994 Accuracy -5.0935\n",
      "Epoch 313 | train: Loss 0.002734 Accuracy 0.8997 | validation: Loss 0.126010 Accuracy -6.9163\n",
      "Epoch 314 | train: Loss 0.002792 Accuracy 0.8976 | validation: Loss 0.101674 Accuracy -5.3875\n",
      "Epoch 315 | train: Loss 0.002800 Accuracy 0.8973 | validation: Loss 0.129383 Accuracy -7.1282\n",
      "Epoch 316 | train: Loss 0.002510 Accuracy 0.9079 | validation: Loss 0.118148 Accuracy -6.4224\n",
      "Epoch 317 | train: Loss 0.002361 Accuracy 0.9134 | validation: Loss 0.117315 Accuracy -6.3701\n",
      "Epoch 318 | train: Loss 0.002376 Accuracy 0.9129 | validation: Loss 0.134534 Accuracy -7.4518\n",
      "Epoch 319 | train: Loss 0.002517 Accuracy 0.9077 | validation: Loss 0.107517 Accuracy -5.7545\n",
      "Epoch 320 | train: Loss 0.002820 Accuracy 0.8966 | validation: Loss 0.140512 Accuracy -7.8274\n",
      "Epoch 321 | train: Loss 0.002808 Accuracy 0.8970 | validation: Loss 0.111996 Accuracy -6.0359\n",
      "Epoch 322 | train: Loss 0.002810 Accuracy 0.8969 | validation: Loss 0.131598 Accuracy -7.2674\n",
      "Epoch 323 | train: Loss 0.002499 Accuracy 0.9084 | validation: Loss 0.126309 Accuracy -6.9351\n",
      "Epoch 324 | train: Loss 0.002347 Accuracy 0.9139 | validation: Loss 0.133864 Accuracy -7.4097\n",
      "Epoch 325 | train: Loss 0.002787 Accuracy 0.8978 | validation: Loss 0.116935 Accuracy -6.3462\n",
      "Epoch 326 | train: Loss 0.003356 Accuracy 0.8769 | validation: Loss 0.138569 Accuracy -7.7053\n",
      "Epoch 327 | train: Loss 0.004947 Accuracy 0.8186 | validation: Loss 0.084928 Accuracy -4.3354\n",
      "Epoch 328 | train: Loss 0.007106 Accuracy 0.7394 | validation: Loss 0.114030 Accuracy -6.1637\n",
      "Epoch 329 | train: Loss 0.008910 Accuracy 0.6732 | validation: Loss 0.077425 Accuracy -3.8641\n",
      "Epoch 330 | train: Loss 0.009197 Accuracy 0.6627 | validation: Loss 0.177194 Accuracy -10.1318\n",
      "Epoch 331 | train: Loss 0.008649 Accuracy 0.6828 | validation: Loss 0.102179 Accuracy -5.4192\n",
      "Epoch 332 | train: Loss 0.007591 Accuracy 0.7216 | validation: Loss 0.153860 Accuracy -8.6659\n",
      "Epoch 333 | train: Loss 0.005929 Accuracy 0.7826 | validation: Loss 0.133126 Accuracy -7.3634\n",
      "Epoch 334 | train: Loss 0.005316 Accuracy 0.8050 | validation: Loss 0.096470 Accuracy -5.0605\n",
      "Epoch 335 | train: Loss 0.005808 Accuracy 0.7870 | validation: Loss 0.130235 Accuracy -7.1817\n",
      "Epoch 336 | train: Loss 0.005512 Accuracy 0.7978 | validation: Loss 0.092111 Accuracy -4.7867\n",
      "Epoch 337 | train: Loss 0.004756 Accuracy 0.8256 | validation: Loss 0.080224 Accuracy -4.0399\n",
      "Epoch 338 | train: Loss 0.004776 Accuracy 0.8249 | validation: Loss 0.110373 Accuracy -5.9340\n",
      "Epoch 339 | train: Loss 0.005006 Accuracy 0.8164 | validation: Loss 0.075562 Accuracy -3.7470\n",
      "Epoch 340 | train: Loss 0.004741 Accuracy 0.8261 | validation: Loss 0.100925 Accuracy -5.3404\n",
      "Epoch 341 | train: Loss 0.004050 Accuracy 0.8515 | validation: Loss 0.105503 Accuracy -5.6280\n",
      "Epoch 342 | train: Loss 0.004116 Accuracy 0.8490 | validation: Loss 0.086096 Accuracy -4.4088\n",
      "Epoch 343 | train: Loss 0.004424 Accuracy 0.8378 | validation: Loss 0.106622 Accuracy -5.6983\n",
      "Epoch 344 | train: Loss 0.004239 Accuracy 0.8445 | validation: Loss 0.078672 Accuracy -3.9424\n",
      "Epoch 345 | train: Loss 0.004038 Accuracy 0.8519 | validation: Loss 0.074054 Accuracy -3.6523\n",
      "Epoch 346 | train: Loss 0.003950 Accuracy 0.8551 | validation: Loss 0.070145 Accuracy -3.4067\n",
      "Epoch 347 | train: Loss 0.003927 Accuracy 0.8560 | validation: Loss 0.050048 Accuracy -2.1442\n",
      "Epoch 348 | train: Loss 0.003941 Accuracy 0.8555 | validation: Loss 0.059872 Accuracy -2.7613\n",
      "Epoch 349 | train: Loss 0.003894 Accuracy 0.8572 | validation: Loss 0.039895 Accuracy -1.5063\n",
      "Epoch 350 | train: Loss 0.003895 Accuracy 0.8572 | validation: Loss 0.047683 Accuracy -1.9956\n",
      "Epoch 351 | train: Loss 0.003657 Accuracy 0.8659 | validation: Loss 0.043986 Accuracy -1.7633\n",
      "Epoch 352 | train: Loss 0.003564 Accuracy 0.8693 | validation: Loss 0.041313 Accuracy -1.5954\n",
      "Epoch 353 | train: Loss 0.003645 Accuracy 0.8663 | validation: Loss 0.052069 Accuracy -2.2711\n",
      "Epoch 354 | train: Loss 0.003650 Accuracy 0.8662 | validation: Loss 0.042994 Accuracy -1.7010\n",
      "Epoch 355 | train: Loss 0.003770 Accuracy 0.8617 | validation: Loss 0.065010 Accuracy -3.0841\n",
      "Epoch 356 | train: Loss 0.003965 Accuracy 0.8546 | validation: Loss 0.045388 Accuracy -1.8514\n",
      "Epoch 357 | train: Loss 0.004193 Accuracy 0.8462 | validation: Loss 0.076065 Accuracy -3.7786\n",
      "Epoch 358 | train: Loss 0.004122 Accuracy 0.8488 | validation: Loss 0.051301 Accuracy -2.2229\n",
      "Epoch 359 | train: Loss 0.004004 Accuracy 0.8532 | validation: Loss 0.070589 Accuracy -3.4346\n",
      "Epoch 360 | train: Loss 0.003612 Accuracy 0.8675 | validation: Loss 0.068784 Accuracy -3.3212\n",
      "Epoch 361 | train: Loss 0.003466 Accuracy 0.8729 | validation: Loss 0.058796 Accuracy -2.6937\n",
      "Epoch 362 | train: Loss 0.003647 Accuracy 0.8662 | validation: Loss 0.089201 Accuracy -4.6039\n",
      "Epoch 363 | train: Loss 0.004026 Accuracy 0.8524 | validation: Loss 0.054038 Accuracy -2.3948\n",
      "Epoch 364 | train: Loss 0.004770 Accuracy 0.8251 | validation: Loss 0.116606 Accuracy -6.3255\n",
      "Epoch 365 | train: Loss 0.005857 Accuracy 0.7852 | validation: Loss 0.052149 Accuracy -2.2761\n",
      "Epoch 366 | train: Loss 0.006664 Accuracy 0.7556 | validation: Loss 0.098559 Accuracy -5.1918\n",
      "Epoch 367 | train: Loss 0.004060 Accuracy 0.8511 | validation: Loss 0.092985 Accuracy -4.8416\n",
      "Epoch 368 | train: Loss 0.003539 Accuracy 0.8702 | validation: Loss 0.060813 Accuracy -2.8205\n",
      "Epoch 369 | train: Loss 0.005237 Accuracy 0.8079 | validation: Loss 0.126122 Accuracy -6.9234\n",
      "Epoch 370 | train: Loss 0.005142 Accuracy 0.8114 | validation: Loss 0.070354 Accuracy -3.4199\n",
      "Epoch 371 | train: Loss 0.004117 Accuracy 0.8490 | validation: Loss 0.100564 Accuracy -5.3177\n",
      "Epoch 372 | train: Loss 0.003145 Accuracy 0.8846 | validation: Loss 0.111756 Accuracy -6.0209\n",
      "Epoch 373 | train: Loss 0.003383 Accuracy 0.8759 | validation: Loss 0.079130 Accuracy -3.9712\n",
      "Epoch 374 | train: Loss 0.004117 Accuracy 0.8490 | validation: Loss 0.121793 Accuracy -6.6514\n",
      "Epoch 375 | train: Loss 0.003860 Accuracy 0.8585 | validation: Loss 0.090258 Accuracy -4.6703\n",
      "Epoch 376 | train: Loss 0.003309 Accuracy 0.8787 | validation: Loss 0.085021 Accuracy -4.3413\n",
      "Epoch 377 | train: Loss 0.003264 Accuracy 0.8803 | validation: Loss 0.106472 Accuracy -5.6889\n",
      "Epoch 378 | train: Loss 0.003563 Accuracy 0.8693 | validation: Loss 0.063530 Accuracy -2.9911\n",
      "Epoch 379 | train: Loss 0.004054 Accuracy 0.8513 | validation: Loss 0.110079 Accuracy -5.9155\n",
      "Epoch 380 | train: Loss 0.004316 Accuracy 0.8417 | validation: Loss 0.060595 Accuracy -2.8067\n",
      "Epoch 381 | train: Loss 0.004384 Accuracy 0.8392 | validation: Loss 0.093831 Accuracy -4.8948\n",
      "Epoch 382 | train: Loss 0.003404 Accuracy 0.8752 | validation: Loss 0.082310 Accuracy -4.1709\n",
      "Epoch 383 | train: Loss 0.003190 Accuracy 0.8830 | validation: Loss 0.062410 Accuracy -2.9208\n",
      "Epoch 384 | train: Loss 0.003738 Accuracy 0.8629 | validation: Loss 0.085238 Accuracy -4.3549\n",
      "Epoch 385 | train: Loss 0.003558 Accuracy 0.8695 | validation: Loss 0.053328 Accuracy -2.3502\n",
      "Epoch 386 | train: Loss 0.003386 Accuracy 0.8758 | validation: Loss 0.063548 Accuracy -2.9923\n",
      "Epoch 387 | train: Loss 0.003034 Accuracy 0.8887 | validation: Loss 0.057109 Accuracy -2.5877\n",
      "Epoch 388 | train: Loss 0.002945 Accuracy 0.8920 | validation: Loss 0.049648 Accuracy -2.1190\n",
      "Epoch 389 | train: Loss 0.003183 Accuracy 0.8833 | validation: Loss 0.072994 Accuracy -3.5857\n",
      "Epoch 390 | train: Loss 0.003439 Accuracy 0.8739 | validation: Loss 0.049248 Accuracy -2.0939\n",
      "Epoch 391 | train: Loss 0.003948 Accuracy 0.8552 | validation: Loss 0.081248 Accuracy -4.1043\n",
      "Epoch 392 | train: Loss 0.003686 Accuracy 0.8648 | validation: Loss 0.058343 Accuracy -2.6653\n",
      "Epoch 393 | train: Loss 0.003347 Accuracy 0.8772 | validation: Loss 0.057891 Accuracy -2.6369\n",
      "Epoch 394 | train: Loss 0.003195 Accuracy 0.8828 | validation: Loss 0.066830 Accuracy -3.1985\n",
      "Epoch 395 | train: Loss 0.003297 Accuracy 0.8791 | validation: Loss 0.041994 Accuracy -1.6382\n",
      "Epoch 396 | train: Loss 0.003643 Accuracy 0.8664 | validation: Loss 0.061555 Accuracy -2.8671\n",
      "Epoch 397 | train: Loss 0.003384 Accuracy 0.8759 | validation: Loss 0.042625 Accuracy -1.6778\n",
      "Epoch 398 | train: Loss 0.003221 Accuracy 0.8819 | validation: Loss 0.054892 Accuracy -2.4485\n",
      "Epoch 399 | train: Loss 0.002936 Accuracy 0.8923 | validation: Loss 0.057014 Accuracy -2.5818\n",
      "Epoch 400 | train: Loss 0.002905 Accuracy 0.8935 | validation: Loss 0.055866 Accuracy -2.5097\n",
      "Epoch 401 | train: Loss 0.002958 Accuracy 0.8915 | validation: Loss 0.068594 Accuracy -3.3093\n",
      "Epoch 402 | train: Loss 0.002948 Accuracy 0.8919 | validation: Loss 0.055030 Accuracy -2.4571\n",
      "Epoch 403 | train: Loss 0.002960 Accuracy 0.8915 | validation: Loss 0.069334 Accuracy -3.3558\n",
      "Epoch 404 | train: Loss 0.002926 Accuracy 0.8927 | validation: Loss 0.052400 Accuracy -2.2919\n",
      "Epoch 405 | train: Loss 0.002996 Accuracy 0.8901 | validation: Loss 0.070616 Accuracy -3.4363\n",
      "Epoch 406 | train: Loss 0.002954 Accuracy 0.8917 | validation: Loss 0.053678 Accuracy -2.3722\n",
      "Epoch 407 | train: Loss 0.003029 Accuracy 0.8889 | validation: Loss 0.079669 Accuracy -4.0051\n",
      "Epoch 408 | train: Loss 0.003133 Accuracy 0.8851 | validation: Loss 0.052387 Accuracy -2.2911\n",
      "Epoch 409 | train: Loss 0.003381 Accuracy 0.8760 | validation: Loss 0.079458 Accuracy -3.9918\n",
      "Epoch 410 | train: Loss 0.003127 Accuracy 0.8853 | validation: Loss 0.054887 Accuracy -2.4481\n",
      "Epoch 411 | train: Loss 0.002811 Accuracy 0.8969 | validation: Loss 0.068012 Accuracy -3.2727\n",
      "Epoch 412 | train: Loss 0.002440 Accuracy 0.9105 | validation: Loss 0.065138 Accuracy -3.0922\n",
      "Epoch 413 | train: Loss 0.002449 Accuracy 0.9102 | validation: Loss 0.064624 Accuracy -3.0599\n",
      "Epoch 414 | train: Loss 0.002522 Accuracy 0.9075 | validation: Loss 0.070522 Accuracy -3.4304\n",
      "Epoch 415 | train: Loss 0.002484 Accuracy 0.9089 | validation: Loss 0.066749 Accuracy -3.1934\n",
      "Epoch 416 | train: Loss 0.002377 Accuracy 0.9128 | validation: Loss 0.079330 Accuracy -3.9838\n",
      "Epoch 417 | train: Loss 0.002470 Accuracy 0.9094 | validation: Loss 0.065932 Accuracy -3.1420\n",
      "Epoch 418 | train: Loss 0.002599 Accuracy 0.9047 | validation: Loss 0.078784 Accuracy -3.9495\n",
      "Epoch 419 | train: Loss 0.002696 Accuracy 0.9011 | validation: Loss 0.060758 Accuracy -2.8170\n",
      "Epoch 420 | train: Loss 0.002729 Accuracy 0.8999 | validation: Loss 0.086000 Accuracy -4.4028\n",
      "Epoch 421 | train: Loss 0.002729 Accuracy 0.8999 | validation: Loss 0.055567 Accuracy -2.4909\n",
      "Epoch 422 | train: Loss 0.004706 Accuracy 0.8274 | validation: Loss 0.129707 Accuracy -7.1485\n",
      "Epoch 423 | train: Loss 0.009294 Accuracy 0.6592 | validation: Loss 0.040623 Accuracy -1.5521\n",
      "Epoch 424 | train: Loss 0.022073 Accuracy 0.1905 | validation: Loss 0.172834 Accuracy -9.8579\n",
      "Epoch 425 | train: Loss 0.015437 Accuracy 0.4339 | validation: Loss 0.056416 Accuracy -2.5442\n",
      "Epoch 426 | train: Loss 0.008615 Accuracy 0.6841 | validation: Loss 0.055527 Accuracy -2.4884\n",
      "Epoch 427 | train: Loss 0.007343 Accuracy 0.7307 | validation: Loss 0.147123 Accuracy -8.2427\n",
      "Epoch 428 | train: Loss 0.011259 Accuracy 0.5871 | validation: Loss 0.055419 Accuracy -2.4816\n",
      "Epoch 429 | train: Loss 0.006767 Accuracy 0.7518 | validation: Loss 0.061192 Accuracy -2.8443\n",
      "Epoch 430 | train: Loss 0.005860 Accuracy 0.7851 | validation: Loss 0.123168 Accuracy -6.7378\n",
      "Epoch 431 | train: Loss 0.007025 Accuracy 0.7424 | validation: Loss 0.084296 Accuracy -4.2957\n",
      "Epoch 432 | train: Loss 0.004835 Accuracy 0.8227 | validation: Loss 0.063781 Accuracy -3.0069\n",
      "Epoch 433 | train: Loss 0.006748 Accuracy 0.7525 | validation: Loss 0.100142 Accuracy -5.2912\n",
      "Epoch 434 | train: Loss 0.004978 Accuracy 0.8174 | validation: Loss 0.121251 Accuracy -6.6173\n",
      "Epoch 435 | train: Loss 0.005719 Accuracy 0.7903 | validation: Loss 0.081592 Accuracy -4.1258\n",
      "Epoch 436 | train: Loss 0.005063 Accuracy 0.8143 | validation: Loss 0.082505 Accuracy -4.1832\n",
      "Epoch 437 | train: Loss 0.005020 Accuracy 0.8159 | validation: Loss 0.126488 Accuracy -6.9464\n",
      "Epoch 438 | train: Loss 0.004699 Accuracy 0.8277 | validation: Loss 0.110527 Accuracy -5.9436\n",
      "Epoch 439 | train: Loss 0.004074 Accuracy 0.8506 | validation: Loss 0.078732 Accuracy -3.9462\n",
      "Epoch 440 | train: Loss 0.004998 Accuracy 0.8167 | validation: Loss 0.113513 Accuracy -6.1312\n",
      "Epoch 441 | train: Loss 0.004071 Accuracy 0.8507 | validation: Loss 0.112485 Accuracy -6.0666\n",
      "Epoch 442 | train: Loss 0.004037 Accuracy 0.8519 | validation: Loss 0.077008 Accuracy -3.8379\n",
      "Epoch 443 | train: Loss 0.004583 Accuracy 0.8319 | validation: Loss 0.101913 Accuracy -5.4025\n",
      "Epoch 444 | train: Loss 0.003753 Accuracy 0.8624 | validation: Loss 0.105332 Accuracy -5.6173\n",
      "Epoch 445 | train: Loss 0.003865 Accuracy 0.8583 | validation: Loss 0.072730 Accuracy -3.5691\n",
      "Epoch 446 | train: Loss 0.004119 Accuracy 0.8490 | validation: Loss 0.087213 Accuracy -4.4790\n",
      "Epoch 447 | train: Loss 0.003515 Accuracy 0.8711 | validation: Loss 0.093428 Accuracy -4.8694\n",
      "Epoch 448 | train: Loss 0.003694 Accuracy 0.8645 | validation: Loss 0.069904 Accuracy -3.3916\n",
      "Epoch 449 | train: Loss 0.003681 Accuracy 0.8650 | validation: Loss 0.069236 Accuracy -3.3496\n",
      "Epoch 450 | train: Loss 0.003535 Accuracy 0.8703 | validation: Loss 0.078551 Accuracy -3.9348\n",
      "Epoch 451 | train: Loss 0.003606 Accuracy 0.8678 | validation: Loss 0.061183 Accuracy -2.8437\n",
      "Epoch 452 | train: Loss 0.003359 Accuracy 0.8768 | validation: Loss 0.054602 Accuracy -2.4302\n",
      "Epoch 453 | train: Loss 0.003488 Accuracy 0.8721 | validation: Loss 0.066259 Accuracy -3.1626\n",
      "Epoch 454 | train: Loss 0.003473 Accuracy 0.8726 | validation: Loss 0.058748 Accuracy -2.6907\n",
      "Epoch 455 | train: Loss 0.003392 Accuracy 0.8756 | validation: Loss 0.055822 Accuracy -2.5069\n",
      "Epoch 456 | train: Loss 0.003529 Accuracy 0.8706 | validation: Loss 0.071412 Accuracy -3.4863\n",
      "Epoch 457 | train: Loss 0.003530 Accuracy 0.8706 | validation: Loss 0.065192 Accuracy -3.0955\n",
      "Epoch 458 | train: Loss 0.003435 Accuracy 0.8740 | validation: Loss 0.062581 Accuracy -2.9315\n",
      "Epoch 459 | train: Loss 0.003524 Accuracy 0.8708 | validation: Loss 0.073966 Accuracy -3.6468\n",
      "Epoch 460 | train: Loss 0.003504 Accuracy 0.8715 | validation: Loss 0.065530 Accuracy -3.1168\n",
      "Epoch 461 | train: Loss 0.003393 Accuracy 0.8756 | validation: Loss 0.059818 Accuracy -2.7579\n",
      "Epoch 462 | train: Loss 0.003397 Accuracy 0.8754 | validation: Loss 0.071424 Accuracy -3.4871\n",
      "Epoch 463 | train: Loss 0.003328 Accuracy 0.8779 | validation: Loss 0.060465 Accuracy -2.7986\n",
      "Epoch 464 | train: Loss 0.003271 Accuracy 0.8801 | validation: Loss 0.067016 Accuracy -3.2101\n",
      "Epoch 465 | train: Loss 0.003073 Accuracy 0.8873 | validation: Loss 0.075551 Accuracy -3.7463\n",
      "Epoch 466 | train: Loss 0.003135 Accuracy 0.8850 | validation: Loss 0.069801 Accuracy -3.3851\n",
      "Epoch 467 | train: Loss 0.003127 Accuracy 0.8853 | validation: Loss 0.084920 Accuracy -4.3349\n",
      "Epoch 468 | train: Loss 0.003019 Accuracy 0.8893 | validation: Loss 0.086361 Accuracy -4.4255\n",
      "Epoch 469 | train: Loss 0.002930 Accuracy 0.8925 | validation: Loss 0.080875 Accuracy -4.0808\n",
      "Epoch 470 | train: Loss 0.002989 Accuracy 0.8904 | validation: Loss 0.093937 Accuracy -4.9014\n",
      "Epoch 471 | train: Loss 0.002931 Accuracy 0.8925 | validation: Loss 0.079429 Accuracy -3.9900\n",
      "Epoch 472 | train: Loss 0.002899 Accuracy 0.8937 | validation: Loss 0.089670 Accuracy -4.6333\n",
      "Epoch 473 | train: Loss 0.002763 Accuracy 0.8987 | validation: Loss 0.083247 Accuracy -4.2298\n",
      "Epoch 474 | train: Loss 0.002699 Accuracy 0.9010 | validation: Loss 0.082068 Accuracy -4.1557\n",
      "Epoch 475 | train: Loss 0.002688 Accuracy 0.9014 | validation: Loss 0.091982 Accuracy -4.7786\n",
      "Epoch 476 | train: Loss 0.002703 Accuracy 0.9009 | validation: Loss 0.082849 Accuracy -4.2048\n",
      "Epoch 477 | train: Loss 0.002771 Accuracy 0.8984 | validation: Loss 0.097035 Accuracy -5.0960\n",
      "Epoch 478 | train: Loss 0.002796 Accuracy 0.8975 | validation: Loss 0.084630 Accuracy -4.3167\n",
      "Epoch 479 | train: Loss 0.002750 Accuracy 0.8991 | validation: Loss 0.096987 Accuracy -5.0930\n",
      "Epoch 480 | train: Loss 0.002634 Accuracy 0.9034 | validation: Loss 0.087930 Accuracy -4.5240\n",
      "Epoch 481 | train: Loss 0.002623 Accuracy 0.9038 | validation: Loss 0.087238 Accuracy -4.4805\n",
      "Epoch 482 | train: Loss 0.002493 Accuracy 0.9086 | validation: Loss 0.093236 Accuracy -4.8573\n",
      "Epoch 483 | train: Loss 0.002542 Accuracy 0.9068 | validation: Loss 0.077400 Accuracy -3.8625\n",
      "Epoch 484 | train: Loss 0.002588 Accuracy 0.9051 | validation: Loss 0.092768 Accuracy -4.8279\n",
      "Epoch 485 | train: Loss 0.002582 Accuracy 0.9053 | validation: Loss 0.077611 Accuracy -3.8758\n",
      "Epoch 486 | train: Loss 0.002708 Accuracy 0.9007 | validation: Loss 0.093397 Accuracy -4.8675\n",
      "Epoch 487 | train: Loss 0.002511 Accuracy 0.9079 | validation: Loss 0.075990 Accuracy -3.7739\n",
      "Epoch 488 | train: Loss 0.002513 Accuracy 0.9078 | validation: Loss 0.098679 Accuracy -5.1993\n",
      "Epoch 489 | train: Loss 0.002584 Accuracy 0.9052 | validation: Loss 0.070780 Accuracy -3.4466\n",
      "Epoch 490 | train: Loss 0.002707 Accuracy 0.9007 | validation: Loss 0.098957 Accuracy -5.2168\n",
      "Epoch 491 | train: Loss 0.002828 Accuracy 0.8963 | validation: Loss 0.069726 Accuracy -3.3804\n",
      "Epoch 492 | train: Loss 0.003206 Accuracy 0.8824 | validation: Loss 0.097546 Accuracy -5.1281\n",
      "Epoch 493 | train: Loss 0.002807 Accuracy 0.8971 | validation: Loss 0.068251 Accuracy -3.2878\n",
      "Epoch 494 | train: Loss 0.002821 Accuracy 0.8966 | validation: Loss 0.098087 Accuracy -5.1621\n",
      "Epoch 495 | train: Loss 0.002594 Accuracy 0.9049 | validation: Loss 0.069063 Accuracy -3.3387\n",
      "Epoch 496 | train: Loss 0.002627 Accuracy 0.9037 | validation: Loss 0.087449 Accuracy -4.4938\n",
      "Epoch 497 | train: Loss 0.002299 Accuracy 0.9157 | validation: Loss 0.071032 Accuracy -3.4625\n",
      "Epoch 498 | train: Loss 0.002192 Accuracy 0.9196 | validation: Loss 0.076288 Accuracy -3.7926\n",
      "Epoch 499 | train: Loss 0.002124 Accuracy 0.9221 | validation: Loss 0.082905 Accuracy -4.2083\n",
      "Epoch 500 | train: Loss 0.002071 Accuracy 0.9240 | validation: Loss 0.077618 Accuracy -3.8762\n",
      "Epoch 501 | train: Loss 0.002102 Accuracy 0.9229 | validation: Loss 0.084802 Accuracy -4.3275\n",
      "Epoch 502 | train: Loss 0.002040 Accuracy 0.9252 | validation: Loss 0.067030 Accuracy -3.2110\n",
      "Epoch 503 | train: Loss 0.002079 Accuracy 0.9238 | validation: Loss 0.086267 Accuracy -4.4196\n",
      "Epoch 504 | train: Loss 0.002187 Accuracy 0.9198 | validation: Loss 0.060488 Accuracy -2.8000\n",
      "Epoch 505 | train: Loss 0.002609 Accuracy 0.9043 | validation: Loss 0.108250 Accuracy -5.8006\n",
      "Epoch 506 | train: Loss 0.003051 Accuracy 0.8881 | validation: Loss 0.061586 Accuracy -2.8690\n",
      "Epoch 507 | train: Loss 0.003681 Accuracy 0.8650 | validation: Loss 0.096133 Accuracy -5.0394\n",
      "Epoch 508 | train: Loss 0.002763 Accuracy 0.8987 | validation: Loss 0.069622 Accuracy -3.3739\n",
      "Epoch 509 | train: Loss 0.002420 Accuracy 0.9113 | validation: Loss 0.062104 Accuracy -2.9016\n",
      "Epoch 510 | train: Loss 0.002103 Accuracy 0.9229 | validation: Loss 0.093458 Accuracy -4.8713\n",
      "Epoch 511 | train: Loss 0.002797 Accuracy 0.8974 | validation: Loss 0.047651 Accuracy -1.9936\n",
      "Epoch 512 | train: Loss 0.004587 Accuracy 0.8318 | validation: Loss 0.123814 Accuracy -6.7783\n",
      "Epoch 513 | train: Loss 0.007045 Accuracy 0.7416 | validation: Loss 0.042131 Accuracy -1.6468\n",
      "Epoch 514 | train: Loss 0.011389 Accuracy 0.5823 | validation: Loss 0.074496 Accuracy -3.6800\n",
      "Epoch 515 | train: Loss 0.007066 Accuracy 0.7409 | validation: Loss 0.089046 Accuracy -4.5941\n",
      "Epoch 516 | train: Loss 0.006673 Accuracy 0.7553 | validation: Loss 0.054877 Accuracy -2.4475\n",
      "Epoch 517 | train: Loss 0.007042 Accuracy 0.7418 | validation: Loss 0.080489 Accuracy -4.0566\n",
      "Epoch 518 | train: Loss 0.010648 Accuracy 0.6095 | validation: Loss 0.048062 Accuracy -2.0194\n",
      "Epoch 519 | train: Loss 0.010743 Accuracy 0.6060 | validation: Loss 0.047761 Accuracy -2.0005\n",
      "Epoch 520 | train: Loss 0.009776 Accuracy 0.6415 | validation: Loss 0.074890 Accuracy -3.7048\n",
      "Epoch 521 | train: Loss 0.008396 Accuracy 0.6921 | validation: Loss 0.054272 Accuracy -2.4095\n",
      "Epoch 522 | train: Loss 0.006429 Accuracy 0.7642 | validation: Loss 0.067147 Accuracy -3.2184\n",
      "Epoch 523 | train: Loss 0.005819 Accuracy 0.7866 | validation: Loss 0.086301 Accuracy -4.4217\n",
      "Epoch 524 | train: Loss 0.005803 Accuracy 0.7872 | validation: Loss 0.070110 Accuracy -3.4045\n",
      "Epoch 525 | train: Loss 0.005315 Accuracy 0.8051 | validation: Loss 0.075667 Accuracy -3.7536\n",
      "Epoch 526 | train: Loss 0.005248 Accuracy 0.8075 | validation: Loss 0.074017 Accuracy -3.6500\n",
      "Epoch 527 | train: Loss 0.005367 Accuracy 0.8032 | validation: Loss 0.051768 Accuracy -2.2522\n",
      "Epoch 528 | train: Loss 0.005607 Accuracy 0.7944 | validation: Loss 0.056885 Accuracy -2.5737\n",
      "Epoch 529 | train: Loss 0.005175 Accuracy 0.8102 | validation: Loss 0.033471 Accuracy -1.1027\n",
      "Epoch 530 | train: Loss 0.005479 Accuracy 0.7991 | validation: Loss 0.038583 Accuracy -1.4239\n",
      "Epoch 531 | train: Loss 0.004922 Accuracy 0.8195 | validation: Loss 0.030255 Accuracy -0.9007\n",
      "Epoch 532 | train: Loss 0.004705 Accuracy 0.8275 | validation: Loss 0.029791 Accuracy -0.8715\n",
      "Epoch 533 | train: Loss 0.004439 Accuracy 0.8372 | validation: Loss 0.031078 Accuracy -0.9524\n",
      "Epoch 534 | train: Loss 0.004193 Accuracy 0.8462 | validation: Loss 0.030306 Accuracy -0.9039\n",
      "Epoch 535 | train: Loss 0.004467 Accuracy 0.8362 | validation: Loss 0.038188 Accuracy -1.3991\n",
      "Epoch 536 | train: Loss 0.004158 Accuracy 0.8475 | validation: Loss 0.036069 Accuracy -1.2660\n",
      "Epoch 537 | train: Loss 0.004712 Accuracy 0.8272 | validation: Loss 0.045564 Accuracy -1.8625\n",
      "Epoch 538 | train: Loss 0.005059 Accuracy 0.8145 | validation: Loss 0.035261 Accuracy -1.2152\n",
      "Epoch 539 | train: Loss 0.007113 Accuracy 0.7391 | validation: Loss 0.051306 Accuracy -2.2232\n",
      "Epoch 540 | train: Loss 0.009624 Accuracy 0.6471 | validation: Loss 0.049923 Accuracy -2.1363\n",
      "Epoch 541 | train: Loss 0.015610 Accuracy 0.4275 | validation: Loss 0.047674 Accuracy -1.9950\n",
      "Epoch 542 | train: Loss 0.010680 Accuracy 0.6083 | validation: Loss 0.031150 Accuracy -0.9569\n",
      "Epoch 543 | train: Loss 0.005882 Accuracy 0.7843 | validation: Loss 0.032644 Accuracy -1.0508\n",
      "Epoch 544 | train: Loss 0.003846 Accuracy 0.8589 | validation: Loss 0.055110 Accuracy -2.4622\n",
      "Epoch 545 | train: Loss 0.006957 Accuracy 0.7449 | validation: Loss 0.035099 Accuracy -1.2050\n",
      "Epoch 546 | train: Loss 0.008484 Accuracy 0.6889 | validation: Loss 0.048083 Accuracy -2.0207\n",
      "Epoch 547 | train: Loss 0.003773 Accuracy 0.8616 | validation: Loss 0.067513 Accuracy -3.2414\n",
      "Epoch 548 | train: Loss 0.005910 Accuracy 0.7833 | validation: Loss 0.035671 Accuracy -1.2410\n",
      "Epoch 549 | train: Loss 0.007790 Accuracy 0.7143 | validation: Loss 0.050531 Accuracy -2.1745\n",
      "Epoch 550 | train: Loss 0.003482 Accuracy 0.8723 | validation: Loss 0.080226 Accuracy -4.0400\n",
      "Epoch 551 | train: Loss 0.006577 Accuracy 0.7588 | validation: Loss 0.038529 Accuracy -1.4205\n",
      "Epoch 552 | train: Loss 0.007107 Accuracy 0.7394 | validation: Loss 0.049379 Accuracy -2.1022\n",
      "Epoch 553 | train: Loss 0.003610 Accuracy 0.8676 | validation: Loss 0.088293 Accuracy -4.5468\n",
      "Epoch 554 | train: Loss 0.007059 Accuracy 0.7411 | validation: Loss 0.042916 Accuracy -1.6961\n",
      "Epoch 555 | train: Loss 0.007128 Accuracy 0.7386 | validation: Loss 0.049954 Accuracy -2.1383\n",
      "Epoch 556 | train: Loss 0.003859 Accuracy 0.8585 | validation: Loss 0.089222 Accuracy -4.6052\n",
      "Epoch 557 | train: Loss 0.007144 Accuracy 0.7380 | validation: Loss 0.050093 Accuracy -2.1470\n",
      "Epoch 558 | train: Loss 0.004419 Accuracy 0.8379 | validation: Loss 0.056012 Accuracy -2.5188\n",
      "Epoch 559 | train: Loss 0.004482 Accuracy 0.8356 | validation: Loss 0.103484 Accuracy -5.5012\n",
      "Epoch 560 | train: Loss 0.005866 Accuracy 0.7849 | validation: Loss 0.060366 Accuracy -2.7924\n",
      "Epoch 561 | train: Loss 0.003708 Accuracy 0.8640 | validation: Loss 0.054193 Accuracy -2.4046\n",
      "Epoch 562 | train: Loss 0.004328 Accuracy 0.8413 | validation: Loss 0.090689 Accuracy -4.6973\n",
      "Epoch 563 | train: Loss 0.003959 Accuracy 0.8548 | validation: Loss 0.080326 Accuracy -4.0463\n",
      "Epoch 564 | train: Loss 0.003288 Accuracy 0.8794 | validation: Loss 0.057934 Accuracy -2.6396\n",
      "Epoch 565 | train: Loss 0.004105 Accuracy 0.8495 | validation: Loss 0.072058 Accuracy -3.5269\n",
      "Epoch 566 | train: Loss 0.003280 Accuracy 0.8797 | validation: Loss 0.079446 Accuracy -3.9910\n",
      "Epoch 567 | train: Loss 0.003732 Accuracy 0.8631 | validation: Loss 0.053306 Accuracy -2.3489\n",
      "Epoch 568 | train: Loss 0.003352 Accuracy 0.8771 | validation: Loss 0.062923 Accuracy -2.9530\n",
      "Epoch 569 | train: Loss 0.002505 Accuracy 0.9081 | validation: Loss 0.073982 Accuracy -3.6478\n",
      "Epoch 570 | train: Loss 0.003276 Accuracy 0.8799 | validation: Loss 0.050012 Accuracy -2.1419\n",
      "Epoch 571 | train: Loss 0.003592 Accuracy 0.8683 | validation: Loss 0.055034 Accuracy -2.4574\n",
      "Epoch 572 | train: Loss 0.003448 Accuracy 0.8736 | validation: Loss 0.060270 Accuracy -2.7864\n",
      "Epoch 573 | train: Loss 0.003660 Accuracy 0.8658 | validation: Loss 0.054371 Accuracy -2.4157\n",
      "Epoch 574 | train: Loss 0.003518 Accuracy 0.8710 | validation: Loss 0.064610 Accuracy -3.0590\n",
      "Epoch 575 | train: Loss 0.002473 Accuracy 0.9093 | validation: Loss 0.073740 Accuracy -3.6326\n",
      "Epoch 576 | train: Loss 0.002813 Accuracy 0.8968 | validation: Loss 0.059979 Accuracy -2.7680\n",
      "Epoch 577 | train: Loss 0.003096 Accuracy 0.8864 | validation: Loss 0.066513 Accuracy -3.1785\n",
      "Epoch 578 | train: Loss 0.002545 Accuracy 0.9067 | validation: Loss 0.071077 Accuracy -3.4652\n",
      "Epoch 579 | train: Loss 0.003098 Accuracy 0.8864 | validation: Loss 0.052863 Accuracy -2.3210\n",
      "Epoch 580 | train: Loss 0.003769 Accuracy 0.8618 | validation: Loss 0.073244 Accuracy -3.6014\n",
      "Epoch 581 | train: Loss 0.002244 Accuracy 0.9177 | validation: Loss 0.071089 Accuracy -3.4660\n",
      "Epoch 582 | train: Loss 0.002390 Accuracy 0.9124 | validation: Loss 0.058441 Accuracy -2.6715\n",
      "Epoch 583 | train: Loss 0.002881 Accuracy 0.8943 | validation: Loss 0.074788 Accuracy -3.6984\n",
      "Epoch 584 | train: Loss 0.002752 Accuracy 0.8991 | validation: Loss 0.061981 Accuracy -2.8938\n",
      "Epoch 585 | train: Loss 0.008512 Accuracy 0.6878 | validation: Loss 0.074735 Accuracy -3.6951\n",
      "Epoch 586 | train: Loss 0.005965 Accuracy 0.7812 | validation: Loss 0.060191 Accuracy -2.7814\n",
      "Epoch 587 | train: Loss 0.007630 Accuracy 0.7202 | validation: Loss 0.063085 Accuracy -2.9632\n",
      "Epoch 588 | train: Loss 0.006898 Accuracy 0.7470 | validation: Loss 0.041093 Accuracy -1.5816\n",
      "Epoch 589 | train: Loss 0.007697 Accuracy 0.7177 | validation: Loss 0.039815 Accuracy -1.5013\n",
      "Epoch 590 | train: Loss 0.007413 Accuracy 0.7282 | validation: Loss 0.028094 Accuracy -0.7649\n",
      "Epoch 591 | train: Loss 0.006997 Accuracy 0.7434 | validation: Loss 0.023833 Accuracy -0.4972\n",
      "Epoch 592 | train: Loss 0.006379 Accuracy 0.7661 | validation: Loss 0.020748 Accuracy -0.3034\n",
      "Epoch 593 | train: Loss 0.005983 Accuracy 0.7806 | validation: Loss 0.019639 Accuracy -0.2338\n",
      "Epoch 594 | train: Loss 0.005766 Accuracy 0.7886 | validation: Loss 0.020771 Accuracy -0.3049\n",
      "Epoch 595 | train: Loss 0.005529 Accuracy 0.7972 | validation: Loss 0.022687 Accuracy -0.4253\n",
      "Epoch 596 | train: Loss 0.005378 Accuracy 0.8028 | validation: Loss 0.027092 Accuracy -0.7020\n",
      "Epoch 597 | train: Loss 0.005270 Accuracy 0.8067 | validation: Loss 0.035483 Accuracy -1.2292\n",
      "Epoch 598 | train: Loss 0.005220 Accuracy 0.8086 | validation: Loss 0.038018 Accuracy -1.3884\n",
      "Epoch 599 | train: Loss 0.005271 Accuracy 0.8067 | validation: Loss 0.043044 Accuracy -1.7042\n",
      "Epoch 600 | train: Loss 0.005279 Accuracy 0.8064 | validation: Loss 0.043593 Accuracy -1.7386\n",
      "Epoch 601 | train: Loss 0.005372 Accuracy 0.8030 | validation: Loss 0.039177 Accuracy -1.4612\n",
      "Epoch 602 | train: Loss 0.005399 Accuracy 0.8020 | validation: Loss 0.035163 Accuracy -1.2090\n",
      "Epoch 603 | train: Loss 0.005041 Accuracy 0.8151 | validation: Loss 0.027639 Accuracy -0.7364\n",
      "Epoch 604 | train: Loss 0.004737 Accuracy 0.8263 | validation: Loss 0.024517 Accuracy -0.5402\n",
      "Epoch 605 | train: Loss 0.004610 Accuracy 0.8309 | validation: Loss 0.022435 Accuracy -0.4094\n",
      "Epoch 606 | train: Loss 0.004636 Accuracy 0.8300 | validation: Loss 0.021877 Accuracy -0.3744\n",
      "Epoch 607 | train: Loss 0.004661 Accuracy 0.8291 | validation: Loss 0.021307 Accuracy -0.3386\n",
      "Epoch 608 | train: Loss 0.004626 Accuracy 0.8304 | validation: Loss 0.022393 Accuracy -0.4068\n",
      "Epoch 609 | train: Loss 0.004580 Accuracy 0.8320 | validation: Loss 0.022170 Accuracy -0.3928\n",
      "Epoch 610 | train: Loss 0.004500 Accuracy 0.8350 | validation: Loss 0.023388 Accuracy -0.4693\n",
      "Epoch 611 | train: Loss 0.004484 Accuracy 0.8356 | validation: Loss 0.023470 Accuracy -0.4745\n",
      "Epoch 612 | train: Loss 0.004426 Accuracy 0.8377 | validation: Loss 0.024028 Accuracy -0.5095\n",
      "Epoch 613 | train: Loss 0.004318 Accuracy 0.8416 | validation: Loss 0.023707 Accuracy -0.4894\n",
      "Epoch 614 | train: Loss 0.004112 Accuracy 0.8492 | validation: Loss 0.023726 Accuracy -0.4906\n",
      "Epoch 615 | train: Loss 0.003879 Accuracy 0.8577 | validation: Loss 0.023318 Accuracy -0.4649\n",
      "Epoch 616 | train: Loss 0.003708 Accuracy 0.8640 | validation: Loss 0.025538 Accuracy -0.6044\n",
      "Epoch 617 | train: Loss 0.003666 Accuracy 0.8656 | validation: Loss 0.023436 Accuracy -0.4723\n",
      "Epoch 618 | train: Loss 0.003732 Accuracy 0.8631 | validation: Loss 0.030633 Accuracy -0.9245\n",
      "Epoch 619 | train: Loss 0.004274 Accuracy 0.8433 | validation: Loss 0.023981 Accuracy -0.5066\n",
      "Epoch 620 | train: Loss 0.005736 Accuracy 0.7896 | validation: Loss 0.051029 Accuracy -2.2058\n",
      "Epoch 621 | train: Loss 0.011718 Accuracy 0.5702 | validation: Loss 0.057255 Accuracy -2.5969\n",
      "Epoch 622 | train: Loss 0.021933 Accuracy 0.1957 | validation: Loss 0.129930 Accuracy -7.1626\n",
      "Epoch 623 | train: Loss 0.049080 Accuracy -0.7999 | validation: Loss 0.074490 Accuracy -3.6797\n",
      "Epoch 624 | train: Loss 0.018479 Accuracy 0.3223 | validation: Loss 0.034646 Accuracy -1.1766\n",
      "Epoch 625 | train: Loss 0.004456 Accuracy 0.8366 | validation: Loss 0.041825 Accuracy -1.6276\n",
      "Epoch 626 | train: Loss 0.015524 Accuracy 0.4307 | validation: Loss 0.050844 Accuracy -2.1942\n",
      "Epoch 627 | train: Loss 0.006487 Accuracy 0.7621 | validation: Loss 0.060522 Accuracy -2.8022\n",
      "Epoch 628 | train: Loss 0.008662 Accuracy 0.6823 | validation: Loss 0.035398 Accuracy -1.2238\n",
      "Epoch 629 | train: Loss 0.010057 Accuracy 0.6312 | validation: Loss 0.031806 Accuracy -0.9982\n",
      "Epoch 630 | train: Loss 0.006575 Accuracy 0.7589 | validation: Loss 0.052148 Accuracy -2.2761\n",
      "Epoch 631 | train: Loss 0.010388 Accuracy 0.6190 | validation: Loss 0.029073 Accuracy -0.8264\n",
      "Epoch 632 | train: Loss 0.004695 Accuracy 0.8278 | validation: Loss 0.029959 Accuracy -0.8821\n",
      "Epoch 633 | train: Loss 0.008789 Accuracy 0.6777 | validation: Loss 0.026620 Accuracy -0.6723\n",
      "Epoch 634 | train: Loss 0.004690 Accuracy 0.8280 | validation: Loss 0.038435 Accuracy -1.4146\n",
      "Epoch 635 | train: Loss 0.008073 Accuracy 0.7039 | validation: Loss 0.025776 Accuracy -0.6193\n",
      "Epoch 636 | train: Loss 0.004916 Accuracy 0.8197 | validation: Loss 0.028071 Accuracy -0.7635\n",
      "Epoch 637 | train: Loss 0.006970 Accuracy 0.7444 | validation: Loss 0.027038 Accuracy -0.6986\n",
      "Epoch 638 | train: Loss 0.004273 Accuracy 0.8433 | validation: Loss 0.030291 Accuracy -0.9030\n",
      "Epoch 639 | train: Loss 0.005980 Accuracy 0.7807 | validation: Loss 0.026215 Accuracy -0.6469\n",
      "Epoch 640 | train: Loss 0.004984 Accuracy 0.8172 | validation: Loss 0.026999 Accuracy -0.6962\n",
      "Epoch 641 | train: Loss 0.005092 Accuracy 0.8133 | validation: Loss 0.027404 Accuracy -0.7216\n",
      "Epoch 642 | train: Loss 0.004726 Accuracy 0.8267 | validation: Loss 0.025458 Accuracy -0.5993\n",
      "Epoch 643 | train: Loss 0.003765 Accuracy 0.8619 | validation: Loss 0.028890 Accuracy -0.8149\n",
      "Epoch 644 | train: Loss 0.004835 Accuracy 0.8227 | validation: Loss 0.025418 Accuracy -0.5968\n",
      "Epoch 645 | train: Loss 0.003430 Accuracy 0.8742 | validation: Loss 0.023880 Accuracy -0.5002\n",
      "Epoch 646 | train: Loss 0.004422 Accuracy 0.8378 | validation: Loss 0.024797 Accuracy -0.5578\n",
      "Epoch 647 | train: Loss 0.003357 Accuracy 0.8769 | validation: Loss 0.029507 Accuracy -0.8537\n",
      "Epoch 648 | train: Loss 0.004012 Accuracy 0.8529 | validation: Loss 0.024086 Accuracy -0.5131\n",
      "Epoch 649 | train: Loss 0.003317 Accuracy 0.8784 | validation: Loss 0.026522 Accuracy -0.6662\n",
      "Epoch 650 | train: Loss 0.003280 Accuracy 0.8797 | validation: Loss 0.034928 Accuracy -1.1943\n",
      "Epoch 651 | train: Loss 0.003689 Accuracy 0.8647 | validation: Loss 0.028759 Accuracy -0.8067\n",
      "Epoch 652 | train: Loss 0.003087 Accuracy 0.8868 | validation: Loss 0.024894 Accuracy -0.5639\n",
      "Epoch 653 | train: Loss 0.003218 Accuracy 0.8820 | validation: Loss 0.032990 Accuracy -1.0725\n",
      "Epoch 654 | train: Loss 0.003511 Accuracy 0.8713 | validation: Loss 0.028900 Accuracy -0.8156\n",
      "Epoch 655 | train: Loss 0.002851 Accuracy 0.8955 | validation: Loss 0.027443 Accuracy -0.7241\n",
      "Epoch 656 | train: Loss 0.003028 Accuracy 0.8889 | validation: Loss 0.033263 Accuracy -1.0897\n",
      "Epoch 657 | train: Loss 0.002735 Accuracy 0.8997 | validation: Loss 0.034793 Accuracy -1.1858\n",
      "Epoch 658 | train: Loss 0.002872 Accuracy 0.8947 | validation: Loss 0.029952 Accuracy -0.8817\n",
      "Epoch 659 | train: Loss 0.002757 Accuracy 0.8989 | validation: Loss 0.032954 Accuracy -1.0702\n",
      "Epoch 660 | train: Loss 0.002657 Accuracy 0.9026 | validation: Loss 0.033757 Accuracy -1.1207\n",
      "Epoch 661 | train: Loss 0.002521 Accuracy 0.9075 | validation: Loss 0.030110 Accuracy -0.8916\n",
      "Epoch 662 | train: Loss 0.002630 Accuracy 0.9035 | validation: Loss 0.033150 Accuracy -1.0826\n",
      "Epoch 663 | train: Loss 0.002342 Accuracy 0.9141 | validation: Loss 0.033032 Accuracy -1.0752\n",
      "Epoch 664 | train: Loss 0.002384 Accuracy 0.9126 | validation: Loss 0.031299 Accuracy -0.9663\n",
      "Epoch 665 | train: Loss 0.002409 Accuracy 0.9116 | validation: Loss 0.034715 Accuracy -1.1809\n",
      "Epoch 666 | train: Loss 0.002371 Accuracy 0.9130 | validation: Loss 0.032335 Accuracy -1.0314\n",
      "Epoch 667 | train: Loss 0.002127 Accuracy 0.9220 | validation: Loss 0.031493 Accuracy -0.9785\n",
      "Epoch 668 | train: Loss 0.002112 Accuracy 0.9225 | validation: Loss 0.034044 Accuracy -1.1388\n",
      "Epoch 669 | train: Loss 0.002194 Accuracy 0.9195 | validation: Loss 0.032923 Accuracy -1.0683\n",
      "Epoch 670 | train: Loss 0.002123 Accuracy 0.9221 | validation: Loss 0.035773 Accuracy -1.2474\n",
      "Epoch 671 | train: Loss 0.002052 Accuracy 0.9247 | validation: Loss 0.035596 Accuracy -1.2362\n",
      "Epoch 672 | train: Loss 0.001869 Accuracy 0.9314 | validation: Loss 0.036726 Accuracy -1.3073\n",
      "Epoch 673 | train: Loss 0.001792 Accuracy 0.9343 | validation: Loss 0.036073 Accuracy -1.2662\n",
      "Epoch 674 | train: Loss 0.001714 Accuracy 0.9371 | validation: Loss 0.035357 Accuracy -1.2212\n",
      "Epoch 675 | train: Loss 0.001674 Accuracy 0.9386 | validation: Loss 0.041098 Accuracy -1.5819\n",
      "Epoch 676 | train: Loss 0.001995 Accuracy 0.9268 | validation: Loss 0.043772 Accuracy -1.7499\n",
      "Epoch 677 | train: Loss 0.002837 Accuracy 0.8959 | validation: Loss 0.047220 Accuracy -1.9665\n",
      "Epoch 678 | train: Loss 0.004770 Accuracy 0.8251 | validation: Loss 0.045344 Accuracy -1.8486\n",
      "Epoch 679 | train: Loss 0.005500 Accuracy 0.7983 | validation: Loss 0.045969 Accuracy -1.8879\n",
      "Epoch 680 | train: Loss 0.006459 Accuracy 0.7631 | validation: Loss 0.053623 Accuracy -2.3688\n",
      "Epoch 681 | train: Loss 0.004014 Accuracy 0.8528 | validation: Loss 0.050144 Accuracy -2.1502\n",
      "Epoch 682 | train: Loss 0.003081 Accuracy 0.8870 | validation: Loss 0.041037 Accuracy -1.5780\n",
      "Epoch 683 | train: Loss 0.003185 Accuracy 0.8832 | validation: Loss 0.038879 Accuracy -1.4425\n",
      "Epoch 684 | train: Loss 0.004831 Accuracy 0.8228 | validation: Loss 0.045501 Accuracy -1.8585\n",
      "Epoch 685 | train: Loss 0.003336 Accuracy 0.8777 | validation: Loss 0.050790 Accuracy -2.1908\n",
      "Epoch 686 | train: Loss 0.007014 Accuracy 0.7428 | validation: Loss 0.049408 Accuracy -2.1039\n",
      "Epoch 687 | train: Loss 0.004487 Accuracy 0.8354 | validation: Loss 0.033280 Accuracy -1.0908\n",
      "Epoch 688 | train: Loss 0.008200 Accuracy 0.6993 | validation: Loss 0.038732 Accuracy -1.4332\n",
      "Epoch 689 | train: Loss 0.005480 Accuracy 0.7990 | validation: Loss 0.036738 Accuracy -1.3080\n",
      "Epoch 690 | train: Loss 0.004733 Accuracy 0.8264 | validation: Loss 0.032630 Accuracy -1.0499\n",
      "Epoch 691 | train: Loss 0.007158 Accuracy 0.7375 | validation: Loss 0.054504 Accuracy -2.4241\n",
      "Epoch 692 | train: Loss 0.005239 Accuracy 0.8079 | validation: Loss 0.052627 Accuracy -2.3062\n",
      "Epoch 693 | train: Loss 0.003599 Accuracy 0.8680 | validation: Loss 0.052692 Accuracy -2.3102\n",
      "Epoch 694 | train: Loss 0.004291 Accuracy 0.8426 | validation: Loss 0.068000 Accuracy -3.2720\n",
      "Epoch 695 | train: Loss 0.004554 Accuracy 0.8330 | validation: Loss 0.046236 Accuracy -1.9047\n",
      "Epoch 696 | train: Loss 0.003212 Accuracy 0.8822 | validation: Loss 0.040734 Accuracy -1.5591\n",
      "Epoch 697 | train: Loss 0.003120 Accuracy 0.8856 | validation: Loss 0.047649 Accuracy -1.9935\n",
      "Epoch 698 | train: Loss 0.003963 Accuracy 0.8547 | validation: Loss 0.038593 Accuracy -1.4245\n",
      "Epoch 699 | train: Loss 0.003154 Accuracy 0.8843 | validation: Loss 0.047860 Accuracy -2.0067\n",
      "Epoch 700 | train: Loss 0.002086 Accuracy 0.9235 | validation: Loss 0.055357 Accuracy -2.4777\n",
      "Epoch 701 | train: Loss 0.002864 Accuracy 0.8950 | validation: Loss 0.045625 Accuracy -1.8663\n",
      "Epoch 702 | train: Loss 0.003658 Accuracy 0.8659 | validation: Loss 0.045886 Accuracy -1.8827\n",
      "Epoch 703 | train: Loss 0.002285 Accuracy 0.9162 | validation: Loss 0.041727 Accuracy -1.6214\n",
      "Epoch 704 | train: Loss 0.002259 Accuracy 0.9172 | validation: Loss 0.037910 Accuracy -1.3816\n",
      "Epoch 705 | train: Loss 0.003121 Accuracy 0.8855 | validation: Loss 0.041777 Accuracy -1.6245\n",
      "Epoch 706 | train: Loss 0.002518 Accuracy 0.9077 | validation: Loss 0.042849 Accuracy -1.6919\n",
      "Epoch 707 | train: Loss 0.001995 Accuracy 0.9269 | validation: Loss 0.048085 Accuracy -2.0208\n",
      "Epoch 708 | train: Loss 0.002422 Accuracy 0.9112 | validation: Loss 0.058127 Accuracy -2.6517\n",
      "Epoch 709 | train: Loss 0.003014 Accuracy 0.8895 | validation: Loss 0.049825 Accuracy -2.1302\n",
      "Epoch 710 | train: Loss 0.003022 Accuracy 0.8892 | validation: Loss 0.046778 Accuracy -1.9387\n",
      "Epoch 711 | train: Loss 0.002031 Accuracy 0.9255 | validation: Loss 0.046320 Accuracy -1.9100\n",
      "Epoch 712 | train: Loss 0.001972 Accuracy 0.9277 | validation: Loss 0.049427 Accuracy -2.1051\n",
      "Epoch 713 | train: Loss 0.002423 Accuracy 0.9111 | validation: Loss 0.056300 Accuracy -2.5369\n",
      "Epoch 714 | train: Loss 0.002263 Accuracy 0.9170 | validation: Loss 0.050110 Accuracy -2.1480\n",
      "Epoch 715 | train: Loss 0.001682 Accuracy 0.9383 | validation: Loss 0.046979 Accuracy -1.9514\n",
      "Epoch 716 | train: Loss 0.001587 Accuracy 0.9418 | validation: Loss 0.051410 Accuracy -2.2297\n",
      "Epoch 717 | train: Loss 0.001841 Accuracy 0.9325 | validation: Loss 0.055741 Accuracy -2.5018\n",
      "Epoch 718 | train: Loss 0.002177 Accuracy 0.9202 | validation: Loss 0.066478 Accuracy -3.1764\n",
      "Epoch 719 | train: Loss 0.002163 Accuracy 0.9207 | validation: Loss 0.061183 Accuracy -2.8437\n",
      "Epoch 720 | train: Loss 0.001924 Accuracy 0.9295 | validation: Loss 0.063205 Accuracy -2.9707\n",
      "Epoch 721 | train: Loss 0.001326 Accuracy 0.9514 | validation: Loss 0.069482 Accuracy -3.3651\n",
      "Epoch 722 | train: Loss 0.001859 Accuracy 0.9318 | validation: Loss 0.062351 Accuracy -2.9171\n",
      "Epoch 723 | train: Loss 0.001737 Accuracy 0.9363 | validation: Loss 0.059412 Accuracy -2.7324\n",
      "Epoch 724 | train: Loss 0.001805 Accuracy 0.9338 | validation: Loss 0.058221 Accuracy -2.6576\n",
      "Epoch 725 | train: Loss 0.001683 Accuracy 0.9383 | validation: Loss 0.063624 Accuracy -2.9971\n",
      "Epoch 726 | train: Loss 0.001713 Accuracy 0.9372 | validation: Loss 0.073720 Accuracy -3.6313\n",
      "Epoch 727 | train: Loss 0.001978 Accuracy 0.9275 | validation: Loss 0.068864 Accuracy -3.3262\n",
      "Epoch 728 | train: Loss 0.003180 Accuracy 0.8834 | validation: Loss 0.086350 Accuracy -4.4248\n",
      "Epoch 729 | train: Loss 0.004392 Accuracy 0.8389 | validation: Loss 0.071655 Accuracy -3.5016\n",
      "Epoch 730 | train: Loss 0.005763 Accuracy 0.7887 | validation: Loss 0.079011 Accuracy -3.9637\n",
      "Epoch 731 | train: Loss 0.003489 Accuracy 0.8720 | validation: Loss 0.071578 Accuracy -3.4967\n",
      "Epoch 732 | train: Loss 0.001732 Accuracy 0.9365 | validation: Loss 0.073590 Accuracy -3.6231\n",
      "Epoch 733 | train: Loss 0.002462 Accuracy 0.9097 | validation: Loss 0.084065 Accuracy -4.2812\n",
      "Epoch 734 | train: Loss 0.003314 Accuracy 0.8785 | validation: Loss 0.067024 Accuracy -3.2106\n",
      "Epoch 735 | train: Loss 0.003407 Accuracy 0.8751 | validation: Loss 0.077419 Accuracy -3.8637\n",
      "Epoch 736 | train: Loss 0.001680 Accuracy 0.9384 | validation: Loss 0.080136 Accuracy -4.0344\n",
      "Epoch 737 | train: Loss 0.001980 Accuracy 0.9274 | validation: Loss 0.079896 Accuracy -4.0193\n",
      "Epoch 738 | train: Loss 0.004028 Accuracy 0.8523 | validation: Loss 0.072928 Accuracy -3.5815\n",
      "Epoch 739 | train: Loss 0.001703 Accuracy 0.9375 | validation: Loss 0.135437 Accuracy -7.5086\n",
      "Epoch 740 | train: Loss 0.050991 Accuracy -0.8700 | validation: Loss 2.290949 Accuracy -142.9241\n",
      "Epoch 741 | train: Loss 1.068541 Accuracy -38.1870 | validation: Loss 0.752232 Accuracy -46.2574\n",
      "Epoch 742 | train: Loss 0.404379 Accuracy -13.8299 | validation: Loss 0.781788 Accuracy -48.1142\n",
      "Epoch 743 | train: Loss 0.493046 Accuracy -17.0817 | validation: Loss 0.654759 Accuracy -40.1339\n",
      "Epoch 744 | train: Loss 0.395250 Accuracy -13.4951 | validation: Loss 0.451469 Accuracy -27.3626\n",
      "Epoch 745 | train: Loss 0.245640 Accuracy -8.0084 | validation: Loss 0.248720 Accuracy -14.6253\n",
      "Epoch 746 | train: Loss 0.113875 Accuracy -3.1762 | validation: Loss 0.100408 Accuracy -5.3080\n",
      "Epoch 747 | train: Loss 0.036874 Accuracy -0.3523 | validation: Loss 0.027341 Accuracy -0.7176\n",
      "Epoch 748 | train: Loss 0.034626 Accuracy -0.2699 | validation: Loss 0.018203 Accuracy -0.1436\n",
      "Epoch 749 | train: Loss 0.085093 Accuracy -2.1206 | validation: Loss 0.040602 Accuracy -1.5508\n",
      "Epoch 750 | train: Loss 0.149609 Accuracy -4.4867 | validation: Loss 0.061183 Accuracy -2.8437\n",
      "Epoch 751 | train: Loss 0.191632 Accuracy -6.0278 | validation: Loss 0.062134 Accuracy -2.9034\n",
      "Epoch 752 | train: Loss 0.193489 Accuracy -6.0959 | validation: Loss 0.043041 Accuracy -1.7040\n",
      "Epoch 753 | train: Loss 0.157458 Accuracy -4.7745 | validation: Loss 0.037730 Accuracy -1.3703\n",
      "Epoch 754 | train: Loss 0.052036 Accuracy -0.9083 | validation: Loss 4.846128 Accuracy -303.4481\n",
      "Epoch 755 | train: Loss 0.812237 Accuracy -28.7875 | validation: Loss 0.029400 Accuracy -0.8470\n",
      "Epoch 756 | train: Loss 0.044035 Accuracy -0.6149 | validation: Loss 0.016106 Accuracy -0.0118\n",
      "Epoch 757 | train: Loss 0.071137 Accuracy -1.6088 | validation: Loss 0.016018 Accuracy -0.0063\n",
      "Epoch 758 | train: Loss 0.060780 Accuracy -1.2290 | validation: Loss 0.018444 Accuracy -0.1587\n",
      "Epoch 759 | train: Loss 0.068090 Accuracy -1.4971 | validation: Loss 0.026662 Accuracy -0.6750\n",
      "Epoch 760 | train: Loss 0.035190 Accuracy -0.2905 | validation: Loss 0.041874 Accuracy -1.6307\n",
      "Epoch 761 | train: Loss 0.028263 Accuracy -0.0365 | validation: Loss 0.062772 Accuracy -2.9435\n",
      "Epoch 762 | train: Loss 0.027834 Accuracy -0.0208 | validation: Loss 0.085783 Accuracy -4.3891\n",
      "Epoch 763 | train: Loss 0.032403 Accuracy -0.1883 | validation: Loss 0.106263 Accuracy -5.6757\n",
      "Epoch 764 | train: Loss 0.038913 Accuracy -0.4271 | validation: Loss 0.120048 Accuracy -6.5418\n",
      "Epoch 765 | train: Loss 0.044176 Accuracy -0.6201 | validation: Loss 0.124717 Accuracy -6.8351\n",
      "Epoch 766 | train: Loss 0.046088 Accuracy -0.6902 | validation: Loss 0.120136 Accuracy -6.5473\n",
      "Epoch 767 | train: Loss 0.044212 Accuracy -0.6214 | validation: Loss 0.108201 Accuracy -5.7975\n",
      "Epoch 768 | train: Loss 0.039616 Accuracy -0.4528 | validation: Loss 0.092001 Accuracy -4.7798\n",
      "Epoch 769 | train: Loss 0.034185 Accuracy -0.2537 | validation: Loss 0.074810 Accuracy -3.6998\n",
      "Epoch 770 | train: Loss 0.029769 Accuracy -0.0917 | validation: Loss 0.059253 Accuracy -2.7224\n",
      "Epoch 771 | train: Loss 0.027508 Accuracy -0.0088 | validation: Loss 0.046846 Accuracy -1.9430\n",
      "Epoch 772 | train: Loss 0.027549 Accuracy -0.0103 | validation: Loss 0.038115 Accuracy -1.3945\n",
      "Epoch 773 | train: Loss 0.029175 Accuracy -0.0700 | validation: Loss 0.032703 Accuracy -1.0545\n",
      "Epoch 774 | train: Loss 0.031250 Accuracy -0.1460 | validation: Loss 0.029737 Accuracy -0.8682\n",
      "Epoch 775 | train: Loss 0.032774 Accuracy -0.2019 | validation: Loss 0.027760 Accuracy -0.7440\n",
      "Epoch 776 | train: Loss 0.035087 Accuracy -0.2868 | validation: Loss 0.031309 Accuracy -0.9669\n",
      "Epoch 777 | train: Loss 0.031974 Accuracy -0.1726 | validation: Loss 0.034868 Accuracy -1.1905\n",
      "Epoch 778 | train: Loss 0.030293 Accuracy -0.1110 | validation: Loss 0.038435 Accuracy -1.4146\n",
      "Epoch 779 | train: Loss 0.028618 Accuracy -0.0495 | validation: Loss 0.069404 Accuracy -3.3602\n",
      "Epoch 780 | train: Loss 0.029189 Accuracy -0.0705 | validation: Loss 0.055049 Accuracy -2.4583\n",
      "Epoch 781 | train: Loss 0.027291 Accuracy -0.0008 | validation: Loss 0.062952 Accuracy -2.9548\n",
      "Epoch 782 | train: Loss 0.027841 Accuracy -0.0210 | validation: Loss 0.069693 Accuracy -3.3783\n",
      "Epoch 783 | train: Loss 0.028781 Accuracy -0.0555 | validation: Loss 0.074299 Accuracy -3.6677\n",
      "Epoch 784 | train: Loss 0.029655 Accuracy -0.0875 | validation: Loss 0.076404 Accuracy -3.7999\n",
      "Epoch 785 | train: Loss 0.030200 Accuracy -0.1075 | validation: Loss 0.075947 Accuracy -3.7712\n",
      "Epoch 786 | train: Loss 0.030106 Accuracy -0.1041 | validation: Loss 0.073351 Accuracy -3.6081\n",
      "Epoch 787 | train: Loss 0.029486 Accuracy -0.0814 | validation: Loss 0.069119 Accuracy -3.3423\n",
      "Epoch 788 | train: Loss 0.028617 Accuracy -0.0495 | validation: Loss 0.063576 Accuracy -2.9940\n",
      "Epoch 789 | train: Loss 0.027747 Accuracy -0.0176 | validation: Loss 0.058471 Accuracy -2.6733\n",
      "Epoch 790 | train: Loss 0.027055 Accuracy 0.0078 | validation: Loss 0.054036 Accuracy -2.3947\n",
      "Epoch 791 | train: Loss 0.026598 Accuracy 0.0246 | validation: Loss 0.050350 Accuracy -2.1631\n",
      "Epoch 792 | train: Loss 0.025940 Accuracy 0.0487 | validation: Loss 0.054544 Accuracy -2.4266\n",
      "Epoch 793 | train: Loss 0.025031 Accuracy 0.0820 | validation: Loss 0.068735 Accuracy -3.3181\n",
      "Epoch 794 | train: Loss 0.024975 Accuracy 0.0841 | validation: Loss 0.077391 Accuracy -3.8619\n",
      "Epoch 795 | train: Loss 0.022656 Accuracy 0.1691 | validation: Loss 0.051078 Accuracy -2.2089\n",
      "Epoch 796 | train: Loss 0.019672 Accuracy 0.2786 | validation: Loss 0.057832 Accuracy -2.6332\n",
      "Epoch 797 | train: Loss 0.017361 Accuracy 0.3633 | validation: Loss 0.134093 Accuracy -7.4241\n",
      "Epoch 798 | train: Loss 0.018211 Accuracy 0.3321 | validation: Loss 0.030516 Accuracy -0.9171\n",
      "Epoch 799 | train: Loss 0.019921 Accuracy 0.2694 | validation: Loss 0.028221 Accuracy -0.7729\n",
      "Epoch 800 | train: Loss 0.018598 Accuracy 0.3179 | validation: Loss 0.062296 Accuracy -2.9136\n",
      "Epoch 801 | train: Loss 0.014759 Accuracy 0.4587 | validation: Loss 0.027701 Accuracy -0.7403\n",
      "Epoch 802 | train: Loss 0.012165 Accuracy 0.5539 | validation: Loss 0.025507 Accuracy -0.6024\n",
      "Epoch 803 | train: Loss 0.012813 Accuracy 0.5301 | validation: Loss 0.032450 Accuracy -1.0386\n",
      "Epoch 804 | train: Loss 0.014220 Accuracy 0.4785 | validation: Loss 0.046748 Accuracy -1.9369\n",
      "Epoch 805 | train: Loss 0.019203 Accuracy 0.2958 | validation: Loss 0.175912 Accuracy -10.0513\n",
      "Epoch 806 | train: Loss 0.011078 Accuracy 0.5937 | validation: Loss 0.331792 Accuracy -19.8441\n",
      "Epoch 807 | train: Loss 0.028754 Accuracy -0.0545 | validation: Loss 0.637838 Accuracy -39.0709\n",
      "Epoch 808 | train: Loss 0.045017 Accuracy -0.6509 | validation: Loss 0.021059 Accuracy -0.3230\n",
      "Epoch 809 | train: Loss 0.042080 Accuracy -0.5432 | validation: Loss 0.023251 Accuracy -0.4607\n",
      "Epoch 810 | train: Loss 0.038716 Accuracy -0.4198 | validation: Loss 0.027861 Accuracy -0.7503\n",
      "Epoch 811 | train: Loss 0.034082 Accuracy -0.2499 | validation: Loss 0.035480 Accuracy -1.2290\n",
      "Epoch 812 | train: Loss 0.029918 Accuracy -0.0972 | validation: Loss 0.046138 Accuracy -1.8985\n",
      "Epoch 813 | train: Loss 0.027575 Accuracy -0.0113 | validation: Loss 0.058745 Accuracy -2.6905\n",
      "Epoch 814 | train: Loss 0.027504 Accuracy -0.0087 | validation: Loss 0.071457 Accuracy -3.4891\n",
      "Epoch 815 | train: Loss 0.029193 Accuracy -0.0706 | validation: Loss 0.082182 Accuracy -4.1629\n",
      "Epoch 816 | train: Loss 0.031503 Accuracy -0.1553 | validation: Loss 0.089157 Accuracy -4.6011\n",
      "Epoch 817 | train: Loss 0.033331 Accuracy -0.2224 | validation: Loss 0.091406 Accuracy -4.7424\n",
      "Epoch 818 | train: Loss 0.033943 Accuracy -0.2448 | validation: Loss 0.088972 Accuracy -4.5895\n",
      "Epoch 819 | train: Loss 0.033205 Accuracy -0.2177 | validation: Loss 0.082782 Accuracy -4.2006\n",
      "Epoch 820 | train: Loss 0.031517 Accuracy -0.1558 | validation: Loss 0.074409 Accuracy -3.6746\n",
      "Epoch 821 | train: Loss 0.029551 Accuracy -0.0837 | validation: Loss 0.065512 Accuracy -3.1156\n",
      "Epoch 822 | train: Loss 0.027952 Accuracy -0.0251 | validation: Loss 0.057545 Accuracy -2.6151\n",
      "Epoch 823 | train: Loss 0.027129 Accuracy 0.0051 | validation: Loss 0.050838 Accuracy -2.1938\n",
      "Epoch 824 | train: Loss 0.027049 Accuracy 0.0080 | validation: Loss 0.040547 Accuracy -1.5473\n",
      "Epoch 825 | train: Loss 0.027803 Accuracy -0.0196 | validation: Loss 0.030832 Accuracy -0.9369\n",
      "Epoch 826 | train: Loss 0.028687 Accuracy -0.0520 | validation: Loss 0.027984 Accuracy -0.7581\n",
      "Epoch 827 | train: Loss 0.028319 Accuracy -0.0386 | validation: Loss 0.044154 Accuracy -1.7739\n",
      "Epoch 828 | train: Loss 0.024434 Accuracy 0.1039 | validation: Loss 0.078916 Accuracy -3.9578\n",
      "Epoch 829 | train: Loss 0.023466 Accuracy 0.1394 | validation: Loss 0.081034 Accuracy -4.0908\n",
      "Epoch 830 | train: Loss 0.021233 Accuracy 0.2213 | validation: Loss 0.037088 Accuracy -1.3300\n",
      "Epoch 831 | train: Loss 0.017524 Accuracy 0.3573 | validation: Loss 0.032876 Accuracy -1.0654\n",
      "Epoch 832 | train: Loss 0.016182 Accuracy 0.4065 | validation: Loss 0.153491 Accuracy -8.6428\n",
      "Epoch 833 | train: Loss 0.017295 Accuracy 0.3658 | validation: Loss 0.033415 Accuracy -1.0992\n",
      "Epoch 834 | train: Loss 0.015139 Accuracy 0.4448 | validation: Loss 0.119369 Accuracy -6.4991\n",
      "Epoch 835 | train: Loss 0.013897 Accuracy 0.4903 | validation: Loss 0.028837 Accuracy -0.8116\n",
      "Epoch 836 | train: Loss 0.013964 Accuracy 0.4879 | validation: Loss 0.088637 Accuracy -4.5684\n",
      "Epoch 837 | train: Loss 0.013249 Accuracy 0.5141 | validation: Loss 0.022597 Accuracy -0.4196\n",
      "Epoch 838 | train: Loss 0.013158 Accuracy 0.5175 | validation: Loss 0.039410 Accuracy -1.4759\n",
      "Epoch 839 | train: Loss 0.011725 Accuracy 0.5700 | validation: Loss 0.062393 Accuracy -2.9197\n",
      "Epoch 840 | train: Loss 0.013692 Accuracy 0.4979 | validation: Loss 0.032834 Accuracy -1.0627\n",
      "Epoch 841 | train: Loss 0.012612 Accuracy 0.5375 | validation: Loss 0.025656 Accuracy -0.6118\n",
      "Epoch 842 | train: Loss 0.011041 Accuracy 0.5951 | validation: Loss 0.049915 Accuracy -2.1358\n",
      "Epoch 843 | train: Loss 0.014813 Accuracy 0.4568 | validation: Loss 0.026467 Accuracy -0.6627\n",
      "Epoch 844 | train: Loss 0.021292 Accuracy 0.2191 | validation: Loss 0.032388 Accuracy -1.0347\n",
      "Epoch 845 | train: Loss 0.012718 Accuracy 0.5336 | validation: Loss 0.025740 Accuracy -0.6171\n",
      "Epoch 846 | train: Loss 0.010251 Accuracy 0.6241 | validation: Loss 0.028302 Accuracy -0.7780\n",
      "Epoch 847 | train: Loss 0.009820 Accuracy 0.6399 | validation: Loss 0.030848 Accuracy -0.9380\n",
      "Epoch 848 | train: Loss 0.010322 Accuracy 0.6215 | validation: Loss 0.025299 Accuracy -0.5894\n",
      "Epoch 849 | train: Loss 0.007944 Accuracy 0.7087 | validation: Loss 0.027741 Accuracy -0.7428\n",
      "Epoch 850 | train: Loss 0.007996 Accuracy 0.7067 | validation: Loss 0.023258 Accuracy -0.4612\n",
      "Epoch 851 | train: Loss 0.009413 Accuracy 0.6548 | validation: Loss 0.050371 Accuracy -2.1645\n",
      "Epoch 852 | train: Loss 0.014958 Accuracy 0.4514 | validation: Loss 0.032363 Accuracy -1.0331\n",
      "Epoch 853 | train: Loss 0.015343 Accuracy 0.4373 | validation: Loss 0.028763 Accuracy -0.8070\n",
      "Epoch 854 | train: Loss 0.019711 Accuracy 0.2771 | validation: Loss 0.030181 Accuracy -0.8960\n",
      "Epoch 855 | train: Loss 0.014187 Accuracy 0.4797 | validation: Loss 0.077772 Accuracy -3.8859\n",
      "Epoch 856 | train: Loss 0.017007 Accuracy 0.3763 | validation: Loss 0.056981 Accuracy -2.5797\n",
      "Epoch 857 | train: Loss 0.016994 Accuracy 0.3768 | validation: Loss 0.038971 Accuracy -1.4483\n",
      "Epoch 858 | train: Loss 0.013452 Accuracy 0.5067 | validation: Loss 0.074617 Accuracy -3.6877\n",
      "Epoch 859 | train: Loss 0.021312 Accuracy 0.2184 | validation: Loss 0.041311 Accuracy -1.5953\n",
      "Epoch 860 | train: Loss 0.014550 Accuracy 0.4664 | validation: Loss 0.036324 Accuracy -1.2820\n",
      "Epoch 861 | train: Loss 0.010555 Accuracy 0.6129 | validation: Loss 0.028603 Accuracy -0.7969\n",
      "Epoch 862 | train: Loss 0.011460 Accuracy 0.5797 | validation: Loss 0.025652 Accuracy -0.6115\n",
      "Epoch 863 | train: Loss 0.009349 Accuracy 0.6571 | validation: Loss 0.024949 Accuracy -0.5674\n",
      "Epoch 864 | train: Loss 0.010458 Accuracy 0.6165 | validation: Loss 0.019791 Accuracy -0.2433\n",
      "Epoch 865 | train: Loss 0.008825 Accuracy 0.6764 | validation: Loss 0.019950 Accuracy -0.2533\n",
      "Epoch 866 | train: Loss 0.009323 Accuracy 0.6581 | validation: Loss 0.020973 Accuracy -0.3176\n",
      "Epoch 867 | train: Loss 0.009607 Accuracy 0.6477 | validation: Loss 0.021255 Accuracy -0.3353\n",
      "Epoch 868 | train: Loss 0.008164 Accuracy 0.7006 | validation: Loss 0.021838 Accuracy -0.3719\n",
      "Epoch 869 | train: Loss 0.008700 Accuracy 0.6809 | validation: Loss 0.023286 Accuracy -0.4629\n",
      "Epoch 870 | train: Loss 0.009538 Accuracy 0.6502 | validation: Loss 0.022794 Accuracy -0.4320\n",
      "Epoch 871 | train: Loss 0.008570 Accuracy 0.6857 | validation: Loss 0.024602 Accuracy -0.5456\n",
      "Epoch 872 | train: Loss 0.007951 Accuracy 0.7084 | validation: Loss 0.027260 Accuracy -0.7126\n",
      "Epoch 873 | train: Loss 0.008654 Accuracy 0.6826 | validation: Loss 0.022156 Accuracy -0.3919\n",
      "Epoch 874 | train: Loss 0.008067 Accuracy 0.7042 | validation: Loss 0.022807 Accuracy -0.4328\n",
      "Epoch 875 | train: Loss 0.006915 Accuracy 0.7464 | validation: Loss 0.024350 Accuracy -0.5297\n",
      "Epoch 876 | train: Loss 0.008593 Accuracy 0.6849 | validation: Loss 0.049740 Accuracy -2.1248\n",
      "Epoch 877 | train: Loss 0.014814 Accuracy 0.4567 | validation: Loss 0.025109 Accuracy -0.5774\n",
      "Epoch 878 | train: Loss 0.011274 Accuracy 0.5866 | validation: Loss 0.028496 Accuracy -0.7902\n",
      "Epoch 879 | train: Loss 0.010057 Accuracy 0.6312 | validation: Loss 0.025164 Accuracy -0.5809\n",
      "Epoch 880 | train: Loss 0.008651 Accuracy 0.6827 | validation: Loss 0.029214 Accuracy -0.8353\n",
      "Epoch 881 | train: Loss 0.009481 Accuracy 0.6523 | validation: Loss 0.025047 Accuracy -0.5735\n",
      "Epoch 882 | train: Loss 0.006933 Accuracy 0.7458 | validation: Loss 0.031001 Accuracy -0.9476\n",
      "Epoch 883 | train: Loss 0.008338 Accuracy 0.6942 | validation: Loss 0.028581 Accuracy -0.7955\n",
      "Epoch 884 | train: Loss 0.005663 Accuracy 0.7923 | validation: Loss 0.027404 Accuracy -0.7216\n",
      "Epoch 885 | train: Loss 0.009478 Accuracy 0.6524 | validation: Loss 0.050009 Accuracy -2.1417\n",
      "Epoch 886 | train: Loss 0.017613 Accuracy 0.3541 | validation: Loss 0.025437 Accuracy -0.5980\n",
      "Epoch 887 | train: Loss 0.007362 Accuracy 0.7300 | validation: Loss 0.051785 Accuracy -2.2533\n",
      "Epoch 888 | train: Loss 0.017824 Accuracy 0.3463 | validation: Loss 0.049967 Accuracy -2.1391\n",
      "Epoch 889 | train: Loss 0.013920 Accuracy 0.4895 | validation: Loss 0.041778 Accuracy -1.6246\n",
      "Epoch 890 | train: Loss 0.016667 Accuracy 0.3888 | validation: Loss 0.033244 Accuracy -1.0885\n",
      "Epoch 891 | train: Loss 0.014975 Accuracy 0.4508 | validation: Loss 0.031941 Accuracy -1.0067\n",
      "Epoch 892 | train: Loss 0.011214 Accuracy 0.5887 | validation: Loss 0.041673 Accuracy -1.6180\n",
      "Epoch 893 | train: Loss 0.012811 Accuracy 0.5302 | validation: Loss 0.020230 Accuracy -0.2709\n",
      "Epoch 894 | train: Loss 0.012536 Accuracy 0.5403 | validation: Loss 0.018827 Accuracy -0.1828\n",
      "Epoch 895 | train: Loss 0.008319 Accuracy 0.6949 | validation: Loss 0.020931 Accuracy -0.3149\n",
      "Epoch 896 | train: Loss 0.009971 Accuracy 0.6343 | validation: Loss 0.038426 Accuracy -1.4140\n",
      "Epoch 897 | train: Loss 0.009299 Accuracy 0.6590 | validation: Loss 0.030752 Accuracy -0.9320\n",
      "Epoch 898 | train: Loss 0.007303 Accuracy 0.7322 | validation: Loss 0.027299 Accuracy -0.7150\n",
      "Epoch 899 | train: Loss 0.009156 Accuracy 0.6642 | validation: Loss 0.027661 Accuracy -0.7377\n",
      "Epoch 900 | train: Loss 0.006954 Accuracy 0.7450 | validation: Loss 0.027413 Accuracy -0.7221\n",
      "Epoch 901 | train: Loss 0.007436 Accuracy 0.7273 | validation: Loss 0.027065 Accuracy -0.7003\n",
      "Epoch 902 | train: Loss 0.006614 Accuracy 0.7575 | validation: Loss 0.027070 Accuracy -0.7006\n",
      "Epoch 903 | train: Loss 0.007253 Accuracy 0.7340 | validation: Loss 0.026869 Accuracy -0.6880\n",
      "Epoch 904 | train: Loss 0.006443 Accuracy 0.7637 | validation: Loss 0.033545 Accuracy -1.1074\n",
      "Epoch 905 | train: Loss 0.006188 Accuracy 0.7731 | validation: Loss 0.042365 Accuracy -1.6615\n",
      "Epoch 906 | train: Loss 0.005277 Accuracy 0.8065 | validation: Loss 0.050386 Accuracy -2.1654\n",
      "Epoch 907 | train: Loss 0.006437 Accuracy 0.7639 | validation: Loss 0.079907 Accuracy -4.0200\n",
      "Epoch 908 | train: Loss 0.006023 Accuracy 0.7791 | validation: Loss 0.076642 Accuracy -3.8148\n",
      "Epoch 909 | train: Loss 0.005953 Accuracy 0.7817 | validation: Loss 0.050399 Accuracy -2.1662\n",
      "Epoch 910 | train: Loss 0.006143 Accuracy 0.7747 | validation: Loss 0.051316 Accuracy -2.2238\n",
      "Epoch 911 | train: Loss 0.005354 Accuracy 0.8037 | validation: Loss 0.033930 Accuracy -1.1316\n",
      "Epoch 912 | train: Loss 0.005207 Accuracy 0.8090 | validation: Loss 0.024326 Accuracy -0.5282\n",
      "Epoch 913 | train: Loss 0.005919 Accuracy 0.7829 | validation: Loss 0.033285 Accuracy -1.0911\n",
      "Epoch 914 | train: Loss 0.007127 Accuracy 0.7386 | validation: Loss 0.030539 Accuracy -0.9186\n",
      "Epoch 915 | train: Loss 0.007449 Accuracy 0.7268 | validation: Loss 0.023211 Accuracy -0.4582\n",
      "Epoch 916 | train: Loss 0.005773 Accuracy 0.7883 | validation: Loss 0.023459 Accuracy -0.4738\n",
      "Epoch 917 | train: Loss 0.006881 Accuracy 0.7477 | validation: Loss 0.023749 Accuracy -0.4920\n",
      "Epoch 918 | train: Loss 0.006156 Accuracy 0.7742 | validation: Loss 0.022279 Accuracy -0.3996\n",
      "Epoch 919 | train: Loss 0.005293 Accuracy 0.8059 | validation: Loss 0.030013 Accuracy -0.8855\n",
      "Epoch 920 | train: Loss 0.006898 Accuracy 0.7470 | validation: Loss 0.022510 Accuracy -0.4141\n",
      "Epoch 921 | train: Loss 0.006315 Accuracy 0.7684 | validation: Loss 0.024881 Accuracy -0.5631\n",
      "Epoch 922 | train: Loss 0.005134 Accuracy 0.8117 | validation: Loss 0.028312 Accuracy -0.7787\n",
      "Epoch 923 | train: Loss 0.005704 Accuracy 0.7908 | validation: Loss 0.025816 Accuracy -0.6218\n",
      "Epoch 924 | train: Loss 0.005798 Accuracy 0.7874 | validation: Loss 0.028521 Accuracy -0.7918\n",
      "Epoch 925 | train: Loss 0.005542 Accuracy 0.7968 | validation: Loss 0.035611 Accuracy -1.2372\n",
      "Epoch 926 | train: Loss 0.005777 Accuracy 0.7881 | validation: Loss 0.033348 Accuracy -1.0950\n",
      "Epoch 927 | train: Loss 0.005290 Accuracy 0.8060 | validation: Loss 0.035736 Accuracy -1.2451\n",
      "Epoch 928 | train: Loss 0.005264 Accuracy 0.8069 | validation: Loss 0.047828 Accuracy -2.0047\n",
      "Epoch 929 | train: Loss 0.005502 Accuracy 0.7982 | validation: Loss 0.038511 Accuracy -1.4194\n",
      "Epoch 930 | train: Loss 0.005368 Accuracy 0.8031 | validation: Loss 0.043886 Accuracy -1.7571\n",
      "Epoch 931 | train: Loss 0.005010 Accuracy 0.8162 | validation: Loss 0.048921 Accuracy -2.0734\n",
      "Epoch 932 | train: Loss 0.005312 Accuracy 0.8052 | validation: Loss 0.037595 Accuracy -1.3618\n",
      "Epoch 933 | train: Loss 0.005440 Accuracy 0.8005 | validation: Loss 0.041548 Accuracy -1.6102\n",
      "Epoch 934 | train: Loss 0.005214 Accuracy 0.8088 | validation: Loss 0.048875 Accuracy -2.0705\n",
      "Epoch 935 | train: Loss 0.005400 Accuracy 0.8020 | validation: Loss 0.039576 Accuracy -1.4863\n",
      "Epoch 936 | train: Loss 0.005316 Accuracy 0.8050 | validation: Loss 0.044967 Accuracy -1.8250\n",
      "Epoch 937 | train: Loss 0.005095 Accuracy 0.8132 | validation: Loss 0.045482 Accuracy -1.8573\n",
      "Epoch 938 | train: Loss 0.005077 Accuracy 0.8138 | validation: Loss 0.042915 Accuracy -1.6960\n",
      "Epoch 939 | train: Loss 0.005146 Accuracy 0.8113 | validation: Loss 0.048065 Accuracy -2.0196\n",
      "Epoch 940 | train: Loss 0.005136 Accuracy 0.8116 | validation: Loss 0.042215 Accuracy -1.6521\n",
      "Epoch 941 | train: Loss 0.005188 Accuracy 0.8098 | validation: Loss 0.045646 Accuracy -1.8676\n",
      "Epoch 942 | train: Loss 0.005160 Accuracy 0.8108 | validation: Loss 0.041772 Accuracy -1.6242\n",
      "Epoch 943 | train: Loss 0.005140 Accuracy 0.8115 | validation: Loss 0.045250 Accuracy -1.8427\n",
      "Epoch 944 | train: Loss 0.005092 Accuracy 0.8133 | validation: Loss 0.040426 Accuracy -1.5397\n",
      "Epoch 945 | train: Loss 0.005140 Accuracy 0.8115 | validation: Loss 0.050596 Accuracy -2.1786\n",
      "Epoch 946 | train: Loss 0.005140 Accuracy 0.8115 | validation: Loss 0.039280 Accuracy -1.4677\n",
      "Epoch 947 | train: Loss 0.005310 Accuracy 0.8053 | validation: Loss 0.054027 Accuracy -2.3941\n",
      "Epoch 948 | train: Loss 0.005618 Accuracy 0.7940 | validation: Loss 0.033337 Accuracy -1.0943\n",
      "Epoch 949 | train: Loss 0.005988 Accuracy 0.7804 | validation: Loss 0.052376 Accuracy -2.2904\n",
      "Epoch 950 | train: Loss 0.005905 Accuracy 0.7834 | validation: Loss 0.033832 Accuracy -1.1254\n",
      "Epoch 951 | train: Loss 0.005505 Accuracy 0.7981 | validation: Loss 0.040454 Accuracy -1.5414\n",
      "Epoch 952 | train: Loss 0.005027 Accuracy 0.8156 | validation: Loss 0.049347 Accuracy -2.1001\n",
      "Epoch 953 | train: Loss 0.005040 Accuracy 0.8152 | validation: Loss 0.040142 Accuracy -1.5218\n",
      "Epoch 954 | train: Loss 0.005801 Accuracy 0.7873 | validation: Loss 0.077072 Accuracy -3.8419\n",
      "Epoch 955 | train: Loss 0.007208 Accuracy 0.7357 | validation: Loss 0.030248 Accuracy -0.9003\n",
      "Epoch 956 | train: Loss 0.011333 Accuracy 0.5844 | validation: Loss 0.094770 Accuracy -4.9537\n",
      "Epoch 957 | train: Loss 0.014705 Accuracy 0.4607 | validation: Loss 0.033088 Accuracy -1.0787\n",
      "Epoch 958 | train: Loss 0.010565 Accuracy 0.6125 | validation: Loss 0.030365 Accuracy -0.9076\n",
      "Epoch 959 | train: Loss 0.008235 Accuracy 0.6980 | validation: Loss 0.042666 Accuracy -1.6804\n",
      "Epoch 960 | train: Loss 0.010024 Accuracy 0.6324 | validation: Loss 0.030172 Accuracy -0.8955\n",
      "Epoch 961 | train: Loss 0.006568 Accuracy 0.7591 | validation: Loss 0.028937 Accuracy -0.8179\n",
      "Epoch 962 | train: Loss 0.009028 Accuracy 0.6689 | validation: Loss 0.028651 Accuracy -0.7999\n",
      "Epoch 963 | train: Loss 0.006061 Accuracy 0.7777 | validation: Loss 0.055025 Accuracy -2.4568\n",
      "Epoch 964 | train: Loss 0.007956 Accuracy 0.7082 | validation: Loss 0.039153 Accuracy -1.4597\n",
      "Epoch 965 | train: Loss 0.005384 Accuracy 0.8025 | validation: Loss 0.033163 Accuracy -1.0834\n",
      "Epoch 966 | train: Loss 0.008355 Accuracy 0.6936 | validation: Loss 0.054523 Accuracy -2.4253\n",
      "Epoch 967 | train: Loss 0.006672 Accuracy 0.7553 | validation: Loss 0.048731 Accuracy -2.0614\n",
      "Epoch 968 | train: Loss 0.006673 Accuracy 0.7553 | validation: Loss 0.030350 Accuracy -0.9067\n",
      "Epoch 969 | train: Loss 0.007278 Accuracy 0.7331 | validation: Loss 0.029938 Accuracy -0.8808\n",
      "Epoch 970 | train: Loss 0.006249 Accuracy 0.7708 | validation: Loss 0.036114 Accuracy -1.2688\n",
      "Epoch 971 | train: Loss 0.006860 Accuracy 0.7484 | validation: Loss 0.031262 Accuracy -0.9640\n",
      "Epoch 972 | train: Loss 0.006054 Accuracy 0.7780 | validation: Loss 0.027058 Accuracy -0.6999\n",
      "Epoch 973 | train: Loss 0.006821 Accuracy 0.7499 | validation: Loss 0.029871 Accuracy -0.8766\n",
      "Epoch 974 | train: Loss 0.005681 Accuracy 0.7917 | validation: Loss 0.041004 Accuracy -1.5760\n",
      "Epoch 975 | train: Loss 0.006289 Accuracy 0.7694 | validation: Loss 0.033894 Accuracy -1.1293\n",
      "Epoch 976 | train: Loss 0.005594 Accuracy 0.7948 | validation: Loss 0.033814 Accuracy -1.1243\n",
      "Epoch 977 | train: Loss 0.005973 Accuracy 0.7810 | validation: Loss 0.046680 Accuracy -1.9326\n",
      "Epoch 978 | train: Loss 0.006327 Accuracy 0.7680 | validation: Loss 0.034071 Accuracy -1.1405\n",
      "Epoch 979 | train: Loss 0.005564 Accuracy 0.7959 | validation: Loss 0.029626 Accuracy -0.8612\n",
      "Epoch 980 | train: Loss 0.006006 Accuracy 0.7797 | validation: Loss 0.039446 Accuracy -1.4781\n",
      "Epoch 981 | train: Loss 0.006082 Accuracy 0.7770 | validation: Loss 0.029452 Accuracy -0.8503\n",
      "Epoch 982 | train: Loss 0.005385 Accuracy 0.8025 | validation: Loss 0.026860 Accuracy -0.6874\n",
      "Epoch 983 | train: Loss 0.005953 Accuracy 0.7817 | validation: Loss 0.040905 Accuracy -1.5698\n",
      "Epoch 984 | train: Loss 0.005888 Accuracy 0.7841 | validation: Loss 0.035181 Accuracy -1.2101\n",
      "Epoch 985 | train: Loss 0.005246 Accuracy 0.8076 | validation: Loss 0.036159 Accuracy -1.2716\n",
      "Epoch 986 | train: Loss 0.005659 Accuracy 0.7925 | validation: Loss 0.062388 Accuracy -2.9194\n",
      "Epoch 987 | train: Loss 0.006279 Accuracy 0.7697 | validation: Loss 0.036701 Accuracy -1.3056\n",
      "Epoch 988 | train: Loss 0.006437 Accuracy 0.7639 | validation: Loss 0.055568 Accuracy -2.4910\n",
      "Epoch 989 | train: Loss 0.005683 Accuracy 0.7916 | validation: Loss 0.037816 Accuracy -1.3757\n",
      "Epoch 990 | train: Loss 0.005203 Accuracy 0.8092 | validation: Loss 0.036871 Accuracy -1.3163\n",
      "Epoch 991 | train: Loss 0.005145 Accuracy 0.8113 | validation: Loss 0.043700 Accuracy -1.7454\n",
      "Epoch 992 | train: Loss 0.005296 Accuracy 0.8058 | validation: Loss 0.035603 Accuracy -1.2367\n",
      "Epoch 993 | train: Loss 0.005372 Accuracy 0.8030 | validation: Loss 0.048675 Accuracy -2.0579\n",
      "Epoch 994 | train: Loss 0.005175 Accuracy 0.8102 | validation: Loss 0.046266 Accuracy -1.9066\n",
      "Epoch 995 | train: Loss 0.005137 Accuracy 0.8116 | validation: Loss 0.048414 Accuracy -2.0415\n",
      "Epoch 996 | train: Loss 0.005146 Accuracy 0.8113 | validation: Loss 0.051675 Accuracy -2.2464\n",
      "Epoch 997 | train: Loss 0.005172 Accuracy 0.8103 | validation: Loss 0.041136 Accuracy -1.5843\n",
      "Epoch 998 | train: Loss 0.005272 Accuracy 0.8067 | validation: Loss 0.047408 Accuracy -1.9783\n",
      "Epoch 999 | train: Loss 0.005308 Accuracy 0.8053 | validation: Loss 0.036934 Accuracy -1.3203\n",
      "Epoch 1000 | train: Loss 0.005403 Accuracy 0.8019 | validation: Loss 0.046204 Accuracy -1.9026\n",
      "Epoch 1001 | train: Loss 0.005299 Accuracy 0.8057 | validation: Loss 0.043102 Accuracy -1.7078\n",
      "Epoch 1002 | train: Loss 0.005241 Accuracy 0.8078 | validation: Loss 0.052108 Accuracy -2.2736\n",
      "Epoch 1003 | train: Loss 0.005166 Accuracy 0.8105 | validation: Loss 0.056445 Accuracy -2.5461\n",
      "Epoch 1004 | train: Loss 0.005189 Accuracy 0.8097 | validation: Loss 0.051427 Accuracy -2.2308\n",
      "Epoch 1005 | train: Loss 0.005191 Accuracy 0.8096 | validation: Loss 0.054369 Accuracy -2.4156\n",
      "Epoch 1006 | train: Loss 0.005178 Accuracy 0.8101 | validation: Loss 0.045025 Accuracy -1.8286\n",
      "Epoch 1007 | train: Loss 0.005268 Accuracy 0.8068 | validation: Loss 0.055018 Accuracy -2.4564\n",
      "Epoch 1008 | train: Loss 0.005272 Accuracy 0.8066 | validation: Loss 0.044348 Accuracy -1.7861\n",
      "Epoch 1009 | train: Loss 0.005405 Accuracy 0.8018 | validation: Loss 0.066209 Accuracy -3.1594\n",
      "Epoch 1010 | train: Loss 0.005593 Accuracy 0.7949 | validation: Loss 0.042004 Accuracy -1.6388\n",
      "Epoch 1011 | train: Loss 0.006269 Accuracy 0.7701 | validation: Loss 0.085732 Accuracy -4.3860\n",
      "Epoch 1012 | train: Loss 0.007414 Accuracy 0.7281 | validation: Loss 0.032688 Accuracy -1.0536\n",
      "Epoch 1013 | train: Loss 0.009499 Accuracy 0.6516 | validation: Loss 0.091747 Accuracy -4.7638\n",
      "Epoch 1014 | train: Loss 0.009838 Accuracy 0.6392 | validation: Loss 0.030842 Accuracy -0.9376\n",
      "Epoch 1015 | train: Loss 0.008287 Accuracy 0.6961 | validation: Loss 0.041107 Accuracy -1.5824\n",
      "Epoch 1016 | train: Loss 0.005597 Accuracy 0.7948 | validation: Loss 0.065612 Accuracy -3.1220\n",
      "Epoch 1017 | train: Loss 0.007350 Accuracy 0.7305 | validation: Loss 0.031351 Accuracy -0.9696\n",
      "Epoch 1018 | train: Loss 0.007964 Accuracy 0.7079 | validation: Loss 0.049816 Accuracy -2.1296\n",
      "Epoch 1019 | train: Loss 0.005379 Accuracy 0.8027 | validation: Loss 0.068780 Accuracy -3.3210\n",
      "Epoch 1020 | train: Loss 0.006495 Accuracy 0.7618 | validation: Loss 0.037739 Accuracy -1.3709\n",
      "Epoch 1021 | train: Loss 0.007660 Accuracy 0.7191 | validation: Loss 0.057264 Accuracy -2.5975\n",
      "Epoch 1022 | train: Loss 0.005707 Accuracy 0.7907 | validation: Loss 0.057195 Accuracy -2.5932\n",
      "Epoch 1023 | train: Loss 0.005975 Accuracy 0.7809 | validation: Loss 0.036243 Accuracy -1.2769\n",
      "Epoch 1024 | train: Loss 0.006756 Accuracy 0.7522 | validation: Loss 0.045013 Accuracy -1.8278\n",
      "Epoch 1025 | train: Loss 0.005721 Accuracy 0.7902 | validation: Loss 0.047974 Accuracy -2.0139\n",
      "Epoch 1026 | train: Loss 0.006092 Accuracy 0.7766 | validation: Loss 0.034005 Accuracy -1.1363\n",
      "Epoch 1027 | train: Loss 0.006149 Accuracy 0.7745 | validation: Loss 0.038147 Accuracy -1.3965\n",
      "Epoch 1028 | train: Loss 0.005632 Accuracy 0.7935 | validation: Loss 0.051839 Accuracy -2.2567\n",
      "Epoch 1029 | train: Loss 0.006062 Accuracy 0.7777 | validation: Loss 0.039494 Accuracy -1.4811\n",
      "Epoch 1030 | train: Loss 0.005864 Accuracy 0.7849 | validation: Loss 0.044543 Accuracy -1.7983\n",
      "Epoch 1031 | train: Loss 0.005530 Accuracy 0.7972 | validation: Loss 0.052943 Accuracy -2.3260\n",
      "Epoch 1032 | train: Loss 0.005839 Accuracy 0.7859 | validation: Loss 0.038548 Accuracy -1.4217\n",
      "Epoch 1033 | train: Loss 0.005914 Accuracy 0.7831 | validation: Loss 0.043785 Accuracy -1.7507\n",
      "Epoch 1034 | train: Loss 0.005490 Accuracy 0.7987 | validation: Loss 0.047159 Accuracy -1.9627\n",
      "Epoch 1035 | train: Loss 0.005612 Accuracy 0.7942 | validation: Loss 0.036210 Accuracy -1.2748\n",
      "Epoch 1036 | train: Loss 0.005812 Accuracy 0.7868 | validation: Loss 0.047402 Accuracy -1.9779\n",
      "Epoch 1037 | train: Loss 0.005490 Accuracy 0.7987 | validation: Loss 0.044985 Accuracy -1.8261\n",
      "Epoch 1038 | train: Loss 0.005331 Accuracy 0.8045 | validation: Loss 0.042814 Accuracy -1.6897\n",
      "Epoch 1039 | train: Loss 0.005548 Accuracy 0.7965 | validation: Loss 0.062343 Accuracy -2.9165\n",
      "Epoch 1040 | train: Loss 0.005704 Accuracy 0.7908 | validation: Loss 0.043831 Accuracy -1.7536\n",
      "Epoch 1041 | train: Loss 0.005935 Accuracy 0.7824 | validation: Loss 0.064084 Accuracy -3.0259\n",
      "Epoch 1042 | train: Loss 0.005773 Accuracy 0.7883 | validation: Loss 0.041539 Accuracy -1.6096\n",
      "Epoch 1043 | train: Loss 0.005693 Accuracy 0.7912 | validation: Loss 0.053575 Accuracy -2.3658\n",
      "Epoch 1044 | train: Loss 0.005351 Accuracy 0.8037 | validation: Loss 0.046201 Accuracy -1.9025\n",
      "Epoch 1045 | train: Loss 0.005219 Accuracy 0.8086 | validation: Loss 0.048784 Accuracy -2.0648\n",
      "Epoch 1046 | train: Loss 0.005172 Accuracy 0.8103 | validation: Loss 0.060196 Accuracy -2.7817\n",
      "Epoch 1047 | train: Loss 0.005307 Accuracy 0.8054 | validation: Loss 0.044341 Accuracy -1.7856\n",
      "Epoch 1048 | train: Loss 0.005941 Accuracy 0.7821 | validation: Loss 0.082742 Accuracy -4.1981\n",
      "Epoch 1049 | train: Loss 0.007164 Accuracy 0.7373 | validation: Loss 0.032113 Accuracy -1.0174\n",
      "Epoch 1050 | train: Loss 0.010507 Accuracy 0.6147 | validation: Loss 0.110171 Accuracy -5.9212\n",
      "Epoch 1051 | train: Loss 0.014167 Accuracy 0.4804 | validation: Loss 0.036305 Accuracy -1.2808\n",
      "Epoch 1052 | train: Loss 0.015243 Accuracy 0.4410 | validation: Loss 0.037714 Accuracy -1.3693\n",
      "Epoch 1053 | train: Loss 0.006366 Accuracy 0.7665 | validation: Loss 0.065187 Accuracy -3.0952\n",
      "Epoch 1054 | train: Loss 0.010921 Accuracy 0.5995 | validation: Loss 0.029810 Accuracy -0.8728\n",
      "Epoch 1055 | train: Loss 0.009544 Accuracy 0.6500 | validation: Loss 0.028964 Accuracy -0.8196\n",
      "Epoch 1056 | train: Loss 0.008605 Accuracy 0.6844 | validation: Loss 0.058773 Accuracy -2.6923\n",
      "Epoch 1057 | train: Loss 0.008556 Accuracy 0.6862 | validation: Loss 0.049033 Accuracy -2.0804\n",
      "Epoch 1058 | train: Loss 0.006290 Accuracy 0.7693 | validation: Loss 0.031539 Accuracy -0.9814\n",
      "Epoch 1059 | train: Loss 0.008971 Accuracy 0.6710 | validation: Loss 0.041201 Accuracy -1.5883\n",
      "Epoch 1060 | train: Loss 0.005914 Accuracy 0.7831 | validation: Loss 0.072408 Accuracy -3.5489\n",
      "Epoch 1061 | train: Loss 0.008370 Accuracy 0.6931 | validation: Loss 0.044087 Accuracy -1.7697\n",
      "Epoch 1062 | train: Loss 0.005934 Accuracy 0.7824 | validation: Loss 0.034757 Accuracy -1.1835\n",
      "Epoch 1063 | train: Loss 0.008202 Accuracy 0.6992 | validation: Loss 0.049677 Accuracy -2.1209\n",
      "Epoch 1064 | train: Loss 0.006408 Accuracy 0.7650 | validation: Loss 0.049854 Accuracy -2.1320\n",
      "Epoch 1065 | train: Loss 0.006944 Accuracy 0.7453 | validation: Loss 0.031675 Accuracy -0.9899\n",
      "Epoch 1066 | train: Loss 0.006661 Accuracy 0.7557 | validation: Loss 0.030535 Accuracy -0.9183\n",
      "Epoch 1067 | train: Loss 0.006818 Accuracy 0.7499 | validation: Loss 0.042590 Accuracy -1.6757\n",
      "Epoch 1068 | train: Loss 0.006469 Accuracy 0.7628 | validation: Loss 0.041569 Accuracy -1.6115\n",
      "Epoch 1069 | train: Loss 0.006154 Accuracy 0.7743 | validation: Loss 0.031149 Accuracy -0.9569\n",
      "Epoch 1070 | train: Loss 0.006927 Accuracy 0.7460 | validation: Loss 0.037137 Accuracy -1.3330\n",
      "Epoch 1071 | train: Loss 0.005767 Accuracy 0.7885 | validation: Loss 0.048223 Accuracy -2.0295\n",
      "Epoch 1072 | train: Loss 0.006568 Accuracy 0.7591 | validation: Loss 0.034226 Accuracy -1.1502\n",
      "Epoch 1073 | train: Loss 0.006023 Accuracy 0.7791 | validation: Loss 0.033291 Accuracy -1.0915\n",
      "Epoch 1074 | train: Loss 0.006036 Accuracy 0.7786 | validation: Loss 0.045490 Accuracy -1.8578\n",
      "Epoch 1075 | train: Loss 0.006143 Accuracy 0.7747 | validation: Loss 0.036136 Accuracy -1.2702\n",
      "Epoch 1076 | train: Loss 0.005561 Accuracy 0.7961 | validation: Loss 0.032585 Accuracy -1.0471\n",
      "Epoch 1077 | train: Loss 0.005970 Accuracy 0.7811 | validation: Loss 0.048349 Accuracy -2.0374\n",
      "Epoch 1078 | train: Loss 0.005922 Accuracy 0.7828 | validation: Loss 0.039387 Accuracy -1.4744\n",
      "Epoch 1079 | train: Loss 0.005497 Accuracy 0.7984 | validation: Loss 0.039815 Accuracy -1.5013\n",
      "Epoch 1080 | train: Loss 0.005609 Accuracy 0.7943 | validation: Loss 0.056528 Accuracy -2.5512\n",
      "Epoch 1081 | train: Loss 0.005848 Accuracy 0.7855 | validation: Loss 0.040748 Accuracy -1.5599\n",
      "Epoch 1082 | train: Loss 0.005729 Accuracy 0.7899 | validation: Loss 0.049864 Accuracy -2.1326\n",
      "Epoch 1083 | train: Loss 0.005309 Accuracy 0.8053 | validation: Loss 0.052536 Accuracy -2.3004\n",
      "Epoch 1084 | train: Loss 0.005296 Accuracy 0.8058 | validation: Loss 0.041147 Accuracy -1.5850\n",
      "Epoch 1085 | train: Loss 0.005609 Accuracy 0.7943 | validation: Loss 0.062348 Accuracy -2.9169\n",
      "Epoch 1086 | train: Loss 0.005785 Accuracy 0.7879 | validation: Loss 0.039030 Accuracy -1.4520\n",
      "Epoch 1087 | train: Loss 0.005847 Accuracy 0.7856 | validation: Loss 0.057959 Accuracy -2.6411\n",
      "Epoch 1088 | train: Loss 0.005405 Accuracy 0.8018 | validation: Loss 0.050705 Accuracy -2.1855\n",
      "Epoch 1089 | train: Loss 0.005220 Accuracy 0.8086 | validation: Loss 0.048440 Accuracy -2.0431\n",
      "Epoch 1090 | train: Loss 0.005333 Accuracy 0.8044 | validation: Loss 0.066896 Accuracy -3.2026\n",
      "Epoch 1091 | train: Loss 0.005685 Accuracy 0.7915 | validation: Loss 0.040056 Accuracy -1.5164\n",
      "Epoch 1092 | train: Loss 0.006518 Accuracy 0.7610 | validation: Loss 0.078282 Accuracy -3.9179\n",
      "Epoch 1093 | train: Loss 0.007371 Accuracy 0.7297 | validation: Loss 0.032016 Accuracy -1.0113\n",
      "Epoch 1094 | train: Loss 0.008623 Accuracy 0.6838 | validation: Loss 0.072935 Accuracy -3.5820\n",
      "Epoch 1095 | train: Loss 0.008332 Accuracy 0.6944 | validation: Loss 0.032728 Accuracy -1.0561\n",
      "Epoch 1096 | train: Loss 0.006818 Accuracy 0.7500 | validation: Loss 0.036901 Accuracy -1.3183\n",
      "Epoch 1097 | train: Loss 0.005972 Accuracy 0.7810 | validation: Loss 0.061751 Accuracy -2.8794\n",
      "Epoch 1098 | train: Loss 0.007138 Accuracy 0.7382 | validation: Loss 0.033994 Accuracy -1.1356\n",
      "Epoch 1099 | train: Loss 0.006867 Accuracy 0.7482 | validation: Loss 0.049268 Accuracy -2.0952\n",
      "Epoch 1100 | train: Loss 0.005378 Accuracy 0.8028 | validation: Loss 0.080000 Accuracy -4.0259\n",
      "Epoch 1101 | train: Loss 0.006314 Accuracy 0.7685 | validation: Loss 0.047777 Accuracy -2.0015\n",
      "Epoch 1102 | train: Loss 0.007495 Accuracy 0.7251 | validation: Loss 0.088279 Accuracy -4.5459\n",
      "Epoch 1103 | train: Loss 0.006157 Accuracy 0.7742 | validation: Loss 0.070038 Accuracy -3.4000\n",
      "Epoch 1104 | train: Loss 0.005568 Accuracy 0.7958 | validation: Loss 0.046443 Accuracy -1.9177\n",
      "Epoch 1105 | train: Loss 0.006397 Accuracy 0.7654 | validation: Loss 0.059026 Accuracy -2.7082\n",
      "Epoch 1106 | train: Loss 0.006164 Accuracy 0.7740 | validation: Loss 0.047620 Accuracy -1.9917\n",
      "Epoch 1107 | train: Loss 0.006059 Accuracy 0.7778 | validation: Loss 0.035427 Accuracy -1.2256\n",
      "Epoch 1108 | train: Loss 0.006547 Accuracy 0.7599 | validation: Loss 0.043242 Accuracy -1.7166\n",
      "Epoch 1109 | train: Loss 0.005887 Accuracy 0.7841 | validation: Loss 0.053408 Accuracy -2.3553\n",
      "Epoch 1110 | train: Loss 0.005936 Accuracy 0.7823 | validation: Loss 0.044435 Accuracy -1.7916\n",
      "Epoch 1111 | train: Loss 0.005830 Accuracy 0.7862 | validation: Loss 0.058103 Accuracy -2.6502\n",
      "Epoch 1112 | train: Loss 0.005520 Accuracy 0.7976 | validation: Loss 0.079921 Accuracy -4.0208\n",
      "Epoch 1113 | train: Loss 0.006057 Accuracy 0.7779 | validation: Loss 0.053948 Accuracy -2.3892\n",
      "Epoch 1114 | train: Loss 0.006359 Accuracy 0.7668 | validation: Loss 0.065468 Accuracy -3.1129\n",
      "Epoch 1115 | train: Loss 0.005640 Accuracy 0.7932 | validation: Loss 0.057839 Accuracy -2.6336\n",
      "Epoch 1116 | train: Loss 0.005645 Accuracy 0.7930 | validation: Loss 0.039027 Accuracy -1.4518\n",
      "Epoch 1117 | train: Loss 0.006086 Accuracy 0.7768 | validation: Loss 0.047952 Accuracy -2.0125\n",
      "Epoch 1118 | train: Loss 0.005698 Accuracy 0.7911 | validation: Loss 0.046776 Accuracy -1.9386\n",
      "Epoch 1119 | train: Loss 0.005628 Accuracy 0.7936 | validation: Loss 0.039853 Accuracy -1.5037\n",
      "Epoch 1120 | train: Loss 0.005892 Accuracy 0.7839 | validation: Loss 0.058224 Accuracy -2.6578\n",
      "Epoch 1121 | train: Loss 0.005641 Accuracy 0.7931 | validation: Loss 0.056169 Accuracy -2.5287\n",
      "Epoch 1122 | train: Loss 0.005586 Accuracy 0.7951 | validation: Loss 0.058179 Accuracy -2.6550\n",
      "Epoch 1123 | train: Loss 0.005601 Accuracy 0.7946 | validation: Loss 0.063829 Accuracy -3.0099\n",
      "Epoch 1124 | train: Loss 0.005603 Accuracy 0.7945 | validation: Loss 0.045863 Accuracy -1.8812\n",
      "Epoch 1125 | train: Loss 0.005735 Accuracy 0.7897 | validation: Loss 0.056652 Accuracy -2.5591\n",
      "Epoch 1126 | train: Loss 0.005626 Accuracy 0.7937 | validation: Loss 0.043580 Accuracy -1.7378\n",
      "Epoch 1127 | train: Loss 0.005533 Accuracy 0.7971 | validation: Loss 0.049394 Accuracy -2.1031\n",
      "Epoch 1128 | train: Loss 0.005383 Accuracy 0.8026 | validation: Loss 0.057417 Accuracy -2.6071\n",
      "Epoch 1129 | train: Loss 0.005380 Accuracy 0.8027 | validation: Loss 0.051198 Accuracy -2.2164\n",
      "Epoch 1130 | train: Loss 0.005669 Accuracy 0.7921 | validation: Loss 0.077645 Accuracy -3.8779\n",
      "Epoch 1131 | train: Loss 0.006038 Accuracy 0.7786 | validation: Loss 0.043428 Accuracy -1.7283\n",
      "Epoch 1132 | train: Loss 0.007011 Accuracy 0.7429 | validation: Loss 0.091560 Accuracy -4.7521\n",
      "Epoch 1133 | train: Loss 0.008249 Accuracy 0.6975 | validation: Loss 0.032320 Accuracy -1.0304\n",
      "Epoch 1134 | train: Loss 0.010078 Accuracy 0.6304 | validation: Loss 0.081247 Accuracy -4.1042\n",
      "Epoch 1135 | train: Loss 0.008350 Accuracy 0.6938 | validation: Loss 0.038112 Accuracy -1.3943\n",
      "Epoch 1136 | train: Loss 0.006387 Accuracy 0.7658 | validation: Loss 0.043510 Accuracy -1.7334\n",
      "Epoch 1137 | train: Loss 0.005869 Accuracy 0.7848 | validation: Loss 0.081486 Accuracy -4.1192\n",
      "Epoch 1138 | train: Loss 0.007007 Accuracy 0.7430 | validation: Loss 0.043158 Accuracy -1.7113\n",
      "Epoch 1139 | train: Loss 0.007611 Accuracy 0.7209 | validation: Loss 0.082571 Accuracy -4.1874\n",
      "Epoch 1140 | train: Loss 0.006184 Accuracy 0.7732 | validation: Loss 0.065005 Accuracy -3.0838\n",
      "Epoch 1141 | train: Loss 0.005473 Accuracy 0.7993 | validation: Loss 0.050997 Accuracy -2.2038\n",
      "Epoch 1142 | train: Loss 0.006306 Accuracy 0.7688 | validation: Loss 0.081172 Accuracy -4.0994\n",
      "Epoch 1143 | train: Loss 0.006648 Accuracy 0.7562 | validation: Loss 0.049029 Accuracy -2.0801\n",
      "Epoch 1144 | train: Loss 0.005877 Accuracy 0.7845 | validation: Loss 0.047747 Accuracy -1.9996\n",
      "Epoch 1145 | train: Loss 0.005894 Accuracy 0.7838 | validation: Loss 0.070625 Accuracy -3.4369\n",
      "Epoch 1146 | train: Loss 0.006328 Accuracy 0.7679 | validation: Loss 0.049032 Accuracy -2.0804\n",
      "Epoch 1147 | train: Loss 0.005933 Accuracy 0.7824 | validation: Loss 0.054887 Accuracy -2.4482\n",
      "Epoch 1148 | train: Loss 0.005613 Accuracy 0.7942 | validation: Loss 0.066542 Accuracy -3.1804\n",
      "Epoch 1149 | train: Loss 0.005902 Accuracy 0.7836 | validation: Loss 0.049131 Accuracy -2.0866\n",
      "Epoch 1150 | train: Loss 0.006037 Accuracy 0.7786 | validation: Loss 0.059828 Accuracy -2.7586\n",
      "Epoch 1151 | train: Loss 0.005673 Accuracy 0.7920 | validation: Loss 0.057135 Accuracy -2.5894\n",
      "Epoch 1152 | train: Loss 0.005623 Accuracy 0.7938 | validation: Loss 0.047064 Accuracy -1.9567\n",
      "Epoch 1153 | train: Loss 0.005857 Accuracy 0.7852 | validation: Loss 0.061787 Accuracy -2.8816\n",
      "Epoch 1154 | train: Loss 0.005774 Accuracy 0.7882 | validation: Loss 0.050137 Accuracy -2.1497\n",
      "Epoch 1155 | train: Loss 0.005610 Accuracy 0.7942 | validation: Loss 0.053180 Accuracy -2.3409\n",
      "Epoch 1156 | train: Loss 0.005523 Accuracy 0.7975 | validation: Loss 0.060405 Accuracy -2.7948\n",
      "Epoch 1157 | train: Loss 0.005589 Accuracy 0.7950 | validation: Loss 0.050063 Accuracy -2.1451\n",
      "Epoch 1158 | train: Loss 0.005776 Accuracy 0.7882 | validation: Loss 0.065894 Accuracy -3.1397\n",
      "Epoch 1159 | train: Loss 0.005805 Accuracy 0.7871 | validation: Loss 0.047346 Accuracy -1.9744\n",
      "Epoch 1160 | train: Loss 0.005832 Accuracy 0.7861 | validation: Loss 0.061661 Accuracy -2.8737\n",
      "Epoch 1161 | train: Loss 0.005513 Accuracy 0.7978 | validation: Loss 0.057625 Accuracy -2.6201\n",
      "Epoch 1162 | train: Loss 0.005397 Accuracy 0.8021 | validation: Loss 0.058662 Accuracy -2.6853\n",
      "Epoch 1163 | train: Loss 0.005393 Accuracy 0.8022 | validation: Loss 0.067671 Accuracy -3.2513\n",
      "Epoch 1164 | train: Loss 0.005509 Accuracy 0.7980 | validation: Loss 0.048795 Accuracy -2.0655\n",
      "Epoch 1165 | train: Loss 0.005985 Accuracy 0.7805 | validation: Loss 0.082130 Accuracy -4.1596\n",
      "Epoch 1166 | train: Loss 0.006416 Accuracy 0.7647 | validation: Loss 0.041076 Accuracy -1.5805\n",
      "Epoch 1167 | train: Loss 0.007763 Accuracy 0.7153 | validation: Loss 0.107302 Accuracy -5.7410\n",
      "Epoch 1168 | train: Loss 0.009941 Accuracy 0.6354 | validation: Loss 0.032653 Accuracy -1.0514\n",
      "Epoch 1169 | train: Loss 0.013306 Accuracy 0.5120 | validation: Loss 0.092181 Accuracy -4.7911\n",
      "Epoch 1170 | train: Loss 0.009882 Accuracy 0.6376 | validation: Loss 0.039361 Accuracy -1.4728\n",
      "Epoch 1171 | train: Loss 0.006494 Accuracy 0.7618 | validation: Loss 0.036026 Accuracy -1.2633\n",
      "Epoch 1172 | train: Loss 0.007130 Accuracy 0.7385 | validation: Loss 0.077279 Accuracy -3.8549\n",
      "Epoch 1173 | train: Loss 0.007948 Accuracy 0.7085 | validation: Loss 0.044002 Accuracy -1.7643\n",
      "Epoch 1174 | train: Loss 0.006163 Accuracy 0.7740 | validation: Loss 0.048029 Accuracy -2.0173\n",
      "Epoch 1175 | train: Loss 0.006204 Accuracy 0.7725 | validation: Loss 0.100126 Accuracy -5.2902\n",
      "Epoch 1176 | train: Loss 0.007364 Accuracy 0.7299 | validation: Loss 0.057998 Accuracy -2.6436\n",
      "Epoch 1177 | train: Loss 0.006947 Accuracy 0.7452 | validation: Loss 0.075648 Accuracy -3.7525\n",
      "Epoch 1178 | train: Loss 0.005770 Accuracy 0.7884 | validation: Loss 0.092514 Accuracy -4.8120\n",
      "Epoch 1179 | train: Loss 0.006746 Accuracy 0.7526 | validation: Loss 0.047421 Accuracy -1.9791\n",
      "Epoch 1180 | train: Loss 0.006984 Accuracy 0.7439 | validation: Loss 0.051915 Accuracy -2.2614\n",
      "Epoch 1181 | train: Loss 0.005846 Accuracy 0.7856 | validation: Loss 0.053450 Accuracy -2.3579\n",
      "Epoch 1182 | train: Loss 0.006413 Accuracy 0.7648 | validation: Loss 0.036548 Accuracy -1.2960\n",
      "Epoch 1183 | train: Loss 0.006383 Accuracy 0.7659 | validation: Loss 0.037192 Accuracy -1.3365\n",
      "Epoch 1184 | train: Loss 0.006175 Accuracy 0.7736 | validation: Loss 0.052141 Accuracy -2.2756\n",
      "Epoch 1185 | train: Loss 0.006178 Accuracy 0.7734 | validation: Loss 0.046756 Accuracy -1.9373\n",
      "Epoch 1186 | train: Loss 0.005643 Accuracy 0.7931 | validation: Loss 0.048368 Accuracy -2.0386\n",
      "Epoch 1187 | train: Loss 0.005944 Accuracy 0.7820 | validation: Loss 0.074781 Accuracy -3.6980\n",
      "Epoch 1188 | train: Loss 0.006255 Accuracy 0.7706 | validation: Loss 0.053062 Accuracy -2.3335\n",
      "Epoch 1189 | train: Loss 0.006091 Accuracy 0.7766 | validation: Loss 0.058752 Accuracy -2.6910\n",
      "Epoch 1190 | train: Loss 0.005576 Accuracy 0.7955 | validation: Loss 0.058513 Accuracy -2.6759\n",
      "Epoch 1191 | train: Loss 0.005741 Accuracy 0.7894 | validation: Loss 0.038465 Accuracy -1.4165\n",
      "Epoch 1192 | train: Loss 0.006034 Accuracy 0.7787 | validation: Loss 0.046656 Accuracy -1.9311\n",
      "Epoch 1193 | train: Loss 0.005589 Accuracy 0.7950 | validation: Loss 0.047802 Accuracy -2.0030\n",
      "Epoch 1194 | train: Loss 0.005537 Accuracy 0.7969 | validation: Loss 0.040860 Accuracy -1.5669\n",
      "Epoch 1195 | train: Loss 0.005847 Accuracy 0.7856 | validation: Loss 0.065881 Accuracy -3.1388\n",
      "Epoch 1196 | train: Loss 0.005663 Accuracy 0.7923 | validation: Loss 0.055914 Accuracy -2.5127\n",
      "Epoch 1197 | train: Loss 0.005625 Accuracy 0.7937 | validation: Loss 0.071900 Accuracy -3.5169\n",
      "Epoch 1198 | train: Loss 0.005438 Accuracy 0.8006 | validation: Loss 0.059450 Accuracy -2.7348\n",
      "Epoch 1199 | train: Loss 0.005301 Accuracy 0.8056 | validation: Loss 0.058521 Accuracy -2.6765\n",
      "Epoch 1200 | train: Loss 0.005202 Accuracy 0.8092 | validation: Loss 0.052154 Accuracy -2.2764\n",
      "Epoch 1201 | train: Loss 0.005209 Accuracy 0.8090 | validation: Loss 0.050986 Accuracy -2.2031\n",
      "Epoch 1202 | train: Loss 0.005198 Accuracy 0.8094 | validation: Loss 0.051823 Accuracy -2.2557\n",
      "Epoch 1203 | train: Loss 0.005152 Accuracy 0.8111 | validation: Loss 0.053419 Accuracy -2.3560\n",
      "Epoch 1204 | train: Loss 0.005094 Accuracy 0.8132 | validation: Loss 0.057834 Accuracy -2.6333\n",
      "Epoch 1205 | train: Loss 0.005036 Accuracy 0.8153 | validation: Loss 0.061246 Accuracy -2.8476\n",
      "Epoch 1206 | train: Loss 0.005028 Accuracy 0.8156 | validation: Loss 0.065258 Accuracy -3.0997\n",
      "Epoch 1207 | train: Loss 0.005036 Accuracy 0.8153 | validation: Loss 0.055167 Accuracy -2.4657\n",
      "Epoch 1208 | train: Loss 0.005096 Accuracy 0.8131 | validation: Loss 0.063778 Accuracy -3.0067\n",
      "Epoch 1209 | train: Loss 0.005326 Accuracy 0.8047 | validation: Loss 0.036036 Accuracy -1.2639\n",
      "Epoch 1210 | train: Loss 0.006464 Accuracy 0.7629 | validation: Loss 0.085314 Accuracy -4.3597\n",
      "Epoch 1211 | train: Loss 0.009201 Accuracy 0.6626 | validation: Loss 0.034389 Accuracy -1.1604\n",
      "Epoch 1212 | train: Loss 0.019299 Accuracy 0.2922 | validation: Loss 0.159719 Accuracy -9.0340\n",
      "Epoch 1213 | train: Loss 0.029500 Accuracy -0.0819 | validation: Loss 0.068741 Accuracy -3.3185\n",
      "Epoch 1214 | train: Loss 0.027108 Accuracy 0.0058 | validation: Loss 0.039750 Accuracy -1.4972\n",
      "Epoch 1215 | train: Loss 0.015630 Accuracy 0.4268 | validation: Loss 0.092829 Accuracy -4.8318\n",
      "Epoch 1216 | train: Loss 0.016480 Accuracy 0.3956 | validation: Loss 0.056427 Accuracy -2.5449\n",
      "Epoch 1217 | train: Loss 0.009428 Accuracy 0.6542 | validation: Loss 0.033920 Accuracy -1.1310\n",
      "Epoch 1218 | train: Loss 0.013037 Accuracy 0.5219 | validation: Loss 0.036891 Accuracy -1.3176\n",
      "Epoch 1219 | train: Loss 0.015455 Accuracy 0.4332 | validation: Loss 0.039985 Accuracy -1.5120\n",
      "Epoch 1220 | train: Loss 0.006525 Accuracy 0.7607 | validation: Loss 0.103309 Accuracy -5.4902\n",
      "Epoch 1221 | train: Loss 0.014271 Accuracy 0.4766 | validation: Loss 0.046069 Accuracy -1.8942\n",
      "Epoch 1222 | train: Loss 0.005770 Accuracy 0.7884 | validation: Loss 0.034114 Accuracy -1.1431\n",
      "Epoch 1223 | train: Loss 0.012872 Accuracy 0.5279 | validation: Loss 0.054844 Accuracy -2.4455\n",
      "Epoch 1224 | train: Loss 0.006250 Accuracy 0.7708 | validation: Loss 0.069867 Accuracy -3.3893\n",
      "Epoch 1225 | train: Loss 0.010713 Accuracy 0.6071 | validation: Loss 0.034369 Accuracy -1.1592\n",
      "Epoch 1226 | train: Loss 0.006630 Accuracy 0.7569 | validation: Loss 0.031239 Accuracy -0.9625\n",
      "Epoch 1227 | train: Loss 0.010307 Accuracy 0.6220 | validation: Loss 0.029470 Accuracy -0.8514\n",
      "Epoch 1228 | train: Loss 0.007919 Accuracy 0.7096 | validation: Loss 0.035353 Accuracy -1.2210\n",
      "Epoch 1229 | train: Loss 0.006807 Accuracy 0.7504 | validation: Loss 0.039919 Accuracy -1.5078\n",
      "Epoch 1230 | train: Loss 0.007651 Accuracy 0.7194 | validation: Loss 0.027469 Accuracy -0.7257\n",
      "Epoch 1231 | train: Loss 0.005547 Accuracy 0.7966 | validation: Loss 0.026426 Accuracy -0.6601\n",
      "Epoch 1232 | train: Loss 0.007896 Accuracy 0.7104 | validation: Loss 0.026107 Accuracy -0.6401\n",
      "Epoch 1233 | train: Loss 0.005585 Accuracy 0.7952 | validation: Loss 0.035081 Accuracy -1.2039\n",
      "Epoch 1234 | train: Loss 0.006761 Accuracy 0.7521 | validation: Loss 0.029573 Accuracy -0.8579\n",
      "Epoch 1235 | train: Loss 0.005429 Accuracy 0.8009 | validation: Loss 0.027656 Accuracy -0.7374\n",
      "Epoch 1236 | train: Loss 0.006391 Accuracy 0.7656 | validation: Loss 0.027869 Accuracy -0.7508\n",
      "Epoch 1237 | train: Loss 0.005774 Accuracy 0.7882 | validation: Loss 0.032894 Accuracy -1.0665\n",
      "Epoch 1238 | train: Loss 0.005916 Accuracy 0.7831 | validation: Loss 0.032111 Accuracy -1.0173\n",
      "Epoch 1239 | train: Loss 0.005634 Accuracy 0.7934 | validation: Loss 0.029398 Accuracy -0.8468\n",
      "Epoch 1240 | train: Loss 0.005696 Accuracy 0.7911 | validation: Loss 0.028835 Accuracy -0.8115\n",
      "Epoch 1241 | train: Loss 0.005248 Accuracy 0.8075 | validation: Loss 0.032268 Accuracy -1.0271\n",
      "Epoch 1242 | train: Loss 0.005397 Accuracy 0.8021 | validation: Loss 0.029518 Accuracy -0.8544\n",
      "Epoch 1243 | train: Loss 0.004766 Accuracy 0.8252 | validation: Loss 0.028381 Accuracy -0.7830\n",
      "Epoch 1244 | train: Loss 0.005474 Accuracy 0.7993 | validation: Loss 0.028144 Accuracy -0.7681\n",
      "Epoch 1245 | train: Loss 0.004406 Accuracy 0.8384 | validation: Loss 0.035605 Accuracy -1.2368\n",
      "Epoch 1246 | train: Loss 0.005095 Accuracy 0.8132 | validation: Loss 0.029929 Accuracy -0.8802\n",
      "Epoch 1247 | train: Loss 0.004460 Accuracy 0.8364 | validation: Loss 0.030866 Accuracy -0.9391\n",
      "Epoch 1248 | train: Loss 0.004673 Accuracy 0.8286 | validation: Loss 0.044087 Accuracy -1.7697\n",
      "Epoch 1249 | train: Loss 0.004674 Accuracy 0.8286 | validation: Loss 0.040659 Accuracy -1.5543\n",
      "Epoch 1250 | train: Loss 0.004268 Accuracy 0.8435 | validation: Loss 0.035063 Accuracy -1.2028\n",
      "Epoch 1251 | train: Loss 0.004809 Accuracy 0.8236 | validation: Loss 0.044938 Accuracy -1.8231\n",
      "Epoch 1252 | train: Loss 0.004298 Accuracy 0.8424 | validation: Loss 0.047030 Accuracy -1.9545\n",
      "Epoch 1253 | train: Loss 0.004259 Accuracy 0.8438 | validation: Loss 0.037118 Accuracy -1.3319\n",
      "Epoch 1254 | train: Loss 0.004573 Accuracy 0.8323 | validation: Loss 0.045947 Accuracy -1.8865\n",
      "Epoch 1255 | train: Loss 0.004279 Accuracy 0.8431 | validation: Loss 0.043598 Accuracy -1.7390\n",
      "Epoch 1256 | train: Loss 0.004205 Accuracy 0.8458 | validation: Loss 0.036918 Accuracy -1.3193\n",
      "Epoch 1257 | train: Loss 0.004435 Accuracy 0.8374 | validation: Loss 0.046381 Accuracy -1.9138\n",
      "Epoch 1258 | train: Loss 0.004282 Accuracy 0.8430 | validation: Loss 0.041767 Accuracy -1.6239\n",
      "Epoch 1259 | train: Loss 0.004107 Accuracy 0.8494 | validation: Loss 0.041245 Accuracy -1.5911\n",
      "Epoch 1260 | train: Loss 0.004155 Accuracy 0.8476 | validation: Loss 0.048263 Accuracy -2.0320\n",
      "Epoch 1261 | train: Loss 0.004357 Accuracy 0.8402 | validation: Loss 0.040103 Accuracy -1.5194\n",
      "Epoch 1262 | train: Loss 0.004351 Accuracy 0.8404 | validation: Loss 0.044385 Accuracy -1.7884\n",
      "Epoch 1263 | train: Loss 0.004109 Accuracy 0.8493 | validation: Loss 0.045220 Accuracy -1.8409\n",
      "Epoch 1264 | train: Loss 0.004059 Accuracy 0.8511 | validation: Loss 0.040794 Accuracy -1.5628\n",
      "Epoch 1265 | train: Loss 0.004214 Accuracy 0.8455 | validation: Loss 0.053086 Accuracy -2.3350\n",
      "Epoch 1266 | train: Loss 0.004428 Accuracy 0.8376 | validation: Loss 0.039550 Accuracy -1.4846\n",
      "Epoch 1267 | train: Loss 0.004896 Accuracy 0.8204 | validation: Loss 0.061273 Accuracy -2.8493\n",
      "Epoch 1268 | train: Loss 0.004858 Accuracy 0.8219 | validation: Loss 0.040522 Accuracy -1.5457\n",
      "Epoch 1269 | train: Loss 0.005231 Accuracy 0.8082 | validation: Loss 0.065312 Accuracy -3.1031\n",
      "Epoch 1270 | train: Loss 0.004901 Accuracy 0.8203 | validation: Loss 0.042119 Accuracy -1.6461\n",
      "Epoch 1271 | train: Loss 0.004980 Accuracy 0.8174 | validation: Loss 0.061526 Accuracy -2.8652\n",
      "Epoch 1272 | train: Loss 0.004516 Accuracy 0.8344 | validation: Loss 0.044831 Accuracy -1.8164\n",
      "Epoch 1273 | train: Loss 0.004425 Accuracy 0.8377 | validation: Loss 0.059101 Accuracy -2.7129\n",
      "Epoch 1274 | train: Loss 0.004177 Accuracy 0.8468 | validation: Loss 0.047179 Accuracy -1.9639\n",
      "Epoch 1275 | train: Loss 0.004145 Accuracy 0.8480 | validation: Loss 0.056817 Accuracy -2.5694\n",
      "Epoch 1276 | train: Loss 0.003999 Accuracy 0.8533 | validation: Loss 0.049527 Accuracy -2.1114\n",
      "Epoch 1277 | train: Loss 0.003943 Accuracy 0.8554 | validation: Loss 0.053400 Accuracy -2.3547\n",
      "Epoch 1278 | train: Loss 0.003836 Accuracy 0.8593 | validation: Loss 0.052807 Accuracy -2.3175\n",
      "Epoch 1279 | train: Loss 0.003790 Accuracy 0.8610 | validation: Loss 0.051845 Accuracy -2.2570\n",
      "Epoch 1280 | train: Loss 0.003794 Accuracy 0.8609 | validation: Loss 0.057203 Accuracy -2.5937\n",
      "Epoch 1281 | train: Loss 0.003816 Accuracy 0.8600 | validation: Loss 0.049710 Accuracy -2.1229\n",
      "Epoch 1282 | train: Loss 0.004027 Accuracy 0.8523 | validation: Loss 0.065579 Accuracy -3.1199\n",
      "Epoch 1283 | train: Loss 0.004590 Accuracy 0.8317 | validation: Loss 0.042230 Accuracy -1.6530\n",
      "Epoch 1284 | train: Loss 0.007895 Accuracy 0.7105 | validation: Loss 0.110687 Accuracy -5.9537\n",
      "Epoch 1285 | train: Loss 0.016777 Accuracy 0.3847 | validation: Loss 0.083342 Accuracy -4.2358\n",
      "Epoch 1286 | train: Loss 0.041935 Accuracy -0.5379 | validation: Loss 0.076320 Accuracy -3.7946\n",
      "Epoch 1287 | train: Loss 0.009486 Accuracy 0.6521 | validation: Loss 0.090682 Accuracy -4.6969\n",
      "Epoch 1288 | train: Loss 0.016022 Accuracy 0.4124 | validation: Loss 0.037181 Accuracy -1.3358\n",
      "Epoch 1289 | train: Loss 0.011549 Accuracy 0.5765 | validation: Loss 0.053670 Accuracy -2.3717\n",
      "Epoch 1290 | train: Loss 0.018885 Accuracy 0.3074 | validation: Loss 0.037543 Accuracy -1.3586\n",
      "Epoch 1291 | train: Loss 0.010773 Accuracy 0.6049 | validation: Loss 0.043454 Accuracy -1.7299\n",
      "Epoch 1292 | train: Loss 0.009755 Accuracy 0.6422 | validation: Loss 0.051332 Accuracy -2.2249\n",
      "Epoch 1293 | train: Loss 0.011101 Accuracy 0.5929 | validation: Loss 0.035646 Accuracy -1.2394\n",
      "Epoch 1294 | train: Loss 0.008806 Accuracy 0.6771 | validation: Loss 0.034183 Accuracy -1.1475\n",
      "Epoch 1295 | train: Loss 0.010178 Accuracy 0.6267 | validation: Loss 0.027957 Accuracy -0.7563\n",
      "Epoch 1296 | train: Loss 0.008253 Accuracy 0.6974 | validation: Loss 0.032054 Accuracy -1.0137\n",
      "Epoch 1297 | train: Loss 0.008223 Accuracy 0.6984 | validation: Loss 0.036359 Accuracy -1.2842\n",
      "Epoch 1298 | train: Loss 0.008357 Accuracy 0.6935 | validation: Loss 0.030383 Accuracy -0.9088\n",
      "Epoch 1299 | train: Loss 0.005866 Accuracy 0.7849 | validation: Loss 0.034698 Accuracy -1.1798\n",
      "Epoch 1300 | train: Loss 0.006615 Accuracy 0.7574 | validation: Loss 0.037109 Accuracy -1.3313\n",
      "Epoch 1301 | train: Loss 0.006769 Accuracy 0.7517 | validation: Loss 0.034199 Accuracy -1.1485\n",
      "Epoch 1302 | train: Loss 0.005603 Accuracy 0.7945 | validation: Loss 0.035259 Accuracy -1.2151\n",
      "Epoch 1303 | train: Loss 0.006742 Accuracy 0.7527 | validation: Loss 0.033878 Accuracy -1.1283\n",
      "Epoch 1304 | train: Loss 0.005433 Accuracy 0.8008 | validation: Loss 0.039529 Accuracy -1.4833\n",
      "Epoch 1305 | train: Loss 0.006015 Accuracy 0.7794 | validation: Loss 0.039952 Accuracy -1.5099\n",
      "Epoch 1306 | train: Loss 0.006132 Accuracy 0.7751 | validation: Loss 0.032445 Accuracy -1.0383\n",
      "Epoch 1307 | train: Loss 0.005110 Accuracy 0.8126 | validation: Loss 0.031732 Accuracy -0.9935\n",
      "Epoch 1308 | train: Loss 0.005710 Accuracy 0.7906 | validation: Loss 0.033987 Accuracy -1.1351\n",
      "Epoch 1309 | train: Loss 0.004843 Accuracy 0.8224 | validation: Loss 0.039992 Accuracy -1.5124\n",
      "Epoch 1310 | train: Loss 0.005632 Accuracy 0.7935 | validation: Loss 0.032756 Accuracy -1.0578\n",
      "Epoch 1311 | train: Loss 0.004723 Accuracy 0.8268 | validation: Loss 0.029669 Accuracy -0.8639\n",
      "Epoch 1312 | train: Loss 0.005102 Accuracy 0.8129 | validation: Loss 0.032693 Accuracy -1.0539\n",
      "Epoch 1313 | train: Loss 0.004652 Accuracy 0.8294 | validation: Loss 0.037163 Accuracy -1.3347\n",
      "Epoch 1314 | train: Loss 0.005315 Accuracy 0.8051 | validation: Loss 0.031289 Accuracy -0.9657\n",
      "Epoch 1315 | train: Loss 0.004741 Accuracy 0.8261 | validation: Loss 0.032249 Accuracy -1.0259\n",
      "Epoch 1316 | train: Loss 0.004869 Accuracy 0.8214 | validation: Loss 0.032973 Accuracy -1.0715\n",
      "Epoch 1317 | train: Loss 0.004481 Accuracy 0.8357 | validation: Loss 0.034637 Accuracy -1.1760\n",
      "Epoch 1318 | train: Loss 0.004777 Accuracy 0.8248 | validation: Loss 0.040082 Accuracy -1.5180\n",
      "Epoch 1319 | train: Loss 0.004409 Accuracy 0.8383 | validation: Loss 0.047212 Accuracy -1.9660\n",
      "Epoch 1320 | train: Loss 0.004437 Accuracy 0.8373 | validation: Loss 0.043482 Accuracy -1.7316\n",
      "Epoch 1321 | train: Loss 0.004442 Accuracy 0.8371 | validation: Loss 0.048874 Accuracy -2.0704\n",
      "Epoch 1322 | train: Loss 0.004216 Accuracy 0.8454 | validation: Loss 0.059357 Accuracy -2.7290\n",
      "Epoch 1323 | train: Loss 0.004288 Accuracy 0.8427 | validation: Loss 0.049584 Accuracy -2.1150\n",
      "Epoch 1324 | train: Loss 0.003978 Accuracy 0.8541 | validation: Loss 0.046164 Accuracy -1.9002\n",
      "Epoch 1325 | train: Loss 0.004017 Accuracy 0.8527 | validation: Loss 0.053916 Accuracy -2.3872\n",
      "Epoch 1326 | train: Loss 0.003896 Accuracy 0.8571 | validation: Loss 0.049700 Accuracy -2.1223\n",
      "Epoch 1327 | train: Loss 0.003716 Accuracy 0.8637 | validation: Loss 0.047099 Accuracy -1.9589\n",
      "Epoch 1328 | train: Loss 0.003825 Accuracy 0.8597 | validation: Loss 0.058009 Accuracy -2.6443\n",
      "Epoch 1329 | train: Loss 0.003742 Accuracy 0.8628 | validation: Loss 0.054767 Accuracy -2.4406\n",
      "Epoch 1330 | train: Loss 0.003635 Accuracy 0.8667 | validation: Loss 0.053971 Accuracy -2.3906\n",
      "Epoch 1331 | train: Loss 0.003670 Accuracy 0.8654 | validation: Loss 0.061897 Accuracy -2.8885\n",
      "Epoch 1332 | train: Loss 0.003714 Accuracy 0.8638 | validation: Loss 0.053482 Accuracy -2.3599\n",
      "Epoch 1333 | train: Loss 0.003734 Accuracy 0.8631 | validation: Loss 0.057072 Accuracy -2.5854\n",
      "Epoch 1334 | train: Loss 0.003666 Accuracy 0.8655 | validation: Loss 0.058524 Accuracy -2.6766\n",
      "Epoch 1335 | train: Loss 0.003716 Accuracy 0.8637 | validation: Loss 0.051604 Accuracy -2.2419\n",
      "Epoch 1336 | train: Loss 0.003799 Accuracy 0.8607 | validation: Loss 0.053836 Accuracy -2.3821\n",
      "Epoch 1337 | train: Loss 0.003720 Accuracy 0.8636 | validation: Loss 0.050429 Accuracy -2.1681\n",
      "Epoch 1338 | train: Loss 0.003687 Accuracy 0.8648 | validation: Loss 0.045567 Accuracy -1.8627\n",
      "Epoch 1339 | train: Loss 0.003751 Accuracy 0.8624 | validation: Loss 0.047487 Accuracy -1.9833\n",
      "Epoch 1340 | train: Loss 0.003752 Accuracy 0.8624 | validation: Loss 0.043032 Accuracy -1.7034\n",
      "Epoch 1341 | train: Loss 0.003753 Accuracy 0.8624 | validation: Loss 0.043976 Accuracy -1.7627\n",
      "Epoch 1342 | train: Loss 0.003682 Accuracy 0.8650 | validation: Loss 0.042919 Accuracy -1.6963\n",
      "Epoch 1343 | train: Loss 0.003693 Accuracy 0.8646 | validation: Loss 0.041440 Accuracy -1.6034\n",
      "Epoch 1344 | train: Loss 0.003704 Accuracy 0.8642 | validation: Loss 0.043153 Accuracy -1.7110\n",
      "Epoch 1345 | train: Loss 0.003686 Accuracy 0.8648 | validation: Loss 0.042130 Accuracy -1.6467\n",
      "Epoch 1346 | train: Loss 0.003684 Accuracy 0.8649 | validation: Loss 0.043928 Accuracy -1.7597\n",
      "Epoch 1347 | train: Loss 0.003623 Accuracy 0.8671 | validation: Loss 0.045078 Accuracy -1.8320\n",
      "Epoch 1348 | train: Loss 0.003603 Accuracy 0.8679 | validation: Loss 0.044962 Accuracy -1.8247\n",
      "Epoch 1349 | train: Loss 0.003626 Accuracy 0.8670 | validation: Loss 0.046941 Accuracy -1.9490\n",
      "Epoch 1350 | train: Loss 0.003603 Accuracy 0.8679 | validation: Loss 0.044296 Accuracy -1.7828\n",
      "Epoch 1351 | train: Loss 0.003643 Accuracy 0.8664 | validation: Loss 0.046776 Accuracy -1.9386\n",
      "Epoch 1352 | train: Loss 0.003599 Accuracy 0.8680 | validation: Loss 0.044844 Accuracy -1.8172\n",
      "Epoch 1353 | train: Loss 0.003622 Accuracy 0.8672 | validation: Loss 0.047342 Accuracy -1.9741\n",
      "Epoch 1354 | train: Loss 0.003572 Accuracy 0.8690 | validation: Loss 0.046608 Accuracy -1.9281\n",
      "Epoch 1355 | train: Loss 0.003573 Accuracy 0.8689 | validation: Loss 0.047881 Accuracy -2.0080\n",
      "Epoch 1356 | train: Loss 0.003554 Accuracy 0.8697 | validation: Loss 0.049088 Accuracy -2.0839\n",
      "Epoch 1357 | train: Loss 0.003537 Accuracy 0.8703 | validation: Loss 0.048860 Accuracy -2.0695\n",
      "Epoch 1358 | train: Loss 0.003556 Accuracy 0.8696 | validation: Loss 0.051746 Accuracy -2.2508\n",
      "Epoch 1359 | train: Loss 0.003545 Accuracy 0.8700 | validation: Loss 0.049731 Accuracy -2.1243\n",
      "Epoch 1360 | train: Loss 0.003605 Accuracy 0.8678 | validation: Loss 0.055435 Accuracy -2.4826\n",
      "Epoch 1361 | train: Loss 0.003619 Accuracy 0.8673 | validation: Loss 0.049612 Accuracy -2.1168\n",
      "Epoch 1362 | train: Loss 0.003824 Accuracy 0.8597 | validation: Loss 0.060066 Accuracy -2.7735\n",
      "Epoch 1363 | train: Loss 0.004017 Accuracy 0.8527 | validation: Loss 0.047532 Accuracy -1.9861\n",
      "Epoch 1364 | train: Loss 0.004875 Accuracy 0.8212 | validation: Loss 0.071787 Accuracy -3.5099\n",
      "Epoch 1365 | train: Loss 0.005908 Accuracy 0.7833 | validation: Loss 0.048059 Accuracy -2.0192\n",
      "Epoch 1366 | train: Loss 0.009294 Accuracy 0.6592 | validation: Loss 0.091045 Accuracy -4.7197\n",
      "Epoch 1367 | train: Loss 0.009671 Accuracy 0.6453 | validation: Loss 0.047542 Accuracy -1.9867\n",
      "Epoch 1368 | train: Loss 0.010615 Accuracy 0.6107 | validation: Loss 0.063450 Accuracy -2.9861\n",
      "Epoch 1369 | train: Loss 0.004456 Accuracy 0.8366 | validation: Loss 0.073319 Accuracy -3.6061\n",
      "Epoch 1370 | train: Loss 0.005706 Accuracy 0.7907 | validation: Loss 0.044034 Accuracy -1.7664\n",
      "Epoch 1371 | train: Loss 0.008390 Accuracy 0.6923 | validation: Loss 0.052841 Accuracy -2.3196\n",
      "Epoch 1372 | train: Loss 0.003907 Accuracy 0.8567 | validation: Loss 0.076973 Accuracy -3.8357\n",
      "Epoch 1373 | train: Loss 0.006927 Accuracy 0.7459 | validation: Loss 0.041041 Accuracy -1.5783\n",
      "Epoch 1374 | train: Loss 0.006114 Accuracy 0.7758 | validation: Loss 0.041196 Accuracy -1.5880\n",
      "Epoch 1375 | train: Loss 0.005088 Accuracy 0.8134 | validation: Loss 0.068278 Accuracy -3.2894\n",
      "Epoch 1376 | train: Loss 0.006413 Accuracy 0.7648 | validation: Loss 0.046607 Accuracy -1.9280\n",
      "Epoch 1377 | train: Loss 0.004004 Accuracy 0.8532 | validation: Loss 0.040239 Accuracy -1.5279\n",
      "Epoch 1378 | train: Loss 0.006092 Accuracy 0.7766 | validation: Loss 0.046523 Accuracy -1.9227\n",
      "Epoch 1379 | train: Loss 0.004026 Accuracy 0.8523 | validation: Loss 0.060212 Accuracy -2.7827\n",
      "Epoch 1380 | train: Loss 0.005495 Accuracy 0.7985 | validation: Loss 0.042928 Accuracy -1.6969\n",
      "Epoch 1381 | train: Loss 0.004358 Accuracy 0.8402 | validation: Loss 0.041441 Accuracy -1.6034\n",
      "Epoch 1382 | train: Loss 0.005170 Accuracy 0.8104 | validation: Loss 0.050471 Accuracy -2.1707\n",
      "Epoch 1383 | train: Loss 0.004270 Accuracy 0.8434 | validation: Loss 0.051214 Accuracy -2.2174\n",
      "Epoch 1384 | train: Loss 0.004431 Accuracy 0.8375 | validation: Loss 0.041561 Accuracy -1.6110\n",
      "Epoch 1385 | train: Loss 0.004767 Accuracy 0.8252 | validation: Loss 0.042404 Accuracy -1.6639\n",
      "Epoch 1386 | train: Loss 0.004120 Accuracy 0.8489 | validation: Loss 0.050895 Accuracy -2.1974\n",
      "Epoch 1387 | train: Loss 0.004758 Accuracy 0.8255 | validation: Loss 0.042630 Accuracy -1.6782\n",
      "Epoch 1388 | train: Loss 0.003892 Accuracy 0.8573 | validation: Loss 0.041519 Accuracy -1.6084\n",
      "Epoch 1389 | train: Loss 0.004551 Accuracy 0.8331 | validation: Loss 0.045123 Accuracy -1.8348\n",
      "Epoch 1390 | train: Loss 0.004064 Accuracy 0.8510 | validation: Loss 0.043566 Accuracy -1.7370\n",
      "Epoch 1391 | train: Loss 0.003873 Accuracy 0.8580 | validation: Loss 0.041269 Accuracy -1.5926\n",
      "Epoch 1392 | train: Loss 0.004395 Accuracy 0.8388 | validation: Loss 0.043096 Accuracy -1.7074\n",
      "Epoch 1393 | train: Loss 0.003773 Accuracy 0.8616 | validation: Loss 0.045895 Accuracy -1.8833\n",
      "Epoch 1394 | train: Loss 0.003997 Accuracy 0.8534 | validation: Loss 0.042590 Accuracy -1.6756\n",
      "Epoch 1395 | train: Loss 0.004216 Accuracy 0.8454 | validation: Loss 0.045139 Accuracy -1.8358\n",
      "Epoch 1396 | train: Loss 0.003686 Accuracy 0.8648 | validation: Loss 0.048940 Accuracy -2.0745\n",
      "Epoch 1397 | train: Loss 0.003891 Accuracy 0.8573 | validation: Loss 0.044247 Accuracy -1.7797\n",
      "Epoch 1398 | train: Loss 0.004167 Accuracy 0.8472 | validation: Loss 0.049237 Accuracy -2.0932\n",
      "Epoch 1399 | train: Loss 0.003647 Accuracy 0.8662 | validation: Loss 0.049827 Accuracy -2.1303\n",
      "Epoch 1400 | train: Loss 0.003595 Accuracy 0.8682 | validation: Loss 0.046128 Accuracy -1.8979\n",
      "Epoch 1401 | train: Loss 0.003976 Accuracy 0.8542 | validation: Loss 0.054780 Accuracy -2.4414\n",
      "Epoch 1402 | train: Loss 0.003878 Accuracy 0.8578 | validation: Loss 0.047348 Accuracy -1.9745\n",
      "Epoch 1403 | train: Loss 0.003717 Accuracy 0.8637 | validation: Loss 0.050316 Accuracy -2.1610\n",
      "Epoch 1404 | train: Loss 0.003464 Accuracy 0.8730 | validation: Loss 0.051498 Accuracy -2.2352\n",
      "Epoch 1405 | train: Loss 0.003492 Accuracy 0.8719 | validation: Loss 0.046742 Accuracy -1.9365\n",
      "Epoch 1406 | train: Loss 0.003755 Accuracy 0.8623 | validation: Loss 0.055507 Accuracy -2.4871\n",
      "Epoch 1407 | train: Loss 0.003864 Accuracy 0.8583 | validation: Loss 0.045894 Accuracy -1.8832\n",
      "Epoch 1408 | train: Loss 0.004296 Accuracy 0.8424 | validation: Loss 0.059560 Accuracy -2.7417\n",
      "Epoch 1409 | train: Loss 0.004417 Accuracy 0.8380 | validation: Loss 0.045715 Accuracy -1.8719\n",
      "Epoch 1410 | train: Loss 0.005260 Accuracy 0.8071 | validation: Loss 0.067183 Accuracy -3.2206\n",
      "Epoch 1411 | train: Loss 0.005669 Accuracy 0.7921 | validation: Loss 0.046360 Accuracy -1.9125\n",
      "Epoch 1412 | train: Loss 0.007579 Accuracy 0.7220 | validation: Loss 0.076115 Accuracy -3.7818\n",
      "Epoch 1413 | train: Loss 0.007594 Accuracy 0.7215 | validation: Loss 0.047157 Accuracy -1.9626\n",
      "Epoch 1414 | train: Loss 0.008694 Accuracy 0.6812 | validation: Loss 0.063638 Accuracy -2.9979\n",
      "Epoch 1415 | train: Loss 0.005216 Accuracy 0.8087 | validation: Loss 0.048076 Accuracy -2.0202\n",
      "Epoch 1416 | train: Loss 0.003607 Accuracy 0.8677 | validation: Loss 0.043741 Accuracy -1.7479\n",
      "Epoch 1417 | train: Loss 0.004746 Accuracy 0.8259 | validation: Loss 0.061991 Accuracy -2.8944\n",
      "Epoch 1418 | train: Loss 0.005016 Accuracy 0.8161 | validation: Loss 0.045023 Accuracy -1.8285\n",
      "Epoch 1419 | train: Loss 0.004072 Accuracy 0.8507 | validation: Loss 0.046304 Accuracy -1.9089\n",
      "Epoch 1420 | train: Loss 0.003882 Accuracy 0.8576 | validation: Loss 0.060773 Accuracy -2.8179\n",
      "Epoch 1421 | train: Loss 0.004549 Accuracy 0.8332 | validation: Loss 0.045977 Accuracy -1.8884\n",
      "Epoch 1422 | train: Loss 0.004198 Accuracy 0.8461 | validation: Loss 0.048855 Accuracy -2.0692\n",
      "Epoch 1423 | train: Loss 0.003722 Accuracy 0.8635 | validation: Loss 0.059844 Accuracy -2.7596\n",
      "Epoch 1424 | train: Loss 0.004335 Accuracy 0.8410 | validation: Loss 0.046366 Accuracy -1.9128\n",
      "Epoch 1425 | train: Loss 0.004288 Accuracy 0.8428 | validation: Loss 0.050191 Accuracy -2.1532\n",
      "Epoch 1426 | train: Loss 0.003636 Accuracy 0.8667 | validation: Loss 0.057817 Accuracy -2.6323\n",
      "Epoch 1427 | train: Loss 0.004180 Accuracy 0.8467 | validation: Loss 0.045797 Accuracy -1.8771\n",
      "Epoch 1428 | train: Loss 0.004376 Accuracy 0.8395 | validation: Loss 0.049962 Accuracy -2.1387\n",
      "Epoch 1429 | train: Loss 0.003587 Accuracy 0.8684 | validation: Loss 0.053650 Accuracy -2.3704\n",
      "Epoch 1430 | train: Loss 0.003956 Accuracy 0.8549 | validation: Loss 0.044735 Accuracy -1.8104\n",
      "Epoch 1431 | train: Loss 0.004469 Accuracy 0.8361 | validation: Loss 0.049666 Accuracy -2.1202\n",
      "Epoch 1432 | train: Loss 0.003653 Accuracy 0.8660 | validation: Loss 0.049099 Accuracy -2.0845\n",
      "Epoch 1433 | train: Loss 0.003620 Accuracy 0.8672 | validation: Loss 0.044394 Accuracy -1.7889\n",
      "Epoch 1434 | train: Loss 0.004245 Accuracy 0.8443 | validation: Loss 0.051045 Accuracy -2.2068\n",
      "Epoch 1435 | train: Loss 0.003895 Accuracy 0.8572 | validation: Loss 0.045968 Accuracy -1.8878\n",
      "Epoch 1436 | train: Loss 0.003553 Accuracy 0.8697 | validation: Loss 0.045837 Accuracy -1.8796\n",
      "Epoch 1437 | train: Loss 0.003607 Accuracy 0.8677 | validation: Loss 0.050725 Accuracy -2.1867\n",
      "Epoch 1438 | train: Loss 0.003861 Accuracy 0.8584 | validation: Loss 0.045660 Accuracy -1.8685\n",
      "Epoch 1439 | train: Loss 0.004142 Accuracy 0.8481 | validation: Loss 0.050692 Accuracy -2.1846\n",
      "Epoch 1440 | train: Loss 0.003797 Accuracy 0.8607 | validation: Loss 0.046824 Accuracy -1.9416\n",
      "Epoch 1441 | train: Loss 0.003628 Accuracy 0.8669 | validation: Loss 0.047802 Accuracy -2.0030\n",
      "Epoch 1442 | train: Loss 0.003498 Accuracy 0.8717 | validation: Loss 0.048955 Accuracy -2.0755\n",
      "Epoch 1443 | train: Loss 0.003545 Accuracy 0.8700 | validation: Loss 0.046173 Accuracy -1.9007\n",
      "Epoch 1444 | train: Loss 0.003793 Accuracy 0.8609 | validation: Loss 0.051546 Accuracy -2.2383\n",
      "Epoch 1445 | train: Loss 0.003927 Accuracy 0.8560 | validation: Loss 0.046433 Accuracy -1.9171\n",
      "Epoch 1446 | train: Loss 0.004528 Accuracy 0.8339 | validation: Loss 0.056602 Accuracy -2.5559\n",
      "Epoch 1447 | train: Loss 0.004982 Accuracy 0.8173 | validation: Loss 0.048932 Accuracy -2.0741\n",
      "Epoch 1448 | train: Loss 0.006860 Accuracy 0.7484 | validation: Loss 0.071942 Accuracy -3.5196\n",
      "Epoch 1449 | train: Loss 0.008300 Accuracy 0.6956 | validation: Loss 0.057175 Accuracy -2.5919\n",
      "Epoch 1450 | train: Loss 0.013258 Accuracy 0.5138 | validation: Loss 0.084097 Accuracy -4.2832\n",
      "Epoch 1451 | train: Loss 0.010056 Accuracy 0.6312 | validation: Loss 0.047455 Accuracy -1.9813\n",
      "Epoch 1452 | train: Loss 0.007375 Accuracy 0.7295 | validation: Loss 0.052007 Accuracy -2.2672\n",
      "Epoch 1453 | train: Loss 0.003711 Accuracy 0.8639 | validation: Loss 0.074541 Accuracy -3.6829\n",
      "Epoch 1454 | train: Loss 0.006503 Accuracy 0.7615 | validation: Loss 0.046154 Accuracy -1.8995\n",
      "Epoch 1455 | train: Loss 0.007874 Accuracy 0.7112 | validation: Loss 0.051350 Accuracy -2.2260\n",
      "Epoch 1456 | train: Loss 0.003770 Accuracy 0.8618 | validation: Loss 0.077330 Accuracy -3.8581\n",
      "Epoch 1457 | train: Loss 0.007146 Accuracy 0.7379 | validation: Loss 0.043396 Accuracy -1.7263\n",
      "Epoch 1458 | train: Loss 0.006719 Accuracy 0.7536 | validation: Loss 0.044817 Accuracy -1.8155\n",
      "Epoch 1459 | train: Loss 0.004546 Accuracy 0.8333 | validation: Loss 0.075619 Accuracy -3.7506\n",
      "Epoch 1460 | train: Loss 0.007489 Accuracy 0.7254 | validation: Loss 0.044757 Accuracy -1.8118\n",
      "Epoch 1461 | train: Loss 0.004590 Accuracy 0.8317 | validation: Loss 0.043458 Accuracy -1.7301\n",
      "Epoch 1462 | train: Loss 0.005842 Accuracy 0.7858 | validation: Loss 0.061614 Accuracy -2.8708\n",
      "Epoch 1463 | train: Loss 0.005149 Accuracy 0.8112 | validation: Loss 0.054086 Accuracy -2.3979\n",
      "Epoch 1464 | train: Loss 0.004196 Accuracy 0.8461 | validation: Loss 0.043227 Accuracy -1.7157\n",
      "Epoch 1465 | train: Loss 0.005717 Accuracy 0.7904 | validation: Loss 0.047958 Accuracy -2.0128\n",
      "Epoch 1466 | train: Loss 0.003919 Accuracy 0.8563 | validation: Loss 0.061183 Accuracy -2.8437\n",
      "Epoch 1467 | train: Loss 0.005342 Accuracy 0.8041 | validation: Loss 0.045000 Accuracy -1.8270\n",
      "Epoch 1468 | train: Loss 0.004433 Accuracy 0.8374 | validation: Loss 0.044781 Accuracy -1.8133\n",
      "Epoch 1469 | train: Loss 0.004511 Accuracy 0.8346 | validation: Loss 0.055250 Accuracy -2.4709\n",
      "Epoch 1470 | train: Loss 0.004778 Accuracy 0.8248 | validation: Loss 0.045901 Accuracy -1.8836\n",
      "Epoch 1471 | train: Loss 0.003728 Accuracy 0.8633 | validation: Loss 0.043488 Accuracy -1.7321\n",
      "Epoch 1472 | train: Loss 0.004637 Accuracy 0.8300 | validation: Loss 0.048083 Accuracy -2.0207\n",
      "Epoch 1473 | train: Loss 0.003979 Accuracy 0.8541 | validation: Loss 0.046304 Accuracy -1.9090\n",
      "Epoch 1474 | train: Loss 0.003781 Accuracy 0.8613 | validation: Loss 0.044050 Accuracy -1.7674\n",
      "Epoch 1475 | train: Loss 0.004439 Accuracy 0.8372 | validation: Loss 0.045493 Accuracy -1.8580\n",
      "Epoch 1476 | train: Loss 0.003697 Accuracy 0.8644 | validation: Loss 0.046740 Accuracy -1.9364\n",
      "Epoch 1477 | train: Loss 0.003878 Accuracy 0.8578 | validation: Loss 0.044640 Accuracy -1.8044\n",
      "Epoch 1478 | train: Loss 0.004270 Accuracy 0.8434 | validation: Loss 0.045998 Accuracy -1.8897\n",
      "Epoch 1479 | train: Loss 0.003612 Accuracy 0.8675 | validation: Loss 0.047535 Accuracy -1.9863\n",
      "Epoch 1480 | train: Loss 0.003751 Accuracy 0.8624 | validation: Loss 0.045925 Accuracy -1.8851\n",
      "Epoch 1481 | train: Loss 0.004196 Accuracy 0.8461 | validation: Loss 0.047981 Accuracy -2.0143\n",
      "Epoch 1482 | train: Loss 0.003683 Accuracy 0.8649 | validation: Loss 0.046693 Accuracy -1.9334\n",
      "Epoch 1483 | train: Loss 0.003474 Accuracy 0.8726 | validation: Loss 0.046008 Accuracy -1.8903\n",
      "Epoch 1484 | train: Loss 0.003786 Accuracy 0.8612 | validation: Loss 0.049508 Accuracy -2.1102\n",
      "Epoch 1485 | train: Loss 0.003858 Accuracy 0.8585 | validation: Loss 0.046366 Accuracy -1.9129\n",
      "Epoch 1486 | train: Loss 0.003855 Accuracy 0.8586 | validation: Loss 0.048206 Accuracy -2.0285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-19 23:54:25,984] Trial 0 failed with parameters: {'hidden_units': 15, 'depth': 1, 'learning_rate': 0.09954748354475935, 'weight_decay': 0.012988817632185535} because of the following error: The number of the values 1500 did not match the number of the objectives 1.\n",
      "[W 2024-05-19 23:54:25,993] Trial 0 failed with value [2.0083858966827393, 0.11816372722387314, 0.07339923083782196, 0.061511073261499405, 0.06497222930192947, 0.09094373881816864, 0.08993729948997498, 0.06118002533912659, 0.04181719571352005, 0.037317998707294464, 0.03344135731458664, 0.06185782700777054, 0.039735324680805206, 0.0307464599609375, 0.030562778934836388, 0.031170129776000977, 0.05972468852996826, 0.06176064908504486, 0.04324307292699814, 0.0787721499800682, 0.06770604103803635, 0.025680219754576683, 0.029057465493679047, 0.030216967687010765, 0.032131895422935486, 0.027336319908499718, 0.05486733838915825, 0.02685409225523472, 0.025757886469364166, 0.05913310497999191, 0.059724293649196625, 0.02377590350806713, 0.021185707300901413, 0.04673356935381889, 0.04021361842751503, 0.01860872097313404, 0.024330945685505867, 0.03392268344759941, 0.023115769028663635, 0.03121366538107395, 0.041086528450250626, 0.2315736711025238, 0.40665754675865173, 0.09616872668266296, 0.442456990480423, 0.05786954239010811, 0.052590154111385345, 0.062288571149110794, 0.049873750656843185, 0.0367639921605587, 0.049145087599754333, 0.07173548638820648, 0.05454619973897934, 0.02977081760764122, 0.029949285089969635, 0.05215424671769142, 0.048088811337947845, 0.039440758526325226, 0.03108164668083191, 0.023563941940665245, 0.03432071954011917, 0.021701794117689133, 0.03556215763092041, 0.02544425241649151, 0.02483479306101799, 0.026918979361653328, 0.04091402515769005, 0.03782381862401962, 0.034084126353263855, 0.04645076394081116, 0.04590512067079544, 0.03446829319000244, 0.037952322512865067, 0.04270847886800766, 0.031027447432279587, 0.03529742732644081, 0.03132043778896332, 0.039917487651109695, 0.04859231412410736, 0.04423481598496437, 0.05944084748625755, 0.029637623578310013, 0.09392010420560837, 0.08793266862630844, 0.06212823837995529, 0.04357016459107399, 0.038232751190662384, 0.04622500762343407, 0.026812952011823654, 0.049688924103975296, 0.028577880933880806, 0.041814371943473816, 0.051083121448755264, 0.0307371374219656, 0.02759614586830139, 0.026452194899320602, 0.028010595589876175, 0.04809975624084473, 0.0458376482129097, 0.026617180556058884, 0.021709926426410675, 0.021784471347928047, 0.030315212905406952, 0.03726835921406746, 0.028052350506186485, 0.028338978067040443, 0.02982906810939312, 0.02618037536740303, 0.030303774401545525, 0.026051731780171394, 0.02524278312921524, 0.025915898382663727, 0.02542083151638508, 0.02595655806362629, 0.04175158962607384, 0.03133363276720047, 0.03488413989543915, 0.05725042149424553, 0.028651731088757515, 0.09466437995433807, 0.04469132423400879, 0.03969593346118927, 0.08711224794387817, 0.1152869164943695, 0.05553818866610527, 0.0571436733007431, 0.11691784113645554, 0.10042131692171097, 0.051620688289403915, 0.10558456927537918, 0.1273350566625595, 0.07261323183774948, 0.10773374140262604, 0.1552160084247589, 0.09291312843561172, 0.07914542406797409, 0.12045058608055115, 0.0996955931186676, 0.06828369200229645, 0.10030948370695114, 0.10225225985050201, 0.0744171291589737, 0.10276288539171219, 0.10752575844526291, 0.07822294533252716, 0.09484080970287323, 0.10271711647510529, 0.07451100647449493, 0.1141362190246582, 0.07500331103801727, 0.10268431901931763, 0.08525880426168442, 0.08316349983215332, 0.10265984386205673, 0.0794612318277359, 0.10587717592716217, 0.11011043190956116, 0.10799327492713928, 0.1465928554534912, 0.05292852967977524, 0.3089883625507355, 0.04348223656415939, 0.045780666172504425, 0.24928954243659973, 0.2162306308746338, 0.06515590101480484, 0.04493344947695732, 0.07675626128911972, 0.17038072645664215, 0.14060519635677338, 0.07717723399400711, 0.06418076902627945, 0.09267795085906982, 0.1664668470621109, 0.21613405644893646, 0.1717582494020462, 0.12325949966907501, 0.15384729206562042, 0.24640583992004395, 0.26282814145088196, 0.1575295478105545, 0.11177418380975723, 0.1308499127626419, 0.1280377358198166, 0.07588651776313782, 0.1317819505929947, 0.08606256544589996, 0.10504241287708282, 0.12317858636379242, 0.08857692778110504, 0.11271613091230392, 0.1000838428735733, 0.08591406792402267, 0.08750826120376587, 0.07051455229520798, 0.06640232354402542, 0.07772308588027954, 0.07188858091831207, 0.06326430290937424, 0.07288315147161484, 0.07366513460874557, 0.06065196171402931, 0.06713265925645828, 0.07499706000089645, 0.06828047335147858, 0.0782582014799118, 0.08228414505720139, 0.06920294463634491, 0.09340090304613113, 0.07460762560367584, 0.08571361750364304, 0.09226668626070023, 0.0790027529001236, 0.10445640236139297, 0.07778884470462799, 0.10309483855962753, 0.07103664427995682, 0.10352656245231628, 0.06267897039651871, 0.0987759456038475, 0.06583812832832336, 0.07827174663543701, 0.08317021280527115, 0.06421481817960739, 0.09914038330316544, 0.05819646641612053, 0.09517551213502884, 0.0631272941827774, 0.07244089245796204, 0.07814589142799377, 0.06147458031773567, 0.08780765533447266, 0.05745271220803261, 0.09272534400224686, 0.05156857892870903, 0.08736929297447205, 0.06405527144670486, 0.06750975549221039, 0.08874737471342087, 0.05730123817920685, 0.090952567756176, 0.0676039531826973, 0.06466011703014374, 0.08339114487171173, 0.055304452776908875, 0.08116118609905243, 0.0651261955499649, 0.07090533524751663, 0.08247994631528854, 0.06400199979543686, 0.09545795619487762, 0.06372351199388504, 0.08388496190309525, 0.07376039773225784, 0.06483828276395798, 0.08942236006259918, 0.06304100900888443, 0.1027427613735199, 0.05948757007718086, 0.10278374701738358, 0.06752333790063858, 0.07049006223678589, 0.09654326736927032, 0.06408441811800003, 0.09085910767316818, 0.07965479791164398, 0.09089279174804688, 0.09530134499073029, 0.0834265649318695, 0.10239546746015549, 0.07558954507112503, 0.10446576774120331, 0.0707961693406105, 0.09380099177360535, 0.07546418905258179, 0.08855392038822174, 0.08705668896436691, 0.08679161965847015, 0.10251720994710922, 0.07536409050226212, 0.1314501315355301, 0.052764587104320526, 0.09689125418663025, 0.11351838707923889, 0.05203397199511528, 0.07169206440448761, 0.1050533577799797, 0.048741523176431656, 0.05827343091368675, 0.09776415675878525, 0.059094253927469254, 0.06095406785607338, 0.10795879364013672, 0.08995475620031357, 0.07392923533916473, 0.10788988322019577, 0.10503462702035904, 0.0732642263174057, 0.0858418419957161, 0.10181448608636856, 0.06890520453453064, 0.07540398091077805, 0.09994805604219437, 0.07159058749675751, 0.11208789050579071, 0.09809818863868713, 0.09458156675100327, 0.12334168702363968, 0.08940105140209198, 0.11724027246236801, 0.10438854247331619, 0.09699408710002899, 0.12600959837436676, 0.1016741469502449, 0.1293828934431076, 0.1181475818157196, 0.11731509119272232, 0.13453440368175507, 0.10751724243164062, 0.14051203429698944, 0.11199606209993362, 0.1315980702638626, 0.12630894780158997, 0.13386361300945282, 0.11693477630615234, 0.1385689675807953, 0.08492833375930786, 0.1140301376581192, 0.07742495089769363, 0.1771937757730484, 0.10217875987291336, 0.15386010706424713, 0.1331261545419693, 0.0964697003364563, 0.13023456931114197, 0.09211139380931854, 0.08022446185350418, 0.11037307977676392, 0.07556191086769104, 0.10092511028051376, 0.10550292581319809, 0.08609560877084732, 0.10662199556827545, 0.07867227494716644, 0.07405369728803635, 0.07014507800340652, 0.050048403441905975, 0.05987193435430527, 0.03989529237151146, 0.047682661563158035, 0.043985515832901, 0.041312869638204575, 0.05206891521811485, 0.04299434274435043, 0.06500976532697678, 0.045388299971818924, 0.07606513053178787, 0.051300834864377975, 0.07058867812156677, 0.0687837302684784, 0.05879606679081917, 0.08920108526945114, 0.05403833091259003, 0.11660625040531158, 0.05214887112379074, 0.09855916351079941, 0.09298533946275711, 0.060813020914793015, 0.12612247467041016, 0.07035432755947113, 0.10056417435407639, 0.11175641417503357, 0.07913047820329666, 0.12179311364889145, 0.09025771170854568, 0.08502147346735, 0.10647224634885788, 0.06352955102920532, 0.11007913947105408, 0.060594893991947174, 0.09383147209882736, 0.08230957388877869, 0.06240968778729439, 0.08523755520582199, 0.05332834646105766, 0.06354816257953644, 0.05710858106613159, 0.049647651612758636, 0.07299387454986572, 0.04924789443612099, 0.08124826848506927, 0.05834328010678291, 0.05789058655500412, 0.06682998687028885, 0.04199448600411415, 0.06155504658818245, 0.04262467846274376, 0.054891910403966904, 0.05701350048184395, 0.05586622655391693, 0.06859402358531952, 0.05502954125404358, 0.06933408975601196, 0.05240023881196976, 0.07061596214771271, 0.05367845669388771, 0.07966947555541992, 0.05238690227270126, 0.07945805788040161, 0.05488679185509682, 0.06801170855760574, 0.06513849645853043, 0.06462389975786209, 0.07052183151245117, 0.06674931943416595, 0.07933026552200317, 0.06593165546655655, 0.07878445833921432, 0.06075819581747055, 0.08599979430437088, 0.05556659400463104, 0.1297065019607544, 0.040622927248477936, 0.17283405363559723, 0.0564156100153923, 0.0555272176861763, 0.14712285995483398, 0.05541885644197464, 0.06119249761104584, 0.12316825240850449, 0.08429624140262604, 0.06378093361854553, 0.1001424714922905, 0.12125060707330704, 0.08159151673316956, 0.08250465244054794, 0.12648804485797882, 0.11052658408880234, 0.07873240113258362, 0.11351320147514343, 0.11248496919870377, 0.07700813561677933, 0.1019129678606987, 0.10533232986927032, 0.07273001968860626, 0.08721347898244858, 0.09342753142118454, 0.06990384310483932, 0.06923594325780869, 0.07855096459388733, 0.06118323653936386, 0.05460156872868538, 0.06625882536172867, 0.05874791741371155, 0.05582159012556076, 0.07141205668449402, 0.06519173830747604, 0.06258055567741394, 0.0739658996462822, 0.06553005427122116, 0.059817537665367126, 0.07142436504364014, 0.06046514958143234, 0.06701558828353882, 0.07555068284273148, 0.06980109214782715, 0.08491963893175125, 0.08636105060577393, 0.08087489008903503, 0.0939374566078186, 0.07942904531955719, 0.08966994285583496, 0.08324724435806274, 0.08206792175769806, 0.09198217839002609, 0.08284945040941238, 0.09703531861305237, 0.0846296027302742, 0.09698674827814102, 0.08793019503355026, 0.08723756670951843, 0.09323585778474808, 0.07740005850791931, 0.09276757389307022, 0.07761147618293762, 0.09339746832847595, 0.07598976045846939, 0.09867873042821884, 0.07077977806329727, 0.09895727038383484, 0.06972608715295792, 0.09754614531993866, 0.0682513564825058, 0.09808708727359772, 0.06906301528215408, 0.08744880557060242, 0.07103221118450165, 0.07628759741783142, 0.08290459215641022, 0.07761790603399277, 0.08480212092399597, 0.06702975183725357, 0.08626744151115417, 0.060487691313028336, 0.10824998468160629, 0.06158582866191864, 0.09613337367773056, 0.06962234526872635, 0.06210440397262573, 0.09345752745866776, 0.04765112325549126, 0.12381357699632645, 0.042131420224905014, 0.07449580729007721, 0.08904552459716797, 0.05487706512212753, 0.08048897981643677, 0.048061683773994446, 0.04776064679026604, 0.07489028573036194, 0.054272186011075974, 0.06714694201946259, 0.08630138635635376, 0.07011020928621292, 0.0756668746471405, 0.07401704788208008, 0.051767826080322266, 0.056884765625, 0.03347079083323479, 0.03858329728245735, 0.03025464341044426, 0.029790515080094337, 0.031078321859240532, 0.030305521562695503, 0.0381883941590786, 0.036069419234991074, 0.04556423798203468, 0.03526139259338379, 0.05130571499466896, 0.049923475831747055, 0.0476740337908268, 0.031149981543421745, 0.03264397010207176, 0.055109672248363495, 0.03509867191314697, 0.04808344319462776, 0.06751300394535065, 0.03567143902182579, 0.05053138732910156, 0.08022594451904297, 0.038529109209775925, 0.04937934875488281, 0.08829252421855927, 0.042915936559438705, 0.049954142421483994, 0.08922206610441208, 0.050092585384845734, 0.05601194500923157, 0.10348410904407501, 0.060365933924913406, 0.05419318377971649, 0.09068875759840012, 0.08032585680484772, 0.05793384835124016, 0.07205761969089508, 0.07944577932357788, 0.05330628529191017, 0.0629233792424202, 0.0739821344614029, 0.05001242458820343, 0.05503350496292114, 0.06027032807469368, 0.05437074601650238, 0.06460952013731003, 0.07374037802219391, 0.05997856706380844, 0.06651293486356735, 0.07107654958963394, 0.05286348611116409, 0.07324422150850296, 0.07108946144580841, 0.05844148248434067, 0.0747876688838005, 0.06198124215006828, 0.07473541796207428, 0.06019093841314316, 0.06308476626873016, 0.041092559695243835, 0.039815232157707214, 0.02809378132224083, 0.0238326508551836, 0.02074786275625229, 0.019639432430267334, 0.020770683884620667, 0.022687353193759918, 0.027092086151242256, 0.03548349067568779, 0.038017675280570984, 0.0430440790951252, 0.043592724949121475, 0.03917695954442024, 0.03516284376382828, 0.02763904258608818, 0.024517318233847618, 0.02243477664887905, 0.02187667414546013, 0.021306810900568962, 0.02239280194044113, 0.02217048965394497, 0.02338823676109314, 0.02347022108733654, 0.02402838133275509, 0.02370738796889782, 0.02372639998793602, 0.023317856714129448, 0.025538183748722076, 0.02343640848994255, 0.03063320554792881, 0.02398115210235119, 0.05102933570742607, 0.05725456029176712, 0.12992985546588898, 0.07448995858430862, 0.0346461683511734, 0.04182516038417816, 0.05084386467933655, 0.060522325336933136, 0.035397812724113464, 0.03180637210607529, 0.052147649228572845, 0.0290729608386755, 0.029959358274936676, 0.02661973237991333, 0.038435231894254684, 0.025775961577892303, 0.028070995584130287, 0.0270383358001709, 0.030290918424725533, 0.02621472254395485, 0.02699935995042324, 0.02740357629954815, 0.02545798383653164, 0.02888977900147438, 0.02541772834956646, 0.02387988567352295, 0.024796871468424797, 0.029507240280508995, 0.024085568264126778, 0.026521844789385796, 0.034928131848573685, 0.028758579865098, 0.024893514811992645, 0.03298993036150932, 0.02890048548579216, 0.027443457394838333, 0.03326317295432091, 0.03479250520467758, 0.029951680451631546, 0.032953668385744095, 0.033757347613573074, 0.030110454186797142, 0.033149603754282, 0.03303207829594612, 0.031298741698265076, 0.034715279936790466, 0.03233461454510689, 0.031493108719587326, 0.03404422849416733, 0.032923005521297455, 0.03577309846878052, 0.0355956107378006, 0.03672639653086662, 0.03607261925935745, 0.035356782376766205, 0.04109828174114227, 0.04377187043428421, 0.04722025990486145, 0.045343589037656784, 0.04596875607967377, 0.05362328141927719, 0.050143834203481674, 0.04103674367070198, 0.038879286497831345, 0.04550114646553993, 0.050790201872587204, 0.049407705664634705, 0.03328009694814682, 0.03873166814446449, 0.03673841804265976, 0.032629966735839844, 0.05450350046157837, 0.05262655019760132, 0.05269169434905052, 0.06800007075071335, 0.04623601213097572, 0.0407344289124012, 0.0476493164896965, 0.03859323263168335, 0.04786001145839691, 0.05535741150379181, 0.045625220984220505, 0.04588630050420761, 0.04172744229435921, 0.037910375744104385, 0.04177653416991234, 0.0428490936756134, 0.048084888607263565, 0.05812664330005646, 0.04982546344399452, 0.04677775129675865, 0.046320077031850815, 0.04942673072218895, 0.056300193071365356, 0.05010983720421791, 0.0469793863594532, 0.05140962451696396, 0.05574079602956772, 0.06647814810276031, 0.06118285283446312, 0.06320487707853317, 0.06948215514421463, 0.06235111504793167, 0.059411752969026566, 0.05822101607918739, 0.06362412124872208, 0.07371982932090759, 0.06886393576860428, 0.08635029196739197, 0.07165460288524628, 0.07901095598936081, 0.07157810032367706, 0.07358989864587784, 0.08406523615121841, 0.06702373176813126, 0.0774189755320549, 0.080135777592659, 0.0798955112695694, 0.07292796671390533, 0.13543729484081268, 2.2909488677978516, 0.7522322535514832, 0.7817876935005188, 0.6547591686248779, 0.45146897435188293, 0.24871982634067535, 0.10040846467018127, 0.02734093740582466, 0.01820335164666176, 0.040602296590805054, 0.0611826628446579, 0.062134142965078354, 0.043040886521339417, 0.03773023560643196, 4.846128463745117, 0.029399951919913292, 0.016106335446238518, 0.016018260270357132, 0.018443886190652847, 0.026662087067961693, 0.04187406972050667, 0.06277196109294891, 0.08578261733055115, 0.10626289248466492, 0.12004785239696503, 0.124716617166996, 0.12013635039329529, 0.10820100456476212, 0.09200083464384079, 0.07481002062559128, 0.05925280973315239, 0.04684600606560707, 0.038114603608846664, 0.032702941447496414, 0.02973683923482895, 0.027760222554206848, 0.0313086099922657, 0.0348682776093483, 0.038435403257608414, 0.06940437108278275, 0.0550491027534008, 0.06295222789049149, 0.06969272345304489, 0.07429870963096619, 0.07640416920185089, 0.07594713568687439, 0.07335060089826584, 0.06911927461624146, 0.06357556581497192, 0.05847073718905449, 0.05403625965118408, 0.050349995493888855, 0.05454373359680176, 0.06873487681150436, 0.07739128172397614, 0.051078058779239655, 0.05783194303512573, 0.1340932846069336, 0.03051583282649517, 0.02822103723883629, 0.06229565292596817, 0.02770092338323593, 0.025507215410470963, 0.03244961053133011, 0.04674820974469185, 0.1759123057126999, 0.33179181814193726, 0.6378381848335266, 0.021058663725852966, 0.023251013830304146, 0.0278607364743948, 0.035480137914419174, 0.046137936413288116, 0.058744631707668304, 0.07145694643259048, 0.08218233287334442, 0.08915673941373825, 0.09140632301568985, 0.088971808552742, 0.08278227597475052, 0.07440943270921707, 0.06551174819469452, 0.057544585317373276, 0.050838395953178406, 0.040547434240579605, 0.030831672251224518, 0.027984337881207466, 0.044153518974781036, 0.07891643792390823, 0.08103428035974503, 0.03708809241652489, 0.03287629783153534, 0.15349110960960388, 0.03341491147875786, 0.1193685233592987, 0.02883727476000786, 0.08863701671361923, 0.022597026079893112, 0.03941034898161888, 0.06239296868443489, 0.03283373638987541, 0.02565612643957138, 0.04991455376148224, 0.026466883718967438, 0.032388363033533096, 0.02573993429541588, 0.02830231748521328, 0.030848395079374313, 0.025299299508333206, 0.02774067409336567, 0.023258458822965622, 0.050371136516332626, 0.032363053411245346, 0.028763266280293465, 0.0301806703209877, 0.07777231186628342, 0.05698138475418091, 0.038970716297626495, 0.07461713254451752, 0.041311319917440414, 0.036324311047792435, 0.028603380545973778, 0.02565169148147106, 0.02494894154369831, 0.019790729507803917, 0.019949758425354958, 0.020972659811377525, 0.021255049854516983, 0.02183820866048336, 0.0232855174690485, 0.022793812677264214, 0.024602005258202553, 0.02725996822118759, 0.02215566858649254, 0.022807223722338676, 0.024350013583898544, 0.04974021390080452, 0.025108905509114265, 0.028495825827121735, 0.025163838639855385, 0.029214419424533844, 0.02504693903028965, 0.031001001596450806, 0.028581028804183006, 0.02740410715341568, 0.050009045749902725, 0.02543681673705578, 0.05178549885749817, 0.04996713623404503, 0.04177828133106232, 0.033243972808122635, 0.0319414921104908, 0.041672736406326294, 0.02022969350218773, 0.018827233463525772, 0.020931022241711617, 0.0384257473051548, 0.03075249493122101, 0.0272989384829998, 0.027660613879561424, 0.027412615716457367, 0.0270647294819355, 0.027069538831710815, 0.026869069784879684, 0.033544812351465225, 0.04236515611410141, 0.0503857359290123, 0.07990723848342896, 0.0766415148973465, 0.05039874091744423, 0.05131565406918526, 0.03393038734793663, 0.024325614795088768, 0.03328537568449974, 0.03053934872150421, 0.023211276158690453, 0.023459099233150482, 0.023749344050884247, 0.02227863296866417, 0.030013414099812508, 0.022509654983878136, 0.024881307035684586, 0.02831231988966465, 0.02581576071679592, 0.028520649299025536, 0.03561103343963623, 0.03334794193506241, 0.035736486315727234, 0.047827545553445816, 0.03851089999079704, 0.04388619586825371, 0.04892083629965782, 0.03759482875466347, 0.04154818877577782, 0.04887513816356659, 0.03957582637667656, 0.044966861605644226, 0.04548157379031181, 0.04291490092873573, 0.04806462302803993, 0.04221479222178459, 0.04564623534679413, 0.04177207499742508, 0.045250196009874344, 0.04042622819542885, 0.050596438348293304, 0.03928017243742943, 0.05402691289782524, 0.033336881548166275, 0.05237596854567528, 0.033831823617219925, 0.04045415297150612, 0.049347344785928726, 0.04014190286397934, 0.07707200944423676, 0.030248083174228668, 0.09477018564939499, 0.03308834880590439, 0.03036465495824814, 0.04266649857163429, 0.030171798542141914, 0.028936590999364853, 0.0286507960408926, 0.055024851113557816, 0.0391528494656086, 0.033163268119096756, 0.05452348664402962, 0.04873105511069298, 0.030350029468536377, 0.029938101768493652, 0.0361141636967659, 0.03126169368624687, 0.027058178558945656, 0.02987080253660679, 0.04100361838936806, 0.033893883228302, 0.03381423279643059, 0.04667971283197403, 0.03407144173979759, 0.02962592802941799, 0.03944610431790352, 0.029452074319124222, 0.026860209181904793, 0.040905021131038666, 0.03518056496977806, 0.036159176379442215, 0.062387604266405106, 0.03670065104961395, 0.05556821823120117, 0.03781607002019882, 0.03687052056193352, 0.043700024485588074, 0.03560316190123558, 0.04867493733763695, 0.046265859156847, 0.048413991928100586, 0.051675014197826385, 0.04113642871379852, 0.0474080964922905, 0.03693396598100662, 0.04620355740189552, 0.043102141469717026, 0.05210842564702034, 0.056445229798555374, 0.051426634192466736, 0.05436891317367554, 0.04502454400062561, 0.05501828342676163, 0.04434848576784134, 0.06620881706476212, 0.0420040562748909, 0.08573231846094131, 0.032688017934560776, 0.0917469710111618, 0.030841799452900887, 0.041106514632701874, 0.06561224162578583, 0.03135097771883011, 0.0498163215816021, 0.06878048926591873, 0.037738993763923645, 0.05726420879364014, 0.05719510093331337, 0.03624262660741806, 0.04501291364431381, 0.04797372967004776, 0.034005045890808105, 0.03814731910824776, 0.051838602870702744, 0.039494119584560394, 0.044543199241161346, 0.05294308811426163, 0.0385475717484951, 0.04378471150994301, 0.04715945944190025, 0.036210089921951294, 0.047401998192071915, 0.044985331594944, 0.0428142286837101, 0.06234263256192207, 0.04383104294538498, 0.06408387422561646, 0.041539158672094345, 0.05357525125145912, 0.04620106518268585, 0.048783984035253525, 0.06019553169608116, 0.044340599328279495, 0.08274165540933609, 0.03211253508925438, 0.11017066240310669, 0.03630514815449715, 0.03771408647298813, 0.06518708169460297, 0.02981020137667656, 0.028964389115571976, 0.05877295136451721, 0.04903257638216019, 0.03153878077864647, 0.0412006601691246, 0.07240770012140274, 0.04408743605017662, 0.034757088869810104, 0.04967712238430977, 0.049854278564453125, 0.031675294041633606, 0.030535317957401276, 0.042590413242578506, 0.04156944155693054, 0.031149445101618767, 0.0371367484331131, 0.048223137855529785, 0.034225862473249435, 0.033291228115558624, 0.04549001529812813, 0.036135729402303696, 0.03258492797613144, 0.04834934324026108, 0.03938651829957962, 0.03981498256325722, 0.056527797132730484, 0.04074830561876297, 0.049863871186971664, 0.052535608410835266, 0.04114745557308197, 0.0623481385409832, 0.03903011605143547, 0.05795879662036896, 0.05070534721016884, 0.04844000190496445, 0.06689552217721939, 0.04005551338195801, 0.07828225195407867, 0.032015521079301834, 0.07293462008237839, 0.0327284075319767, 0.036901332437992096, 0.061751242727041245, 0.03399423137307167, 0.0492679737508297, 0.08000043779611588, 0.04777659848332405, 0.08827866613864899, 0.07003800570964813, 0.04644310846924782, 0.05902599170804024, 0.04762035980820656, 0.03542704880237579, 0.04324157163500786, 0.0534084178507328, 0.04443533718585968, 0.05810309574007988, 0.07992058992385864, 0.053948257118463516, 0.06546766310930252, 0.05783916264772415, 0.03902667388319969, 0.04795241355895996, 0.04677635803818703, 0.03985299542546272, 0.058224234730005264, 0.05616878345608711, 0.05817888677120209, 0.06382870674133301, 0.04586264118552208, 0.05665209889411926, 0.043580129742622375, 0.04939385876059532, 0.05741674825549126, 0.05119815468788147, 0.0776447206735611, 0.043427854776382446, 0.0915602445602417, 0.03232017531991005, 0.0812467411160469, 0.038112301379442215, 0.043509501963853836, 0.08148574084043503, 0.043158113956451416, 0.08257106691598892, 0.06500515341758728, 0.050997063517570496, 0.08117160946130753, 0.049028534442186356, 0.04774694889783859, 0.0706254318356514, 0.04903244972229004, 0.05488748475909233, 0.06654230505228043, 0.04913138225674629, 0.059828076511621475, 0.05713453143835068, 0.04706432297825813, 0.061786629259586334, 0.050136543810367584, 0.05318038910627365, 0.06040465459227562, 0.05006272718310356, 0.0658944621682167, 0.04734603688120842, 0.06166088208556175, 0.057624511420726776, 0.058661967515945435, 0.06767117232084274, 0.0487951897084713, 0.08212967962026596, 0.04107648879289627, 0.10730179399251938, 0.03265313059091568, 0.09218103438615799, 0.03936120495200157, 0.036026451736688614, 0.07727904617786407, 0.04400156810879707, 0.04802907630801201, 0.10012635588645935, 0.05799790471792221, 0.07564839720726013, 0.09251440316438675, 0.04742060601711273, 0.051914721727371216, 0.05344961956143379, 0.0365477129817009, 0.03719150274991989, 0.052140627056360245, 0.0467555969953537, 0.04836813732981682, 0.07478107511997223, 0.05306192860007286, 0.05875164270401001, 0.0585128590464592, 0.038464613258838654, 0.04665592312812805, 0.04780171811580658, 0.0408598892390728, 0.06588089466094971, 0.05591440945863724, 0.07189951837062836, 0.059450265020132065, 0.05852137506008148, 0.052153587341308594, 0.05098605901002884, 0.05182277038693428, 0.053419329226017, 0.05783362314105034, 0.061245691031217575, 0.06525785475969315, 0.055166762322187424, 0.06377778202295303, 0.03603597730398178, 0.08531398326158524, 0.03438853845000267, 0.1597185730934143, 0.06874129921197891, 0.039750393480062485, 0.0928293913602829, 0.0564265139400959, 0.03392045944929123, 0.03689078614115715, 0.03998524695634842, 0.10330929607152939, 0.04606926813721657, 0.034113556146621704, 0.054844245314598083, 0.06986749917268753, 0.03436921164393425, 0.031238922849297523, 0.029470086097717285, 0.03535265848040581, 0.039919059723615646, 0.027469027787446976, 0.026425814256072044, 0.026106545701622963, 0.03508097678422928, 0.029573168605566025, 0.027655741199851036, 0.027868986129760742, 0.03289388120174408, 0.0321112722158432, 0.0293976329267025, 0.0288351159542799, 0.032267551869153976, 0.029517894610762596, 0.02838130295276642, 0.028144143521785736, 0.03560534864664078, 0.029928820207715034, 0.030865775421261787, 0.04408708214759827, 0.040658850222826004, 0.03506310284137726, 0.04493752494454384, 0.04702971130609512, 0.03711829334497452, 0.045946717262268066, 0.04359836503863335, 0.036918479949235916, 0.0463809110224247, 0.041766975075006485, 0.04124464839696884, 0.04826310649514198, 0.040103066712617874, 0.0443853996694088, 0.04522048309445381, 0.04079405963420868, 0.0530855767428875, 0.039550021290779114, 0.061272770166397095, 0.04052179679274559, 0.06531158834695816, 0.04211943596601486, 0.061525873839855194, 0.0448312871158123, 0.059101078659296036, 0.0471787191927433, 0.056816648691892624, 0.04952651634812355, 0.05339980125427246, 0.05280674248933792, 0.05184485390782356, 0.05720313638448715, 0.0497097484767437, 0.06557945162057877, 0.042230453342199326, 0.11068723350763321, 0.08334210515022278, 0.07631967216730118, 0.09068231284618378, 0.03718065097928047, 0.05366998910903931, 0.037543416023254395, 0.043453995138406754, 0.051332417875528336, 0.035646338015794754, 0.034182798117399216, 0.027957122772932053, 0.03205356374382973, 0.03635882958769798, 0.030383136123418808, 0.034697599709033966, 0.03710886090993881, 0.034199316054582596, 0.035258837044239044, 0.033878497779369354, 0.039528749883174896, 0.03995155915617943, 0.032444700598716736, 0.031731754541397095, 0.03398673236370087, 0.03999187797307968, 0.03275614231824875, 0.029668791219592094, 0.032692693173885345, 0.03716327250003815, 0.03128878027200699, 0.03224852308630943, 0.03297347202897072, 0.034636884927749634, 0.04008158668875694, 0.04721219465136528, 0.04348166286945343, 0.04887351021170616, 0.05935715511441231, 0.049584269523620605, 0.0461643747985363, 0.0539158433675766, 0.049700409173965454, 0.04709881171584129, 0.05800879746675491, 0.05476651340723038, 0.05397115647792816, 0.061896584928035736, 0.05348199978470802, 0.057071663439273834, 0.058523714542388916, 0.05160437896847725, 0.053835779428482056, 0.050429243594408035, 0.04556748643517494, 0.04748746380209923, 0.043032240122556686, 0.04397572576999664, 0.0429188534617424, 0.04144005477428436, 0.043153248727321625, 0.04212958365678787, 0.043928101658821106, 0.04507835954427719, 0.04496211186051369, 0.04694068059325218, 0.04429623857140541, 0.04677627235651016, 0.044844113290309906, 0.047341715544462204, 0.046608079224824905, 0.04788109287619591, 0.049088262021541595, 0.04885990545153618, 0.05174556374549866, 0.04973122105002403, 0.0554354190826416, 0.04961186274886131, 0.06006563454866409, 0.04753199592232704, 0.07178722321987152, 0.04805859178304672, 0.09104519337415695, 0.047542259097099304, 0.06345047801733017, 0.07331890612840652, 0.0440342091023922, 0.05284091457724571, 0.07697313278913498, 0.04104095697402954, 0.04119551181793213, 0.06827766448259354, 0.04660724103450775, 0.04023901745676994, 0.046523455530405045, 0.06021173298358917, 0.04292833432555199, 0.041440945118665695, 0.05047066509723663, 0.05121385678648949, 0.041561029851436615, 0.04240373894572258, 0.05089542642235756, 0.04263018071651459, 0.04151920974254608, 0.045123253017663956, 0.04356621950864792, 0.04126900061964989, 0.0430961474776268, 0.045895468443632126, 0.04258950427174568, 0.04513943940401077, 0.04893987998366356, 0.04424689710140228, 0.04923708364367485, 0.049826573580503464, 0.04612825810909271, 0.0547800175845623, 0.04734807834029198, 0.05031634494662285, 0.05149752274155617, 0.046741779893636703, 0.05550725758075714, 0.04589436948299408, 0.059559863060712814, 0.04571479931473732, 0.06718319654464722, 0.046359993517398834, 0.0761151909828186, 0.04715745151042938, 0.06363820284605026, 0.04807556793093681, 0.04374116286635399, 0.06199068948626518, 0.04502285271883011, 0.04630369320511818, 0.060772690922021866, 0.04597661271691322, 0.048855260014534, 0.05984381213784218, 0.04636598750948906, 0.05019111931324005, 0.05781732499599457, 0.04579718038439751, 0.049961742013692856, 0.05364985764026642, 0.044735394418239594, 0.04966604337096214, 0.049098920077085495, 0.04439364746212959, 0.05104541406035423, 0.04596757888793945, 0.04583711549639702, 0.05072498321533203, 0.04565999284386635, 0.050691764801740646, 0.04682443290948868, 0.04780173301696777, 0.04895523563027382, 0.04617312178015709, 0.051545996218919754, 0.046433161944150925, 0.05660231038928032, 0.04893222823739052, 0.07194206863641739, 0.05717475339770317, 0.08409740030765533, 0.04745515435934067, 0.05200672149658203, 0.07454115152359009, 0.046153511852025986, 0.05135013535618782, 0.07733022421598434, 0.04339619353413582, 0.04481653869152069, 0.07561908662319183, 0.044756900519132614, 0.04345766827464104, 0.06161430850625038, 0.054086148738861084, 0.04322739318013191, 0.04795750603079796, 0.06118331849575043, 0.04499975964426994, 0.04478144645690918, 0.05524972453713417, 0.04590080678462982, 0.04348832741379738, 0.04808319732546806, 0.04630410671234131, 0.044050272554159164, 0.045493461191654205, 0.04674042388796806, 0.04463979974389076, 0.045997802168130875, 0.0475354865193367, 0.04592475667595863, 0.047981008887290955, 0.04669264331459999, 0.046007782220840454, 0.049507834017276764, 0.04636602848768234, 0.04820631816983223, 0.047483354806900024, 0.04687478020787239, 0.051344599574804306, 0.04704776778817177, 0.05344053730368614, 0.04645054042339325, 0.058316003531217575, 0.04853357747197151, 0.06947153061628342, 0.05263945087790489, 0.07678769528865814, 0.04403345286846161, 0.0664840117096901, 0.06427981704473495].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1487 | train: Loss 0.003475 Accuracy 0.8726 | validation: Loss 0.047483 Accuracy -1.9830\n",
      "Epoch 1488 | train: Loss 0.003385 Accuracy 0.8759 | validation: Loss 0.046875 Accuracy -1.9448\n",
      "Epoch 1489 | train: Loss 0.003506 Accuracy 0.8714 | validation: Loss 0.051345 Accuracy -2.2256\n",
      "Epoch 1490 | train: Loss 0.003620 Accuracy 0.8672 | validation: Loss 0.047048 Accuracy -1.9557\n",
      "Epoch 1491 | train: Loss 0.003903 Accuracy 0.8569 | validation: Loss 0.053441 Accuracy -2.3573\n",
      "Epoch 1492 | train: Loss 0.003918 Accuracy 0.8563 | validation: Loss 0.046451 Accuracy -1.9182\n",
      "Epoch 1493 | train: Loss 0.004397 Accuracy 0.8388 | validation: Loss 0.058316 Accuracy -2.6636\n",
      "Epoch 1494 | train: Loss 0.004692 Accuracy 0.8279 | validation: Loss 0.048534 Accuracy -2.0490\n",
      "Epoch 1495 | train: Loss 0.006039 Accuracy 0.7785 | validation: Loss 0.069472 Accuracy -3.3644\n",
      "Epoch 1496 | train: Loss 0.006876 Accuracy 0.7478 | validation: Loss 0.052639 Accuracy -2.3070\n",
      "Epoch 1497 | train: Loss 0.010032 Accuracy 0.6321 | validation: Loss 0.076788 Accuracy -3.8240\n",
      "Epoch 1498 | train: Loss 0.008013 Accuracy 0.7061 | validation: Loss 0.044033 Accuracy -1.7663\n",
      "Epoch 1499 | train: Loss 0.008111 Accuracy 0.7025 | validation: Loss 0.066484 Accuracy -3.1767\n",
      "Epoch 1500 | train: Loss 0.004969 Accuracy 0.8178 | validation: Loss 0.064280 Accuracy -3.0382\n",
      "Epoch 1500 | train: Loss 0.004969 Accuracy 0.8178 | validation: Loss 0.064280 Accuracy -3.0382\n",
      "1 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84acce7471f34afc9be27ecd9d3e2370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train: Loss 1.330652 Accuracy -47.7995 | validation: Loss 2.106875 Accuracy -131.3601\n",
      "Epoch 2 | train: Loss 1.321851 Accuracy -47.4767 | validation: Loss 2.089412 Accuracy -130.2630\n",
      "Epoch 3 | train: Loss 1.313113 Accuracy -47.1563 | validation: Loss 2.072069 Accuracy -129.1734\n",
      "Epoch 4 | train: Loss 1.304438 Accuracy -46.8381 | validation: Loss 2.054845 Accuracy -128.0914\n",
      "Epoch 5 | train: Loss 1.295784 Accuracy -46.5208 | validation: Loss 2.037653 Accuracy -127.0113\n",
      "Epoch 6 | train: Loss 1.287192 Accuracy -46.2056 | validation: Loss 2.020586 Accuracy -125.9392\n",
      "Epoch 7 | train: Loss 1.278678 Accuracy -45.8934 | validation: Loss 2.003644 Accuracy -124.8748\n",
      "Epoch 8 | train: Loss 1.270229 Accuracy -45.5836 | validation: Loss 1.986817 Accuracy -123.8177\n",
      "Epoch 9 | train: Loss 1.261880 Accuracy -45.2774 | validation: Loss 1.970017 Accuracy -122.7623\n",
      "Epoch 10 | train: Loss 1.253619 Accuracy -44.9744 | validation: Loss 1.953335 Accuracy -121.7142\n",
      "Epoch 11 | train: Loss 1.245425 Accuracy -44.6739 | validation: Loss 1.936768 Accuracy -120.6735\n",
      "Epoch 12 | train: Loss 1.237326 Accuracy -44.3769 | validation: Loss 1.920308 Accuracy -119.6394\n",
      "Epoch 13 | train: Loss 1.229345 Accuracy -44.0842 | validation: Loss 1.903876 Accuracy -118.6071\n",
      "Epoch 14 | train: Loss 1.221501 Accuracy -43.7966 | validation: Loss 1.887569 Accuracy -117.5826\n",
      "Epoch 15 | train: Loss 1.213755 Accuracy -43.5125 | validation: Loss 1.871261 Accuracy -116.5581\n",
      "Epoch 16 | train: Loss 1.206131 Accuracy -43.2329 | validation: Loss 1.855065 Accuracy -115.5406\n",
      "Epoch 17 | train: Loss 1.198572 Accuracy -42.9557 | validation: Loss 1.838833 Accuracy -114.5209\n",
      "Epoch 18 | train: Loss 1.191098 Accuracy -42.6816 | validation: Loss 1.822726 Accuracy -113.5090\n",
      "Epoch 19 | train: Loss 1.183674 Accuracy -42.4093 | validation: Loss 1.806773 Accuracy -112.5068\n",
      "Epoch 20 | train: Loss 1.176317 Accuracy -42.1395 | validation: Loss 1.790939 Accuracy -111.5121\n",
      "Epoch 21 | train: Loss 1.168982 Accuracy -41.8705 | validation: Loss 1.775215 Accuracy -110.5242\n",
      "Epoch 22 | train: Loss 1.161682 Accuracy -41.6028 | validation: Loss 1.759703 Accuracy -109.5497\n",
      "Epoch 23 | train: Loss 1.154428 Accuracy -41.3367 | validation: Loss 1.744380 Accuracy -108.5871\n",
      "Epoch 24 | train: Loss 1.147215 Accuracy -41.0722 | validation: Loss 1.729166 Accuracy -107.6313\n",
      "Epoch 25 | train: Loss 1.140027 Accuracy -40.8086 | validation: Loss 1.714174 Accuracy -106.6895\n",
      "Epoch 26 | train: Loss 1.132868 Accuracy -40.5461 | validation: Loss 1.699334 Accuracy -105.7571\n",
      "Epoch 27 | train: Loss 1.125751 Accuracy -40.2851 | validation: Loss 1.684676 Accuracy -104.8363\n",
      "Epoch 28 | train: Loss 1.118674 Accuracy -40.0255 | validation: Loss 1.670079 Accuracy -103.9193\n",
      "Epoch 29 | train: Loss 1.111665 Accuracy -39.7685 | validation: Loss 1.655489 Accuracy -103.0027\n",
      "Epoch 30 | train: Loss 1.104691 Accuracy -39.5127 | validation: Loss 1.641024 Accuracy -102.0939\n",
      "Epoch 31 | train: Loss 1.097755 Accuracy -39.2584 | validation: Loss 1.626575 Accuracy -101.1862\n",
      "Epoch 32 | train: Loss 1.090840 Accuracy -39.0048 | validation: Loss 1.612127 Accuracy -100.2785\n",
      "Epoch 33 | train: Loss 1.083921 Accuracy -38.7510 | validation: Loss 1.597714 Accuracy -99.3731\n",
      "Epoch 34 | train: Loss 1.077024 Accuracy -38.4981 | validation: Loss 1.583508 Accuracy -98.4806\n",
      "Epoch 35 | train: Loss 1.070151 Accuracy -38.2460 | validation: Loss 1.569295 Accuracy -97.5877\n",
      "Epoch 36 | train: Loss 1.063293 Accuracy -37.9945 | validation: Loss 1.555167 Accuracy -96.7002\n",
      "Epoch 37 | train: Loss 1.056474 Accuracy -37.7445 | validation: Loss 1.541097 Accuracy -95.8163\n",
      "Epoch 38 | train: Loss 1.049662 Accuracy -37.4946 | validation: Loss 1.527154 Accuracy -94.9403\n",
      "Epoch 39 | train: Loss 1.042841 Accuracy -37.2445 | validation: Loss 1.513179 Accuracy -94.0624\n",
      "Epoch 40 | train: Loss 1.036023 Accuracy -36.9944 | validation: Loss 1.499188 Accuracy -93.1834\n",
      "Epoch 41 | train: Loss 1.029217 Accuracy -36.7449 | validation: Loss 1.485411 Accuracy -92.3179\n",
      "Epoch 42 | train: Loss 1.022437 Accuracy -36.4962 | validation: Loss 1.471546 Accuracy -91.4468\n",
      "Epoch 43 | train: Loss 1.015689 Accuracy -36.2487 | validation: Loss 1.457638 Accuracy -90.5731\n",
      "Epoch 44 | train: Loss 1.008952 Accuracy -36.0017 | validation: Loss 1.443802 Accuracy -89.7039\n",
      "Epoch 45 | train: Loss 1.002217 Accuracy -35.7547 | validation: Loss 1.429966 Accuracy -88.8347\n",
      "Epoch 46 | train: Loss 0.995470 Accuracy -35.5073 | validation: Loss 1.416190 Accuracy -87.9692\n",
      "Epoch 47 | train: Loss 0.988730 Accuracy -35.2600 | validation: Loss 1.402602 Accuracy -87.1156\n",
      "Epoch 48 | train: Loss 0.981996 Accuracy -35.0131 | validation: Loss 1.389064 Accuracy -86.2651\n",
      "Epoch 49 | train: Loss 0.975275 Accuracy -34.7666 | validation: Loss 1.375623 Accuracy -85.4207\n",
      "Epoch 50 | train: Loss 0.968546 Accuracy -34.5199 | validation: Loss 1.362339 Accuracy -84.5862\n",
      "Epoch 51 | train: Loss 0.961814 Accuracy -34.2730 | validation: Loss 1.349102 Accuracy -83.7545\n",
      "Epoch 52 | train: Loss 0.955102 Accuracy -34.0268 | validation: Loss 1.335998 Accuracy -82.9313\n",
      "Epoch 53 | train: Loss 0.948398 Accuracy -33.7809 | validation: Loss 1.322987 Accuracy -82.1140\n",
      "Epoch 54 | train: Loss 0.941651 Accuracy -33.5335 | validation: Loss 1.310028 Accuracy -81.2998\n",
      "Epoch 55 | train: Loss 0.934929 Accuracy -33.2870 | validation: Loss 1.297243 Accuracy -80.4966\n",
      "Epoch 56 | train: Loss 0.928210 Accuracy -33.0406 | validation: Loss 1.284498 Accuracy -79.6960\n",
      "Epoch 57 | train: Loss 0.921470 Accuracy -32.7934 | validation: Loss 1.271868 Accuracy -78.9025\n",
      "Epoch 58 | train: Loss 0.914716 Accuracy -32.5457 | validation: Loss 1.259341 Accuracy -78.1155\n",
      "Epoch 59 | train: Loss 0.907985 Accuracy -32.2989 | validation: Loss 1.246862 Accuracy -77.3315\n",
      "Epoch 60 | train: Loss 0.901272 Accuracy -32.0527 | validation: Loss 1.234535 Accuracy -76.5571\n",
      "Epoch 61 | train: Loss 0.894539 Accuracy -31.8058 | validation: Loss 1.222261 Accuracy -75.7860\n",
      "Epoch 62 | train: Loss 0.887794 Accuracy -31.5584 | validation: Loss 1.210087 Accuracy -75.0212\n",
      "Epoch 63 | train: Loss 0.881047 Accuracy -31.3110 | validation: Loss 1.198022 Accuracy -74.2633\n",
      "Epoch 64 | train: Loss 0.874331 Accuracy -31.0647 | validation: Loss 1.186001 Accuracy -73.5081\n",
      "Epoch 65 | train: Loss 0.867611 Accuracy -30.8182 | validation: Loss 1.174152 Accuracy -72.7637\n",
      "Epoch 66 | train: Loss 0.860895 Accuracy -30.5719 | validation: Loss 1.162344 Accuracy -72.0219\n",
      "Epoch 67 | train: Loss 0.854166 Accuracy -30.3251 | validation: Loss 1.150502 Accuracy -71.2779\n",
      "Epoch 68 | train: Loss 0.847472 Accuracy -30.0797 | validation: Loss 1.138571 Accuracy -70.5284\n",
      "Epoch 69 | train: Loss 0.840791 Accuracy -29.8346 | validation: Loss 1.126560 Accuracy -69.7738\n",
      "Epoch 70 | train: Loss 0.834112 Accuracy -29.5897 | validation: Loss 1.114493 Accuracy -69.0158\n",
      "Epoch 71 | train: Loss 0.827439 Accuracy -29.3450 | validation: Loss 1.102368 Accuracy -68.2540\n",
      "Epoch 72 | train: Loss 0.820777 Accuracy -29.1006 | validation: Loss 1.090213 Accuracy -67.4904\n",
      "Epoch 73 | train: Loss 0.814117 Accuracy -28.8564 | validation: Loss 1.078086 Accuracy -66.7285\n",
      "Epoch 74 | train: Loss 0.807466 Accuracy -28.6125 | validation: Loss 1.066006 Accuracy -65.9696\n",
      "Epoch 75 | train: Loss 0.800823 Accuracy -28.3689 | validation: Loss 1.053973 Accuracy -65.2137\n",
      "Epoch 76 | train: Loss 0.794179 Accuracy -28.1252 | validation: Loss 1.041985 Accuracy -64.4606\n",
      "Epoch 77 | train: Loss 0.787533 Accuracy -27.8815 | validation: Loss 1.030043 Accuracy -63.7103\n",
      "Epoch 78 | train: Loss 0.780885 Accuracy -27.6377 | validation: Loss 1.018146 Accuracy -62.9629\n",
      "Epoch 79 | train: Loss 0.774236 Accuracy -27.3939 | validation: Loss 1.006296 Accuracy -62.2185\n",
      "Epoch 80 | train: Loss 0.767586 Accuracy -27.1500 | validation: Loss 0.994490 Accuracy -61.4768\n",
      "Epoch 81 | train: Loss 0.760923 Accuracy -26.9056 | validation: Loss 0.982724 Accuracy -60.7376\n",
      "Epoch 82 | train: Loss 0.754240 Accuracy -26.6605 | validation: Loss 0.970999 Accuracy -60.0010\n",
      "Epoch 83 | train: Loss 0.747543 Accuracy -26.4149 | validation: Loss 0.959314 Accuracy -59.2669\n",
      "Epoch 84 | train: Loss 0.740835 Accuracy -26.1689 | validation: Loss 0.947701 Accuracy -58.5374\n",
      "Epoch 85 | train: Loss 0.734107 Accuracy -25.9222 | validation: Loss 0.936263 Accuracy -57.8188\n",
      "Epoch 86 | train: Loss 0.727347 Accuracy -25.6743 | validation: Loss 0.925058 Accuracy -57.1149\n",
      "Epoch 87 | train: Loss 0.720570 Accuracy -25.4257 | validation: Loss 0.914066 Accuracy -56.4243\n",
      "Epoch 88 | train: Loss 0.713774 Accuracy -25.1765 | validation: Loss 0.903338 Accuracy -55.7503\n",
      "Epoch 89 | train: Loss 0.706962 Accuracy -24.9267 | validation: Loss 0.892738 Accuracy -55.0844\n",
      "Epoch 90 | train: Loss 0.700131 Accuracy -24.6762 | validation: Loss 0.882321 Accuracy -54.4300\n",
      "Epoch 91 | train: Loss 0.693285 Accuracy -24.4251 | validation: Loss 0.872027 Accuracy -53.7833\n",
      "Epoch 92 | train: Loss 0.686437 Accuracy -24.1739 | validation: Loss 0.861856 Accuracy -53.1443\n",
      "Epoch 93 | train: Loss 0.679570 Accuracy -23.9221 | validation: Loss 0.851801 Accuracy -52.5127\n",
      "Epoch 94 | train: Loss 0.672684 Accuracy -23.6696 | validation: Loss 0.841900 Accuracy -51.8906\n",
      "Epoch 95 | train: Loss 0.665778 Accuracy -23.4163 | validation: Loss 0.831998 Accuracy -51.2686\n",
      "Epoch 96 | train: Loss 0.658857 Accuracy -23.1625 | validation: Loss 0.822081 Accuracy -50.6455\n",
      "Epoch 97 | train: Loss 0.651925 Accuracy -22.9083 | validation: Loss 0.812157 Accuracy -50.0221\n",
      "Epoch 98 | train: Loss 0.644980 Accuracy -22.6536 | validation: Loss 0.802242 Accuracy -49.3992\n",
      "Epoch 99 | train: Loss 0.638029 Accuracy -22.3987 | validation: Loss 0.792314 Accuracy -48.7755\n",
      "Epoch 100 | train: Loss 0.631109 Accuracy -22.1449 | validation: Loss 0.782387 Accuracy -48.1518\n",
      "Epoch 101 | train: Loss 0.624234 Accuracy -21.8927 | validation: Loss 0.772468 Accuracy -47.5287\n",
      "Epoch 102 | train: Loss 0.617392 Accuracy -21.6419 | validation: Loss 0.762548 Accuracy -46.9055\n",
      "Epoch 103 | train: Loss 0.610569 Accuracy -21.3916 | validation: Loss 0.752654 Accuracy -46.2840\n",
      "Epoch 104 | train: Loss 0.603793 Accuracy -21.1431 | validation: Loss 0.742835 Accuracy -45.6671\n",
      "Epoch 105 | train: Loss 0.597081 Accuracy -20.8970 | validation: Loss 0.733072 Accuracy -45.0538\n",
      "Epoch 106 | train: Loss 0.590446 Accuracy -20.6536 | validation: Loss 0.723342 Accuracy -44.4425\n",
      "Epoch 107 | train: Loss 0.583847 Accuracy -20.4116 | validation: Loss 0.713735 Accuracy -43.8389\n",
      "Epoch 108 | train: Loss 0.577340 Accuracy -20.1730 | validation: Loss 0.704167 Accuracy -43.2378\n",
      "Epoch 109 | train: Loss 0.570927 Accuracy -19.9378 | validation: Loss 0.694642 Accuracy -42.6394\n",
      "Epoch 110 | train: Loss 0.564606 Accuracy -19.7060 | validation: Loss 0.685229 Accuracy -42.0481\n",
      "Epoch 111 | train: Loss 0.558347 Accuracy -19.4765 | validation: Loss 0.675853 Accuracy -41.4591\n",
      "Epoch 112 | train: Loss 0.552208 Accuracy -19.2513 | validation: Loss 0.666557 Accuracy -40.8751\n",
      "Epoch 113 | train: Loss 0.546163 Accuracy -19.0296 | validation: Loss 0.657345 Accuracy -40.2964\n",
      "Epoch 114 | train: Loss 0.540207 Accuracy -18.8112 | validation: Loss 0.648176 Accuracy -39.7203\n",
      "Epoch 115 | train: Loss 0.534328 Accuracy -18.5956 | validation: Loss 0.639117 Accuracy -39.1512\n",
      "Epoch 116 | train: Loss 0.528563 Accuracy -18.3842 | validation: Loss 0.630123 Accuracy -38.5862\n",
      "Epoch 117 | train: Loss 0.522891 Accuracy -18.1762 | validation: Loss 0.621176 Accuracy -38.0241\n",
      "Epoch 118 | train: Loss 0.517295 Accuracy -17.9710 | validation: Loss 0.612332 Accuracy -37.4685\n",
      "Epoch 119 | train: Loss 0.511802 Accuracy -17.7695 | validation: Loss 0.603536 Accuracy -36.9159\n",
      "Epoch 120 | train: Loss 0.506391 Accuracy -17.5711 | validation: Loss 0.594806 Accuracy -36.3675\n",
      "Epoch 121 | train: Loss 0.501033 Accuracy -17.3746 | validation: Loss 0.586173 Accuracy -35.8251\n",
      "Epoch 122 | train: Loss 0.495737 Accuracy -17.1804 | validation: Loss 0.577589 Accuracy -35.2858\n",
      "Epoch 123 | train: Loss 0.490512 Accuracy -16.9887 | validation: Loss 0.569076 Accuracy -34.7510\n",
      "Epoch 124 | train: Loss 0.485339 Accuracy -16.7990 | validation: Loss 0.560650 Accuracy -34.2217\n",
      "Epoch 125 | train: Loss 0.480183 Accuracy -16.6099 | validation: Loss 0.552272 Accuracy -33.6953\n",
      "Epoch 126 | train: Loss 0.475100 Accuracy -16.4235 | validation: Loss 0.543949 Accuracy -33.1725\n",
      "Epoch 127 | train: Loss 0.470073 Accuracy -16.2391 | validation: Loss 0.535717 Accuracy -32.6553\n",
      "Epoch 128 | train: Loss 0.465065 Accuracy -16.0555 | validation: Loss 0.527533 Accuracy -32.1412\n",
      "Epoch 129 | train: Loss 0.460098 Accuracy -15.8733 | validation: Loss 0.519398 Accuracy -31.6301\n",
      "Epoch 130 | train: Loss 0.455165 Accuracy -15.6924 | validation: Loss 0.511348 Accuracy -31.1244\n",
      "Epoch 131 | train: Loss 0.450257 Accuracy -15.5124 | validation: Loss 0.503358 Accuracy -30.6224\n",
      "Epoch 132 | train: Loss 0.445390 Accuracy -15.3340 | validation: Loss 0.495421 Accuracy -30.1238\n",
      "Epoch 133 | train: Loss 0.440540 Accuracy -15.1561 | validation: Loss 0.487539 Accuracy -29.6286\n",
      "Epoch 134 | train: Loss 0.435729 Accuracy -14.9796 | validation: Loss 0.479750 Accuracy -29.1393\n",
      "Epoch 135 | train: Loss 0.430957 Accuracy -14.8046 | validation: Loss 0.472017 Accuracy -28.6535\n",
      "Epoch 136 | train: Loss 0.426195 Accuracy -14.6300 | validation: Loss 0.464337 Accuracy -28.1710\n",
      "Epoch 137 | train: Loss 0.421450 Accuracy -14.4560 | validation: Loss 0.456713 Accuracy -27.6920\n",
      "Epoch 138 | train: Loss 0.416732 Accuracy -14.2829 | validation: Loss 0.449143 Accuracy -27.2165\n",
      "Epoch 139 | train: Loss 0.412030 Accuracy -14.1105 | validation: Loss 0.441631 Accuracy -26.7445\n",
      "Epoch 140 | train: Loss 0.407370 Accuracy -13.9396 | validation: Loss 0.434176 Accuracy -26.2762\n",
      "Epoch 141 | train: Loss 0.402736 Accuracy -13.7697 | validation: Loss 0.426780 Accuracy -25.8116\n",
      "Epoch 142 | train: Loss 0.398122 Accuracy -13.6005 | validation: Loss 0.419444 Accuracy -25.3507\n",
      "Epoch 143 | train: Loss 0.393520 Accuracy -13.4317 | validation: Loss 0.412168 Accuracy -24.8936\n",
      "Epoch 144 | train: Loss 0.388928 Accuracy -13.2633 | validation: Loss 0.404951 Accuracy -24.4402\n",
      "Epoch 145 | train: Loss 0.384360 Accuracy -13.0958 | validation: Loss 0.397797 Accuracy -23.9908\n",
      "Epoch 146 | train: Loss 0.379815 Accuracy -12.9291 | validation: Loss 0.390705 Accuracy -23.5453\n",
      "Epoch 147 | train: Loss 0.375288 Accuracy -12.7631 | validation: Loss 0.383676 Accuracy -23.1037\n",
      "Epoch 148 | train: Loss 0.370775 Accuracy -12.5976 | validation: Loss 0.376711 Accuracy -22.6661\n",
      "Epoch 149 | train: Loss 0.366284 Accuracy -12.4328 | validation: Loss 0.369812 Accuracy -22.2327\n",
      "Epoch 150 | train: Loss 0.361815 Accuracy -12.2690 | validation: Loss 0.362979 Accuracy -21.8034\n",
      "Epoch 151 | train: Loss 0.357359 Accuracy -12.1056 | validation: Loss 0.356212 Accuracy -21.3783\n",
      "Epoch 152 | train: Loss 0.352917 Accuracy -11.9426 | validation: Loss 0.349512 Accuracy -20.9574\n",
      "Epoch 153 | train: Loss 0.348492 Accuracy -11.7804 | validation: Loss 0.342881 Accuracy -20.5408\n",
      "Epoch 154 | train: Loss 0.344086 Accuracy -11.6188 | validation: Loss 0.336318 Accuracy -20.1285\n",
      "Epoch 155 | train: Loss 0.339700 Accuracy -11.4579 | validation: Loss 0.329825 Accuracy -19.7206\n",
      "Epoch 156 | train: Loss 0.335336 Accuracy -11.2979 | validation: Loss 0.323405 Accuracy -19.3173\n",
      "Epoch 157 | train: Loss 0.330996 Accuracy -11.1388 | validation: Loss 0.317058 Accuracy -18.9185\n",
      "Epoch 158 | train: Loss 0.326672 Accuracy -10.9802 | validation: Loss 0.310783 Accuracy -18.5243\n",
      "Epoch 159 | train: Loss 0.322364 Accuracy -10.8222 | validation: Loss 0.304583 Accuracy -18.1348\n",
      "Epoch 160 | train: Loss 0.318076 Accuracy -10.6649 | validation: Loss 0.298472 Accuracy -17.7509\n",
      "Epoch 161 | train: Loss 0.313812 Accuracy -10.5085 | validation: Loss 0.292435 Accuracy -17.3717\n",
      "Epoch 162 | train: Loss 0.309573 Accuracy -10.3531 | validation: Loss 0.286477 Accuracy -16.9973\n",
      "Epoch 163 | train: Loss 0.305358 Accuracy -10.1985 | validation: Loss 0.280596 Accuracy -16.6279\n",
      "Epoch 164 | train: Loss 0.301166 Accuracy -10.0448 | validation: Loss 0.274795 Accuracy -16.2634\n",
      "Epoch 165 | train: Loss 0.297002 Accuracy -9.8921 | validation: Loss 0.269095 Accuracy -15.9053\n",
      "Epoch 166 | train: Loss 0.292859 Accuracy -9.7401 | validation: Loss 0.263477 Accuracy -15.5524\n",
      "Epoch 167 | train: Loss 0.288735 Accuracy -9.5889 | validation: Loss 0.257939 Accuracy -15.2045\n",
      "Epoch 168 | train: Loss 0.284634 Accuracy -9.4385 | validation: Loss 0.252483 Accuracy -14.8617\n",
      "Epoch 169 | train: Loss 0.280565 Accuracy -9.2893 | validation: Loss 0.247115 Accuracy -14.5245\n",
      "Epoch 170 | train: Loss 0.276517 Accuracy -9.1408 | validation: Loss 0.241843 Accuracy -14.1933\n",
      "Epoch 171 | train: Loss 0.272491 Accuracy -8.9932 | validation: Loss 0.236654 Accuracy -13.8673\n",
      "Epoch 172 | train: Loss 0.268488 Accuracy -8.8463 | validation: Loss 0.231548 Accuracy -13.5465\n",
      "Epoch 173 | train: Loss 0.264510 Accuracy -8.7005 | validation: Loss 0.226526 Accuracy -13.2310\n",
      "Epoch 174 | train: Loss 0.260556 Accuracy -8.5555 | validation: Loss 0.221595 Accuracy -12.9213\n",
      "Epoch 175 | train: Loss 0.256625 Accuracy -8.4113 | validation: Loss 0.216762 Accuracy -12.6176\n",
      "Epoch 176 | train: Loss 0.252718 Accuracy -8.2680 | validation: Loss 0.212014 Accuracy -12.3194\n",
      "Epoch 177 | train: Loss 0.248834 Accuracy -8.1256 | validation: Loss 0.207352 Accuracy -12.0264\n",
      "Epoch 178 | train: Loss 0.244975 Accuracy -7.9841 | validation: Loss 0.202775 Accuracy -11.7389\n",
      "Epoch 179 | train: Loss 0.241141 Accuracy -7.8435 | validation: Loss 0.198300 Accuracy -11.4578\n",
      "Epoch 180 | train: Loss 0.237332 Accuracy -7.7038 | validation: Loss 0.193919 Accuracy -11.1825\n",
      "Epoch 181 | train: Loss 0.233549 Accuracy -7.5650 | validation: Loss 0.189625 Accuracy -10.9128\n",
      "Epoch 182 | train: Loss 0.229792 Accuracy -7.4272 | validation: Loss 0.185419 Accuracy -10.6486\n",
      "Epoch 183 | train: Loss 0.226061 Accuracy -7.2904 | validation: Loss 0.181303 Accuracy -10.3900\n",
      "Epoch 184 | train: Loss 0.222358 Accuracy -7.1546 | validation: Loss 0.177290 Accuracy -10.1379\n",
      "Epoch 185 | train: Loss 0.218685 Accuracy -7.0199 | validation: Loss 0.173367 Accuracy -9.8914\n",
      "Epoch 186 | train: Loss 0.215040 Accuracy -6.8862 | validation: Loss 0.169535 Accuracy -9.6507\n",
      "Epoch 187 | train: Loss 0.211424 Accuracy -6.7536 | validation: Loss 0.165793 Accuracy -9.4156\n",
      "Epoch 188 | train: Loss 0.207837 Accuracy -6.6221 | validation: Loss 0.162154 Accuracy -9.1870\n",
      "Epoch 189 | train: Loss 0.204280 Accuracy -6.4916 | validation: Loss 0.158614 Accuracy -8.9646\n",
      "Epoch 190 | train: Loss 0.200753 Accuracy -6.3623 | validation: Loss 0.155165 Accuracy -8.7479\n",
      "Epoch 191 | train: Loss 0.197256 Accuracy -6.2340 | validation: Loss 0.151809 Accuracy -8.5371\n",
      "Epoch 192 | train: Loss 0.193791 Accuracy -6.1070 | validation: Loss 0.148552 Accuracy -8.3325\n",
      "Epoch 193 | train: Loss 0.190357 Accuracy -5.9810 | validation: Loss 0.145399 Accuracy -8.1344\n",
      "Epoch 194 | train: Loss 0.186955 Accuracy -5.8563 | validation: Loss 0.142339 Accuracy -7.9421\n",
      "Epoch 195 | train: Loss 0.183587 Accuracy -5.7327 | validation: Loss 0.139372 Accuracy -7.7557\n",
      "Epoch 196 | train: Loss 0.180251 Accuracy -5.6104 | validation: Loss 0.136503 Accuracy -7.5755\n",
      "Epoch 197 | train: Loss 0.176949 Accuracy -5.4893 | validation: Loss 0.133735 Accuracy -7.4016\n",
      "Epoch 198 | train: Loss 0.173682 Accuracy -5.3695 | validation: Loss 0.131060 Accuracy -7.2336\n",
      "Epoch 199 | train: Loss 0.170452 Accuracy -5.2511 | validation: Loss 0.128480 Accuracy -7.0715\n",
      "Epoch 200 | train: Loss 0.167262 Accuracy -5.1341 | validation: Loss 0.125998 Accuracy -6.9155\n",
      "Epoch 201 | train: Loss 0.164110 Accuracy -5.0185 | validation: Loss 0.123616 Accuracy -6.7659\n",
      "Epoch 202 | train: Loss 0.160994 Accuracy -4.9042 | validation: Loss 0.121329 Accuracy -6.6222\n",
      "Epoch 203 | train: Loss 0.157917 Accuracy -4.7914 | validation: Loss 0.119135 Accuracy -6.4844\n",
      "Epoch 204 | train: Loss 0.154884 Accuracy -4.6801 | validation: Loss 0.117040 Accuracy -6.3528\n",
      "Epoch 205 | train: Loss 0.151889 Accuracy -4.5703 | validation: Loss 0.115044 Accuracy -6.2274\n",
      "Epoch 206 | train: Loss 0.148933 Accuracy -4.4619 | validation: Loss 0.113141 Accuracy -6.1078\n",
      "Epoch 207 | train: Loss 0.146017 Accuracy -4.3549 | validation: Loss 0.111330 Accuracy -5.9941\n",
      "Epoch 208 | train: Loss 0.143145 Accuracy -4.2496 | validation: Loss 0.109612 Accuracy -5.8861\n",
      "Epoch 209 | train: Loss 0.140313 Accuracy -4.1458 | validation: Loss 0.107986 Accuracy -5.7840\n",
      "Epoch 210 | train: Loss 0.137523 Accuracy -4.0434 | validation: Loss 0.106451 Accuracy -5.6875\n",
      "Epoch 211 | train: Loss 0.134773 Accuracy -3.9426 | validation: Loss 0.105006 Accuracy -5.5968\n",
      "Epoch 212 | train: Loss 0.132065 Accuracy -3.8433 | validation: Loss 0.103653 Accuracy -5.5118\n",
      "Epoch 213 | train: Loss 0.129398 Accuracy -3.7455 | validation: Loss 0.102389 Accuracy -5.4324\n",
      "Epoch 214 | train: Loss 0.126775 Accuracy -3.6493 | validation: Loss 0.101215 Accuracy -5.3586\n",
      "Epoch 215 | train: Loss 0.124192 Accuracy -3.5545 | validation: Loss 0.100129 Accuracy -5.2904\n",
      "Epoch 216 | train: Loss 0.121644 Accuracy -3.4611 | validation: Loss 0.099132 Accuracy -5.2277\n",
      "Epoch 217 | train: Loss 0.119138 Accuracy -3.3692 | validation: Loss 0.098221 Accuracy -5.1705\n",
      "Epoch 218 | train: Loss 0.116676 Accuracy -3.2789 | validation: Loss 0.097397 Accuracy -5.1187\n",
      "Epoch 219 | train: Loss 0.114257 Accuracy -3.1902 | validation: Loss 0.096658 Accuracy -5.0724\n",
      "Epoch 220 | train: Loss 0.111880 Accuracy -3.1030 | validation: Loss 0.096005 Accuracy -5.0313\n",
      "Epoch 221 | train: Loss 0.109545 Accuracy -3.0174 | validation: Loss 0.095435 Accuracy -4.9955\n",
      "Epoch 222 | train: Loss 0.107254 Accuracy -2.9334 | validation: Loss 0.094948 Accuracy -4.9649\n",
      "Epoch 223 | train: Loss 0.105008 Accuracy -2.8510 | validation: Loss 0.094543 Accuracy -4.9395\n",
      "Epoch 224 | train: Loss 0.102805 Accuracy -2.7702 | validation: Loss 0.094219 Accuracy -4.9191\n",
      "Epoch 225 | train: Loss 0.100644 Accuracy -2.6909 | validation: Loss 0.093974 Accuracy -4.9037\n",
      "Epoch 226 | train: Loss 0.098526 Accuracy -2.6133 | validation: Loss 0.093808 Accuracy -4.8933\n",
      "Epoch 227 | train: Loss 0.096452 Accuracy -2.5372 | validation: Loss 0.093718 Accuracy -4.8877\n",
      "Epoch 228 | train: Loss 0.094422 Accuracy -2.4628 | validation: Loss 0.093705 Accuracy -4.8868\n",
      "Epoch 229 | train: Loss 0.092435 Accuracy -2.3899 | validation: Loss 0.093765 Accuracy -4.8906\n",
      "Epoch 230 | train: Loss 0.090490 Accuracy -2.3186 | validation: Loss 0.093898 Accuracy -4.8990\n",
      "Epoch 231 | train: Loss 0.088589 Accuracy -2.2488 | validation: Loss 0.094103 Accuracy -4.9118\n",
      "Epoch 232 | train: Loss 0.086729 Accuracy -2.1807 | validation: Loss 0.094377 Accuracy -4.9290\n",
      "Epoch 233 | train: Loss 0.084913 Accuracy -2.1141 | validation: Loss 0.094719 Accuracy -4.9505\n",
      "Epoch 234 | train: Loss 0.083141 Accuracy -2.0491 | validation: Loss 0.095128 Accuracy -4.9762\n",
      "Epoch 235 | train: Loss 0.081410 Accuracy -1.9856 | validation: Loss 0.095601 Accuracy -5.0059\n",
      "Epoch 236 | train: Loss 0.079722 Accuracy -1.9237 | validation: Loss 0.096137 Accuracy -5.0396\n",
      "Epoch 237 | train: Loss 0.078074 Accuracy -1.8633 | validation: Loss 0.096735 Accuracy -5.0772\n",
      "Epoch 238 | train: Loss 0.076468 Accuracy -1.8044 | validation: Loss 0.097392 Accuracy -5.1184\n",
      "Epoch 239 | train: Loss 0.074904 Accuracy -1.7470 | validation: Loss 0.098106 Accuracy -5.1633\n",
      "Epoch 240 | train: Loss 0.073381 Accuracy -1.6911 | validation: Loss 0.098876 Accuracy -5.2117\n",
      "Epoch 241 | train: Loss 0.071898 Accuracy -1.6368 | validation: Loss 0.099700 Accuracy -5.2634\n",
      "Epoch 242 | train: Loss 0.070456 Accuracy -1.5839 | validation: Loss 0.100575 Accuracy -5.3184\n",
      "Epoch 243 | train: Loss 0.069053 Accuracy -1.5324 | validation: Loss 0.101500 Accuracy -5.3766\n",
      "Epoch 244 | train: Loss 0.067690 Accuracy -1.4824 | validation: Loss 0.102473 Accuracy -5.4377\n",
      "Epoch 245 | train: Loss 0.066365 Accuracy -1.4338 | validation: Loss 0.103492 Accuracy -5.5017\n",
      "Epoch 246 | train: Loss 0.065078 Accuracy -1.3866 | validation: Loss 0.104555 Accuracy -5.5684\n",
      "Epoch 247 | train: Loss 0.063830 Accuracy -1.3409 | validation: Loss 0.105659 Accuracy -5.6378\n",
      "Epoch 248 | train: Loss 0.062619 Accuracy -1.2965 | validation: Loss 0.106803 Accuracy -5.7097\n",
      "Epoch 249 | train: Loss 0.061446 Accuracy -1.2534 | validation: Loss 0.107984 Accuracy -5.7839\n",
      "Epoch 250 | train: Loss 0.060308 Accuracy -1.2117 | validation: Loss 0.109201 Accuracy -5.8604\n",
      "Epoch 251 | train: Loss 0.059206 Accuracy -1.1713 | validation: Loss 0.110452 Accuracy -5.9389\n",
      "Epoch 252 | train: Loss 0.058139 Accuracy -1.1322 | validation: Loss 0.111733 Accuracy -6.0194\n",
      "Epoch 253 | train: Loss 0.057107 Accuracy -1.0943 | validation: Loss 0.113044 Accuracy -6.1018\n",
      "Epoch 254 | train: Loss 0.056108 Accuracy -1.0577 | validation: Loss 0.114383 Accuracy -6.1858\n",
      "Epoch 255 | train: Loss 0.055143 Accuracy -1.0223 | validation: Loss 0.115746 Accuracy -6.2715\n",
      "Epoch 256 | train: Loss 0.054211 Accuracy -0.9881 | validation: Loss 0.117132 Accuracy -6.3586\n",
      "Epoch 257 | train: Loss 0.053311 Accuracy -0.9551 | validation: Loss 0.118539 Accuracy -6.4470\n",
      "Epoch 258 | train: Loss 0.052442 Accuracy -0.9232 | validation: Loss 0.119965 Accuracy -6.5366\n",
      "Epoch 259 | train: Loss 0.051603 Accuracy -0.8925 | validation: Loss 0.121408 Accuracy -6.6272\n",
      "Epoch 260 | train: Loss 0.050795 Accuracy -0.8628 | validation: Loss 0.122866 Accuracy -6.7188\n",
      "Epoch 261 | train: Loss 0.050017 Accuracy -0.8343 | validation: Loss 0.124338 Accuracy -6.8112\n",
      "Epoch 262 | train: Loss 0.049267 Accuracy -0.8068 | validation: Loss 0.125820 Accuracy -6.9044\n",
      "Epoch 263 | train: Loss 0.048545 Accuracy -0.7803 | validation: Loss 0.127311 Accuracy -6.9980\n",
      "Epoch 264 | train: Loss 0.047850 Accuracy -0.7548 | validation: Loss 0.128809 Accuracy -7.0922\n",
      "Epoch 265 | train: Loss 0.047182 Accuracy -0.7303 | validation: Loss 0.130313 Accuracy -7.1867\n",
      "Epoch 266 | train: Loss 0.046541 Accuracy -0.7068 | validation: Loss 0.131821 Accuracy -7.2814\n",
      "Epoch 267 | train: Loss 0.045924 Accuracy -0.6842 | validation: Loss 0.133330 Accuracy -7.3762\n",
      "Epoch 268 | train: Loss 0.045333 Accuracy -0.6625 | validation: Loss 0.134839 Accuracy -7.4710\n",
      "Epoch 269 | train: Loss 0.044765 Accuracy -0.6417 | validation: Loss 0.136347 Accuracy -7.5657\n",
      "Epoch 270 | train: Loss 0.044220 Accuracy -0.6217 | validation: Loss 0.137851 Accuracy -7.6602\n",
      "Epoch 271 | train: Loss 0.043698 Accuracy -0.6026 | validation: Loss 0.139350 Accuracy -7.7544\n",
      "Epoch 272 | train: Loss 0.043199 Accuracy -0.5842 | validation: Loss 0.140843 Accuracy -7.8482\n",
      "Epoch 273 | train: Loss 0.042720 Accuracy -0.5667 | validation: Loss 0.142328 Accuracy -7.9415\n",
      "Epoch 274 | train: Loss 0.042262 Accuracy -0.5499 | validation: Loss 0.143804 Accuracy -8.0342\n",
      "Epoch 275 | train: Loss 0.041823 Accuracy -0.5338 | validation: Loss 0.145269 Accuracy -8.1262\n",
      "Epoch 276 | train: Loss 0.041404 Accuracy -0.5184 | validation: Loss 0.146721 Accuracy -8.2175\n",
      "Epoch 277 | train: Loss 0.041004 Accuracy -0.5038 | validation: Loss 0.148160 Accuracy -8.3079\n",
      "Epoch 278 | train: Loss 0.040622 Accuracy -0.4897 | validation: Loss 0.149585 Accuracy -8.3974\n",
      "Epoch 279 | train: Loss 0.040256 Accuracy -0.4763 | validation: Loss 0.150993 Accuracy -8.4858\n",
      "Epoch 280 | train: Loss 0.039908 Accuracy -0.4636 | validation: Loss 0.152385 Accuracy -8.5732\n",
      "Epoch 281 | train: Loss 0.039575 Accuracy -0.4514 | validation: Loss 0.153758 Accuracy -8.6595\n",
      "Epoch 282 | train: Loss 0.039258 Accuracy -0.4397 | validation: Loss 0.155112 Accuracy -8.7446\n",
      "Epoch 283 | train: Loss 0.038956 Accuracy -0.4287 | validation: Loss 0.156446 Accuracy -8.8284\n",
      "Epoch 284 | train: Loss 0.038668 Accuracy -0.4181 | validation: Loss 0.157758 Accuracy -8.9108\n",
      "Epoch 285 | train: Loss 0.038394 Accuracy -0.4080 | validation: Loss 0.159049 Accuracy -8.9919\n",
      "Epoch 286 | train: Loss 0.038133 Accuracy -0.3985 | validation: Loss 0.160318 Accuracy -9.0716\n",
      "Epoch 287 | train: Loss 0.037885 Accuracy -0.3894 | validation: Loss 0.161562 Accuracy -9.1498\n",
      "Epoch 288 | train: Loss 0.037648 Accuracy -0.3807 | validation: Loss 0.162783 Accuracy -9.2265\n",
      "Epoch 289 | train: Loss 0.037423 Accuracy -0.3724 | validation: Loss 0.163979 Accuracy -9.3016\n",
      "Epoch 290 | train: Loss 0.037209 Accuracy -0.3646 | validation: Loss 0.165150 Accuracy -9.3752\n",
      "Epoch 291 | train: Loss 0.037005 Accuracy -0.3571 | validation: Loss 0.166294 Accuracy -9.4471\n",
      "Epoch 292 | train: Loss 0.036812 Accuracy -0.3500 | validation: Loss 0.167413 Accuracy -9.5174\n",
      "Epoch 293 | train: Loss 0.036628 Accuracy -0.3433 | validation: Loss 0.168505 Accuracy -9.5860\n",
      "Epoch 294 | train: Loss 0.036453 Accuracy -0.3368 | validation: Loss 0.169570 Accuracy -9.6529\n",
      "Epoch 295 | train: Loss 0.036287 Accuracy -0.3308 | validation: Loss 0.170607 Accuracy -9.7180\n",
      "Epoch 296 | train: Loss 0.036128 Accuracy -0.3250 | validation: Loss 0.171617 Accuracy -9.7815\n",
      "Epoch 297 | train: Loss 0.035978 Accuracy -0.3194 | validation: Loss 0.172599 Accuracy -9.8432\n",
      "Epoch 298 | train: Loss 0.035835 Accuracy -0.3142 | validation: Loss 0.173553 Accuracy -9.9031\n",
      "Epoch 299 | train: Loss 0.035699 Accuracy -0.3092 | validation: Loss 0.174479 Accuracy -9.9613\n",
      "Epoch 300 | train: Loss 0.035570 Accuracy -0.3045 | validation: Loss 0.175377 Accuracy -10.0177\n",
      "Epoch 301 | train: Loss 0.035447 Accuracy -0.3000 | validation: Loss 0.176247 Accuracy -10.0723\n",
      "Epoch 302 | train: Loss 0.035330 Accuracy -0.2957 | validation: Loss 0.177088 Accuracy -10.1252\n",
      "Epoch 303 | train: Loss 0.035219 Accuracy -0.2916 | validation: Loss 0.177901 Accuracy -10.1763\n",
      "Epoch 304 | train: Loss 0.035113 Accuracy -0.2877 | validation: Loss 0.178687 Accuracy -10.2256\n",
      "Epoch 305 | train: Loss 0.035012 Accuracy -0.2840 | validation: Loss 0.179444 Accuracy -10.2732\n",
      "Epoch 306 | train: Loss 0.034915 Accuracy -0.2805 | validation: Loss 0.180174 Accuracy -10.3191\n",
      "Epoch 307 | train: Loss 0.034824 Accuracy -0.2771 | validation: Loss 0.180876 Accuracy -10.3632\n",
      "Epoch 308 | train: Loss 0.034736 Accuracy -0.2739 | validation: Loss 0.181551 Accuracy -10.4056\n",
      "Epoch 309 | train: Loss 0.034652 Accuracy -0.2708 | validation: Loss 0.182199 Accuracy -10.4463\n",
      "Epoch 310 | train: Loss 0.034572 Accuracy -0.2679 | validation: Loss 0.182820 Accuracy -10.4853\n",
      "Epoch 311 | train: Loss 0.034495 Accuracy -0.2651 | validation: Loss 0.183415 Accuracy -10.5227\n",
      "Epoch 312 | train: Loss 0.034422 Accuracy -0.2624 | validation: Loss 0.183984 Accuracy -10.5584\n",
      "Epoch 313 | train: Loss 0.034352 Accuracy -0.2598 | validation: Loss 0.184526 Accuracy -10.5925\n",
      "Epoch 314 | train: Loss 0.034284 Accuracy -0.2573 | validation: Loss 0.185044 Accuracy -10.6250\n",
      "Epoch 315 | train: Loss 0.034219 Accuracy -0.2549 | validation: Loss 0.185536 Accuracy -10.6559\n",
      "Epoch 316 | train: Loss 0.034157 Accuracy -0.2526 | validation: Loss 0.186004 Accuracy -10.6853\n",
      "Epoch 317 | train: Loss 0.034097 Accuracy -0.2504 | validation: Loss 0.186447 Accuracy -10.7132\n",
      "Epoch 318 | train: Loss 0.034039 Accuracy -0.2483 | validation: Loss 0.186867 Accuracy -10.7396\n",
      "Epoch 319 | train: Loss 0.033983 Accuracy -0.2463 | validation: Loss 0.187264 Accuracy -10.7645\n",
      "Epoch 320 | train: Loss 0.033929 Accuracy -0.2443 | validation: Loss 0.187638 Accuracy -10.7880\n",
      "Epoch 321 | train: Loss 0.033877 Accuracy -0.2424 | validation: Loss 0.187989 Accuracy -10.8100\n",
      "Epoch 322 | train: Loss 0.033826 Accuracy -0.2405 | validation: Loss 0.188319 Accuracy -10.8308\n",
      "Epoch 323 | train: Loss 0.033777 Accuracy -0.2387 | validation: Loss 0.188627 Accuracy -10.8501\n",
      "Epoch 324 | train: Loss 0.033729 Accuracy -0.2370 | validation: Loss 0.188914 Accuracy -10.8681\n",
      "Epoch 325 | train: Loss 0.033682 Accuracy -0.2352 | validation: Loss 0.189180 Accuracy -10.8848\n",
      "Epoch 326 | train: Loss 0.033637 Accuracy -0.2336 | validation: Loss 0.189426 Accuracy -10.9003\n",
      "Epoch 327 | train: Loss 0.033593 Accuracy -0.2320 | validation: Loss 0.189653 Accuracy -10.9145\n",
      "Epoch 328 | train: Loss 0.033550 Accuracy -0.2304 | validation: Loss 0.189860 Accuracy -10.9276\n",
      "Epoch 329 | train: Loss 0.033507 Accuracy -0.2288 | validation: Loss 0.190049 Accuracy -10.9395\n",
      "Epoch 330 | train: Loss 0.033466 Accuracy -0.2273 | validation: Loss 0.190221 Accuracy -10.9502\n",
      "Epoch 331 | train: Loss 0.033425 Accuracy -0.2258 | validation: Loss 0.190374 Accuracy -10.9599\n",
      "Epoch 332 | train: Loss 0.033385 Accuracy -0.2244 | validation: Loss 0.190511 Accuracy -10.9685\n",
      "Epoch 333 | train: Loss 0.033346 Accuracy -0.2229 | validation: Loss 0.190632 Accuracy -10.9760\n",
      "Epoch 334 | train: Loss 0.033308 Accuracy -0.2215 | validation: Loss 0.190736 Accuracy -10.9826\n",
      "Epoch 335 | train: Loss 0.033270 Accuracy -0.2201 | validation: Loss 0.190825 Accuracy -10.9882\n",
      "Epoch 336 | train: Loss 0.033232 Accuracy -0.2187 | validation: Loss 0.190900 Accuracy -10.9929\n",
      "Epoch 337 | train: Loss 0.033195 Accuracy -0.2174 | validation: Loss 0.190960 Accuracy -10.9967\n",
      "Epoch 338 | train: Loss 0.033159 Accuracy -0.2161 | validation: Loss 0.191006 Accuracy -10.9996\n",
      "Epoch 339 | train: Loss 0.033123 Accuracy -0.2147 | validation: Loss 0.191039 Accuracy -11.0016\n",
      "Epoch 340 | train: Loss 0.033088 Accuracy -0.2134 | validation: Loss 0.191059 Accuracy -11.0029\n",
      "Epoch 341 | train: Loss 0.033052 Accuracy -0.2121 | validation: Loss 0.191066 Accuracy -11.0033\n",
      "Epoch 342 | train: Loss 0.033017 Accuracy -0.2109 | validation: Loss 0.191061 Accuracy -11.0030\n",
      "Epoch 343 | train: Loss 0.032983 Accuracy -0.2096 | validation: Loss 0.191045 Accuracy -11.0020\n",
      "Epoch 344 | train: Loss 0.032949 Accuracy -0.2083 | validation: Loss 0.191019 Accuracy -11.0003\n",
      "Epoch 345 | train: Loss 0.032915 Accuracy -0.2071 | validation: Loss 0.190982 Accuracy -10.9980\n",
      "Epoch 346 | train: Loss 0.032881 Accuracy -0.2059 | validation: Loss 0.190935 Accuracy -10.9951\n",
      "Epoch 347 | train: Loss 0.032848 Accuracy -0.2046 | validation: Loss 0.190879 Accuracy -10.9916\n",
      "Epoch 348 | train: Loss 0.032815 Accuracy -0.2034 | validation: Loss 0.190813 Accuracy -10.9875\n",
      "Epoch 349 | train: Loss 0.032782 Accuracy -0.2022 | validation: Loss 0.190740 Accuracy -10.9828\n",
      "Epoch 350 | train: Loss 0.032749 Accuracy -0.2010 | validation: Loss 0.190657 Accuracy -10.9776\n",
      "Epoch 351 | train: Loss 0.032716 Accuracy -0.1998 | validation: Loss 0.190567 Accuracy -10.9720\n",
      "Epoch 352 | train: Loss 0.032684 Accuracy -0.1986 | validation: Loss 0.190469 Accuracy -10.9658\n",
      "Epoch 353 | train: Loss 0.032652 Accuracy -0.1974 | validation: Loss 0.190363 Accuracy -10.9592\n",
      "Epoch 354 | train: Loss 0.032619 Accuracy -0.1963 | validation: Loss 0.190251 Accuracy -10.9521\n",
      "Epoch 355 | train: Loss 0.032587 Accuracy -0.1951 | validation: Loss 0.190132 Accuracy -10.9446\n",
      "Epoch 356 | train: Loss 0.032556 Accuracy -0.1939 | validation: Loss 0.190006 Accuracy -10.9367\n",
      "Epoch 357 | train: Loss 0.032524 Accuracy -0.1928 | validation: Loss 0.189874 Accuracy -10.9285\n",
      "Epoch 358 | train: Loss 0.032492 Accuracy -0.1916 | validation: Loss 0.189737 Accuracy -10.9198\n",
      "Epoch 359 | train: Loss 0.032461 Accuracy -0.1904 | validation: Loss 0.189594 Accuracy -10.9108\n",
      "Epoch 360 | train: Loss 0.032429 Accuracy -0.1893 | validation: Loss 0.189445 Accuracy -10.9015\n",
      "Epoch 361 | train: Loss 0.032398 Accuracy -0.1881 | validation: Loss 0.189291 Accuracy -10.8918\n",
      "Epoch 362 | train: Loss 0.032367 Accuracy -0.1870 | validation: Loss 0.189133 Accuracy -10.8819\n",
      "Epoch 363 | train: Loss 0.032336 Accuracy -0.1859 | validation: Loss 0.188970 Accuracy -10.8716\n",
      "Epoch 364 | train: Loss 0.032305 Accuracy -0.1847 | validation: Loss 0.188803 Accuracy -10.8611\n",
      "Epoch 365 | train: Loss 0.032274 Accuracy -0.1836 | validation: Loss 0.188631 Accuracy -10.8504\n",
      "Epoch 366 | train: Loss 0.032243 Accuracy -0.1825 | validation: Loss 0.188456 Accuracy -10.8393\n",
      "Epoch 367 | train: Loss 0.032212 Accuracy -0.1813 | validation: Loss 0.188277 Accuracy -10.8281\n",
      "Epoch 368 | train: Loss 0.032181 Accuracy -0.1802 | validation: Loss 0.188094 Accuracy -10.8166\n",
      "Epoch 369 | train: Loss 0.032151 Accuracy -0.1791 | validation: Loss 0.187908 Accuracy -10.8050\n",
      "Epoch 370 | train: Loss 0.032120 Accuracy -0.1780 | validation: Loss 0.187720 Accuracy -10.7931\n",
      "Epoch 371 | train: Loss 0.032090 Accuracy -0.1768 | validation: Loss 0.187528 Accuracy -10.7811\n",
      "Epoch 372 | train: Loss 0.032060 Accuracy -0.1757 | validation: Loss 0.187334 Accuracy -10.7688\n",
      "Epoch 373 | train: Loss 0.032029 Accuracy -0.1746 | validation: Loss 0.187137 Accuracy -10.7565\n",
      "Epoch 374 | train: Loss 0.031999 Accuracy -0.1735 | validation: Loss 0.186937 Accuracy -10.7439\n",
      "Epoch 375 | train: Loss 0.031969 Accuracy -0.1724 | validation: Loss 0.186736 Accuracy -10.7313\n",
      "Epoch 376 | train: Loss 0.031939 Accuracy -0.1713 | validation: Loss 0.186532 Accuracy -10.7185\n",
      "Epoch 377 | train: Loss 0.031909 Accuracy -0.1702 | validation: Loss 0.186326 Accuracy -10.7056\n",
      "Epoch 378 | train: Loss 0.031879 Accuracy -0.1691 | validation: Loss 0.186119 Accuracy -10.6925\n",
      "Epoch 379 | train: Loss 0.031849 Accuracy -0.1680 | validation: Loss 0.185910 Accuracy -10.6794\n",
      "Epoch 380 | train: Loss 0.031819 Accuracy -0.1669 | validation: Loss 0.185699 Accuracy -10.6662\n",
      "Epoch 381 | train: Loss 0.031789 Accuracy -0.1658 | validation: Loss 0.185487 Accuracy -10.6528\n",
      "Epoch 382 | train: Loss 0.031760 Accuracy -0.1647 | validation: Loss 0.185273 Accuracy -10.6394\n",
      "Epoch 383 | train: Loss 0.031730 Accuracy -0.1636 | validation: Loss 0.185059 Accuracy -10.6259\n",
      "Epoch 384 | train: Loss 0.031700 Accuracy -0.1626 | validation: Loss 0.184843 Accuracy -10.6124\n",
      "Epoch 385 | train: Loss 0.031671 Accuracy -0.1615 | validation: Loss 0.184626 Accuracy -10.5987\n",
      "Epoch 386 | train: Loss 0.031642 Accuracy -0.1604 | validation: Loss 0.184408 Accuracy -10.5850\n",
      "Epoch 387 | train: Loss 0.031612 Accuracy -0.1593 | validation: Loss 0.184189 Accuracy -10.5713\n",
      "Epoch 388 | train: Loss 0.031583 Accuracy -0.1583 | validation: Loss 0.183970 Accuracy -10.5575\n",
      "Epoch 389 | train: Loss 0.031554 Accuracy -0.1572 | validation: Loss 0.183749 Accuracy -10.5437\n",
      "Epoch 390 | train: Loss 0.031524 Accuracy -0.1561 | validation: Loss 0.183526 Accuracy -10.5297\n",
      "Epoch 391 | train: Loss 0.031494 Accuracy -0.1550 | validation: Loss 0.183301 Accuracy -10.5155\n",
      "Epoch 392 | train: Loss 0.031463 Accuracy -0.1539 | validation: Loss 0.183073 Accuracy -10.5012\n",
      "Epoch 393 | train: Loss 0.031432 Accuracy -0.1527 | validation: Loss 0.182843 Accuracy -10.4867\n",
      "Epoch 394 | train: Loss 0.031402 Accuracy -0.1516 | validation: Loss 0.182611 Accuracy -10.4721\n",
      "Epoch 395 | train: Loss 0.031371 Accuracy -0.1505 | validation: Loss 0.182377 Accuracy -10.4574\n",
      "Epoch 396 | train: Loss 0.031340 Accuracy -0.1493 | validation: Loss 0.182141 Accuracy -10.4426\n",
      "Epoch 397 | train: Loss 0.031309 Accuracy -0.1482 | validation: Loss 0.181904 Accuracy -10.4277\n",
      "Epoch 398 | train: Loss 0.031278 Accuracy -0.1471 | validation: Loss 0.181665 Accuracy -10.4127\n",
      "Epoch 399 | train: Loss 0.031247 Accuracy -0.1459 | validation: Loss 0.181425 Accuracy -10.3977\n",
      "Epoch 400 | train: Loss 0.031216 Accuracy -0.1448 | validation: Loss 0.181185 Accuracy -10.3826\n",
      "Epoch 401 | train: Loss 0.031185 Accuracy -0.1437 | validation: Loss 0.180943 Accuracy -10.3674\n",
      "Epoch 402 | train: Loss 0.031154 Accuracy -0.1425 | validation: Loss 0.180701 Accuracy -10.3522\n",
      "Epoch 403 | train: Loss 0.031123 Accuracy -0.1414 | validation: Loss 0.180458 Accuracy -10.3369\n",
      "Epoch 404 | train: Loss 0.031091 Accuracy -0.1402 | validation: Loss 0.180213 Accuracy -10.3215\n",
      "Epoch 405 | train: Loss 0.031059 Accuracy -0.1390 | validation: Loss 0.179967 Accuracy -10.3060\n",
      "Epoch 406 | train: Loss 0.031027 Accuracy -0.1379 | validation: Loss 0.179720 Accuracy -10.2905\n",
      "Epoch 407 | train: Loss 0.030996 Accuracy -0.1367 | validation: Loss 0.179472 Accuracy -10.2750\n",
      "Epoch 408 | train: Loss 0.030964 Accuracy -0.1355 | validation: Loss 0.179224 Accuracy -10.2594\n",
      "Epoch 409 | train: Loss 0.030932 Accuracy -0.1344 | validation: Loss 0.178974 Accuracy -10.2437\n",
      "Epoch 410 | train: Loss 0.030899 Accuracy -0.1332 | validation: Loss 0.178724 Accuracy -10.2279\n",
      "Epoch 411 | train: Loss 0.030867 Accuracy -0.1320 | validation: Loss 0.178472 Accuracy -10.2122\n",
      "Epoch 412 | train: Loss 0.030834 Accuracy -0.1308 | validation: Loss 0.178220 Accuracy -10.1963\n",
      "Epoch 413 | train: Loss 0.030801 Accuracy -0.1296 | validation: Loss 0.177967 Accuracy -10.1804\n",
      "Epoch 414 | train: Loss 0.030769 Accuracy -0.1284 | validation: Loss 0.177714 Accuracy -10.1645\n",
      "Epoch 415 | train: Loss 0.030735 Accuracy -0.1271 | validation: Loss 0.177457 Accuracy -10.1484\n",
      "Epoch 416 | train: Loss 0.030700 Accuracy -0.1259 | validation: Loss 0.177198 Accuracy -10.1321\n",
      "Epoch 417 | train: Loss 0.030665 Accuracy -0.1246 | validation: Loss 0.176937 Accuracy -10.1157\n",
      "Epoch 418 | train: Loss 0.030630 Accuracy -0.1233 | validation: Loss 0.176674 Accuracy -10.0992\n",
      "Epoch 419 | train: Loss 0.030594 Accuracy -0.1220 | validation: Loss 0.176410 Accuracy -10.0826\n",
      "Epoch 420 | train: Loss 0.030559 Accuracy -0.1207 | validation: Loss 0.176144 Accuracy -10.0659\n",
      "Epoch 421 | train: Loss 0.030523 Accuracy -0.1194 | validation: Loss 0.175877 Accuracy -10.0491\n",
      "Epoch 422 | train: Loss 0.030488 Accuracy -0.1181 | validation: Loss 0.175610 Accuracy -10.0324\n",
      "Epoch 423 | train: Loss 0.030452 Accuracy -0.1168 | validation: Loss 0.175343 Accuracy -10.0155\n",
      "Epoch 424 | train: Loss 0.030416 Accuracy -0.1155 | validation: Loss 0.175075 Accuracy -9.9987\n",
      "Epoch 425 | train: Loss 0.030380 Accuracy -0.1141 | validation: Loss 0.174806 Accuracy -9.9818\n",
      "Epoch 426 | train: Loss 0.030344 Accuracy -0.1128 | validation: Loss 0.174537 Accuracy -9.9649\n",
      "Epoch 427 | train: Loss 0.030307 Accuracy -0.1115 | validation: Loss 0.174267 Accuracy -9.9479\n",
      "Epoch 428 | train: Loss 0.030271 Accuracy -0.1101 | validation: Loss 0.173997 Accuracy -9.9310\n",
      "Epoch 429 | train: Loss 0.030234 Accuracy -0.1088 | validation: Loss 0.173727 Accuracy -9.9140\n",
      "Epoch 430 | train: Loss 0.030197 Accuracy -0.1074 | validation: Loss 0.173457 Accuracy -9.8971\n",
      "Epoch 431 | train: Loss 0.030161 Accuracy -0.1061 | validation: Loss 0.173187 Accuracy -9.8801\n",
      "Epoch 432 | train: Loss 0.030124 Accuracy -0.1047 | validation: Loss 0.172918 Accuracy -9.8632\n",
      "Epoch 433 | train: Loss 0.030087 Accuracy -0.1034 | validation: Loss 0.172650 Accuracy -9.8464\n",
      "Epoch 434 | train: Loss 0.030050 Accuracy -0.1020 | validation: Loss 0.172382 Accuracy -9.8295\n",
      "Epoch 435 | train: Loss 0.030013 Accuracy -0.1007 | validation: Loss 0.172114 Accuracy -9.8127\n",
      "Epoch 436 | train: Loss 0.029976 Accuracy -0.0993 | validation: Loss 0.171848 Accuracy -9.7960\n",
      "Epoch 437 | train: Loss 0.029940 Accuracy -0.0980 | validation: Loss 0.171582 Accuracy -9.7793\n",
      "Epoch 438 | train: Loss 0.029903 Accuracy -0.0966 | validation: Loss 0.171317 Accuracy -9.7626\n",
      "Epoch 439 | train: Loss 0.029867 Accuracy -0.0953 | validation: Loss 0.171053 Accuracy -9.7460\n",
      "Epoch 440 | train: Loss 0.029830 Accuracy -0.0940 | validation: Loss 0.170790 Accuracy -9.7295\n",
      "Epoch 441 | train: Loss 0.029792 Accuracy -0.0926 | validation: Loss 0.170525 Accuracy -9.7129\n",
      "Epoch 442 | train: Loss 0.029754 Accuracy -0.0912 | validation: Loss 0.170261 Accuracy -9.6963\n",
      "Epoch 443 | train: Loss 0.029715 Accuracy -0.0898 | validation: Loss 0.169996 Accuracy -9.6796\n",
      "Epoch 444 | train: Loss 0.029676 Accuracy -0.0883 | validation: Loss 0.169731 Accuracy -9.6630\n",
      "Epoch 445 | train: Loss 0.029638 Accuracy -0.0869 | validation: Loss 0.169466 Accuracy -9.6464\n",
      "Epoch 446 | train: Loss 0.029599 Accuracy -0.0855 | validation: Loss 0.169202 Accuracy -9.6297\n",
      "Epoch 447 | train: Loss 0.029560 Accuracy -0.0841 | validation: Loss 0.168938 Accuracy -9.6132\n",
      "Epoch 448 | train: Loss 0.029522 Accuracy -0.0827 | validation: Loss 0.168674 Accuracy -9.5966\n",
      "Epoch 449 | train: Loss 0.029483 Accuracy -0.0812 | validation: Loss 0.168409 Accuracy -9.5800\n",
      "Epoch 450 | train: Loss 0.029444 Accuracy -0.0798 | validation: Loss 0.168146 Accuracy -9.5634\n",
      "Epoch 451 | train: Loss 0.029405 Accuracy -0.0784 | validation: Loss 0.167883 Accuracy -9.5469\n",
      "Epoch 452 | train: Loss 0.029366 Accuracy -0.0769 | validation: Loss 0.167620 Accuracy -9.5304\n",
      "Epoch 453 | train: Loss 0.029326 Accuracy -0.0755 | validation: Loss 0.167357 Accuracy -9.5139\n",
      "Epoch 454 | train: Loss 0.029287 Accuracy -0.0740 | validation: Loss 0.167095 Accuracy -9.4974\n",
      "Epoch 455 | train: Loss 0.029247 Accuracy -0.0726 | validation: Loss 0.166834 Accuracy -9.4810\n",
      "Epoch 456 | train: Loss 0.029208 Accuracy -0.0711 | validation: Loss 0.166573 Accuracy -9.4646\n",
      "Epoch 457 | train: Loss 0.029168 Accuracy -0.0697 | validation: Loss 0.166313 Accuracy -9.4483\n",
      "Epoch 458 | train: Loss 0.029128 Accuracy -0.0682 | validation: Loss 0.166052 Accuracy -9.4319\n",
      "Epoch 459 | train: Loss 0.029089 Accuracy -0.0668 | validation: Loss 0.165791 Accuracy -9.4155\n",
      "Epoch 460 | train: Loss 0.029050 Accuracy -0.0654 | validation: Loss 0.165533 Accuracy -9.3993\n",
      "Epoch 461 | train: Loss 0.029012 Accuracy -0.0640 | validation: Loss 0.165277 Accuracy -9.3832\n",
      "Epoch 462 | train: Loss 0.028974 Accuracy -0.0626 | validation: Loss 0.165023 Accuracy -9.3672\n",
      "Epoch 463 | train: Loss 0.028936 Accuracy -0.0612 | validation: Loss 0.164771 Accuracy -9.3514\n",
      "Epoch 464 | train: Loss 0.028898 Accuracy -0.0598 | validation: Loss 0.164522 Accuracy -9.3357\n",
      "Epoch 465 | train: Loss 0.028860 Accuracy -0.0584 | validation: Loss 0.164274 Accuracy -9.3202\n",
      "Epoch 466 | train: Loss 0.028822 Accuracy -0.0570 | validation: Loss 0.164027 Accuracy -9.3047\n",
      "Epoch 467 | train: Loss 0.028785 Accuracy -0.0556 | validation: Loss 0.163780 Accuracy -9.2891\n",
      "Epoch 468 | train: Loss 0.028747 Accuracy -0.0542 | validation: Loss 0.163534 Accuracy -9.2737\n",
      "Epoch 469 | train: Loss 0.028709 Accuracy -0.0529 | validation: Loss 0.163290 Accuracy -9.2584\n",
      "Epoch 470 | train: Loss 0.028671 Accuracy -0.0515 | validation: Loss 0.163047 Accuracy -9.2431\n",
      "Epoch 471 | train: Loss 0.028634 Accuracy -0.0501 | validation: Loss 0.162807 Accuracy -9.2280\n",
      "Epoch 472 | train: Loss 0.028598 Accuracy -0.0488 | validation: Loss 0.162569 Accuracy -9.2131\n",
      "Epoch 473 | train: Loss 0.028562 Accuracy -0.0474 | validation: Loss 0.162334 Accuracy -9.1983\n",
      "Epoch 474 | train: Loss 0.028525 Accuracy -0.0461 | validation: Loss 0.162101 Accuracy -9.1836\n",
      "Epoch 475 | train: Loss 0.028489 Accuracy -0.0448 | validation: Loss 0.161870 Accuracy -9.1691\n",
      "Epoch 476 | train: Loss 0.028454 Accuracy -0.0435 | validation: Loss 0.161642 Accuracy -9.1548\n",
      "Epoch 477 | train: Loss 0.028418 Accuracy -0.0422 | validation: Loss 0.161417 Accuracy -9.1407\n",
      "Epoch 478 | train: Loss 0.028382 Accuracy -0.0409 | validation: Loss 0.161193 Accuracy -9.1266\n",
      "Epoch 479 | train: Loss 0.028347 Accuracy -0.0396 | validation: Loss 0.160973 Accuracy -9.1128\n",
      "Epoch 480 | train: Loss 0.028312 Accuracy -0.0383 | validation: Loss 0.160754 Accuracy -9.0990\n",
      "Epoch 481 | train: Loss 0.028278 Accuracy -0.0370 | validation: Loss 0.160537 Accuracy -9.0854\n",
      "Epoch 482 | train: Loss 0.028243 Accuracy -0.0358 | validation: Loss 0.160322 Accuracy -9.0719\n",
      "Epoch 483 | train: Loss 0.028211 Accuracy -0.0346 | validation: Loss 0.160112 Accuracy -9.0587\n",
      "Epoch 484 | train: Loss 0.028179 Accuracy -0.0334 | validation: Loss 0.159905 Accuracy -9.0457\n",
      "Epoch 485 | train: Loss 0.028148 Accuracy -0.0323 | validation: Loss 0.159703 Accuracy -9.0330\n",
      "Epoch 486 | train: Loss 0.028117 Accuracy -0.0311 | validation: Loss 0.159505 Accuracy -9.0206\n",
      "Epoch 487 | train: Loss 0.028086 Accuracy -0.0300 | validation: Loss 0.159310 Accuracy -9.0083\n",
      "Epoch 488 | train: Loss 0.028055 Accuracy -0.0289 | validation: Loss 0.159118 Accuracy -8.9963\n",
      "Epoch 489 | train: Loss 0.028025 Accuracy -0.0278 | validation: Loss 0.158929 Accuracy -8.9844\n",
      "Epoch 490 | train: Loss 0.027995 Accuracy -0.0267 | validation: Loss 0.158742 Accuracy -8.9727\n",
      "Epoch 491 | train: Loss 0.027965 Accuracy -0.0256 | validation: Loss 0.158558 Accuracy -8.9611\n",
      "Epoch 492 | train: Loss 0.027935 Accuracy -0.0245 | validation: Loss 0.158376 Accuracy -8.9497\n",
      "Epoch 493 | train: Loss 0.027906 Accuracy -0.0234 | validation: Loss 0.158196 Accuracy -8.9383\n",
      "Epoch 494 | train: Loss 0.027877 Accuracy -0.0223 | validation: Loss 0.158018 Accuracy -8.9272\n",
      "Epoch 495 | train: Loss 0.027847 Accuracy -0.0213 | validation: Loss 0.157842 Accuracy -8.9161\n",
      "Epoch 496 | train: Loss 0.027818 Accuracy -0.0202 | validation: Loss 0.157666 Accuracy -8.9051\n",
      "Epoch 497 | train: Loss 0.027789 Accuracy -0.0191 | validation: Loss 0.157492 Accuracy -8.8941\n",
      "Epoch 498 | train: Loss 0.027760 Accuracy -0.0181 | validation: Loss 0.157320 Accuracy -8.8833\n",
      "Epoch 499 | train: Loss 0.027732 Accuracy -0.0170 | validation: Loss 0.157150 Accuracy -8.8726\n",
      "Epoch 500 | train: Loss 0.027704 Accuracy -0.0160 | validation: Loss 0.156981 Accuracy -8.8620\n",
      "Epoch 501 | train: Loss 0.027676 Accuracy -0.0150 | validation: Loss 0.156814 Accuracy -8.8515\n",
      "Epoch 502 | train: Loss 0.027648 Accuracy -0.0139 | validation: Loss 0.156648 Accuracy -8.8411\n",
      "Epoch 503 | train: Loss 0.027620 Accuracy -0.0129 | validation: Loss 0.156484 Accuracy -8.8308\n",
      "Epoch 504 | train: Loss 0.027593 Accuracy -0.0119 | validation: Loss 0.156321 Accuracy -8.8206\n",
      "Epoch 505 | train: Loss 0.027565 Accuracy -0.0109 | validation: Loss 0.156160 Accuracy -8.8104\n",
      "Epoch 506 | train: Loss 0.027539 Accuracy -0.0099 | validation: Loss 0.156001 Accuracy -8.8004\n",
      "Epoch 507 | train: Loss 0.027512 Accuracy -0.0090 | validation: Loss 0.155845 Accuracy -8.7907\n",
      "Epoch 508 | train: Loss 0.027486 Accuracy -0.0080 | validation: Loss 0.155693 Accuracy -8.7811\n",
      "Epoch 509 | train: Loss 0.027459 Accuracy -0.0070 | validation: Loss 0.155541 Accuracy -8.7716\n",
      "Epoch 510 | train: Loss 0.027433 Accuracy -0.0061 | validation: Loss 0.155392 Accuracy -8.7622\n",
      "Epoch 511 | train: Loss 0.027407 Accuracy -0.0051 | validation: Loss 0.155244 Accuracy -8.7529\n",
      "Epoch 512 | train: Loss 0.027382 Accuracy -0.0042 | validation: Loss 0.155096 Accuracy -8.7436\n",
      "Epoch 513 | train: Loss 0.027356 Accuracy -0.0032 | validation: Loss 0.154950 Accuracy -8.7344\n",
      "Epoch 514 | train: Loss 0.027330 Accuracy -0.0023 | validation: Loss 0.154805 Accuracy -8.7253\n",
      "Epoch 515 | train: Loss 0.027305 Accuracy -0.0014 | validation: Loss 0.154661 Accuracy -8.7163\n",
      "Epoch 516 | train: Loss 0.027279 Accuracy -0.0004 | validation: Loss 0.154519 Accuracy -8.7073\n",
      "Epoch 517 | train: Loss 0.027255 Accuracy 0.0005 | validation: Loss 0.154378 Accuracy -8.6985\n",
      "Epoch 518 | train: Loss 0.027230 Accuracy 0.0014 | validation: Loss 0.154239 Accuracy -8.6897\n",
      "Epoch 519 | train: Loss 0.027206 Accuracy 0.0023 | validation: Loss 0.154101 Accuracy -8.6811\n",
      "Epoch 520 | train: Loss 0.027182 Accuracy 0.0032 | validation: Loss 0.153965 Accuracy -8.6725\n",
      "Epoch 521 | train: Loss 0.027159 Accuracy 0.0040 | validation: Loss 0.153833 Accuracy -8.6642\n",
      "Epoch 522 | train: Loss 0.027136 Accuracy 0.0048 | validation: Loss 0.153703 Accuracy -8.6561\n",
      "Epoch 523 | train: Loss 0.027114 Accuracy 0.0057 | validation: Loss 0.153576 Accuracy -8.6481\n",
      "Epoch 524 | train: Loss 0.027091 Accuracy 0.0065 | validation: Loss 0.153451 Accuracy -8.6402\n",
      "Epoch 525 | train: Loss 0.027069 Accuracy 0.0073 | validation: Loss 0.153328 Accuracy -8.6325\n",
      "Epoch 526 | train: Loss 0.027047 Accuracy 0.0081 | validation: Loss 0.153206 Accuracy -8.6249\n",
      "Epoch 527 | train: Loss 0.027026 Accuracy 0.0089 | validation: Loss 0.153086 Accuracy -8.6173\n",
      "Epoch 528 | train: Loss 0.027004 Accuracy 0.0097 | validation: Loss 0.152967 Accuracy -8.6099\n",
      "Epoch 529 | train: Loss 0.026982 Accuracy 0.0105 | validation: Loss 0.152850 Accuracy -8.6025\n",
      "Epoch 530 | train: Loss 0.026961 Accuracy 0.0112 | validation: Loss 0.152734 Accuracy -8.5952\n",
      "Epoch 531 | train: Loss 0.026940 Accuracy 0.0120 | validation: Loss 0.152618 Accuracy -8.5879\n",
      "Epoch 532 | train: Loss 0.026919 Accuracy 0.0128 | validation: Loss 0.152504 Accuracy -8.5807\n",
      "Epoch 533 | train: Loss 0.026897 Accuracy 0.0136 | validation: Loss 0.152390 Accuracy -8.5736\n",
      "Epoch 534 | train: Loss 0.026876 Accuracy 0.0144 | validation: Loss 0.152276 Accuracy -8.5664\n",
      "Epoch 535 | train: Loss 0.026855 Accuracy 0.0151 | validation: Loss 0.152162 Accuracy -8.5593\n",
      "Epoch 536 | train: Loss 0.026833 Accuracy 0.0159 | validation: Loss 0.152048 Accuracy -8.5521\n",
      "Epoch 537 | train: Loss 0.026812 Accuracy 0.0167 | validation: Loss 0.151934 Accuracy -8.5449\n",
      "Epoch 538 | train: Loss 0.026790 Accuracy 0.0175 | validation: Loss 0.151819 Accuracy -8.5377\n",
      "Epoch 539 | train: Loss 0.026769 Accuracy 0.0183 | validation: Loss 0.151705 Accuracy -8.5306\n",
      "Epoch 540 | train: Loss 0.026748 Accuracy 0.0191 | validation: Loss 0.151591 Accuracy -8.5234\n",
      "Epoch 541 | train: Loss 0.026727 Accuracy 0.0198 | validation: Loss 0.151478 Accuracy -8.5163\n",
      "Epoch 542 | train: Loss 0.026706 Accuracy 0.0206 | validation: Loss 0.151363 Accuracy -8.5091\n",
      "Epoch 543 | train: Loss 0.026685 Accuracy 0.0214 | validation: Loss 0.151249 Accuracy -8.5019\n",
      "Epoch 544 | train: Loss 0.026664 Accuracy 0.0222 | validation: Loss 0.151134 Accuracy -8.4947\n",
      "Epoch 545 | train: Loss 0.026642 Accuracy 0.0229 | validation: Loss 0.151019 Accuracy -8.4875\n",
      "Epoch 546 | train: Loss 0.026621 Accuracy 0.0237 | validation: Loss 0.150904 Accuracy -8.4802\n",
      "Epoch 547 | train: Loss 0.026600 Accuracy 0.0245 | validation: Loss 0.150789 Accuracy -8.4730\n",
      "Epoch 548 | train: Loss 0.026579 Accuracy 0.0252 | validation: Loss 0.150673 Accuracy -8.4657\n",
      "Epoch 549 | train: Loss 0.026558 Accuracy 0.0260 | validation: Loss 0.150557 Accuracy -8.4585\n",
      "Epoch 550 | train: Loss 0.026537 Accuracy 0.0268 | validation: Loss 0.150442 Accuracy -8.4512\n",
      "Epoch 551 | train: Loss 0.026516 Accuracy 0.0276 | validation: Loss 0.150326 Accuracy -8.4439\n",
      "Epoch 552 | train: Loss 0.026495 Accuracy 0.0283 | validation: Loss 0.150211 Accuracy -8.4367\n",
      "Epoch 553 | train: Loss 0.026475 Accuracy 0.0291 | validation: Loss 0.150095 Accuracy -8.4294\n",
      "Epoch 554 | train: Loss 0.026454 Accuracy 0.0298 | validation: Loss 0.149979 Accuracy -8.4221\n",
      "Epoch 555 | train: Loss 0.026433 Accuracy 0.0306 | validation: Loss 0.149864 Accuracy -8.4149\n",
      "Epoch 556 | train: Loss 0.026412 Accuracy 0.0314 | validation: Loss 0.149752 Accuracy -8.4078\n",
      "Epoch 557 | train: Loss 0.026391 Accuracy 0.0321 | validation: Loss 0.149639 Accuracy -8.4008\n",
      "Epoch 558 | train: Loss 0.026371 Accuracy 0.0329 | validation: Loss 0.149526 Accuracy -8.3937\n",
      "Epoch 559 | train: Loss 0.026350 Accuracy 0.0337 | validation: Loss 0.149414 Accuracy -8.3866\n",
      "Epoch 560 | train: Loss 0.026329 Accuracy 0.0344 | validation: Loss 0.149302 Accuracy -8.3796\n",
      "Epoch 561 | train: Loss 0.026309 Accuracy 0.0352 | validation: Loss 0.149190 Accuracy -8.3726\n",
      "Epoch 562 | train: Loss 0.026288 Accuracy 0.0359 | validation: Loss 0.149079 Accuracy -8.3656\n",
      "Epoch 563 | train: Loss 0.026268 Accuracy 0.0367 | validation: Loss 0.148968 Accuracy -8.3586\n",
      "Epoch 564 | train: Loss 0.026248 Accuracy 0.0374 | validation: Loss 0.148857 Accuracy -8.3517\n",
      "Epoch 565 | train: Loss 0.026228 Accuracy 0.0381 | validation: Loss 0.148748 Accuracy -8.3448\n",
      "Epoch 566 | train: Loss 0.026208 Accuracy 0.0389 | validation: Loss 0.148639 Accuracy -8.3380\n",
      "Epoch 567 | train: Loss 0.026188 Accuracy 0.0396 | validation: Loss 0.148531 Accuracy -8.3312\n",
      "Epoch 568 | train: Loss 0.026169 Accuracy 0.0403 | validation: Loss 0.148425 Accuracy -8.3245\n",
      "Epoch 569 | train: Loss 0.026149 Accuracy 0.0410 | validation: Loss 0.148319 Accuracy -8.3178\n",
      "Epoch 570 | train: Loss 0.026130 Accuracy 0.0417 | validation: Loss 0.148213 Accuracy -8.3112\n",
      "Epoch 571 | train: Loss 0.026110 Accuracy 0.0424 | validation: Loss 0.148109 Accuracy -8.3046\n",
      "Epoch 572 | train: Loss 0.026091 Accuracy 0.0432 | validation: Loss 0.148005 Accuracy -8.2981\n",
      "Epoch 573 | train: Loss 0.026072 Accuracy 0.0439 | validation: Loss 0.147901 Accuracy -8.2916\n",
      "Epoch 574 | train: Loss 0.026052 Accuracy 0.0446 | validation: Loss 0.147797 Accuracy -8.2851\n",
      "Epoch 575 | train: Loss 0.026033 Accuracy 0.0453 | validation: Loss 0.147694 Accuracy -8.2786\n",
      "Epoch 576 | train: Loss 0.026014 Accuracy 0.0460 | validation: Loss 0.147591 Accuracy -8.2721\n",
      "Epoch 577 | train: Loss 0.025995 Accuracy 0.0467 | validation: Loss 0.147489 Accuracy -8.2657\n",
      "Epoch 578 | train: Loss 0.025976 Accuracy 0.0474 | validation: Loss 0.147387 Accuracy -8.2593\n",
      "Epoch 579 | train: Loss 0.025957 Accuracy 0.0481 | validation: Loss 0.147285 Accuracy -8.2529\n",
      "Epoch 580 | train: Loss 0.025937 Accuracy 0.0488 | validation: Loss 0.147182 Accuracy -8.2464\n",
      "Epoch 581 | train: Loss 0.025918 Accuracy 0.0495 | validation: Loss 0.147080 Accuracy -8.2400\n",
      "Epoch 582 | train: Loss 0.025898 Accuracy 0.0502 | validation: Loss 0.146978 Accuracy -8.2336\n",
      "Epoch 583 | train: Loss 0.025879 Accuracy 0.0509 | validation: Loss 0.146875 Accuracy -8.2271\n",
      "Epoch 584 | train: Loss 0.025860 Accuracy 0.0516 | validation: Loss 0.146772 Accuracy -8.2207\n",
      "Epoch 585 | train: Loss 0.025840 Accuracy 0.0524 | validation: Loss 0.146669 Accuracy -8.2142\n",
      "Epoch 586 | train: Loss 0.025821 Accuracy 0.0531 | validation: Loss 0.146566 Accuracy -8.2077\n",
      "Epoch 587 | train: Loss 0.025801 Accuracy 0.0538 | validation: Loss 0.146464 Accuracy -8.2013\n",
      "Epoch 588 | train: Loss 0.025782 Accuracy 0.0545 | validation: Loss 0.146361 Accuracy -8.1948\n",
      "Epoch 589 | train: Loss 0.025762 Accuracy 0.0552 | validation: Loss 0.146258 Accuracy -8.1884\n",
      "Epoch 590 | train: Loss 0.025743 Accuracy 0.0559 | validation: Loss 0.146156 Accuracy -8.1819\n",
      "Epoch 591 | train: Loss 0.025724 Accuracy 0.0566 | validation: Loss 0.146053 Accuracy -8.1755\n",
      "Epoch 592 | train: Loss 0.025704 Accuracy 0.0573 | validation: Loss 0.145952 Accuracy -8.1691\n",
      "Epoch 593 | train: Loss 0.025685 Accuracy 0.0580 | validation: Loss 0.145850 Accuracy -8.1627\n",
      "Epoch 594 | train: Loss 0.025666 Accuracy 0.0588 | validation: Loss 0.145749 Accuracy -8.1564\n",
      "Epoch 595 | train: Loss 0.025646 Accuracy 0.0595 | validation: Loss 0.145648 Accuracy -8.1500\n",
      "Epoch 596 | train: Loss 0.025627 Accuracy 0.0602 | validation: Loss 0.145548 Accuracy -8.1437\n",
      "Epoch 597 | train: Loss 0.025608 Accuracy 0.0609 | validation: Loss 0.145447 Accuracy -8.1374\n",
      "Epoch 598 | train: Loss 0.025589 Accuracy 0.0616 | validation: Loss 0.145348 Accuracy -8.1312\n",
      "Epoch 599 | train: Loss 0.025570 Accuracy 0.0623 | validation: Loss 0.145248 Accuracy -8.1249\n",
      "Epoch 600 | train: Loss 0.025551 Accuracy 0.0630 | validation: Loss 0.145150 Accuracy -8.1187\n",
      "Epoch 601 | train: Loss 0.025532 Accuracy 0.0637 | validation: Loss 0.145052 Accuracy -8.1126\n",
      "Epoch 602 | train: Loss 0.025513 Accuracy 0.0644 | validation: Loss 0.144955 Accuracy -8.1065\n",
      "Epoch 603 | train: Loss 0.025494 Accuracy 0.0650 | validation: Loss 0.144858 Accuracy -8.1004\n",
      "Epoch 604 | train: Loss 0.025475 Accuracy 0.0657 | validation: Loss 0.144763 Accuracy -8.0944\n",
      "Epoch 605 | train: Loss 0.025457 Accuracy 0.0664 | validation: Loss 0.144667 Accuracy -8.0884\n",
      "Epoch 606 | train: Loss 0.025438 Accuracy 0.0671 | validation: Loss 0.144573 Accuracy -8.0825\n",
      "Epoch 607 | train: Loss 0.025419 Accuracy 0.0678 | validation: Loss 0.144478 Accuracy -8.0766\n",
      "Epoch 608 | train: Loss 0.025401 Accuracy 0.0685 | validation: Loss 0.144385 Accuracy -8.0707\n",
      "Epoch 609 | train: Loss 0.025382 Accuracy 0.0692 | validation: Loss 0.144292 Accuracy -8.0649\n",
      "Epoch 610 | train: Loss 0.025363 Accuracy 0.0698 | validation: Loss 0.144200 Accuracy -8.0591\n",
      "Epoch 611 | train: Loss 0.025345 Accuracy 0.0705 | validation: Loss 0.144108 Accuracy -8.0533\n",
      "Epoch 612 | train: Loss 0.025327 Accuracy 0.0712 | validation: Loss 0.144016 Accuracy -8.0475\n",
      "Epoch 613 | train: Loss 0.025308 Accuracy 0.0719 | validation: Loss 0.143925 Accuracy -8.0418\n",
      "Epoch 614 | train: Loss 0.025290 Accuracy 0.0725 | validation: Loss 0.143835 Accuracy -8.0361\n",
      "Epoch 615 | train: Loss 0.025272 Accuracy 0.0732 | validation: Loss 0.143744 Accuracy -8.0304\n",
      "Epoch 616 | train: Loss 0.025254 Accuracy 0.0739 | validation: Loss 0.143655 Accuracy -8.0248\n",
      "Epoch 617 | train: Loss 0.025236 Accuracy 0.0745 | validation: Loss 0.143565 Accuracy -8.0192\n",
      "Epoch 618 | train: Loss 0.025217 Accuracy 0.0752 | validation: Loss 0.143476 Accuracy -8.0136\n",
      "Epoch 619 | train: Loss 0.025199 Accuracy 0.0759 | validation: Loss 0.143388 Accuracy -8.0080\n",
      "Epoch 620 | train: Loss 0.025181 Accuracy 0.0765 | validation: Loss 0.143300 Accuracy -8.0025\n",
      "Epoch 621 | train: Loss 0.025163 Accuracy 0.0772 | validation: Loss 0.143212 Accuracy -7.9970\n",
      "Epoch 622 | train: Loss 0.025146 Accuracy 0.0778 | validation: Loss 0.143125 Accuracy -7.9915\n",
      "Epoch 623 | train: Loss 0.025128 Accuracy 0.0785 | validation: Loss 0.143038 Accuracy -7.9861\n",
      "Epoch 624 | train: Loss 0.025110 Accuracy 0.0791 | validation: Loss 0.142952 Accuracy -7.9806\n",
      "Epoch 625 | train: Loss 0.025092 Accuracy 0.0798 | validation: Loss 0.142865 Accuracy -7.9752\n",
      "Epoch 626 | train: Loss 0.025074 Accuracy 0.0804 | validation: Loss 0.142779 Accuracy -7.9698\n",
      "Epoch 627 | train: Loss 0.025057 Accuracy 0.0811 | validation: Loss 0.142694 Accuracy -7.9644\n",
      "Epoch 628 | train: Loss 0.025039 Accuracy 0.0817 | validation: Loss 0.142609 Accuracy -7.9591\n",
      "Epoch 629 | train: Loss 0.025021 Accuracy 0.0824 | validation: Loss 0.142524 Accuracy -7.9538\n",
      "Epoch 630 | train: Loss 0.025004 Accuracy 0.0830 | validation: Loss 0.142439 Accuracy -7.9485\n",
      "Epoch 631 | train: Loss 0.024986 Accuracy 0.0837 | validation: Loss 0.142355 Accuracy -7.9431\n",
      "Epoch 632 | train: Loss 0.024968 Accuracy 0.0843 | validation: Loss 0.142271 Accuracy -7.9379\n",
      "Epoch 633 | train: Loss 0.024951 Accuracy 0.0850 | validation: Loss 0.142186 Accuracy -7.9326\n",
      "Epoch 634 | train: Loss 0.024933 Accuracy 0.0856 | validation: Loss 0.142103 Accuracy -7.9273\n",
      "Epoch 635 | train: Loss 0.024916 Accuracy 0.0863 | validation: Loss 0.142019 Accuracy -7.9220\n",
      "Epoch 636 | train: Loss 0.024898 Accuracy 0.0869 | validation: Loss 0.141935 Accuracy -7.9168\n",
      "Epoch 637 | train: Loss 0.024881 Accuracy 0.0875 | validation: Loss 0.141852 Accuracy -7.9116\n",
      "Epoch 638 | train: Loss 0.024863 Accuracy 0.0882 | validation: Loss 0.141769 Accuracy -7.9063\n",
      "Epoch 639 | train: Loss 0.024846 Accuracy 0.0888 | validation: Loss 0.141686 Accuracy -7.9011\n",
      "Epoch 640 | train: Loss 0.024828 Accuracy 0.0895 | validation: Loss 0.141603 Accuracy -7.8959\n",
      "Epoch 641 | train: Loss 0.024811 Accuracy 0.0901 | validation: Loss 0.141520 Accuracy -7.8907\n",
      "Epoch 642 | train: Loss 0.024794 Accuracy 0.0907 | validation: Loss 0.141439 Accuracy -7.8856\n",
      "Epoch 643 | train: Loss 0.024777 Accuracy 0.0914 | validation: Loss 0.141358 Accuracy -7.8805\n",
      "Epoch 644 | train: Loss 0.024760 Accuracy 0.0920 | validation: Loss 0.141278 Accuracy -7.8755\n",
      "Epoch 645 | train: Loss 0.024743 Accuracy 0.0926 | validation: Loss 0.141199 Accuracy -7.8705\n",
      "Epoch 646 | train: Loss 0.024727 Accuracy 0.0932 | validation: Loss 0.141120 Accuracy -7.8656\n",
      "Epoch 647 | train: Loss 0.024710 Accuracy 0.0938 | validation: Loss 0.141043 Accuracy -7.8607\n",
      "Epoch 648 | train: Loss 0.024693 Accuracy 0.0944 | validation: Loss 0.140965 Accuracy -7.8558\n",
      "Epoch 649 | train: Loss 0.024677 Accuracy 0.0950 | validation: Loss 0.140888 Accuracy -7.8510\n",
      "Epoch 650 | train: Loss 0.024661 Accuracy 0.0956 | validation: Loss 0.140812 Accuracy -7.8462\n",
      "Epoch 651 | train: Loss 0.024644 Accuracy 0.0962 | validation: Loss 0.140736 Accuracy -7.8415\n",
      "Epoch 652 | train: Loss 0.024628 Accuracy 0.0968 | validation: Loss 0.140660 Accuracy -7.8367\n",
      "Epoch 653 | train: Loss 0.024611 Accuracy 0.0974 | validation: Loss 0.140585 Accuracy -7.8320\n",
      "Epoch 654 | train: Loss 0.024595 Accuracy 0.0980 | validation: Loss 0.140509 Accuracy -7.8272\n",
      "Epoch 655 | train: Loss 0.024579 Accuracy 0.0986 | validation: Loss 0.140434 Accuracy -7.8225\n",
      "Epoch 656 | train: Loss 0.024563 Accuracy 0.0992 | validation: Loss 0.140359 Accuracy -7.8178\n",
      "Epoch 657 | train: Loss 0.024546 Accuracy 0.0998 | validation: Loss 0.140284 Accuracy -7.8131\n",
      "Epoch 658 | train: Loss 0.024530 Accuracy 0.1004 | validation: Loss 0.140209 Accuracy -7.8083\n",
      "Epoch 659 | train: Loss 0.024514 Accuracy 0.1010 | validation: Loss 0.140134 Accuracy -7.8036\n",
      "Epoch 660 | train: Loss 0.024498 Accuracy 0.1016 | validation: Loss 0.140059 Accuracy -7.7989\n",
      "Epoch 661 | train: Loss 0.024482 Accuracy 0.1022 | validation: Loss 0.139984 Accuracy -7.7942\n",
      "Epoch 662 | train: Loss 0.024465 Accuracy 0.1028 | validation: Loss 0.139909 Accuracy -7.7895\n",
      "Epoch 663 | train: Loss 0.024449 Accuracy 0.1034 | validation: Loss 0.139834 Accuracy -7.7848\n",
      "Epoch 664 | train: Loss 0.024433 Accuracy 0.1039 | validation: Loss 0.139759 Accuracy -7.7801\n",
      "Epoch 665 | train: Loss 0.024417 Accuracy 0.1045 | validation: Loss 0.139684 Accuracy -7.7754\n",
      "Epoch 666 | train: Loss 0.024401 Accuracy 0.1051 | validation: Loss 0.139610 Accuracy -7.7707\n",
      "Epoch 667 | train: Loss 0.024385 Accuracy 0.1057 | validation: Loss 0.139535 Accuracy -7.7660\n",
      "Epoch 668 | train: Loss 0.024369 Accuracy 0.1063 | validation: Loss 0.139460 Accuracy -7.7613\n",
      "Epoch 669 | train: Loss 0.024353 Accuracy 0.1069 | validation: Loss 0.139384 Accuracy -7.7565\n",
      "Epoch 670 | train: Loss 0.024338 Accuracy 0.1075 | validation: Loss 0.139310 Accuracy -7.7519\n",
      "Epoch 671 | train: Loss 0.024322 Accuracy 0.1080 | validation: Loss 0.139235 Accuracy -7.7472\n",
      "Epoch 672 | train: Loss 0.024306 Accuracy 0.1086 | validation: Loss 0.139162 Accuracy -7.7425\n",
      "Epoch 673 | train: Loss 0.024291 Accuracy 0.1092 | validation: Loss 0.139088 Accuracy -7.7379\n",
      "Epoch 674 | train: Loss 0.024275 Accuracy 0.1097 | validation: Loss 0.139014 Accuracy -7.7333\n",
      "Epoch 675 | train: Loss 0.024260 Accuracy 0.1103 | validation: Loss 0.138941 Accuracy -7.7287\n",
      "Epoch 676 | train: Loss 0.024244 Accuracy 0.1109 | validation: Loss 0.138868 Accuracy -7.7241\n",
      "Epoch 677 | train: Loss 0.024229 Accuracy 0.1114 | validation: Loss 0.138795 Accuracy -7.7195\n",
      "Epoch 678 | train: Loss 0.024213 Accuracy 0.1120 | validation: Loss 0.138723 Accuracy -7.7150\n",
      "Epoch 679 | train: Loss 0.024198 Accuracy 0.1126 | validation: Loss 0.138650 Accuracy -7.7104\n",
      "Epoch 680 | train: Loss 0.024183 Accuracy 0.1131 | validation: Loss 0.138578 Accuracy -7.7059\n",
      "Epoch 681 | train: Loss 0.024167 Accuracy 0.1137 | validation: Loss 0.138505 Accuracy -7.7013\n",
      "Epoch 682 | train: Loss 0.024152 Accuracy 0.1143 | validation: Loss 0.138433 Accuracy -7.6968\n",
      "Epoch 683 | train: Loss 0.024137 Accuracy 0.1148 | validation: Loss 0.138361 Accuracy -7.6923\n",
      "Epoch 684 | train: Loss 0.024122 Accuracy 0.1154 | validation: Loss 0.138289 Accuracy -7.6877\n",
      "Epoch 685 | train: Loss 0.024106 Accuracy 0.1159 | validation: Loss 0.138217 Accuracy -7.6832\n",
      "Epoch 686 | train: Loss 0.024091 Accuracy 0.1165 | validation: Loss 0.138145 Accuracy -7.6787\n",
      "Epoch 687 | train: Loss 0.024076 Accuracy 0.1171 | validation: Loss 0.138073 Accuracy -7.6741\n",
      "Epoch 688 | train: Loss 0.024061 Accuracy 0.1176 | validation: Loss 0.138001 Accuracy -7.6696\n",
      "Epoch 689 | train: Loss 0.024046 Accuracy 0.1182 | validation: Loss 0.137928 Accuracy -7.6651\n",
      "Epoch 690 | train: Loss 0.024030 Accuracy 0.1187 | validation: Loss 0.137856 Accuracy -7.6605\n",
      "Epoch 691 | train: Loss 0.024015 Accuracy 0.1193 | validation: Loss 0.137784 Accuracy -7.6560\n",
      "Epoch 692 | train: Loss 0.024000 Accuracy 0.1198 | validation: Loss 0.137712 Accuracy -7.6515\n",
      "Epoch 693 | train: Loss 0.023985 Accuracy 0.1204 | validation: Loss 0.137640 Accuracy -7.6470\n",
      "Epoch 694 | train: Loss 0.023970 Accuracy 0.1209 | validation: Loss 0.137568 Accuracy -7.6424\n",
      "Epoch 695 | train: Loss 0.023955 Accuracy 0.1215 | validation: Loss 0.137496 Accuracy -7.6379\n",
      "Epoch 696 | train: Loss 0.023940 Accuracy 0.1220 | validation: Loss 0.137424 Accuracy -7.6334\n",
      "Epoch 697 | train: Loss 0.023925 Accuracy 0.1226 | validation: Loss 0.137352 Accuracy -7.6288\n",
      "Epoch 698 | train: Loss 0.023910 Accuracy 0.1231 | validation: Loss 0.137280 Accuracy -7.6243\n",
      "Epoch 699 | train: Loss 0.023895 Accuracy 0.1237 | validation: Loss 0.137208 Accuracy -7.6198\n",
      "Epoch 700 | train: Loss 0.023880 Accuracy 0.1242 | validation: Loss 0.137136 Accuracy -7.6153\n",
      "Epoch 701 | train: Loss 0.023865 Accuracy 0.1248 | validation: Loss 0.137064 Accuracy -7.6107\n",
      "Epoch 702 | train: Loss 0.023850 Accuracy 0.1253 | validation: Loss 0.136992 Accuracy -7.6063\n",
      "Epoch 703 | train: Loss 0.023836 Accuracy 0.1259 | validation: Loss 0.136922 Accuracy -7.6018\n",
      "Epoch 704 | train: Loss 0.023821 Accuracy 0.1264 | validation: Loss 0.136852 Accuracy -7.5975\n",
      "Epoch 705 | train: Loss 0.023807 Accuracy 0.1269 | validation: Loss 0.136783 Accuracy -7.5931\n",
      "Epoch 706 | train: Loss 0.023793 Accuracy 0.1274 | validation: Loss 0.136715 Accuracy -7.5888\n",
      "Epoch 707 | train: Loss 0.023779 Accuracy 0.1280 | validation: Loss 0.136647 Accuracy -7.5846\n",
      "Epoch 708 | train: Loss 0.023764 Accuracy 0.1285 | validation: Loss 0.136580 Accuracy -7.5804\n",
      "Epoch 709 | train: Loss 0.023750 Accuracy 0.1290 | validation: Loss 0.136513 Accuracy -7.5762\n",
      "Epoch 710 | train: Loss 0.023736 Accuracy 0.1295 | validation: Loss 0.136447 Accuracy -7.5720\n",
      "Epoch 711 | train: Loss 0.023722 Accuracy 0.1300 | validation: Loss 0.136381 Accuracy -7.5678\n",
      "Epoch 712 | train: Loss 0.023708 Accuracy 0.1306 | validation: Loss 0.136315 Accuracy -7.5637\n",
      "Epoch 713 | train: Loss 0.023694 Accuracy 0.1311 | validation: Loss 0.136250 Accuracy -7.5596\n",
      "Epoch 714 | train: Loss 0.023680 Accuracy 0.1316 | validation: Loss 0.136184 Accuracy -7.5555\n",
      "Epoch 715 | train: Loss 0.023666 Accuracy 0.1321 | validation: Loss 0.136119 Accuracy -7.5514\n",
      "Epoch 716 | train: Loss 0.023652 Accuracy 0.1326 | validation: Loss 0.136054 Accuracy -7.5473\n",
      "Epoch 717 | train: Loss 0.023638 Accuracy 0.1331 | validation: Loss 0.135989 Accuracy -7.5432\n",
      "Epoch 718 | train: Loss 0.023624 Accuracy 0.1336 | validation: Loss 0.135924 Accuracy -7.5391\n",
      "Epoch 719 | train: Loss 0.023610 Accuracy 0.1341 | validation: Loss 0.135858 Accuracy -7.5350\n",
      "Epoch 720 | train: Loss 0.023596 Accuracy 0.1346 | validation: Loss 0.135793 Accuracy -7.5309\n",
      "Epoch 721 | train: Loss 0.023582 Accuracy 0.1352 | validation: Loss 0.135728 Accuracy -7.5268\n",
      "Epoch 722 | train: Loss 0.023568 Accuracy 0.1357 | validation: Loss 0.135663 Accuracy -7.5227\n",
      "Epoch 723 | train: Loss 0.023555 Accuracy 0.1362 | validation: Loss 0.135598 Accuracy -7.5187\n",
      "Epoch 724 | train: Loss 0.023541 Accuracy 0.1367 | validation: Loss 0.135533 Accuracy -7.5146\n",
      "Epoch 725 | train: Loss 0.023527 Accuracy 0.1372 | validation: Loss 0.135467 Accuracy -7.5104\n",
      "Epoch 726 | train: Loss 0.023513 Accuracy 0.1377 | validation: Loss 0.135402 Accuracy -7.5063\n",
      "Epoch 727 | train: Loss 0.023499 Accuracy 0.1382 | validation: Loss 0.135336 Accuracy -7.5022\n",
      "Epoch 728 | train: Loss 0.023486 Accuracy 0.1387 | validation: Loss 0.135271 Accuracy -7.4981\n",
      "Epoch 729 | train: Loss 0.023472 Accuracy 0.1392 | validation: Loss 0.135205 Accuracy -7.4940\n",
      "Epoch 730 | train: Loss 0.023458 Accuracy 0.1397 | validation: Loss 0.135139 Accuracy -7.4898\n",
      "Epoch 731 | train: Loss 0.023444 Accuracy 0.1402 | validation: Loss 0.135073 Accuracy -7.4857\n",
      "Epoch 732 | train: Loss 0.023431 Accuracy 0.1407 | validation: Loss 0.135007 Accuracy -7.4816\n",
      "Epoch 733 | train: Loss 0.023417 Accuracy 0.1412 | validation: Loss 0.134941 Accuracy -7.4774\n",
      "Epoch 734 | train: Loss 0.023403 Accuracy 0.1417 | validation: Loss 0.134875 Accuracy -7.4733\n",
      "Epoch 735 | train: Loss 0.023390 Accuracy 0.1422 | validation: Loss 0.134811 Accuracy -7.4692\n",
      "Epoch 736 | train: Loss 0.023377 Accuracy 0.1427 | validation: Loss 0.134748 Accuracy -7.4653\n",
      "Epoch 737 | train: Loss 0.023364 Accuracy 0.1432 | validation: Loss 0.134686 Accuracy -7.4614\n",
      "Epoch 738 | train: Loss 0.023351 Accuracy 0.1436 | validation: Loss 0.134626 Accuracy -7.4576\n",
      "Epoch 739 | train: Loss 0.023338 Accuracy 0.1441 | validation: Loss 0.134566 Accuracy -7.4538\n",
      "Epoch 740 | train: Loss 0.023325 Accuracy 0.1446 | validation: Loss 0.134507 Accuracy -7.4502\n",
      "Epoch 741 | train: Loss 0.023313 Accuracy 0.1450 | validation: Loss 0.134449 Accuracy -7.4465\n",
      "Epoch 742 | train: Loss 0.023300 Accuracy 0.1455 | validation: Loss 0.134392 Accuracy -7.4429\n",
      "Epoch 743 | train: Loss 0.023287 Accuracy 0.1460 | validation: Loss 0.134335 Accuracy -7.4393\n",
      "Epoch 744 | train: Loss 0.023275 Accuracy 0.1464 | validation: Loss 0.134278 Accuracy -7.4357\n",
      "Epoch 745 | train: Loss 0.023262 Accuracy 0.1469 | validation: Loss 0.134222 Accuracy -7.4322\n",
      "Epoch 746 | train: Loss 0.023249 Accuracy 0.1474 | validation: Loss 0.134165 Accuracy -7.4287\n",
      "Epoch 747 | train: Loss 0.023237 Accuracy 0.1478 | validation: Loss 0.134109 Accuracy -7.4251\n",
      "Epoch 748 | train: Loss 0.023224 Accuracy 0.1483 | validation: Loss 0.134053 Accuracy -7.4216\n",
      "Epoch 749 | train: Loss 0.023212 Accuracy 0.1487 | validation: Loss 0.133996 Accuracy -7.4180\n",
      "Epoch 750 | train: Loss 0.023199 Accuracy 0.1492 | validation: Loss 0.133940 Accuracy -7.4145\n",
      "Epoch 751 | train: Loss 0.023187 Accuracy 0.1497 | validation: Loss 0.133883 Accuracy -7.4110\n",
      "Epoch 752 | train: Loss 0.023174 Accuracy 0.1501 | validation: Loss 0.133827 Accuracy -7.4074\n",
      "Epoch 753 | train: Loss 0.023162 Accuracy 0.1506 | validation: Loss 0.133770 Accuracy -7.4038\n",
      "Epoch 754 | train: Loss 0.023149 Accuracy 0.1510 | validation: Loss 0.133712 Accuracy -7.4002\n",
      "Epoch 755 | train: Loss 0.023137 Accuracy 0.1515 | validation: Loss 0.133655 Accuracy -7.3966\n",
      "Epoch 756 | train: Loss 0.023125 Accuracy 0.1519 | validation: Loss 0.133597 Accuracy -7.3929\n",
      "Epoch 757 | train: Loss 0.023112 Accuracy 0.1524 | validation: Loss 0.133538 Accuracy -7.3893\n",
      "Epoch 758 | train: Loss 0.023100 Accuracy 0.1528 | validation: Loss 0.133480 Accuracy -7.3856\n",
      "Epoch 759 | train: Loss 0.023088 Accuracy 0.1533 | validation: Loss 0.133421 Accuracy -7.3819\n",
      "Epoch 760 | train: Loss 0.023075 Accuracy 0.1538 | validation: Loss 0.133362 Accuracy -7.3782\n",
      "Epoch 761 | train: Loss 0.023063 Accuracy 0.1542 | validation: Loss 0.133302 Accuracy -7.3744\n",
      "Epoch 762 | train: Loss 0.023051 Accuracy 0.1547 | validation: Loss 0.133242 Accuracy -7.3707\n",
      "Epoch 763 | train: Loss 0.023038 Accuracy 0.1551 | validation: Loss 0.133181 Accuracy -7.3669\n",
      "Epoch 764 | train: Loss 0.023026 Accuracy 0.1556 | validation: Loss 0.133121 Accuracy -7.3630\n",
      "Epoch 765 | train: Loss 0.023014 Accuracy 0.1560 | validation: Loss 0.133060 Accuracy -7.3592\n",
      "Epoch 766 | train: Loss 0.023002 Accuracy 0.1565 | validation: Loss 0.132998 Accuracy -7.3553\n",
      "Epoch 767 | train: Loss 0.022989 Accuracy 0.1569 | validation: Loss 0.132936 Accuracy -7.3514\n",
      "Epoch 768 | train: Loss 0.022977 Accuracy 0.1573 | validation: Loss 0.132874 Accuracy -7.3475\n",
      "Epoch 769 | train: Loss 0.022965 Accuracy 0.1578 | validation: Loss 0.132812 Accuracy -7.3436\n",
      "Epoch 770 | train: Loss 0.022953 Accuracy 0.1582 | validation: Loss 0.132749 Accuracy -7.3397\n",
      "Epoch 771 | train: Loss 0.022941 Accuracy 0.1587 | validation: Loss 0.132686 Accuracy -7.3358\n",
      "Epoch 772 | train: Loss 0.022929 Accuracy 0.1591 | validation: Loss 0.132623 Accuracy -7.3318\n",
      "Epoch 773 | train: Loss 0.022916 Accuracy 0.1596 | validation: Loss 0.132560 Accuracy -7.3278\n",
      "Epoch 774 | train: Loss 0.022904 Accuracy 0.1600 | validation: Loss 0.132497 Accuracy -7.3238\n",
      "Epoch 775 | train: Loss 0.022892 Accuracy 0.1605 | validation: Loss 0.132433 Accuracy -7.3198\n",
      "Epoch 776 | train: Loss 0.022880 Accuracy 0.1609 | validation: Loss 0.132370 Accuracy -7.3159\n",
      "Epoch 777 | train: Loss 0.022869 Accuracy 0.1613 | validation: Loss 0.132306 Accuracy -7.3118\n",
      "Epoch 778 | train: Loss 0.022857 Accuracy 0.1618 | validation: Loss 0.132242 Accuracy -7.3079\n",
      "Epoch 779 | train: Loss 0.022845 Accuracy 0.1622 | validation: Loss 0.132179 Accuracy -7.3039\n",
      "Epoch 780 | train: Loss 0.022833 Accuracy 0.1626 | validation: Loss 0.132115 Accuracy -7.2999\n",
      "Epoch 781 | train: Loss 0.022821 Accuracy 0.1631 | validation: Loss 0.132051 Accuracy -7.2959\n",
      "Epoch 782 | train: Loss 0.022809 Accuracy 0.1635 | validation: Loss 0.131988 Accuracy -7.2919\n",
      "Epoch 783 | train: Loss 0.022797 Accuracy 0.1640 | validation: Loss 0.131924 Accuracy -7.2879\n",
      "Epoch 784 | train: Loss 0.022785 Accuracy 0.1644 | validation: Loss 0.131860 Accuracy -7.2839\n",
      "Epoch 785 | train: Loss 0.022773 Accuracy 0.1648 | validation: Loss 0.131797 Accuracy -7.2799\n",
      "Epoch 786 | train: Loss 0.022762 Accuracy 0.1653 | validation: Loss 0.131733 Accuracy -7.2759\n",
      "Epoch 787 | train: Loss 0.022750 Accuracy 0.1657 | validation: Loss 0.131670 Accuracy -7.2719\n",
      "Epoch 788 | train: Loss 0.022738 Accuracy 0.1661 | validation: Loss 0.131607 Accuracy -7.2679\n",
      "Epoch 789 | train: Loss 0.022726 Accuracy 0.1666 | validation: Loss 0.131544 Accuracy -7.2640\n",
      "Epoch 790 | train: Loss 0.022715 Accuracy 0.1670 | validation: Loss 0.131481 Accuracy -7.2600\n",
      "Epoch 791 | train: Loss 0.022703 Accuracy 0.1674 | validation: Loss 0.131418 Accuracy -7.2561\n",
      "Epoch 792 | train: Loss 0.022691 Accuracy 0.1678 | validation: Loss 0.131355 Accuracy -7.2521\n",
      "Epoch 793 | train: Loss 0.022679 Accuracy 0.1683 | validation: Loss 0.131293 Accuracy -7.2482\n",
      "Epoch 794 | train: Loss 0.022668 Accuracy 0.1687 | validation: Loss 0.131231 Accuracy -7.2443\n",
      "Epoch 795 | train: Loss 0.022656 Accuracy 0.1691 | validation: Loss 0.131169 Accuracy -7.2404\n",
      "Epoch 796 | train: Loss 0.022644 Accuracy 0.1696 | validation: Loss 0.131107 Accuracy -7.2365\n",
      "Epoch 797 | train: Loss 0.022632 Accuracy 0.1700 | validation: Loss 0.131046 Accuracy -7.2327\n",
      "Epoch 798 | train: Loss 0.022621 Accuracy 0.1704 | validation: Loss 0.130984 Accuracy -7.2288\n",
      "Epoch 799 | train: Loss 0.022609 Accuracy 0.1708 | validation: Loss 0.130923 Accuracy -7.2250\n",
      "Epoch 800 | train: Loss 0.022597 Accuracy 0.1713 | validation: Loss 0.130862 Accuracy -7.2211\n",
      "Epoch 801 | train: Loss 0.022586 Accuracy 0.1717 | validation: Loss 0.130801 Accuracy -7.2173\n",
      "Epoch 802 | train: Loss 0.022574 Accuracy 0.1721 | validation: Loss 0.130741 Accuracy -7.2135\n",
      "Epoch 803 | train: Loss 0.022563 Accuracy 0.1726 | validation: Loss 0.130680 Accuracy -7.2097\n",
      "Epoch 804 | train: Loss 0.022551 Accuracy 0.1730 | validation: Loss 0.130620 Accuracy -7.2059\n",
      "Epoch 805 | train: Loss 0.022539 Accuracy 0.1734 | validation: Loss 0.130561 Accuracy -7.2022\n",
      "Epoch 806 | train: Loss 0.022528 Accuracy 0.1738 | validation: Loss 0.130503 Accuracy -7.1986\n",
      "Epoch 807 | train: Loss 0.022517 Accuracy 0.1742 | validation: Loss 0.130445 Accuracy -7.1949\n",
      "Epoch 808 | train: Loss 0.022505 Accuracy 0.1747 | validation: Loss 0.130387 Accuracy -7.1913\n",
      "Epoch 809 | train: Loss 0.022494 Accuracy 0.1751 | validation: Loss 0.130331 Accuracy -7.1878\n",
      "Epoch 810 | train: Loss 0.022482 Accuracy 0.1755 | validation: Loss 0.130275 Accuracy -7.1843\n",
      "Epoch 811 | train: Loss 0.022471 Accuracy 0.1759 | validation: Loss 0.130220 Accuracy -7.1808\n",
      "Epoch 812 | train: Loss 0.022460 Accuracy 0.1763 | validation: Loss 0.130165 Accuracy -7.1773\n",
      "Epoch 813 | train: Loss 0.022448 Accuracy 0.1767 | validation: Loss 0.130110 Accuracy -7.1739\n",
      "Epoch 814 | train: Loss 0.022437 Accuracy 0.1772 | validation: Loss 0.130056 Accuracy -7.1705\n",
      "Epoch 815 | train: Loss 0.022426 Accuracy 0.1776 | validation: Loss 0.130001 Accuracy -7.1671\n",
      "Epoch 816 | train: Loss 0.022415 Accuracy 0.1780 | validation: Loss 0.129948 Accuracy -7.1637\n",
      "Epoch 817 | train: Loss 0.022403 Accuracy 0.1784 | validation: Loss 0.129894 Accuracy -7.1603\n",
      "Epoch 818 | train: Loss 0.022392 Accuracy 0.1788 | validation: Loss 0.129841 Accuracy -7.1570\n",
      "Epoch 819 | train: Loss 0.022381 Accuracy 0.1792 | validation: Loss 0.129787 Accuracy -7.1536\n",
      "Epoch 820 | train: Loss 0.022370 Accuracy 0.1796 | validation: Loss 0.129734 Accuracy -7.1503\n",
      "Epoch 821 | train: Loss 0.022359 Accuracy 0.1800 | validation: Loss 0.129681 Accuracy -7.1469\n",
      "Epoch 822 | train: Loss 0.022347 Accuracy 0.1804 | validation: Loss 0.129628 Accuracy -7.1436\n",
      "Epoch 823 | train: Loss 0.022336 Accuracy 0.1809 | validation: Loss 0.129575 Accuracy -7.1403\n",
      "Epoch 824 | train: Loss 0.022325 Accuracy 0.1813 | validation: Loss 0.129522 Accuracy -7.1370\n",
      "Epoch 825 | train: Loss 0.022314 Accuracy 0.1817 | validation: Loss 0.129469 Accuracy -7.1336\n",
      "Epoch 826 | train: Loss 0.022303 Accuracy 0.1821 | validation: Loss 0.129416 Accuracy -7.1303\n",
      "Epoch 827 | train: Loss 0.022292 Accuracy 0.1825 | validation: Loss 0.129363 Accuracy -7.1270\n",
      "Epoch 828 | train: Loss 0.022280 Accuracy 0.1829 | validation: Loss 0.129311 Accuracy -7.1237\n",
      "Epoch 829 | train: Loss 0.022269 Accuracy 0.1833 | validation: Loss 0.129258 Accuracy -7.1203\n",
      "Epoch 830 | train: Loss 0.022258 Accuracy 0.1837 | validation: Loss 0.129205 Accuracy -7.1170\n",
      "Epoch 831 | train: Loss 0.022247 Accuracy 0.1841 | validation: Loss 0.129152 Accuracy -7.1137\n",
      "Epoch 832 | train: Loss 0.022236 Accuracy 0.1845 | validation: Loss 0.129099 Accuracy -7.1104\n",
      "Epoch 833 | train: Loss 0.022225 Accuracy 0.1849 | validation: Loss 0.129046 Accuracy -7.1070\n",
      "Epoch 834 | train: Loss 0.022214 Accuracy 0.1853 | validation: Loss 0.128993 Accuracy -7.1037\n",
      "Epoch 835 | train: Loss 0.022203 Accuracy 0.1857 | validation: Loss 0.128940 Accuracy -7.1004\n",
      "Epoch 836 | train: Loss 0.022192 Accuracy 0.1862 | validation: Loss 0.128887 Accuracy -7.0971\n",
      "Epoch 837 | train: Loss 0.022181 Accuracy 0.1866 | validation: Loss 0.128834 Accuracy -7.0938\n",
      "Epoch 838 | train: Loss 0.022170 Accuracy 0.1870 | validation: Loss 0.128781 Accuracy -7.0904\n",
      "Epoch 839 | train: Loss 0.022159 Accuracy 0.1874 | validation: Loss 0.128729 Accuracy -7.0871\n",
      "Epoch 840 | train: Loss 0.022148 Accuracy 0.1878 | validation: Loss 0.128676 Accuracy -7.0838\n",
      "Epoch 841 | train: Loss 0.022137 Accuracy 0.1882 | validation: Loss 0.128623 Accuracy -7.0805\n",
      "Epoch 842 | train: Loss 0.022126 Accuracy 0.1886 | validation: Loss 0.128570 Accuracy -7.0772\n",
      "Epoch 843 | train: Loss 0.022115 Accuracy 0.1890 | validation: Loss 0.128518 Accuracy -7.0739\n",
      "Epoch 844 | train: Loss 0.022104 Accuracy 0.1894 | validation: Loss 0.128465 Accuracy -7.0705\n",
      "Epoch 845 | train: Loss 0.022093 Accuracy 0.1898 | validation: Loss 0.128412 Accuracy -7.0672\n",
      "Epoch 846 | train: Loss 0.022082 Accuracy 0.1902 | validation: Loss 0.128360 Accuracy -7.0639\n",
      "Epoch 847 | train: Loss 0.022071 Accuracy 0.1906 | validation: Loss 0.128307 Accuracy -7.0606\n",
      "Epoch 848 | train: Loss 0.022060 Accuracy 0.1910 | validation: Loss 0.128255 Accuracy -7.0574\n",
      "Epoch 849 | train: Loss 0.022049 Accuracy 0.1914 | validation: Loss 0.128203 Accuracy -7.0541\n",
      "Epoch 850 | train: Loss 0.022038 Accuracy 0.1918 | validation: Loss 0.128150 Accuracy -7.0508\n",
      "Epoch 851 | train: Loss 0.022027 Accuracy 0.1922 | validation: Loss 0.128098 Accuracy -7.0475\n",
      "Epoch 852 | train: Loss 0.022016 Accuracy 0.1926 | validation: Loss 0.128046 Accuracy -7.0442\n",
      "Epoch 853 | train: Loss 0.022006 Accuracy 0.1930 | validation: Loss 0.127994 Accuracy -7.0410\n",
      "Epoch 854 | train: Loss 0.021995 Accuracy 0.1934 | validation: Loss 0.127943 Accuracy -7.0377\n",
      "Epoch 855 | train: Loss 0.021984 Accuracy 0.1938 | validation: Loss 0.127891 Accuracy -7.0345\n",
      "Epoch 856 | train: Loss 0.021973 Accuracy 0.1942 | validation: Loss 0.127839 Accuracy -7.0312\n",
      "Epoch 857 | train: Loss 0.021962 Accuracy 0.1946 | validation: Loss 0.127788 Accuracy -7.0280\n",
      "Epoch 858 | train: Loss 0.021951 Accuracy 0.1950 | validation: Loss 0.127737 Accuracy -7.0248\n",
      "Epoch 859 | train: Loss 0.021940 Accuracy 0.1954 | validation: Loss 0.127686 Accuracy -7.0216\n",
      "Epoch 860 | train: Loss 0.021930 Accuracy 0.1958 | validation: Loss 0.127636 Accuracy -7.0184\n",
      "Epoch 861 | train: Loss 0.021919 Accuracy 0.1962 | validation: Loss 0.127585 Accuracy -7.0153\n",
      "Epoch 862 | train: Loss 0.021908 Accuracy 0.1966 | validation: Loss 0.127535 Accuracy -7.0121\n",
      "Epoch 863 | train: Loss 0.021897 Accuracy 0.1970 | validation: Loss 0.127485 Accuracy -7.0090\n",
      "Epoch 864 | train: Loss 0.021886 Accuracy 0.1974 | validation: Loss 0.127435 Accuracy -7.0058\n",
      "Epoch 865 | train: Loss 0.021876 Accuracy 0.1977 | validation: Loss 0.127385 Accuracy -7.0027\n",
      "Epoch 866 | train: Loss 0.021865 Accuracy 0.1981 | validation: Loss 0.127335 Accuracy -6.9996\n",
      "Epoch 867 | train: Loss 0.021854 Accuracy 0.1985 | validation: Loss 0.127286 Accuracy -6.9965\n",
      "Epoch 868 | train: Loss 0.021843 Accuracy 0.1989 | validation: Loss 0.127236 Accuracy -6.9934\n",
      "Epoch 869 | train: Loss 0.021833 Accuracy 0.1993 | validation: Loss 0.127189 Accuracy -6.9904\n",
      "Epoch 870 | train: Loss 0.021822 Accuracy 0.1997 | validation: Loss 0.127144 Accuracy -6.9876\n",
      "Epoch 871 | train: Loss 0.021812 Accuracy 0.2001 | validation: Loss 0.127101 Accuracy -6.9849\n",
      "Epoch 872 | train: Loss 0.021802 Accuracy 0.2005 | validation: Loss 0.127060 Accuracy -6.9823\n",
      "Epoch 873 | train: Loss 0.021791 Accuracy 0.2008 | validation: Loss 0.127019 Accuracy -6.9797\n",
      "Epoch 874 | train: Loss 0.021781 Accuracy 0.2012 | validation: Loss 0.126980 Accuracy -6.9773\n",
      "Epoch 875 | train: Loss 0.021771 Accuracy 0.2016 | validation: Loss 0.126943 Accuracy -6.9749\n",
      "Epoch 876 | train: Loss 0.021760 Accuracy 0.2020 | validation: Loss 0.126905 Accuracy -6.9726\n",
      "Epoch 877 | train: Loss 0.021750 Accuracy 0.2023 | validation: Loss 0.126869 Accuracy -6.9703\n",
      "Epoch 878 | train: Loss 0.021740 Accuracy 0.2027 | validation: Loss 0.126833 Accuracy -6.9680\n",
      "Epoch 879 | train: Loss 0.021730 Accuracy 0.2031 | validation: Loss 0.126798 Accuracy -6.9658\n",
      "Epoch 880 | train: Loss 0.021719 Accuracy 0.2035 | validation: Loss 0.126762 Accuracy -6.9636\n",
      "Epoch 881 | train: Loss 0.021709 Accuracy 0.2039 | validation: Loss 0.126727 Accuracy -6.9614\n",
      "Epoch 882 | train: Loss 0.021699 Accuracy 0.2042 | validation: Loss 0.126692 Accuracy -6.9591\n",
      "Epoch 883 | train: Loss 0.021689 Accuracy 0.2046 | validation: Loss 0.126656 Accuracy -6.9569\n",
      "Epoch 884 | train: Loss 0.021679 Accuracy 0.2050 | validation: Loss 0.126621 Accuracy -6.9547\n",
      "Epoch 885 | train: Loss 0.021668 Accuracy 0.2053 | validation: Loss 0.126585 Accuracy -6.9524\n",
      "Epoch 886 | train: Loss 0.021658 Accuracy 0.2057 | validation: Loss 0.126548 Accuracy -6.9501\n",
      "Epoch 887 | train: Loss 0.021648 Accuracy 0.2061 | validation: Loss 0.126511 Accuracy -6.9478\n",
      "Epoch 888 | train: Loss 0.021638 Accuracy 0.2065 | validation: Loss 0.126474 Accuracy -6.9455\n",
      "Epoch 889 | train: Loss 0.021628 Accuracy 0.2068 | validation: Loss 0.126436 Accuracy -6.9431\n",
      "Epoch 890 | train: Loss 0.021618 Accuracy 0.2072 | validation: Loss 0.126398 Accuracy -6.9407\n",
      "Epoch 891 | train: Loss 0.021607 Accuracy 0.2076 | validation: Loss 0.126359 Accuracy -6.9383\n",
      "Epoch 892 | train: Loss 0.021597 Accuracy 0.2080 | validation: Loss 0.126320 Accuracy -6.9358\n",
      "Epoch 893 | train: Loss 0.021587 Accuracy 0.2083 | validation: Loss 0.126279 Accuracy -6.9332\n",
      "Epoch 894 | train: Loss 0.021577 Accuracy 0.2087 | validation: Loss 0.126239 Accuracy -6.9307\n",
      "Epoch 895 | train: Loss 0.021567 Accuracy 0.2091 | validation: Loss 0.126197 Accuracy -6.9281\n",
      "Epoch 896 | train: Loss 0.021557 Accuracy 0.2094 | validation: Loss 0.126155 Accuracy -6.9255\n",
      "Epoch 897 | train: Loss 0.021547 Accuracy 0.2098 | validation: Loss 0.126113 Accuracy -6.9228\n",
      "Epoch 898 | train: Loss 0.021537 Accuracy 0.2102 | validation: Loss 0.126070 Accuracy -6.9201\n",
      "Epoch 899 | train: Loss 0.021527 Accuracy 0.2105 | validation: Loss 0.126026 Accuracy -6.9173\n",
      "Epoch 900 | train: Loss 0.021517 Accuracy 0.2109 | validation: Loss 0.125982 Accuracy -6.9146\n",
      "Epoch 901 | train: Loss 0.021507 Accuracy 0.2113 | validation: Loss 0.125937 Accuracy -6.9118\n",
      "Epoch 902 | train: Loss 0.021497 Accuracy 0.2116 | validation: Loss 0.125892 Accuracy -6.9089\n",
      "Epoch 903 | train: Loss 0.021487 Accuracy 0.2120 | validation: Loss 0.125847 Accuracy -6.9061\n",
      "Epoch 904 | train: Loss 0.021477 Accuracy 0.2124 | validation: Loss 0.125801 Accuracy -6.9032\n",
      "Epoch 905 | train: Loss 0.021467 Accuracy 0.2127 | validation: Loss 0.125755 Accuracy -6.9003\n",
      "Epoch 906 | train: Loss 0.021457 Accuracy 0.2131 | validation: Loss 0.125708 Accuracy -6.8974\n",
      "Epoch 907 | train: Loss 0.021447 Accuracy 0.2135 | validation: Loss 0.125662 Accuracy -6.8944\n",
      "Epoch 908 | train: Loss 0.021437 Accuracy 0.2138 | validation: Loss 0.125615 Accuracy -6.8915\n",
      "Epoch 909 | train: Loss 0.021427 Accuracy 0.2142 | validation: Loss 0.125567 Accuracy -6.8885\n",
      "Epoch 910 | train: Loss 0.021417 Accuracy 0.2146 | validation: Loss 0.125520 Accuracy -6.8855\n",
      "Epoch 911 | train: Loss 0.021407 Accuracy 0.2149 | validation: Loss 0.125472 Accuracy -6.8825\n",
      "Epoch 912 | train: Loss 0.021397 Accuracy 0.2153 | validation: Loss 0.125425 Accuracy -6.8796\n",
      "Epoch 913 | train: Loss 0.021387 Accuracy 0.2157 | validation: Loss 0.125377 Accuracy -6.8766\n",
      "Epoch 914 | train: Loss 0.021377 Accuracy 0.2160 | validation: Loss 0.125329 Accuracy -6.8736\n",
      "Epoch 915 | train: Loss 0.021367 Accuracy 0.2164 | validation: Loss 0.125282 Accuracy -6.8706\n",
      "Epoch 916 | train: Loss 0.021357 Accuracy 0.2168 | validation: Loss 0.125234 Accuracy -6.8676\n",
      "Epoch 917 | train: Loss 0.021347 Accuracy 0.2171 | validation: Loss 0.125187 Accuracy -6.8646\n",
      "Epoch 918 | train: Loss 0.021337 Accuracy 0.2175 | validation: Loss 0.125139 Accuracy -6.8616\n",
      "Epoch 919 | train: Loss 0.021327 Accuracy 0.2179 | validation: Loss 0.125091 Accuracy -6.8586\n",
      "Epoch 920 | train: Loss 0.021318 Accuracy 0.2182 | validation: Loss 0.125044 Accuracy -6.8556\n",
      "Epoch 921 | train: Loss 0.021308 Accuracy 0.2186 | validation: Loss 0.124997 Accuracy -6.8527\n",
      "Epoch 922 | train: Loss 0.021298 Accuracy 0.2189 | validation: Loss 0.124950 Accuracy -6.8497\n",
      "Epoch 923 | train: Loss 0.021288 Accuracy 0.2193 | validation: Loss 0.124903 Accuracy -6.8468\n",
      "Epoch 924 | train: Loss 0.021278 Accuracy 0.2197 | validation: Loss 0.124857 Accuracy -6.8439\n",
      "Epoch 925 | train: Loss 0.021269 Accuracy 0.2200 | validation: Loss 0.124810 Accuracy -6.8409\n",
      "Epoch 926 | train: Loss 0.021259 Accuracy 0.2204 | validation: Loss 0.124764 Accuracy -6.8380\n",
      "Epoch 927 | train: Loss 0.021249 Accuracy 0.2207 | validation: Loss 0.124718 Accuracy -6.8351\n",
      "Epoch 928 | train: Loss 0.021239 Accuracy 0.2211 | validation: Loss 0.124672 Accuracy -6.8323\n",
      "Epoch 929 | train: Loss 0.021229 Accuracy 0.2214 | validation: Loss 0.124627 Accuracy -6.8294\n",
      "Epoch 930 | train: Loss 0.021220 Accuracy 0.2218 | validation: Loss 0.124581 Accuracy -6.8266\n",
      "Epoch 931 | train: Loss 0.021210 Accuracy 0.2222 | validation: Loss 0.124537 Accuracy -6.8238\n",
      "Epoch 932 | train: Loss 0.021200 Accuracy 0.2225 | validation: Loss 0.124492 Accuracy -6.8209\n",
      "Epoch 933 | train: Loss 0.021190 Accuracy 0.2229 | validation: Loss 0.124447 Accuracy -6.8181\n",
      "Epoch 934 | train: Loss 0.021181 Accuracy 0.2232 | validation: Loss 0.124403 Accuracy -6.8154\n",
      "Epoch 935 | train: Loss 0.021171 Accuracy 0.2236 | validation: Loss 0.124359 Accuracy -6.8126\n",
      "Epoch 936 | train: Loss 0.021161 Accuracy 0.2239 | validation: Loss 0.124315 Accuracy -6.8099\n",
      "Epoch 937 | train: Loss 0.021152 Accuracy 0.2243 | validation: Loss 0.124272 Accuracy -6.8071\n",
      "Epoch 938 | train: Loss 0.021142 Accuracy 0.2247 | validation: Loss 0.124229 Accuracy -6.8044\n",
      "Epoch 939 | train: Loss 0.021132 Accuracy 0.2250 | validation: Loss 0.124186 Accuracy -6.8017\n",
      "Epoch 940 | train: Loss 0.021123 Accuracy 0.2254 | validation: Loss 0.124143 Accuracy -6.7991\n",
      "Epoch 941 | train: Loss 0.021113 Accuracy 0.2257 | validation: Loss 0.124101 Accuracy -6.7964\n",
      "Epoch 942 | train: Loss 0.021103 Accuracy 0.2261 | validation: Loss 0.124059 Accuracy -6.7937\n",
      "Epoch 943 | train: Loss 0.021094 Accuracy 0.2264 | validation: Loss 0.124017 Accuracy -6.7911\n",
      "Epoch 944 | train: Loss 0.021084 Accuracy 0.2268 | validation: Loss 0.123975 Accuracy -6.7885\n",
      "Epoch 945 | train: Loss 0.021074 Accuracy 0.2271 | validation: Loss 0.123934 Accuracy -6.7859\n",
      "Epoch 946 | train: Loss 0.021065 Accuracy 0.2275 | validation: Loss 0.123893 Accuracy -6.7833\n",
      "Epoch 947 | train: Loss 0.021055 Accuracy 0.2278 | validation: Loss 0.123852 Accuracy -6.7807\n",
      "Epoch 948 | train: Loss 0.021045 Accuracy 0.2282 | validation: Loss 0.123811 Accuracy -6.7782\n",
      "Epoch 949 | train: Loss 0.021036 Accuracy 0.2285 | validation: Loss 0.123771 Accuracy -6.7756\n",
      "Epoch 950 | train: Loss 0.021026 Accuracy 0.2289 | validation: Loss 0.123730 Accuracy -6.7731\n",
      "Epoch 951 | train: Loss 0.021017 Accuracy 0.2292 | validation: Loss 0.123690 Accuracy -6.7706\n",
      "Epoch 952 | train: Loss 0.021007 Accuracy 0.2296 | validation: Loss 0.123650 Accuracy -6.7681\n",
      "Epoch 953 | train: Loss 0.020997 Accuracy 0.2300 | validation: Loss 0.123611 Accuracy -6.7656\n",
      "Epoch 954 | train: Loss 0.020988 Accuracy 0.2303 | validation: Loss 0.123571 Accuracy -6.7631\n",
      "Epoch 955 | train: Loss 0.020978 Accuracy 0.2307 | validation: Loss 0.123532 Accuracy -6.7606\n",
      "Epoch 956 | train: Loss 0.020969 Accuracy 0.2310 | validation: Loss 0.123493 Accuracy -6.7582\n",
      "Epoch 957 | train: Loss 0.020959 Accuracy 0.2314 | validation: Loss 0.123454 Accuracy -6.7557\n",
      "Epoch 958 | train: Loss 0.020950 Accuracy 0.2317 | validation: Loss 0.123415 Accuracy -6.7533\n",
      "Epoch 959 | train: Loss 0.020940 Accuracy 0.2321 | validation: Loss 0.123376 Accuracy -6.7509\n",
      "Epoch 960 | train: Loss 0.020931 Accuracy 0.2324 | validation: Loss 0.123338 Accuracy -6.7485\n",
      "Epoch 961 | train: Loss 0.020921 Accuracy 0.2328 | validation: Loss 0.123300 Accuracy -6.7461\n",
      "Epoch 962 | train: Loss 0.020912 Accuracy 0.2331 | validation: Loss 0.123262 Accuracy -6.7437\n",
      "Epoch 963 | train: Loss 0.020902 Accuracy 0.2334 | validation: Loss 0.123224 Accuracy -6.7413\n",
      "Epoch 964 | train: Loss 0.020893 Accuracy 0.2338 | validation: Loss 0.123186 Accuracy -6.7389\n",
      "Epoch 965 | train: Loss 0.020883 Accuracy 0.2341 | validation: Loss 0.123149 Accuracy -6.7366\n",
      "Epoch 966 | train: Loss 0.020874 Accuracy 0.2345 | validation: Loss 0.123111 Accuracy -6.7342\n",
      "Epoch 967 | train: Loss 0.020864 Accuracy 0.2348 | validation: Loss 0.123074 Accuracy -6.7319\n",
      "Epoch 968 | train: Loss 0.020855 Accuracy 0.2352 | validation: Loss 0.123037 Accuracy -6.7296\n",
      "Epoch 969 | train: Loss 0.020845 Accuracy 0.2355 | validation: Loss 0.123000 Accuracy -6.7272\n",
      "Epoch 970 | train: Loss 0.020836 Accuracy 0.2359 | validation: Loss 0.122963 Accuracy -6.7249\n",
      "Epoch 971 | train: Loss 0.020826 Accuracy 0.2362 | validation: Loss 0.122927 Accuracy -6.7226\n",
      "Epoch 972 | train: Loss 0.020817 Accuracy 0.2366 | validation: Loss 0.122890 Accuracy -6.7203\n",
      "Epoch 973 | train: Loss 0.020808 Accuracy 0.2369 | validation: Loss 0.122854 Accuracy -6.7181\n",
      "Epoch 974 | train: Loss 0.020798 Accuracy 0.2373 | validation: Loss 0.122818 Accuracy -6.7158\n",
      "Epoch 975 | train: Loss 0.020789 Accuracy 0.2376 | validation: Loss 0.122782 Accuracy -6.7135\n",
      "Epoch 976 | train: Loss 0.020779 Accuracy 0.2380 | validation: Loss 0.122746 Accuracy -6.7113\n",
      "Epoch 977 | train: Loss 0.020770 Accuracy 0.2383 | validation: Loss 0.122711 Accuracy -6.7090\n",
      "Epoch 978 | train: Loss 0.020761 Accuracy 0.2386 | validation: Loss 0.122675 Accuracy -6.7068\n",
      "Epoch 979 | train: Loss 0.020751 Accuracy 0.2390 | validation: Loss 0.122640 Accuracy -6.7046\n",
      "Epoch 980 | train: Loss 0.020742 Accuracy 0.2393 | validation: Loss 0.122604 Accuracy -6.7024\n",
      "Epoch 981 | train: Loss 0.020732 Accuracy 0.2397 | validation: Loss 0.122569 Accuracy -6.7002\n",
      "Epoch 982 | train: Loss 0.020723 Accuracy 0.2400 | validation: Loss 0.122534 Accuracy -6.6980\n",
      "Epoch 983 | train: Loss 0.020714 Accuracy 0.2404 | validation: Loss 0.122500 Accuracy -6.6958\n",
      "Epoch 984 | train: Loss 0.020704 Accuracy 0.2407 | validation: Loss 0.122465 Accuracy -6.6936\n",
      "Epoch 985 | train: Loss 0.020695 Accuracy 0.2410 | validation: Loss 0.122430 Accuracy -6.6914\n",
      "Epoch 986 | train: Loss 0.020686 Accuracy 0.2414 | validation: Loss 0.122396 Accuracy -6.6893\n",
      "Epoch 987 | train: Loss 0.020676 Accuracy 0.2417 | validation: Loss 0.122362 Accuracy -6.6871\n",
      "Epoch 988 | train: Loss 0.020667 Accuracy 0.2421 | validation: Loss 0.122328 Accuracy -6.6850\n",
      "Epoch 989 | train: Loss 0.020658 Accuracy 0.2424 | validation: Loss 0.122294 Accuracy -6.6829\n",
      "Epoch 990 | train: Loss 0.020648 Accuracy 0.2428 | validation: Loss 0.122260 Accuracy -6.6807\n",
      "Epoch 991 | train: Loss 0.020639 Accuracy 0.2431 | validation: Loss 0.122226 Accuracy -6.6786\n",
      "Epoch 992 | train: Loss 0.020630 Accuracy 0.2434 | validation: Loss 0.122193 Accuracy -6.6765\n",
      "Epoch 993 | train: Loss 0.020621 Accuracy 0.2438 | validation: Loss 0.122160 Accuracy -6.6744\n",
      "Epoch 994 | train: Loss 0.020611 Accuracy 0.2441 | validation: Loss 0.122127 Accuracy -6.6723\n",
      "Epoch 995 | train: Loss 0.020602 Accuracy 0.2445 | validation: Loss 0.122093 Accuracy -6.6703\n",
      "Epoch 996 | train: Loss 0.020593 Accuracy 0.2448 | validation: Loss 0.122061 Accuracy -6.6682\n",
      "Epoch 997 | train: Loss 0.020583 Accuracy 0.2451 | validation: Loss 0.122028 Accuracy -6.6662\n",
      "Epoch 998 | train: Loss 0.020574 Accuracy 0.2455 | validation: Loss 0.121995 Accuracy -6.6641\n",
      "Epoch 999 | train: Loss 0.020565 Accuracy 0.2458 | validation: Loss 0.121963 Accuracy -6.6621\n",
      "Epoch 1000 | train: Loss 0.020556 Accuracy 0.2462 | validation: Loss 0.121931 Accuracy -6.6600\n",
      "Epoch 1001 | train: Loss 0.020546 Accuracy 0.2465 | validation: Loss 0.121898 Accuracy -6.6580\n",
      "Epoch 1002 | train: Loss 0.020537 Accuracy 0.2468 | validation: Loss 0.121867 Accuracy -6.6560\n",
      "Epoch 1003 | train: Loss 0.020528 Accuracy 0.2472 | validation: Loss 0.121835 Accuracy -6.6540\n",
      "Epoch 1004 | train: Loss 0.020519 Accuracy 0.2475 | validation: Loss 0.121803 Accuracy -6.6520\n",
      "Epoch 1005 | train: Loss 0.020510 Accuracy 0.2478 | validation: Loss 0.121772 Accuracy -6.6501\n",
      "Epoch 1006 | train: Loss 0.020500 Accuracy 0.2482 | validation: Loss 0.121741 Accuracy -6.6481\n",
      "Epoch 1007 | train: Loss 0.020491 Accuracy 0.2485 | validation: Loss 0.121709 Accuracy -6.6461\n",
      "Epoch 1008 | train: Loss 0.020482 Accuracy 0.2489 | validation: Loss 0.121678 Accuracy -6.6442\n",
      "Epoch 1009 | train: Loss 0.020473 Accuracy 0.2492 | validation: Loss 0.121647 Accuracy -6.6423\n",
      "Epoch 1010 | train: Loss 0.020464 Accuracy 0.2495 | validation: Loss 0.121617 Accuracy -6.6403\n",
      "Epoch 1011 | train: Loss 0.020455 Accuracy 0.2499 | validation: Loss 0.121586 Accuracy -6.6384\n",
      "Epoch 1012 | train: Loss 0.020445 Accuracy 0.2502 | validation: Loss 0.121556 Accuracy -6.6365\n",
      "Epoch 1013 | train: Loss 0.020436 Accuracy 0.2505 | validation: Loss 0.121526 Accuracy -6.6346\n",
      "Epoch 1014 | train: Loss 0.020427 Accuracy 0.2509 | validation: Loss 0.121496 Accuracy -6.6327\n",
      "Epoch 1015 | train: Loss 0.020418 Accuracy 0.2512 | validation: Loss 0.121466 Accuracy -6.6308\n",
      "Epoch 1016 | train: Loss 0.020409 Accuracy 0.2515 | validation: Loss 0.121436 Accuracy -6.6290\n",
      "Epoch 1017 | train: Loss 0.020400 Accuracy 0.2519 | validation: Loss 0.121407 Accuracy -6.6271\n",
      "Epoch 1018 | train: Loss 0.020391 Accuracy 0.2522 | validation: Loss 0.121378 Accuracy -6.6253\n",
      "Epoch 1019 | train: Loss 0.020381 Accuracy 0.2525 | validation: Loss 0.121348 Accuracy -6.6235\n",
      "Epoch 1020 | train: Loss 0.020372 Accuracy 0.2529 | validation: Loss 0.121319 Accuracy -6.6216\n",
      "Epoch 1021 | train: Loss 0.020363 Accuracy 0.2532 | validation: Loss 0.121290 Accuracy -6.6198\n",
      "Epoch 1022 | train: Loss 0.020354 Accuracy 0.2535 | validation: Loss 0.121262 Accuracy -6.6180\n",
      "Epoch 1023 | train: Loss 0.020345 Accuracy 0.2539 | validation: Loss 0.121233 Accuracy -6.6162\n",
      "Epoch 1024 | train: Loss 0.020336 Accuracy 0.2542 | validation: Loss 0.121205 Accuracy -6.6144\n",
      "Epoch 1025 | train: Loss 0.020327 Accuracy 0.2545 | validation: Loss 0.121177 Accuracy -6.6127\n",
      "Epoch 1026 | train: Loss 0.020318 Accuracy 0.2549 | validation: Loss 0.121149 Accuracy -6.6109\n",
      "Epoch 1027 | train: Loss 0.020309 Accuracy 0.2552 | validation: Loss 0.121121 Accuracy -6.6092\n",
      "Epoch 1028 | train: Loss 0.020300 Accuracy 0.2555 | validation: Loss 0.121093 Accuracy -6.6074\n",
      "Epoch 1029 | train: Loss 0.020291 Accuracy 0.2559 | validation: Loss 0.121065 Accuracy -6.6057\n",
      "Epoch 1030 | train: Loss 0.020281 Accuracy 0.2562 | validation: Loss 0.121038 Accuracy -6.6040\n",
      "Epoch 1031 | train: Loss 0.020272 Accuracy 0.2565 | validation: Loss 0.121011 Accuracy -6.6022\n",
      "Epoch 1032 | train: Loss 0.020263 Accuracy 0.2569 | validation: Loss 0.120984 Accuracy -6.6005\n",
      "Epoch 1033 | train: Loss 0.020254 Accuracy 0.2572 | validation: Loss 0.120957 Accuracy -6.5989\n",
      "Epoch 1034 | train: Loss 0.020245 Accuracy 0.2575 | validation: Loss 0.120930 Accuracy -6.5972\n",
      "Epoch 1035 | train: Loss 0.020236 Accuracy 0.2579 | validation: Loss 0.120903 Accuracy -6.5955\n",
      "Epoch 1036 | train: Loss 0.020227 Accuracy 0.2582 | validation: Loss 0.120877 Accuracy -6.5938\n",
      "Epoch 1037 | train: Loss 0.020218 Accuracy 0.2585 | validation: Loss 0.120851 Accuracy -6.5922\n",
      "Epoch 1038 | train: Loss 0.020209 Accuracy 0.2589 | validation: Loss 0.120825 Accuracy -6.5906\n",
      "Epoch 1039 | train: Loss 0.020200 Accuracy 0.2592 | validation: Loss 0.120799 Accuracy -6.5889\n",
      "Epoch 1040 | train: Loss 0.020191 Accuracy 0.2595 | validation: Loss 0.120773 Accuracy -6.5873\n",
      "Epoch 1041 | train: Loss 0.020182 Accuracy 0.2598 | validation: Loss 0.120747 Accuracy -6.5857\n",
      "Epoch 1042 | train: Loss 0.020173 Accuracy 0.2602 | validation: Loss 0.120722 Accuracy -6.5841\n",
      "Epoch 1043 | train: Loss 0.020164 Accuracy 0.2605 | validation: Loss 0.120696 Accuracy -6.5825\n",
      "Epoch 1044 | train: Loss 0.020155 Accuracy 0.2608 | validation: Loss 0.120671 Accuracy -6.5809\n",
      "Epoch 1045 | train: Loss 0.020146 Accuracy 0.2612 | validation: Loss 0.120646 Accuracy -6.5793\n",
      "Epoch 1046 | train: Loss 0.020137 Accuracy 0.2615 | validation: Loss 0.120621 Accuracy -6.5778\n",
      "Epoch 1047 | train: Loss 0.020129 Accuracy 0.2618 | validation: Loss 0.120597 Accuracy -6.5762\n",
      "Epoch 1048 | train: Loss 0.020120 Accuracy 0.2621 | validation: Loss 0.120572 Accuracy -6.5747\n",
      "Epoch 1049 | train: Loss 0.020111 Accuracy 0.2625 | validation: Loss 0.120548 Accuracy -6.5732\n",
      "Epoch 1050 | train: Loss 0.020102 Accuracy 0.2628 | validation: Loss 0.120523 Accuracy -6.5716\n",
      "Epoch 1051 | train: Loss 0.020093 Accuracy 0.2631 | validation: Loss 0.120499 Accuracy -6.5701\n",
      "Epoch 1052 | train: Loss 0.020084 Accuracy 0.2635 | validation: Loss 0.120475 Accuracy -6.5686\n",
      "Epoch 1053 | train: Loss 0.020075 Accuracy 0.2638 | validation: Loss 0.120452 Accuracy -6.5671\n",
      "Epoch 1054 | train: Loss 0.020066 Accuracy 0.2641 | validation: Loss 0.120428 Accuracy -6.5656\n",
      "Epoch 1055 | train: Loss 0.020057 Accuracy 0.2644 | validation: Loss 0.120405 Accuracy -6.5642\n",
      "Epoch 1056 | train: Loss 0.020048 Accuracy 0.2648 | validation: Loss 0.120381 Accuracy -6.5627\n",
      "Epoch 1057 | train: Loss 0.020039 Accuracy 0.2651 | validation: Loss 0.120358 Accuracy -6.5612\n",
      "Epoch 1058 | train: Loss 0.020030 Accuracy 0.2654 | validation: Loss 0.120335 Accuracy -6.5598\n",
      "Epoch 1059 | train: Loss 0.020022 Accuracy 0.2657 | validation: Loss 0.120312 Accuracy -6.5584\n",
      "Epoch 1060 | train: Loss 0.020013 Accuracy 0.2661 | validation: Loss 0.120290 Accuracy -6.5569\n",
      "Epoch 1061 | train: Loss 0.020004 Accuracy 0.2664 | validation: Loss 0.120267 Accuracy -6.5555\n",
      "Epoch 1062 | train: Loss 0.019995 Accuracy 0.2667 | validation: Loss 0.120245 Accuracy -6.5541\n",
      "Epoch 1063 | train: Loss 0.019986 Accuracy 0.2670 | validation: Loss 0.120222 Accuracy -6.5527\n",
      "Epoch 1064 | train: Loss 0.019977 Accuracy 0.2674 | validation: Loss 0.120200 Accuracy -6.5513\n",
      "Epoch 1065 | train: Loss 0.019968 Accuracy 0.2677 | validation: Loss 0.120179 Accuracy -6.5500\n",
      "Epoch 1066 | train: Loss 0.019960 Accuracy 0.2680 | validation: Loss 0.120157 Accuracy -6.5486\n",
      "Epoch 1067 | train: Loss 0.019951 Accuracy 0.2683 | validation: Loss 0.120135 Accuracy -6.5472\n",
      "Epoch 1068 | train: Loss 0.019942 Accuracy 0.2687 | validation: Loss 0.120114 Accuracy -6.5459\n",
      "Epoch 1069 | train: Loss 0.019933 Accuracy 0.2690 | validation: Loss 0.120092 Accuracy -6.5446\n",
      "Epoch 1070 | train: Loss 0.019924 Accuracy 0.2693 | validation: Loss 0.120071 Accuracy -6.5432\n",
      "Epoch 1071 | train: Loss 0.019915 Accuracy 0.2696 | validation: Loss 0.120050 Accuracy -6.5419\n",
      "Epoch 1072 | train: Loss 0.019907 Accuracy 0.2700 | validation: Loss 0.120030 Accuracy -6.5406\n",
      "Epoch 1073 | train: Loss 0.019898 Accuracy 0.2703 | validation: Loss 0.120009 Accuracy -6.5393\n",
      "Epoch 1074 | train: Loss 0.019889 Accuracy 0.2706 | validation: Loss 0.119988 Accuracy -6.5380\n",
      "Epoch 1075 | train: Loss 0.019880 Accuracy 0.2709 | validation: Loss 0.119968 Accuracy -6.5367\n",
      "Epoch 1076 | train: Loss 0.019871 Accuracy 0.2713 | validation: Loss 0.119948 Accuracy -6.5355\n",
      "Epoch 1077 | train: Loss 0.019863 Accuracy 0.2716 | validation: Loss 0.119928 Accuracy -6.5342\n",
      "Epoch 1078 | train: Loss 0.019854 Accuracy 0.2719 | validation: Loss 0.119908 Accuracy -6.5330\n",
      "Epoch 1079 | train: Loss 0.019845 Accuracy 0.2722 | validation: Loss 0.119888 Accuracy -6.5317\n",
      "Epoch 1080 | train: Loss 0.019836 Accuracy 0.2725 | validation: Loss 0.119868 Accuracy -6.5305\n",
      "Epoch 1081 | train: Loss 0.019827 Accuracy 0.2729 | validation: Loss 0.119849 Accuracy -6.5293\n",
      "Epoch 1082 | train: Loss 0.019819 Accuracy 0.2732 | validation: Loss 0.119830 Accuracy -6.5281\n",
      "Epoch 1083 | train: Loss 0.019810 Accuracy 0.2735 | validation: Loss 0.119811 Accuracy -6.5269\n",
      "Epoch 1084 | train: Loss 0.019801 Accuracy 0.2738 | validation: Loss 0.119792 Accuracy -6.5257\n",
      "Epoch 1085 | train: Loss 0.019792 Accuracy 0.2741 | validation: Loss 0.119773 Accuracy -6.5245\n",
      "Epoch 1086 | train: Loss 0.019784 Accuracy 0.2745 | validation: Loss 0.119754 Accuracy -6.5233\n",
      "Epoch 1087 | train: Loss 0.019775 Accuracy 0.2748 | validation: Loss 0.119735 Accuracy -6.5221\n",
      "Epoch 1088 | train: Loss 0.019766 Accuracy 0.2751 | validation: Loss 0.119717 Accuracy -6.5210\n",
      "Epoch 1089 | train: Loss 0.019758 Accuracy 0.2754 | validation: Loss 0.119699 Accuracy -6.5198\n",
      "Epoch 1090 | train: Loss 0.019749 Accuracy 0.2757 | validation: Loss 0.119681 Accuracy -6.5187\n",
      "Epoch 1091 | train: Loss 0.019740 Accuracy 0.2761 | validation: Loss 0.119663 Accuracy -6.5176\n",
      "Epoch 1092 | train: Loss 0.019731 Accuracy 0.2764 | validation: Loss 0.119645 Accuracy -6.5165\n",
      "Epoch 1093 | train: Loss 0.019723 Accuracy 0.2767 | validation: Loss 0.119627 Accuracy -6.5153\n",
      "Epoch 1094 | train: Loss 0.019714 Accuracy 0.2770 | validation: Loss 0.119610 Accuracy -6.5142\n",
      "Epoch 1095 | train: Loss 0.019705 Accuracy 0.2773 | validation: Loss 0.119593 Accuracy -6.5132\n",
      "Epoch 1096 | train: Loss 0.019697 Accuracy 0.2777 | validation: Loss 0.119575 Accuracy -6.5121\n",
      "Epoch 1097 | train: Loss 0.019688 Accuracy 0.2780 | validation: Loss 0.119558 Accuracy -6.5110\n",
      "Epoch 1098 | train: Loss 0.019679 Accuracy 0.2783 | validation: Loss 0.119542 Accuracy -6.5100\n",
      "Epoch 1099 | train: Loss 0.019671 Accuracy 0.2786 | validation: Loss 0.119525 Accuracy -6.5089\n",
      "Epoch 1100 | train: Loss 0.019662 Accuracy 0.2789 | validation: Loss 0.119508 Accuracy -6.5079\n",
      "Epoch 1101 | train: Loss 0.019653 Accuracy 0.2792 | validation: Loss 0.119492 Accuracy -6.5068\n",
      "Epoch 1102 | train: Loss 0.019645 Accuracy 0.2796 | validation: Loss 0.119476 Accuracy -6.5058\n",
      "Epoch 1103 | train: Loss 0.019636 Accuracy 0.2799 | validation: Loss 0.119460 Accuracy -6.5048\n",
      "Epoch 1104 | train: Loss 0.019627 Accuracy 0.2802 | validation: Loss 0.119444 Accuracy -6.5038\n",
      "Epoch 1105 | train: Loss 0.019619 Accuracy 0.2805 | validation: Loss 0.119428 Accuracy -6.5028\n",
      "Epoch 1106 | train: Loss 0.019610 Accuracy 0.2808 | validation: Loss 0.119412 Accuracy -6.5018\n",
      "Epoch 1107 | train: Loss 0.019601 Accuracy 0.2812 | validation: Loss 0.119397 Accuracy -6.5008\n",
      "Epoch 1108 | train: Loss 0.019593 Accuracy 0.2815 | validation: Loss 0.119381 Accuracy -6.4999\n",
      "Epoch 1109 | train: Loss 0.019584 Accuracy 0.2818 | validation: Loss 0.119366 Accuracy -6.4989\n",
      "Epoch 1110 | train: Loss 0.019575 Accuracy 0.2821 | validation: Loss 0.119351 Accuracy -6.4980\n",
      "Epoch 1111 | train: Loss 0.019567 Accuracy 0.2824 | validation: Loss 0.119336 Accuracy -6.4970\n",
      "Epoch 1112 | train: Loss 0.019558 Accuracy 0.2827 | validation: Loss 0.119321 Accuracy -6.4961\n",
      "Epoch 1113 | train: Loss 0.019550 Accuracy 0.2830 | validation: Loss 0.119306 Accuracy -6.4952\n",
      "Epoch 1114 | train: Loss 0.019541 Accuracy 0.2834 | validation: Loss 0.119292 Accuracy -6.4943\n",
      "Epoch 1115 | train: Loss 0.019532 Accuracy 0.2837 | validation: Loss 0.119278 Accuracy -6.4934\n",
      "Epoch 1116 | train: Loss 0.019524 Accuracy 0.2840 | validation: Loss 0.119263 Accuracy -6.4925\n",
      "Epoch 1117 | train: Loss 0.019515 Accuracy 0.2843 | validation: Loss 0.119249 Accuracy -6.4916\n",
      "Epoch 1118 | train: Loss 0.019507 Accuracy 0.2846 | validation: Loss 0.119236 Accuracy -6.4907\n",
      "Epoch 1119 | train: Loss 0.019498 Accuracy 0.2849 | validation: Loss 0.119222 Accuracy -6.4899\n",
      "Epoch 1120 | train: Loss 0.019490 Accuracy 0.2853 | validation: Loss 0.119208 Accuracy -6.4890\n",
      "Epoch 1121 | train: Loss 0.019481 Accuracy 0.2856 | validation: Loss 0.119195 Accuracy -6.4882\n",
      "Epoch 1122 | train: Loss 0.019472 Accuracy 0.2859 | validation: Loss 0.119181 Accuracy -6.4873\n",
      "Epoch 1123 | train: Loss 0.019464 Accuracy 0.2862 | validation: Loss 0.119168 Accuracy -6.4865\n",
      "Epoch 1124 | train: Loss 0.019455 Accuracy 0.2865 | validation: Loss 0.119155 Accuracy -6.4857\n",
      "Epoch 1125 | train: Loss 0.019447 Accuracy 0.2868 | validation: Loss 0.119142 Accuracy -6.4849\n",
      "Epoch 1126 | train: Loss 0.019438 Accuracy 0.2871 | validation: Loss 0.119130 Accuracy -6.4841\n",
      "Epoch 1127 | train: Loss 0.019430 Accuracy 0.2874 | validation: Loss 0.119117 Accuracy -6.4833\n",
      "Epoch 1128 | train: Loss 0.019421 Accuracy 0.2878 | validation: Loss 0.119105 Accuracy -6.4825\n",
      "Epoch 1129 | train: Loss 0.019413 Accuracy 0.2881 | validation: Loss 0.119093 Accuracy -6.4817\n",
      "Epoch 1130 | train: Loss 0.019404 Accuracy 0.2884 | validation: Loss 0.119080 Accuracy -6.4810\n",
      "Epoch 1131 | train: Loss 0.019396 Accuracy 0.2887 | validation: Loss 0.119068 Accuracy -6.4802\n",
      "Epoch 1132 | train: Loss 0.019387 Accuracy 0.2890 | validation: Loss 0.119057 Accuracy -6.4795\n",
      "Epoch 1133 | train: Loss 0.019379 Accuracy 0.2893 | validation: Loss 0.119045 Accuracy -6.4787\n",
      "Epoch 1134 | train: Loss 0.019370 Accuracy 0.2896 | validation: Loss 0.119033 Accuracy -6.4780\n",
      "Epoch 1135 | train: Loss 0.019362 Accuracy 0.2899 | validation: Loss 0.119022 Accuracy -6.4773\n",
      "Epoch 1136 | train: Loss 0.019353 Accuracy 0.2903 | validation: Loss 0.119011 Accuracy -6.4766\n",
      "Epoch 1137 | train: Loss 0.019345 Accuracy 0.2906 | validation: Loss 0.119000 Accuracy -6.4759\n",
      "Epoch 1138 | train: Loss 0.019336 Accuracy 0.2909 | validation: Loss 0.118989 Accuracy -6.4752\n",
      "Epoch 1139 | train: Loss 0.019328 Accuracy 0.2912 | validation: Loss 0.118978 Accuracy -6.4745\n",
      "Epoch 1140 | train: Loss 0.019319 Accuracy 0.2915 | validation: Loss 0.118967 Accuracy -6.4739\n",
      "Epoch 1141 | train: Loss 0.019311 Accuracy 0.2918 | validation: Loss 0.118957 Accuracy -6.4732\n",
      "Epoch 1142 | train: Loss 0.019302 Accuracy 0.2921 | validation: Loss 0.118946 Accuracy -6.4726\n",
      "Epoch 1143 | train: Loss 0.019294 Accuracy 0.2924 | validation: Loss 0.118936 Accuracy -6.4719\n",
      "Epoch 1144 | train: Loss 0.019285 Accuracy 0.2927 | validation: Loss 0.118926 Accuracy -6.4713\n",
      "Epoch 1145 | train: Loss 0.019277 Accuracy 0.2931 | validation: Loss 0.118916 Accuracy -6.4707\n",
      "Epoch 1146 | train: Loss 0.019268 Accuracy 0.2934 | validation: Loss 0.118906 Accuracy -6.4700\n",
      "Epoch 1147 | train: Loss 0.019260 Accuracy 0.2937 | validation: Loss 0.118897 Accuracy -6.4694\n",
      "Epoch 1148 | train: Loss 0.019251 Accuracy 0.2940 | validation: Loss 0.118887 Accuracy -6.4688\n",
      "Epoch 1149 | train: Loss 0.019243 Accuracy 0.2943 | validation: Loss 0.118878 Accuracy -6.4682\n",
      "Epoch 1150 | train: Loss 0.019235 Accuracy 0.2946 | validation: Loss 0.118868 Accuracy -6.4677\n",
      "Epoch 1151 | train: Loss 0.019226 Accuracy 0.2949 | validation: Loss 0.118859 Accuracy -6.4671\n",
      "Epoch 1152 | train: Loss 0.019218 Accuracy 0.2952 | validation: Loss 0.118850 Accuracy -6.4665\n",
      "Epoch 1153 | train: Loss 0.019209 Accuracy 0.2955 | validation: Loss 0.118842 Accuracy -6.4660\n",
      "Epoch 1154 | train: Loss 0.019201 Accuracy 0.2958 | validation: Loss 0.118833 Accuracy -6.4654\n",
      "Epoch 1155 | train: Loss 0.019192 Accuracy 0.2961 | validation: Loss 0.118825 Accuracy -6.4649\n",
      "Epoch 1156 | train: Loss 0.019184 Accuracy 0.2965 | validation: Loss 0.118816 Accuracy -6.4644\n",
      "Epoch 1157 | train: Loss 0.019176 Accuracy 0.2968 | validation: Loss 0.118808 Accuracy -6.4639\n",
      "Epoch 1158 | train: Loss 0.019167 Accuracy 0.2971 | validation: Loss 0.118800 Accuracy -6.4634\n",
      "Epoch 1159 | train: Loss 0.019159 Accuracy 0.2974 | validation: Loss 0.118792 Accuracy -6.4629\n",
      "Epoch 1160 | train: Loss 0.019150 Accuracy 0.2977 | validation: Loss 0.118784 Accuracy -6.4624\n",
      "Epoch 1161 | train: Loss 0.019142 Accuracy 0.2980 | validation: Loss 0.118777 Accuracy -6.4619\n",
      "Epoch 1162 | train: Loss 0.019134 Accuracy 0.2983 | validation: Loss 0.118769 Accuracy -6.4614\n",
      "Epoch 1163 | train: Loss 0.019125 Accuracy 0.2986 | validation: Loss 0.118762 Accuracy -6.4610\n",
      "Epoch 1164 | train: Loss 0.019117 Accuracy 0.2989 | validation: Loss 0.118755 Accuracy -6.4605\n",
      "Epoch 1165 | train: Loss 0.019109 Accuracy 0.2992 | validation: Loss 0.118747 Accuracy -6.4601\n",
      "Epoch 1166 | train: Loss 0.019100 Accuracy 0.2995 | validation: Loss 0.118741 Accuracy -6.4596\n",
      "Epoch 1167 | train: Loss 0.019092 Accuracy 0.2998 | validation: Loss 0.118734 Accuracy -6.4592\n",
      "Epoch 1168 | train: Loss 0.019083 Accuracy 0.3001 | validation: Loss 0.118727 Accuracy -6.4588\n",
      "Epoch 1169 | train: Loss 0.019075 Accuracy 0.3005 | validation: Loss 0.118721 Accuracy -6.4584\n",
      "Epoch 1170 | train: Loss 0.019067 Accuracy 0.3008 | validation: Loss 0.118714 Accuracy -6.4580\n",
      "Epoch 1171 | train: Loss 0.019058 Accuracy 0.3011 | validation: Loss 0.118708 Accuracy -6.4576\n",
      "Epoch 1172 | train: Loss 0.019050 Accuracy 0.3014 | validation: Loss 0.118702 Accuracy -6.4572\n",
      "Epoch 1173 | train: Loss 0.019042 Accuracy 0.3017 | validation: Loss 0.118696 Accuracy -6.4568\n",
      "Epoch 1174 | train: Loss 0.019033 Accuracy 0.3020 | validation: Loss 0.118690 Accuracy -6.4565\n",
      "Epoch 1175 | train: Loss 0.019025 Accuracy 0.3023 | validation: Loss 0.118685 Accuracy -6.4561\n",
      "Epoch 1176 | train: Loss 0.019017 Accuracy 0.3026 | validation: Loss 0.118679 Accuracy -6.4558\n",
      "Epoch 1177 | train: Loss 0.019008 Accuracy 0.3029 | validation: Loss 0.118674 Accuracy -6.4554\n",
      "Epoch 1178 | train: Loss 0.019000 Accuracy 0.3032 | validation: Loss 0.118668 Accuracy -6.4551\n",
      "Epoch 1179 | train: Loss 0.018992 Accuracy 0.3035 | validation: Loss 0.118663 Accuracy -6.4548\n",
      "Epoch 1180 | train: Loss 0.018984 Accuracy 0.3038 | validation: Loss 0.118658 Accuracy -6.4545\n",
      "Epoch 1181 | train: Loss 0.018975 Accuracy 0.3041 | validation: Loss 0.118653 Accuracy -6.4542\n",
      "Epoch 1182 | train: Loss 0.018967 Accuracy 0.3044 | validation: Loss 0.118649 Accuracy -6.4539\n",
      "Epoch 1183 | train: Loss 0.018959 Accuracy 0.3047 | validation: Loss 0.118644 Accuracy -6.4536\n",
      "Epoch 1184 | train: Loss 0.018950 Accuracy 0.3050 | validation: Loss 0.118640 Accuracy -6.4533\n",
      "Epoch 1185 | train: Loss 0.018942 Accuracy 0.3053 | validation: Loss 0.118636 Accuracy -6.4530\n",
      "Epoch 1186 | train: Loss 0.018934 Accuracy 0.3056 | validation: Loss 0.118631 Accuracy -6.4528\n",
      "Epoch 1187 | train: Loss 0.018925 Accuracy 0.3059 | validation: Loss 0.118628 Accuracy -6.4525\n",
      "Epoch 1188 | train: Loss 0.018917 Accuracy 0.3062 | validation: Loss 0.118624 Accuracy -6.4523\n",
      "Epoch 1189 | train: Loss 0.018909 Accuracy 0.3065 | validation: Loss 0.118620 Accuracy -6.4521\n",
      "Epoch 1190 | train: Loss 0.018901 Accuracy 0.3068 | validation: Loss 0.118616 Accuracy -6.4518\n",
      "Epoch 1191 | train: Loss 0.018892 Accuracy 0.3072 | validation: Loss 0.118613 Accuracy -6.4516\n",
      "Epoch 1192 | train: Loss 0.018884 Accuracy 0.3075 | validation: Loss 0.118610 Accuracy -6.4514\n",
      "Epoch 1193 | train: Loss 0.018876 Accuracy 0.3078 | validation: Loss 0.118607 Accuracy -6.4512\n",
      "Epoch 1194 | train: Loss 0.018868 Accuracy 0.3081 | validation: Loss 0.118604 Accuracy -6.4510\n",
      "Epoch 1195 | train: Loss 0.018859 Accuracy 0.3084 | validation: Loss 0.118601 Accuracy -6.4508\n",
      "Epoch 1196 | train: Loss 0.018851 Accuracy 0.3087 | validation: Loss 0.118598 Accuracy -6.4507\n",
      "Epoch 1197 | train: Loss 0.018843 Accuracy 0.3090 | validation: Loss 0.118595 Accuracy -6.4505\n",
      "Epoch 1198 | train: Loss 0.018835 Accuracy 0.3093 | validation: Loss 0.118593 Accuracy -6.4504\n",
      "Epoch 1199 | train: Loss 0.018826 Accuracy 0.3096 | validation: Loss 0.118591 Accuracy -6.4502\n",
      "Epoch 1200 | train: Loss 0.018818 Accuracy 0.3099 | validation: Loss 0.118589 Accuracy -6.4501\n",
      "Epoch 1201 | train: Loss 0.018810 Accuracy 0.3102 | validation: Loss 0.118586 Accuracy -6.4499\n",
      "Epoch 1202 | train: Loss 0.018802 Accuracy 0.3105 | validation: Loss 0.118585 Accuracy -6.4498\n",
      "Epoch 1203 | train: Loss 0.018794 Accuracy 0.3108 | validation: Loss 0.118583 Accuracy -6.4497\n",
      "Epoch 1204 | train: Loss 0.018785 Accuracy 0.3111 | validation: Loss 0.118581 Accuracy -6.4496\n",
      "Epoch 1205 | train: Loss 0.018777 Accuracy 0.3114 | validation: Loss 0.118580 Accuracy -6.4495\n",
      "Epoch 1206 | train: Loss 0.018769 Accuracy 0.3117 | validation: Loss 0.118578 Accuracy -6.4494\n",
      "Epoch 1207 | train: Loss 0.018761 Accuracy 0.3120 | validation: Loss 0.118577 Accuracy -6.4494\n",
      "Epoch 1208 | train: Loss 0.018752 Accuracy 0.3123 | validation: Loss 0.118576 Accuracy -6.4493\n",
      "Epoch 1209 | train: Loss 0.018744 Accuracy 0.3126 | validation: Loss 0.118575 Accuracy -6.4492\n",
      "Epoch 1210 | train: Loss 0.018736 Accuracy 0.3129 | validation: Loss 0.118574 Accuracy -6.4492\n",
      "Epoch 1211 | train: Loss 0.018728 Accuracy 0.3132 | validation: Loss 0.118574 Accuracy -6.4492\n",
      "Epoch 1212 | train: Loss 0.018720 Accuracy 0.3135 | validation: Loss 0.118573 Accuracy -6.4491\n",
      "Epoch 1213 | train: Loss 0.018712 Accuracy 0.3138 | validation: Loss 0.118575 Accuracy -6.4492\n",
      "Epoch 1214 | train: Loss 0.018703 Accuracy 0.3141 | validation: Loss 0.118578 Accuracy -6.4494\n",
      "Epoch 1215 | train: Loss 0.018695 Accuracy 0.3144 | validation: Loss 0.118584 Accuracy -6.4498\n",
      "Epoch 1216 | train: Loss 0.018687 Accuracy 0.3147 | validation: Loss 0.118590 Accuracy -6.4502\n",
      "Epoch 1217 | train: Loss 0.018679 Accuracy 0.3150 | validation: Loss 0.118598 Accuracy -6.4507\n",
      "Epoch 1218 | train: Loss 0.018671 Accuracy 0.3153 | validation: Loss 0.118607 Accuracy -6.4513\n",
      "Epoch 1219 | train: Loss 0.018663 Accuracy 0.3156 | validation: Loss 0.118617 Accuracy -6.4519\n",
      "Epoch 1220 | train: Loss 0.018655 Accuracy 0.3159 | validation: Loss 0.118625 Accuracy -6.4524\n",
      "Epoch 1221 | train: Loss 0.018646 Accuracy 0.3162 | validation: Loss 0.118632 Accuracy -6.4528\n",
      "Epoch 1222 | train: Loss 0.018638 Accuracy 0.3165 | validation: Loss 0.118638 Accuracy -6.4532\n",
      "Epoch 1223 | train: Loss 0.018630 Accuracy 0.3168 | validation: Loss 0.118642 Accuracy -6.4534\n",
      "Epoch 1224 | train: Loss 0.018622 Accuracy 0.3171 | validation: Loss 0.118645 Accuracy -6.4536\n",
      "Epoch 1225 | train: Loss 0.018614 Accuracy 0.3174 | validation: Loss 0.118647 Accuracy -6.4538\n",
      "Epoch 1226 | train: Loss 0.018606 Accuracy 0.3177 | validation: Loss 0.118648 Accuracy -6.4538\n",
      "Epoch 1227 | train: Loss 0.018598 Accuracy 0.3180 | validation: Loss 0.118648 Accuracy -6.4538\n",
      "Epoch 1228 | train: Loss 0.018590 Accuracy 0.3183 | validation: Loss 0.118647 Accuracy -6.4538\n",
      "Epoch 1229 | train: Loss 0.018582 Accuracy 0.3186 | validation: Loss 0.118646 Accuracy -6.4537\n",
      "Epoch 1230 | train: Loss 0.018573 Accuracy 0.3188 | validation: Loss 0.118644 Accuracy -6.4536\n",
      "Epoch 1231 | train: Loss 0.018565 Accuracy 0.3191 | validation: Loss 0.118643 Accuracy -6.4535\n",
      "Epoch 1232 | train: Loss 0.018557 Accuracy 0.3194 | validation: Loss 0.118644 Accuracy -6.4536\n",
      "Epoch 1233 | train: Loss 0.018549 Accuracy 0.3197 | validation: Loss 0.118647 Accuracy -6.4537\n",
      "Epoch 1234 | train: Loss 0.018541 Accuracy 0.3200 | validation: Loss 0.118650 Accuracy -6.4539\n",
      "Epoch 1235 | train: Loss 0.018533 Accuracy 0.3203 | validation: Loss 0.118652 Accuracy -6.4541\n",
      "Epoch 1236 | train: Loss 0.018525 Accuracy 0.3206 | validation: Loss 0.118654 Accuracy -6.4542\n",
      "Epoch 1237 | train: Loss 0.018517 Accuracy 0.3209 | validation: Loss 0.118654 Accuracy -6.4542\n",
      "Epoch 1238 | train: Loss 0.018509 Accuracy 0.3212 | validation: Loss 0.118654 Accuracy -6.4542\n",
      "Epoch 1239 | train: Loss 0.018501 Accuracy 0.3215 | validation: Loss 0.118653 Accuracy -6.4542\n",
      "Epoch 1240 | train: Loss 0.018493 Accuracy 0.3218 | validation: Loss 0.118652 Accuracy -6.4541\n",
      "Epoch 1241 | train: Loss 0.018485 Accuracy 0.3221 | validation: Loss 0.118652 Accuracy -6.4541\n",
      "Epoch 1242 | train: Loss 0.018477 Accuracy 0.3224 | validation: Loss 0.118654 Accuracy -6.4542\n",
      "Epoch 1243 | train: Loss 0.018469 Accuracy 0.3227 | validation: Loss 0.118657 Accuracy -6.4544\n",
      "Epoch 1244 | train: Loss 0.018461 Accuracy 0.3230 | validation: Loss 0.118660 Accuracy -6.4546\n",
      "Epoch 1245 | train: Loss 0.018452 Accuracy 0.3233 | validation: Loss 0.118661 Accuracy -6.4547\n",
      "Epoch 1246 | train: Loss 0.018444 Accuracy 0.3236 | validation: Loss 0.118662 Accuracy -6.4547\n",
      "Epoch 1247 | train: Loss 0.018436 Accuracy 0.3239 | validation: Loss 0.118663 Accuracy -6.4547\n",
      "Epoch 1248 | train: Loss 0.018428 Accuracy 0.3242 | validation: Loss 0.118662 Accuracy -6.4547\n",
      "Epoch 1249 | train: Loss 0.018420 Accuracy 0.3245 | validation: Loss 0.118662 Accuracy -6.4547\n",
      "Epoch 1250 | train: Loss 0.018412 Accuracy 0.3248 | validation: Loss 0.118663 Accuracy -6.4548\n",
      "Epoch 1251 | train: Loss 0.018404 Accuracy 0.3251 | validation: Loss 0.118666 Accuracy -6.4550\n",
      "Epoch 1252 | train: Loss 0.018396 Accuracy 0.3253 | validation: Loss 0.118671 Accuracy -6.4552\n",
      "Epoch 1253 | train: Loss 0.018388 Accuracy 0.3256 | validation: Loss 0.118674 Accuracy -6.4555\n",
      "Epoch 1254 | train: Loss 0.018380 Accuracy 0.3259 | validation: Loss 0.118677 Accuracy -6.4556\n",
      "Epoch 1255 | train: Loss 0.018372 Accuracy 0.3262 | validation: Loss 0.118680 Accuracy -6.4558\n",
      "Epoch 1256 | train: Loss 0.018364 Accuracy 0.3265 | validation: Loss 0.118682 Accuracy -6.4559\n",
      "Epoch 1257 | train: Loss 0.018356 Accuracy 0.3268 | validation: Loss 0.118683 Accuracy -6.4560\n",
      "Epoch 1258 | train: Loss 0.018348 Accuracy 0.3271 | validation: Loss 0.118686 Accuracy -6.4562\n",
      "Epoch 1259 | train: Loss 0.018340 Accuracy 0.3274 | validation: Loss 0.118691 Accuracy -6.4565\n",
      "Epoch 1260 | train: Loss 0.018332 Accuracy 0.3277 | validation: Loss 0.118697 Accuracy -6.4569\n",
      "Epoch 1261 | train: Loss 0.018324 Accuracy 0.3280 | validation: Loss 0.118703 Accuracy -6.4573\n",
      "Epoch 1262 | train: Loss 0.018316 Accuracy 0.3283 | validation: Loss 0.118708 Accuracy -6.4576\n",
      "Epoch 1263 | train: Loss 0.018308 Accuracy 0.3286 | validation: Loss 0.118712 Accuracy -6.4578\n",
      "Epoch 1264 | train: Loss 0.018300 Accuracy 0.3289 | validation: Loss 0.118715 Accuracy -6.4580\n",
      "Epoch 1265 | train: Loss 0.018292 Accuracy 0.3292 | validation: Loss 0.118718 Accuracy -6.4582\n",
      "Epoch 1266 | train: Loss 0.018284 Accuracy 0.3294 | validation: Loss 0.118721 Accuracy -6.4584\n",
      "Epoch 1267 | train: Loss 0.018277 Accuracy 0.3297 | validation: Loss 0.118725 Accuracy -6.4587\n",
      "Epoch 1268 | train: Loss 0.018269 Accuracy 0.3300 | validation: Loss 0.118731 Accuracy -6.4590\n",
      "Epoch 1269 | train: Loss 0.018261 Accuracy 0.3303 | validation: Loss 0.118739 Accuracy -6.4595\n",
      "Epoch 1270 | train: Loss 0.018253 Accuracy 0.3306 | validation: Loss 0.118745 Accuracy -6.4599\n",
      "Epoch 1271 | train: Loss 0.018245 Accuracy 0.3309 | validation: Loss 0.118751 Accuracy -6.4603\n",
      "Epoch 1272 | train: Loss 0.018237 Accuracy 0.3312 | validation: Loss 0.118756 Accuracy -6.4606\n",
      "Epoch 1273 | train: Loss 0.018229 Accuracy 0.3315 | validation: Loss 0.118761 Accuracy -6.4609\n",
      "Epoch 1274 | train: Loss 0.018221 Accuracy 0.3318 | validation: Loss 0.118765 Accuracy -6.4612\n",
      "Epoch 1275 | train: Loss 0.018213 Accuracy 0.3321 | validation: Loss 0.118771 Accuracy -6.4616\n",
      "Epoch 1276 | train: Loss 0.018205 Accuracy 0.3324 | validation: Loss 0.118779 Accuracy -6.4620\n",
      "Epoch 1277 | train: Loss 0.018197 Accuracy 0.3327 | validation: Loss 0.118785 Accuracy -6.4624\n",
      "Epoch 1278 | train: Loss 0.018189 Accuracy 0.3329 | validation: Loss 0.118791 Accuracy -6.4628\n",
      "Epoch 1279 | train: Loss 0.018181 Accuracy 0.3332 | validation: Loss 0.118797 Accuracy -6.4632\n",
      "Epoch 1280 | train: Loss 0.018173 Accuracy 0.3335 | validation: Loss 0.118804 Accuracy -6.4636\n",
      "Epoch 1281 | train: Loss 0.018165 Accuracy 0.3338 | validation: Loss 0.118813 Accuracy -6.4642\n",
      "Epoch 1282 | train: Loss 0.018158 Accuracy 0.3341 | validation: Loss 0.118821 Accuracy -6.4647\n",
      "Epoch 1283 | train: Loss 0.018150 Accuracy 0.3344 | validation: Loss 0.118828 Accuracy -6.4651\n",
      "Epoch 1284 | train: Loss 0.018142 Accuracy 0.3347 | validation: Loss 0.118834 Accuracy -6.4655\n",
      "Epoch 1285 | train: Loss 0.018134 Accuracy 0.3350 | validation: Loss 0.118840 Accuracy -6.4659\n",
      "Epoch 1286 | train: Loss 0.018126 Accuracy 0.3353 | validation: Loss 0.118848 Accuracy -6.4664\n",
      "Epoch 1287 | train: Loss 0.018118 Accuracy 0.3356 | validation: Loss 0.118857 Accuracy -6.4669\n",
      "Epoch 1288 | train: Loss 0.018110 Accuracy 0.3358 | validation: Loss 0.118865 Accuracy -6.4674\n",
      "Epoch 1289 | train: Loss 0.018102 Accuracy 0.3361 | validation: Loss 0.118873 Accuracy -6.4679\n",
      "Epoch 1290 | train: Loss 0.018094 Accuracy 0.3364 | validation: Loss 0.118880 Accuracy -6.4684\n",
      "Epoch 1291 | train: Loss 0.018086 Accuracy 0.3367 | validation: Loss 0.118888 Accuracy -6.4689\n",
      "Epoch 1292 | train: Loss 0.018079 Accuracy 0.3370 | validation: Loss 0.118898 Accuracy -6.4695\n",
      "Epoch 1293 | train: Loss 0.018071 Accuracy 0.3373 | validation: Loss 0.118907 Accuracy -6.4701\n",
      "Epoch 1294 | train: Loss 0.018063 Accuracy 0.3376 | validation: Loss 0.118916 Accuracy -6.4706\n",
      "Epoch 1295 | train: Loss 0.018055 Accuracy 0.3379 | validation: Loss 0.118923 Accuracy -6.4711\n",
      "Epoch 1296 | train: Loss 0.018047 Accuracy 0.3382 | validation: Loss 0.118931 Accuracy -6.4716\n",
      "Epoch 1297 | train: Loss 0.018039 Accuracy 0.3384 | validation: Loss 0.118939 Accuracy -6.4721\n",
      "Epoch 1298 | train: Loss 0.018031 Accuracy 0.3387 | validation: Loss 0.118950 Accuracy -6.4728\n",
      "Epoch 1299 | train: Loss 0.018024 Accuracy 0.3390 | validation: Loss 0.118959 Accuracy -6.4734\n",
      "Epoch 1300 | train: Loss 0.018016 Accuracy 0.3393 | validation: Loss 0.118968 Accuracy -6.4739\n",
      "Epoch 1301 | train: Loss 0.018008 Accuracy 0.3396 | validation: Loss 0.118976 Accuracy -6.4744\n",
      "Epoch 1302 | train: Loss 0.018000 Accuracy 0.3399 | validation: Loss 0.118986 Accuracy -6.4750\n",
      "Epoch 1303 | train: Loss 0.017992 Accuracy 0.3402 | validation: Loss 0.118997 Accuracy -6.4757\n",
      "Epoch 1304 | train: Loss 0.017984 Accuracy 0.3405 | validation: Loss 0.119007 Accuracy -6.4764\n",
      "Epoch 1305 | train: Loss 0.017977 Accuracy 0.3407 | validation: Loss 0.119017 Accuracy -6.4770\n",
      "Epoch 1306 | train: Loss 0.017969 Accuracy 0.3410 | validation: Loss 0.119026 Accuracy -6.4775\n",
      "Epoch 1307 | train: Loss 0.017961 Accuracy 0.3413 | validation: Loss 0.119034 Accuracy -6.4781\n",
      "Epoch 1308 | train: Loss 0.017953 Accuracy 0.3416 | validation: Loss 0.119044 Accuracy -6.4787\n",
      "Epoch 1309 | train: Loss 0.017945 Accuracy 0.3419 | validation: Loss 0.119055 Accuracy -6.4794\n",
      "Epoch 1310 | train: Loss 0.017937 Accuracy 0.3422 | validation: Loss 0.119066 Accuracy -6.4801\n",
      "Epoch 1311 | train: Loss 0.017930 Accuracy 0.3425 | validation: Loss 0.119076 Accuracy -6.4807\n",
      "Epoch 1312 | train: Loss 0.017922 Accuracy 0.3427 | validation: Loss 0.119087 Accuracy -6.4814\n",
      "Epoch 1313 | train: Loss 0.017914 Accuracy 0.3430 | validation: Loss 0.119098 Accuracy -6.4821\n",
      "Epoch 1314 | train: Loss 0.017906 Accuracy 0.3433 | validation: Loss 0.119108 Accuracy -6.4827\n",
      "Epoch 1315 | train: Loss 0.017898 Accuracy 0.3436 | validation: Loss 0.119120 Accuracy -6.4834\n",
      "Epoch 1316 | train: Loss 0.017891 Accuracy 0.3439 | validation: Loss 0.119131 Accuracy -6.4841\n",
      "Epoch 1317 | train: Loss 0.017883 Accuracy 0.3442 | validation: Loss 0.119143 Accuracy -6.4849\n",
      "Epoch 1318 | train: Loss 0.017875 Accuracy 0.3445 | validation: Loss 0.119154 Accuracy -6.4856\n",
      "Epoch 1319 | train: Loss 0.017867 Accuracy 0.3448 | validation: Loss 0.119165 Accuracy -6.4863\n",
      "Epoch 1320 | train: Loss 0.017859 Accuracy 0.3450 | validation: Loss 0.119175 Accuracy -6.4869\n",
      "Epoch 1321 | train: Loss 0.017852 Accuracy 0.3453 | validation: Loss 0.119187 Accuracy -6.4877\n",
      "Epoch 1322 | train: Loss 0.017844 Accuracy 0.3456 | validation: Loss 0.119201 Accuracy -6.4885\n",
      "Epoch 1323 | train: Loss 0.017836 Accuracy 0.3459 | validation: Loss 0.119213 Accuracy -6.4893\n",
      "Epoch 1324 | train: Loss 0.017828 Accuracy 0.3462 | validation: Loss 0.119225 Accuracy -6.4900\n",
      "Epoch 1325 | train: Loss 0.017820 Accuracy 0.3465 | validation: Loss 0.119236 Accuracy -6.4907\n",
      "Epoch 1326 | train: Loss 0.017813 Accuracy 0.3467 | validation: Loss 0.119248 Accuracy -6.4915\n",
      "Epoch 1327 | train: Loss 0.017805 Accuracy 0.3470 | validation: Loss 0.119262 Accuracy -6.4924\n",
      "Epoch 1328 | train: Loss 0.017797 Accuracy 0.3473 | validation: Loss 0.119276 Accuracy -6.4932\n",
      "Epoch 1329 | train: Loss 0.017789 Accuracy 0.3476 | validation: Loss 0.119288 Accuracy -6.4940\n",
      "Epoch 1330 | train: Loss 0.017782 Accuracy 0.3479 | validation: Loss 0.119300 Accuracy -6.4947\n",
      "Epoch 1331 | train: Loss 0.017774 Accuracy 0.3482 | validation: Loss 0.119310 Accuracy -6.4954\n",
      "Epoch 1332 | train: Loss 0.017766 Accuracy 0.3485 | validation: Loss 0.119323 Accuracy -6.4962\n",
      "Epoch 1333 | train: Loss 0.017758 Accuracy 0.3487 | validation: Loss 0.119338 Accuracy -6.4971\n",
      "Epoch 1334 | train: Loss 0.017751 Accuracy 0.3490 | validation: Loss 0.119353 Accuracy -6.4981\n",
      "Epoch 1335 | train: Loss 0.017743 Accuracy 0.3493 | validation: Loss 0.119368 Accuracy -6.4991\n",
      "Epoch 1336 | train: Loss 0.017735 Accuracy 0.3496 | validation: Loss 0.119382 Accuracy -6.4999\n",
      "Epoch 1337 | train: Loss 0.017727 Accuracy 0.3499 | validation: Loss 0.119395 Accuracy -6.5007\n",
      "Epoch 1338 | train: Loss 0.017720 Accuracy 0.3502 | validation: Loss 0.119407 Accuracy -6.5015\n",
      "Epoch 1339 | train: Loss 0.017712 Accuracy 0.3504 | validation: Loss 0.119419 Accuracy -6.5022\n",
      "Epoch 1340 | train: Loss 0.017704 Accuracy 0.3507 | validation: Loss 0.119432 Accuracy -6.5031\n",
      "Epoch 1341 | train: Loss 0.017697 Accuracy 0.3510 | validation: Loss 0.119447 Accuracy -6.5040\n",
      "Epoch 1342 | train: Loss 0.017689 Accuracy 0.3513 | validation: Loss 0.119463 Accuracy -6.5050\n",
      "Epoch 1343 | train: Loss 0.017681 Accuracy 0.3516 | validation: Loss 0.119479 Accuracy -6.5060\n",
      "Epoch 1344 | train: Loss 0.017673 Accuracy 0.3519 | validation: Loss 0.119493 Accuracy -6.5069\n",
      "Epoch 1345 | train: Loss 0.017666 Accuracy 0.3521 | validation: Loss 0.119507 Accuracy -6.5078\n",
      "Epoch 1346 | train: Loss 0.017658 Accuracy 0.3524 | validation: Loss 0.119520 Accuracy -6.5086\n",
      "Epoch 1347 | train: Loss 0.017650 Accuracy 0.3527 | validation: Loss 0.119533 Accuracy -6.5094\n",
      "Epoch 1348 | train: Loss 0.017643 Accuracy 0.3530 | validation: Loss 0.119547 Accuracy -6.5103\n",
      "Epoch 1349 | train: Loss 0.017635 Accuracy 0.3533 | validation: Loss 0.119563 Accuracy -6.5113\n",
      "Epoch 1350 | train: Loss 0.017627 Accuracy 0.3536 | validation: Loss 0.119580 Accuracy -6.5124\n",
      "Epoch 1351 | train: Loss 0.017620 Accuracy 0.3538 | validation: Loss 0.119596 Accuracy -6.5134\n",
      "Epoch 1352 | train: Loss 0.017612 Accuracy 0.3541 | validation: Loss 0.119612 Accuracy -6.5144\n",
      "Epoch 1353 | train: Loss 0.017604 Accuracy 0.3544 | validation: Loss 0.119627 Accuracy -6.5153\n",
      "Epoch 1354 | train: Loss 0.017596 Accuracy 0.3547 | validation: Loss 0.119643 Accuracy -6.5163\n",
      "Epoch 1355 | train: Loss 0.017589 Accuracy 0.3550 | validation: Loss 0.119658 Accuracy -6.5173\n",
      "Epoch 1356 | train: Loss 0.017581 Accuracy 0.3552 | validation: Loss 0.119673 Accuracy -6.5182\n",
      "Epoch 1357 | train: Loss 0.017573 Accuracy 0.3555 | validation: Loss 0.119689 Accuracy -6.5192\n",
      "Epoch 1358 | train: Loss 0.017566 Accuracy 0.3558 | validation: Loss 0.119707 Accuracy -6.5204\n",
      "Epoch 1359 | train: Loss 0.017558 Accuracy 0.3561 | validation: Loss 0.119724 Accuracy -6.5214\n",
      "Epoch 1360 | train: Loss 0.017550 Accuracy 0.3564 | validation: Loss 0.119740 Accuracy -6.5224\n",
      "Epoch 1361 | train: Loss 0.017543 Accuracy 0.3566 | validation: Loss 0.119755 Accuracy -6.5234\n",
      "Epoch 1362 | train: Loss 0.017535 Accuracy 0.3569 | validation: Loss 0.119772 Accuracy -6.5244\n",
      "Epoch 1363 | train: Loss 0.017527 Accuracy 0.3572 | validation: Loss 0.119791 Accuracy -6.5256\n",
      "Epoch 1364 | train: Loss 0.017520 Accuracy 0.3575 | validation: Loss 0.119808 Accuracy -6.5267\n",
      "Epoch 1365 | train: Loss 0.017512 Accuracy 0.3578 | validation: Loss 0.119825 Accuracy -6.5277\n",
      "Epoch 1366 | train: Loss 0.017504 Accuracy 0.3581 | validation: Loss 0.119840 Accuracy -6.5287\n",
      "Epoch 1367 | train: Loss 0.017497 Accuracy 0.3583 | validation: Loss 0.119855 Accuracy -6.5297\n",
      "Epoch 1368 | train: Loss 0.017489 Accuracy 0.3586 | validation: Loss 0.119872 Accuracy -6.5307\n",
      "Epoch 1369 | train: Loss 0.017482 Accuracy 0.3589 | validation: Loss 0.119891 Accuracy -6.5319\n",
      "Epoch 1370 | train: Loss 0.017474 Accuracy 0.3592 | validation: Loss 0.119910 Accuracy -6.5331\n",
      "Epoch 1371 | train: Loss 0.017466 Accuracy 0.3595 | validation: Loss 0.119929 Accuracy -6.5343\n",
      "Epoch 1372 | train: Loss 0.017459 Accuracy 0.3597 | validation: Loss 0.119947 Accuracy -6.5354\n",
      "Epoch 1373 | train: Loss 0.017451 Accuracy 0.3600 | validation: Loss 0.119964 Accuracy -6.5365\n",
      "Epoch 1374 | train: Loss 0.017443 Accuracy 0.3603 | validation: Loss 0.119980 Accuracy -6.5375\n",
      "Epoch 1375 | train: Loss 0.017436 Accuracy 0.3606 | validation: Loss 0.119998 Accuracy -6.5387\n",
      "Epoch 1376 | train: Loss 0.017428 Accuracy 0.3609 | validation: Loss 0.120018 Accuracy -6.5399\n",
      "Epoch 1377 | train: Loss 0.017421 Accuracy 0.3611 | validation: Loss 0.120039 Accuracy -6.5412\n",
      "Epoch 1378 | train: Loss 0.017413 Accuracy 0.3614 | validation: Loss 0.120059 Accuracy -6.5424\n",
      "Epoch 1379 | train: Loss 0.017405 Accuracy 0.3617 | validation: Loss 0.120077 Accuracy -6.5436\n",
      "Epoch 1380 | train: Loss 0.017398 Accuracy 0.3620 | validation: Loss 0.120095 Accuracy -6.5447\n",
      "Epoch 1381 | train: Loss 0.017390 Accuracy 0.3622 | validation: Loss 0.120112 Accuracy -6.5458\n",
      "Epoch 1382 | train: Loss 0.017383 Accuracy 0.3625 | validation: Loss 0.120128 Accuracy -6.5468\n",
      "Epoch 1383 | train: Loss 0.017375 Accuracy 0.3628 | validation: Loss 0.120147 Accuracy -6.5480\n",
      "Epoch 1384 | train: Loss 0.017367 Accuracy 0.3631 | validation: Loss 0.120166 Accuracy -6.5492\n",
      "Epoch 1385 | train: Loss 0.017360 Accuracy 0.3634 | validation: Loss 0.120188 Accuracy -6.5505\n",
      "Epoch 1386 | train: Loss 0.017352 Accuracy 0.3636 | validation: Loss 0.120208 Accuracy -6.5518\n",
      "Epoch 1387 | train: Loss 0.017345 Accuracy 0.3639 | validation: Loss 0.120227 Accuracy -6.5530\n",
      "Epoch 1388 | train: Loss 0.017337 Accuracy 0.3642 | validation: Loss 0.120245 Accuracy -6.5542\n",
      "Epoch 1389 | train: Loss 0.017329 Accuracy 0.3645 | validation: Loss 0.120263 Accuracy -6.5553\n",
      "Epoch 1390 | train: Loss 0.017322 Accuracy 0.3648 | validation: Loss 0.120282 Accuracy -6.5565\n",
      "Epoch 1391 | train: Loss 0.017314 Accuracy 0.3650 | validation: Loss 0.120303 Accuracy -6.5578\n",
      "Epoch 1392 | train: Loss 0.017307 Accuracy 0.3653 | validation: Loss 0.120323 Accuracy -6.5591\n",
      "Epoch 1393 | train: Loss 0.017299 Accuracy 0.3656 | validation: Loss 0.120343 Accuracy -6.5603\n",
      "Epoch 1394 | train: Loss 0.017291 Accuracy 0.3659 | validation: Loss 0.120364 Accuracy -6.5616\n",
      "Epoch 1395 | train: Loss 0.017284 Accuracy 0.3661 | validation: Loss 0.120384 Accuracy -6.5629\n",
      "Epoch 1396 | train: Loss 0.017276 Accuracy 0.3664 | validation: Loss 0.120403 Accuracy -6.5641\n",
      "Epoch 1397 | train: Loss 0.017269 Accuracy 0.3667 | validation: Loss 0.120424 Accuracy -6.5654\n",
      "Epoch 1398 | train: Loss 0.017261 Accuracy 0.3670 | validation: Loss 0.120447 Accuracy -6.5668\n",
      "Epoch 1399 | train: Loss 0.017254 Accuracy 0.3673 | validation: Loss 0.120468 Accuracy -6.5682\n",
      "Epoch 1400 | train: Loss 0.017246 Accuracy 0.3675 | validation: Loss 0.120489 Accuracy -6.5695\n",
      "Epoch 1401 | train: Loss 0.017239 Accuracy 0.3678 | validation: Loss 0.120509 Accuracy -6.5707\n",
      "Epoch 1402 | train: Loss 0.017231 Accuracy 0.3681 | validation: Loss 0.120528 Accuracy -6.5720\n",
      "Epoch 1403 | train: Loss 0.017223 Accuracy 0.3684 | validation: Loss 0.120550 Accuracy -6.5733\n",
      "Epoch 1404 | train: Loss 0.017216 Accuracy 0.3686 | validation: Loss 0.120572 Accuracy -6.5747\n",
      "Epoch 1405 | train: Loss 0.017208 Accuracy 0.3689 | validation: Loss 0.120597 Accuracy -6.5762\n",
      "Epoch 1406 | train: Loss 0.017201 Accuracy 0.3692 | validation: Loss 0.120620 Accuracy -6.5777\n",
      "Epoch 1407 | train: Loss 0.017193 Accuracy 0.3695 | validation: Loss 0.120642 Accuracy -6.5791\n",
      "Epoch 1408 | train: Loss 0.017186 Accuracy 0.3697 | validation: Loss 0.120664 Accuracy -6.5804\n",
      "Epoch 1409 | train: Loss 0.017178 Accuracy 0.3700 | validation: Loss 0.120684 Accuracy -6.5817\n",
      "Epoch 1410 | train: Loss 0.017171 Accuracy 0.3703 | validation: Loss 0.120704 Accuracy -6.5830\n",
      "Epoch 1411 | train: Loss 0.017163 Accuracy 0.3706 | validation: Loss 0.120726 Accuracy -6.5844\n",
      "Epoch 1412 | train: Loss 0.017156 Accuracy 0.3708 | validation: Loss 0.120750 Accuracy -6.5859\n",
      "Epoch 1413 | train: Loss 0.017148 Accuracy 0.3711 | validation: Loss 0.120775 Accuracy -6.5874\n",
      "Epoch 1414 | train: Loss 0.017141 Accuracy 0.3714 | validation: Loss 0.120801 Accuracy -6.5891\n",
      "Epoch 1415 | train: Loss 0.017133 Accuracy 0.3717 | validation: Loss 0.120827 Accuracy -6.5907\n",
      "Epoch 1416 | train: Loss 0.017126 Accuracy 0.3719 | validation: Loss 0.120851 Accuracy -6.5922\n",
      "Epoch 1417 | train: Loss 0.017118 Accuracy 0.3722 | validation: Loss 0.120874 Accuracy -6.5937\n",
      "Epoch 1418 | train: Loss 0.017111 Accuracy 0.3725 | validation: Loss 0.120896 Accuracy -6.5951\n",
      "Epoch 1419 | train: Loss 0.017103 Accuracy 0.3728 | validation: Loss 0.120918 Accuracy -6.5964\n",
      "Epoch 1420 | train: Loss 0.017095 Accuracy 0.3731 | validation: Loss 0.120938 Accuracy -6.5977\n",
      "Epoch 1421 | train: Loss 0.017088 Accuracy 0.3733 | validation: Loss 0.120961 Accuracy -6.5991\n",
      "Epoch 1422 | train: Loss 0.017080 Accuracy 0.3736 | validation: Loss 0.120985 Accuracy -6.6006\n",
      "Epoch 1423 | train: Loss 0.017073 Accuracy 0.3739 | validation: Loss 0.121011 Accuracy -6.6023\n",
      "Epoch 1424 | train: Loss 0.017066 Accuracy 0.3742 | validation: Loss 0.121036 Accuracy -6.6038\n",
      "Epoch 1425 | train: Loss 0.017058 Accuracy 0.3744 | validation: Loss 0.121060 Accuracy -6.6053\n",
      "Epoch 1426 | train: Loss 0.017051 Accuracy 0.3747 | validation: Loss 0.121083 Accuracy -6.6068\n",
      "Epoch 1427 | train: Loss 0.017043 Accuracy 0.3750 | validation: Loss 0.121108 Accuracy -6.6083\n",
      "Epoch 1428 | train: Loss 0.017036 Accuracy 0.3752 | validation: Loss 0.121134 Accuracy -6.6100\n",
      "Epoch 1429 | train: Loss 0.017028 Accuracy 0.3755 | validation: Loss 0.121159 Accuracy -6.6116\n",
      "Epoch 1430 | train: Loss 0.017021 Accuracy 0.3758 | validation: Loss 0.121183 Accuracy -6.6131\n",
      "Epoch 1431 | train: Loss 0.017013 Accuracy 0.3761 | validation: Loss 0.121207 Accuracy -6.6146\n",
      "Epoch 1432 | train: Loss 0.017006 Accuracy 0.3763 | validation: Loss 0.121232 Accuracy -6.6161\n",
      "Epoch 1433 | train: Loss 0.016998 Accuracy 0.3766 | validation: Loss 0.121256 Accuracy -6.6177\n",
      "Epoch 1434 | train: Loss 0.016991 Accuracy 0.3769 | validation: Loss 0.121282 Accuracy -6.6193\n",
      "Epoch 1435 | train: Loss 0.016983 Accuracy 0.3772 | validation: Loss 0.121309 Accuracy -6.6210\n",
      "Epoch 1436 | train: Loss 0.016976 Accuracy 0.3774 | validation: Loss 0.121336 Accuracy -6.6227\n",
      "Epoch 1437 | train: Loss 0.016968 Accuracy 0.3777 | validation: Loss 0.121361 Accuracy -6.6243\n",
      "Epoch 1438 | train: Loss 0.016961 Accuracy 0.3780 | validation: Loss 0.121385 Accuracy -6.6258\n",
      "Epoch 1439 | train: Loss 0.016953 Accuracy 0.3783 | validation: Loss 0.121409 Accuracy -6.6273\n",
      "Epoch 1440 | train: Loss 0.016946 Accuracy 0.3785 | validation: Loss 0.121434 Accuracy -6.6289\n",
      "Epoch 1441 | train: Loss 0.016939 Accuracy 0.3788 | validation: Loss 0.121461 Accuracy -6.6306\n",
      "Epoch 1442 | train: Loss 0.016931 Accuracy 0.3791 | validation: Loss 0.121490 Accuracy -6.6323\n",
      "Epoch 1443 | train: Loss 0.016924 Accuracy 0.3794 | validation: Loss 0.121517 Accuracy -6.6341\n",
      "Epoch 1444 | train: Loss 0.016916 Accuracy 0.3796 | validation: Loss 0.121543 Accuracy -6.6357\n",
      "Epoch 1445 | train: Loss 0.016909 Accuracy 0.3799 | validation: Loss 0.121568 Accuracy -6.6373\n",
      "Epoch 1446 | train: Loss 0.016901 Accuracy 0.3802 | validation: Loss 0.121593 Accuracy -6.6388\n",
      "Epoch 1447 | train: Loss 0.016894 Accuracy 0.3804 | validation: Loss 0.121619 Accuracy -6.6405\n",
      "Epoch 1448 | train: Loss 0.016887 Accuracy 0.3807 | validation: Loss 0.121647 Accuracy -6.6422\n",
      "Epoch 1449 | train: Loss 0.016879 Accuracy 0.3810 | validation: Loss 0.121674 Accuracy -6.6439\n",
      "Epoch 1450 | train: Loss 0.016872 Accuracy 0.3813 | validation: Loss 0.121700 Accuracy -6.6455\n",
      "Epoch 1451 | train: Loss 0.016864 Accuracy 0.3815 | validation: Loss 0.121727 Accuracy -6.6473\n",
      "Epoch 1452 | train: Loss 0.016857 Accuracy 0.3818 | validation: Loss 0.121756 Accuracy -6.6491\n",
      "Epoch 1453 | train: Loss 0.016849 Accuracy 0.3821 | validation: Loss 0.121784 Accuracy -6.6508\n",
      "Epoch 1454 | train: Loss 0.016842 Accuracy 0.3823 | validation: Loss 0.121811 Accuracy -6.6525\n",
      "Epoch 1455 | train: Loss 0.016835 Accuracy 0.3826 | validation: Loss 0.121837 Accuracy -6.6542\n",
      "Epoch 1456 | train: Loss 0.016827 Accuracy 0.3829 | validation: Loss 0.121865 Accuracy -6.6559\n",
      "Epoch 1457 | train: Loss 0.016820 Accuracy 0.3832 | validation: Loss 0.121891 Accuracy -6.6576\n",
      "Epoch 1458 | train: Loss 0.016812 Accuracy 0.3834 | validation: Loss 0.121919 Accuracy -6.6593\n",
      "Epoch 1459 | train: Loss 0.016805 Accuracy 0.3837 | validation: Loss 0.121949 Accuracy -6.6612\n",
      "Epoch 1460 | train: Loss 0.016798 Accuracy 0.3840 | validation: Loss 0.121978 Accuracy -6.6630\n",
      "Epoch 1461 | train: Loss 0.016790 Accuracy 0.3842 | validation: Loss 0.122005 Accuracy -6.6647\n",
      "Epoch 1462 | train: Loss 0.016783 Accuracy 0.3845 | validation: Loss 0.122032 Accuracy -6.6664\n",
      "Epoch 1463 | train: Loss 0.016776 Accuracy 0.3848 | validation: Loss 0.122058 Accuracy -6.6680\n",
      "Epoch 1464 | train: Loss 0.016768 Accuracy 0.3851 | validation: Loss 0.122085 Accuracy -6.6698\n",
      "Epoch 1465 | train: Loss 0.016761 Accuracy 0.3853 | validation: Loss 0.122115 Accuracy -6.6716\n",
      "Epoch 1466 | train: Loss 0.016753 Accuracy 0.3856 | validation: Loss 0.122146 Accuracy -6.6735\n",
      "Epoch 1467 | train: Loss 0.016746 Accuracy 0.3859 | validation: Loss 0.122175 Accuracy -6.6754\n",
      "Epoch 1468 | train: Loss 0.016739 Accuracy 0.3861 | validation: Loss 0.122204 Accuracy -6.6772\n",
      "Epoch 1469 | train: Loss 0.016731 Accuracy 0.3864 | validation: Loss 0.122231 Accuracy -6.6789\n",
      "Epoch 1470 | train: Loss 0.016724 Accuracy 0.3867 | validation: Loss 0.122261 Accuracy -6.6808\n",
      "Epoch 1471 | train: Loss 0.016717 Accuracy 0.3869 | validation: Loss 0.122291 Accuracy -6.6827\n",
      "Epoch 1472 | train: Loss 0.016709 Accuracy 0.3872 | validation: Loss 0.122321 Accuracy -6.6846\n",
      "Epoch 1473 | train: Loss 0.016702 Accuracy 0.3875 | validation: Loss 0.122349 Accuracy -6.6863\n",
      "Epoch 1474 | train: Loss 0.016694 Accuracy 0.3878 | validation: Loss 0.122377 Accuracy -6.6881\n",
      "Epoch 1475 | train: Loss 0.016687 Accuracy 0.3880 | validation: Loss 0.122406 Accuracy -6.6899\n",
      "Epoch 1476 | train: Loss 0.016680 Accuracy 0.3883 | validation: Loss 0.122437 Accuracy -6.6919\n",
      "Epoch 1477 | train: Loss 0.016672 Accuracy 0.3886 | validation: Loss 0.122467 Accuracy -6.6937\n",
      "Epoch 1478 | train: Loss 0.016665 Accuracy 0.3888 | validation: Loss 0.122496 Accuracy -6.6956\n",
      "Epoch 1479 | train: Loss 0.016658 Accuracy 0.3891 | validation: Loss 0.122526 Accuracy -6.6975\n",
      "Epoch 1480 | train: Loss 0.016650 Accuracy 0.3894 | validation: Loss 0.122558 Accuracy -6.6995\n",
      "Epoch 1481 | train: Loss 0.016643 Accuracy 0.3896 | validation: Loss 0.122589 Accuracy -6.7014\n",
      "Epoch 1482 | train: Loss 0.016636 Accuracy 0.3899 | validation: Loss 0.122618 Accuracy -6.7033\n",
      "Epoch 1483 | train: Loss 0.016628 Accuracy 0.3902 | validation: Loss 0.122647 Accuracy -6.7050\n",
      "Epoch 1484 | train: Loss 0.016621 Accuracy 0.3904 | validation: Loss 0.122675 Accuracy -6.7068\n",
      "Epoch 1485 | train: Loss 0.016614 Accuracy 0.3907 | validation: Loss 0.122704 Accuracy -6.7087\n",
      "Epoch 1486 | train: Loss 0.016606 Accuracy 0.3910 | validation: Loss 0.122736 Accuracy -6.7106\n",
      "Epoch 1487 | train: Loss 0.016599 Accuracy 0.3913 | validation: Loss 0.122769 Accuracy -6.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-19 23:54:40,043] Trial 1 failed with parameters: {'hidden_units': 11, 'depth': 5, 'learning_rate': 0.00036889369248063454, 'weight_decay': 2.7077652675170757e-05} because of the following error: The number of the values 1500 did not match the number of the objectives 1.\n",
      "[W 2024-05-19 23:54:40,043] Trial 1 failed with value [2.106875419616699, 2.0894124507904053, 2.072068691253662, 2.054845094680786, 2.0376529693603516, 2.0205862522125244, 2.0036444664001465, 1.9868167638778687, 1.9700170755386353, 1.9533348083496094, 1.9367682933807373, 1.9203077554702759, 1.9038761854171753, 1.8875685930252075, 1.871261477470398, 1.8550647497177124, 1.8388328552246094, 1.8227256536483765, 1.80677330493927, 1.7909389734268188, 1.7752150297164917, 1.7597026824951172, 1.7443804740905762, 1.729166030883789, 1.7141742706298828, 1.6993337869644165, 1.6846755743026733, 1.6700793504714966, 1.6554893255233765, 1.6410235166549683, 1.6265747547149658, 1.6121270656585693, 1.5977140665054321, 1.5835075378417969, 1.5692949295043945, 1.5551667213439941, 1.5410971641540527, 1.527153730392456, 1.5131791830062866, 1.4991880655288696, 1.4854108095169067, 1.4715458154678345, 1.4576376676559448, 1.4438021183013916, 1.4299659729003906, 1.4161896705627441, 1.402601957321167, 1.3890639543533325, 1.3756234645843506, 1.3623392581939697, 1.3491019010543823, 1.3359978199005127, 1.3229873180389404, 1.310028314590454, 1.2972431182861328, 1.2844982147216797, 1.2718675136566162, 1.2593411207199097, 1.2468615770339966, 1.2345350980758667, 1.2222610712051392, 1.2100870609283447, 1.1980220079421997, 1.1860014200210571, 1.174152135848999, 1.1623437404632568, 1.1505017280578613, 1.13857102394104, 1.1265604496002197, 1.114493489265442, 1.1023682355880737, 1.0902132987976074, 1.0780857801437378, 1.0660063028335571, 1.053972840309143, 1.041985273361206, 1.0300425291061401, 1.0181461572647095, 1.0062956809997559, 0.9944901466369629, 0.9827244281768799, 0.9709988832473755, 0.9593137502670288, 0.9477012157440186, 0.9362626075744629, 0.9250583052635193, 0.9140658974647522, 0.9033376574516296, 0.8927378058433533, 0.8823205828666687, 0.8720266222953796, 0.861855685710907, 0.8518013954162598, 0.8419000506401062, 0.831997811794281, 0.8220809102058411, 0.812157392501831, 0.8022416830062866, 0.7923137545585632, 0.7823866009712219, 0.7724675536155701, 0.7625484466552734, 0.7526543736457825, 0.7428352236747742, 0.7330723404884338, 0.7233418822288513, 0.7137351632118225, 0.7041667103767395, 0.6946419477462769, 0.685228705406189, 0.6758530735969543, 0.666556715965271, 0.6573454737663269, 0.648176372051239, 0.6391170024871826, 0.6301226019859314, 0.6211758852005005, 0.6123321056365967, 0.603536069393158, 0.594805896282196, 0.586173415184021, 0.5775889754295349, 0.5690755248069763, 0.5606504082679749, 0.5522717833518982, 0.5439490675926208, 0.5357167720794678, 0.5275331139564514, 0.5193983912467957, 0.5113483667373657, 0.5033575892448425, 0.4954207241535187, 0.4875389337539673, 0.47975045442581177, 0.47201675176620483, 0.46433740854263306, 0.4567128121852875, 0.44914302229881287, 0.44163069128990173, 0.4341762661933899, 0.4267800748348236, 0.4194439649581909, 0.4121676981449127, 0.40495145320892334, 0.39779695868492126, 0.39070528745651245, 0.38367635011672974, 0.3767109811306, 0.36981165409088135, 0.36297866702079773, 0.3562120199203491, 0.349511981010437, 0.34288063645362854, 0.33631807565689087, 0.32982537150382996, 0.3234052360057831, 0.3170578181743622, 0.3107832670211792, 0.3045831024646759, 0.29847198724746704, 0.2924354672431946, 0.2864767909049988, 0.28059595823287964, 0.2747945487499237, 0.2690947353839874, 0.26347699761390686, 0.25793907046318054, 0.25248295068740845, 0.24711455404758453, 0.24184295535087585, 0.23665374517440796, 0.23154808580875397, 0.22652626037597656, 0.22159510850906372, 0.21676231920719147, 0.2120143324136734, 0.2073516845703125, 0.20277485251426697, 0.19829963147640228, 0.19391870498657227, 0.1896250694990158, 0.1854192167520523, 0.1813029795885086, 0.17729033529758453, 0.17336741089820862, 0.16953466832637787, 0.1657925248146057, 0.16215433180332184, 0.15861386060714722, 0.15516513586044312, 0.15180860459804535, 0.14855210483074188, 0.14539895951747894, 0.14233878254890442, 0.13937176764011383, 0.13650332391262054, 0.13373512029647827, 0.1310604363679886, 0.12847967445850372, 0.1259976178407669, 0.1236163005232811, 0.12132865190505981, 0.11913470923900604, 0.11703968048095703, 0.11504384875297546, 0.11314075440168381, 0.11133024096488953, 0.10961193591356277, 0.10798556357622147, 0.10645053535699844, 0.10500647872686386, 0.10365288704633713, 0.10238924622535706, 0.10121507197618484, 0.10012933611869812, 0.0991315096616745, 0.09822086244821548, 0.09739670157432556, 0.09665825217962265, 0.09600463509559631, 0.09543487429618835, 0.09494809806346893, 0.09454314410686493, 0.0942189022898674, 0.09397425502538681, 0.09380783885717392, 0.09371840208768845, 0.0937046930193901, 0.09376516938209534, 0.09389840066432953, 0.0941028743982315, 0.09437695145606995, 0.09471921622753143, 0.09512781351804733, 0.0956011712551117, 0.09613747894763947, 0.09673493355512619, 0.09739179164171219, 0.09810613840818405, 0.09887608885765076, 0.09969978034496307, 0.10057523101568222, 0.10150046646595001, 0.10247346758842468, 0.10349231958389282, 0.10455483943223953, 0.1056591272354126, 0.10680294036865234, 0.10798443108797073, 0.10920140892267227, 0.11045176535844803, 0.11173345893621445, 0.11304444819688797, 0.1143825426697731, 0.11574569344520569, 0.11713196337223053, 0.11853919923305511, 0.11996535211801529, 0.12140846252441406, 0.12286649644374847, 0.12433751672506332, 0.1258196234703064, 0.12731078267097473, 0.12880922853946686, 0.13031308352947235, 0.13182051479816437, 0.1333296298980713, 0.13483890891075134, 0.13634654879570007, 0.13785094022750854, 0.13935035467147827, 0.14084337651729584, 0.1423283964395523, 0.14380396902561188, 0.14526869356632233, 0.1467212736606598, 0.14816036820411682, 0.14958469569683075, 0.15099304914474487, 0.15238457918167114, 0.1537577360868454, 0.15511168539524078, 0.15644557774066925, 0.15775839984416962, 0.15904928743839264, 0.1603175401687622, 0.16156242787837982, 0.16278304159641266, 0.1639789193868637, 0.16514959931373596, 0.16629436612129211, 0.16741293668746948, 0.1685047447681427, 0.16956953704357147, 0.1706070899963379, 0.17161688208580017, 0.1725989282131195, 0.1735529899597168, 0.174479141831398, 0.17537683248519897, 0.1762465536594391, 0.17708812654018402, 0.17790143191814423, 0.17868684232234955, 0.1794443428516388, 0.18017418682575226, 0.18087638914585114, 0.18155130743980408, 0.18219918012619019, 0.18282033503055573, 0.18341505527496338, 0.18398350477218628, 0.18452639877796173, 0.18504370748996735, 0.18553605675697327, 0.18600381910800934, 0.18644748628139496, 0.1868673712015152, 0.18726405501365662, 0.1876378059387207, 0.18798944354057312, 0.18831899762153625, 0.1886269897222519, 0.18891391158103943, 0.18918003141880035, 0.1894262284040451, 0.18965281546115875, 0.18986040353775024, 0.19004946947097778, 0.19022054970264435, 0.1903742402791977, 0.19051116704940796, 0.1906316727399826, 0.19073618948459625, 0.19082549214363098, 0.19089989364147186, 0.1909598857164383, 0.19100604951381683, 0.1910388171672821, 0.19105854630470276, 0.19106611609458923, 0.1910613477230072, 0.19104528427124023, 0.19101853668689728, 0.19098152220249176, 0.19093486666679382, 0.19087867438793182, 0.19081339240074158, 0.19073954224586487, 0.19065718352794647, 0.19056688249111176, 0.19046883285045624, 0.19036343693733215, 0.1902509182691574, 0.1901317983865738, 0.19000612199306488, 0.18987442553043365, 0.18973678350448608, 0.18959350883960724, 0.18944501876831055, 0.18929141759872437, 0.18913285434246063, 0.1889699101448059, 0.18880252540111542, 0.18863104283809662, 0.1884557157754898, 0.18827661871910095, 0.1880941390991211, 0.18790845572948456, 0.1877196729183197, 0.18752790987491608, 0.18733352422714233, 0.18713654577732086, 0.18693724274635315, 0.18673570454120636, 0.1865319311618805, 0.1863262802362442, 0.18611888587474823, 0.1859096884727478, 0.18569892644882202, 0.1854868233203888, 0.185273215174675, 0.18505850434303284, 0.18484270572662354, 0.18462572991847992, 0.1844078004360199, 0.18418912589550018, 0.18396958708763123, 0.18374936282634735, 0.18352626264095306, 0.18330079317092896, 0.18307286500930786, 0.18284288048744202, 0.18261079490184784, 0.18237680196762085, 0.18214093148708344, 0.1819036453962326, 0.1816650778055191, 0.18142539262771606, 0.1811848133802414, 0.1809433549642563, 0.180701345205307, 0.1804577112197876, 0.1802128702402115, 0.1799667775630951, 0.17971977591514587, 0.17947201430797577, 0.1792236715555191, 0.17897413671016693, 0.17872366309165955, 0.1784723401069641, 0.17822013795375824, 0.17796720564365387, 0.17771358788013458, 0.17745722830295563, 0.17719818651676178, 0.1769370436668396, 0.17667408287525177, 0.17640957236289978, 0.17614397406578064, 0.17587748169898987, 0.1756102442741394, 0.17534270882606506, 0.17507469654083252, 0.17480593919754028, 0.17453652620315552, 0.17426671087741852, 0.17399674654006958, 0.17372676730155945, 0.17345689237117767, 0.1731872856616974, 0.1729181706905365, 0.17264951765537262, 0.17238159477710724, 0.17211441695690155, 0.17184795439243317, 0.17158213257789612, 0.17131701111793518, 0.17105279862880707, 0.17078956961631775, 0.1705254167318344, 0.17026077210903168, 0.1699959933757782, 0.1697309911251068, 0.16946613788604736, 0.16920159757137299, 0.16893763840198517, 0.16867408156394958, 0.16840940713882446, 0.16814583539962769, 0.16788262128829956, 0.16761969029903412, 0.16735722124576569, 0.16709530353546143, 0.1668340116739273, 0.16657331585884094, 0.1663132607936859, 0.1660517156124115, 0.16579116880893707, 0.1655329167842865, 0.16527685523033142, 0.16502298414707184, 0.16477125883102417, 0.16452156007289886, 0.16427397727966309, 0.16402725875377655, 0.1637801229953766, 0.16353435814380646, 0.16329003870487213, 0.16304731369018555, 0.16280685365200043, 0.16256903111934662, 0.1623336225748062, 0.16210071742534637, 0.16186991333961487, 0.16164235770702362, 0.16141682863235474, 0.16119340062141418, 0.16097258031368256, 0.1607539802789688, 0.16053730249404907, 0.16032235324382782, 0.16011150181293488, 0.15990488231182098, 0.15970304608345032, 0.15950515866279602, 0.15931041538715363, 0.1591183990240097, 0.15892912447452545, 0.1587422639131546, 0.15855816006660461, 0.15837611258029938, 0.15819600224494934, 0.15801802277565002, 0.1578415483236313, 0.15766620635986328, 0.15749229490756989, 0.15732020139694214, 0.15714992582798004, 0.15698117017745972, 0.1568140685558319, 0.15664833784103394, 0.15648403763771057, 0.15632112324237823, 0.15616008639335632, 0.15600070357322693, 0.15584520995616913, 0.15569256246089935, 0.1555412858724594, 0.15539176762104034, 0.15524353086948395, 0.15509644150733948, 0.15495046973228455, 0.15480531752109528, 0.15466122329235077, 0.15451885759830475, 0.15437789261341095, 0.15423868596553802, 0.15410085022449493, 0.1539648473262787, 0.15383268892765045, 0.15370316803455353, 0.15357592701911926, 0.15345092117786407, 0.1533275991678238, 0.15320605039596558, 0.15308605134487152, 0.1529674530029297, 0.15285015106201172, 0.1527339369058609, 0.1526184380054474, 0.15250375866889954, 0.15238966047763824, 0.15227612853050232, 0.15216192603111267, 0.15204782783985138, 0.15193383395671844, 0.1518193632364273, 0.15170516073703766, 0.1515914350748062, 0.15147751569747925, 0.15136337280273438, 0.15124891698360443, 0.15113414824008942, 0.15101905167102814, 0.1509038209915161, 0.15078851580619812, 0.150673046708107, 0.1505574733018875, 0.15044189989566803, 0.15032626688480377, 0.15021073818206787, 0.15009506046772003, 0.1499793827533722, 0.14986446499824524, 0.14975173771381378, 0.14963890612125397, 0.14952616393566132, 0.14941374957561493, 0.1493016928434372, 0.1491900384426117, 0.14907866716384888, 0.148967906832695, 0.14885737001895905, 0.14874781668186188, 0.1486392468214035, 0.14853143692016602, 0.14842471480369568, 0.1483185738325119, 0.14821334183216095, 0.1481086164712906, 0.14800462126731873, 0.1479010134935379, 0.14779731631278992, 0.14769405126571655, 0.1475912481546402, 0.1474890261888504, 0.14738677442073822, 0.14728473126888275, 0.14718246459960938, 0.147080197930336, 0.14697760343551636, 0.14687491953372955, 0.1467720866203308, 0.1466691792011261, 0.14656642079353333, 0.1464635580778122, 0.1463608741760254, 0.14625820517539978, 0.14615578949451447, 0.14605343341827393, 0.14595159888267517, 0.1458500474691391, 0.1457490473985672, 0.1456480473279953, 0.1455475091934204, 0.145447239279747, 0.14534753561019897, 0.14524832367897034, 0.14514993131160736, 0.14505209028720856, 0.14495494961738586, 0.1448584496974945, 0.14476260542869568, 0.14466728270053864, 0.14457255601882935, 0.14447841048240662, 0.1443849354982376, 0.14429210126399994, 0.14419983327388763, 0.14410781860351562, 0.1440163552761078, 0.14392529428005219, 0.1438346654176712, 0.14374446868896484, 0.14365480840206146, 0.14356543123722076, 0.1434764713048935, 0.14338786900043488, 0.1432998776435852, 0.14321227371692657, 0.14312508702278137, 0.14303816854953766, 0.14295156300067902, 0.1428654044866562, 0.14277946949005127, 0.142693892121315, 0.14260876178741455, 0.14252394437789917, 0.14243926107883453, 0.1423548310995102, 0.1422705501317978, 0.1421864926815033, 0.14210250973701477, 0.14201875030994415, 0.14193524420261383, 0.14185187220573425, 0.1417686492204666, 0.14168570935726166, 0.14160287380218506, 0.14152026176452637, 0.14143875241279602, 0.14135803282260895, 0.14127817749977112, 0.14119906723499298, 0.14112043380737305, 0.14104260504245758, 0.14096510410308838, 0.14088833332061768, 0.14081205427646637, 0.14073604345321655, 0.14066031575202942, 0.14058484137058258, 0.1405094563961029, 0.1404341608285904, 0.140359029173851, 0.14028401672840118, 0.14020904898643494, 0.14013409614562988, 0.14005908370018005, 0.1399841606616974, 0.13990917801856995, 0.1398341953754425, 0.13975922763347626, 0.1396844983100891, 0.13960959017276764, 0.13953469693660736, 0.13945956528186798, 0.1393844336271286, 0.13930979371070862, 0.13923543691635132, 0.13916151225566864, 0.1390877217054367, 0.13901422917842865, 0.13894101977348328, 0.13886797428131104, 0.13879530131816864, 0.13872267305850983, 0.13865020871162415, 0.13857783377170563, 0.1385054886341095, 0.13843335211277008, 0.1383611410856247, 0.13828912377357483, 0.1382170021533966, 0.13814492523670197, 0.13807278871536255, 0.13800060749053955, 0.13792848587036133, 0.13785623013973236, 0.13778406381607056, 0.1377120018005371, 0.13764002919197083, 0.13756808638572693, 0.13749611377716064, 0.13742397725582123, 0.1373518407344818, 0.13727983832359314, 0.13720767199993134, 0.1371355950832367, 0.13706353306770325, 0.13699224591255188, 0.1369219720363617, 0.13685233891010284, 0.1367833912372589, 0.13671515882015228, 0.13664735853672028, 0.13658002018928528, 0.13651332259178162, 0.13644707202911377, 0.1363808959722519, 0.13631513714790344, 0.13624954223632812, 0.136183962225914, 0.13611865043640137, 0.1360536813735962, 0.13598856329917908, 0.13592351973056793, 0.13585849106311798, 0.13579334318637848, 0.13572829961776733, 0.13566303253173828, 0.135597825050354, 0.13553251326084137, 0.13546708226203918, 0.13540159165859222, 0.13533614575862885, 0.1352706253528595, 0.1352049708366394, 0.13513925671577454, 0.13507340848445892, 0.13500748574733734, 0.13494142889976501, 0.13487538695335388, 0.13481105864048004, 0.13474814593791962, 0.1346864551305771, 0.13462583720684052, 0.13456624746322632, 0.1345074474811554, 0.1344493329524994, 0.13439181447029114, 0.13433468341827393, 0.13427793979644775, 0.13422150909900665, 0.1341652125120163, 0.13410891592502594, 0.13405266404151917, 0.1339963674545288, 0.13394001126289368, 0.13388346135616302, 0.1338265836238861, 0.13376961648464203, 0.1337122917175293, 0.13365468382835388, 0.13359658420085907, 0.13353846967220306, 0.1334800273180008, 0.13342107832431793, 0.13336177170276642, 0.13330204784870148, 0.1332419514656067, 0.13318149745464325, 0.1331208348274231, 0.13305959105491638, 0.13299813866615295, 0.1329362839460373, 0.1328742355108261, 0.13281187415122986, 0.13274917006492615, 0.13268640637397766, 0.1326233297586441, 0.13256016373634338, 0.13249677419662476, 0.13243325054645538, 0.13236969709396362, 0.13230592012405396, 0.13224241137504578, 0.13217872381210327, 0.13211511075496674, 0.13205140829086304, 0.13198763132095337, 0.13192400336265564, 0.13186044991016388, 0.1317969709634781, 0.1317334771156311, 0.13167016208171844, 0.13160693645477295, 0.1315438151359558, 0.13148082792758942, 0.13141804933547974, 0.1313553750514984, 0.1312929391860962, 0.13123072683811188, 0.13116882741451263, 0.13110701739788055, 0.13104550540447235, 0.13098405301570892, 0.1309228390455246, 0.13086198270320892, 0.1308012455701828, 0.1307407170534134, 0.13068045675754547, 0.13062024116516113, 0.13056108355522156, 0.13050253689289093, 0.13044473528862, 0.13038748502731323, 0.13033097982406616, 0.13027505576610565, 0.13021968305110931, 0.13016456365585327, 0.13010987639427185, 0.13005554676055908, 0.13000145554542542, 0.12994764745235443, 0.12989409267902374, 0.12984059751033783, 0.12978726625442505, 0.12973415851593018, 0.12968097627162933, 0.12962797284126282, 0.1295749992132187, 0.12952204048633575, 0.12946918606758118, 0.12941628694534302, 0.12936340272426605, 0.1293105036020279, 0.12925761938095093, 0.12920470535755157, 0.1291518211364746, 0.1290988326072693, 0.12904594838619232, 0.1289929300546646, 0.1289399415254593, 0.12888719141483307, 0.1288343220949173, 0.12878145277500153, 0.1287287175655365, 0.1286759227514267, 0.12862306833267212, 0.1285703480243683, 0.12851768732070923, 0.1284649670124054, 0.12841233611106873, 0.12835977971553802, 0.1283072531223297, 0.12825489044189453, 0.12820252776145935, 0.12815025448799133, 0.12809818983078003, 0.12804614007472992, 0.1279943585395813, 0.12794263660907745, 0.12789100408554077, 0.12783943116664886, 0.1277882158756256, 0.12773704528808594, 0.1276862472295761, 0.12763559818267822, 0.1275850534439087, 0.1275346726179123, 0.127484530210495, 0.12743450701236725, 0.12738469243049622, 0.1273350566625595, 0.12728555500507355, 0.12723636627197266, 0.1271892786026001, 0.1271442323923111, 0.1271010935306549, 0.12705953419208527, 0.12701939046382904, 0.12698045372962952, 0.12694257497787476, 0.1269054412841797, 0.12686900794506073, 0.12683312594890594, 0.12679755687713623, 0.12676209211349487, 0.1267269253730774, 0.12669160962104797, 0.12665612995624542, 0.12662051618099213, 0.1265845149755478, 0.126548171043396, 0.12651140987873077, 0.12647421658039093, 0.12643632292747498, 0.1263979822397232, 0.12635910511016846, 0.12631957232952118, 0.12627938389778137, 0.1262386292219162, 0.12619732320308685, 0.12615542113780975, 0.12611286342144012, 0.1260698437690735, 0.12602615356445312, 0.12598207592964172, 0.12593746185302734, 0.12589247524738312, 0.12584693729877472, 0.12580111622810364, 0.1257549226284027, 0.12570840120315552, 0.12566165626049042, 0.12561458349227905, 0.12556733191013336, 0.1255200207233429, 0.12547241151332855, 0.12542486190795898, 0.12537723779678345, 0.12532949447631836, 0.12528175115585327, 0.12523412704467773, 0.1251865178346634, 0.12513896822929382, 0.12509149312973022, 0.12504427134990692, 0.12499707192182541, 0.12495001405477524, 0.12490317970514297, 0.12485656887292862, 0.12481007725000381, 0.12476387619972229, 0.12471792101860046, 0.12467216700315475, 0.12462669610977173, 0.12458143383264542, 0.12453652918338776, 0.12449172884225845, 0.12444721162319183, 0.12440299987792969, 0.12435906380414963, 0.12431541085243225, 0.12427198886871338, 0.1242288202047348, 0.12418599426746368, 0.12414337694644928, 0.12410089373588562, 0.12405889481306076, 0.12401697039604187, 0.12397540360689163, 0.12393396347761154, 0.12389276176691055, 0.12385189533233643, 0.12381117045879364, 0.12377064675092697, 0.12373041361570358, 0.12369027733802795, 0.12365037202835083, 0.1236107125878334, 0.12357121706008911, 0.12353184074163437, 0.12349269539117813, 0.12345385551452637, 0.1234150379896164, 0.12337636947631836, 0.12333805114030838, 0.12329980731010437, 0.12326175719499588, 0.12322382628917694, 0.12318620085716248, 0.12314865738153458, 0.12311124801635742, 0.12307403236627579, 0.12303711473941803, 0.12300018966197968, 0.12296342849731445, 0.12292685359716415, 0.12289045006036758, 0.12285416573286057, 0.12281811237335205, 0.1227821409702301, 0.12274635583162308, 0.12271059304475784, 0.12267515063285828, 0.12263976037502289, 0.12260448932647705, 0.12256946414709091, 0.12253446877002716, 0.12249967455863953, 0.12246502190828323, 0.12243048846721649, 0.12239615619182587, 0.122361920773983, 0.12232782691717148, 0.12229392677545547, 0.12226011604070663, 0.12222647666931152, 0.12219297885894775, 0.12215964496135712, 0.1221265122294426, 0.12209345400333405, 0.12206059694290161, 0.12202789634466171, 0.12199527770280838, 0.12196291238069534, 0.12193064391613007, 0.12189849466085434, 0.12186668068170547, 0.12183484435081482, 0.12180320918560028, 0.12177174538373947, 0.12174050509929657, 0.12170925736427307, 0.12167835235595703, 0.12164747714996338, 0.12161681801080704, 0.12158636003732681, 0.12155601382255554, 0.12152589112520218, 0.12149586528539658, 0.12146593630313873, 0.12143630534410477, 0.12140683084726334, 0.12137753516435623, 0.12134833633899689, 0.12131936103105545, 0.12129046022891998, 0.12126176059246063, 0.12123319506645203, 0.12120478600263596, 0.12117665261030197, 0.12114858627319336, 0.12112066894769669, 0.12109293788671494, 0.1210654154419899, 0.12103796005249023, 0.12101072818040848, 0.1209835559129715, 0.12095676362514496, 0.12092997133731842, 0.12090331315994263, 0.1208769753575325, 0.12085068225860596, 0.12082453072071075, 0.12079863250255585, 0.12077280879020691, 0.12074710428714752, 0.12072163820266724, 0.12069632112979889, 0.12067113816738129, 0.12064611911773682, 0.1206212043762207, 0.1205965131521225, 0.12057202309370041, 0.12054761499166489, 0.12052329629659653, 0.12049923837184906, 0.1204754114151001, 0.12045159190893173, 0.12042799592018127, 0.12040454149246216, 0.12038126587867737, 0.12035807222127914, 0.12033510208129883, 0.12031221389770508, 0.12028958648443222, 0.1202671229839325, 0.12024464458227158, 0.12022244930267334, 0.12020047754049301, 0.12017852813005447, 0.12015672028064728, 0.12013515084981918, 0.12011376023292542, 0.12009241431951523, 0.12007127702236176, 0.12005028128623962, 0.12002955377101898, 0.12000878900289536, 0.11998830735683441, 0.11996788531541824, 0.11994776129722595, 0.11992766708135605, 0.11990778148174286, 0.11988801509141922, 0.1198684424161911, 0.11984898895025253, 0.11982973664999008, 0.11981060355901718, 0.11979158967733383, 0.11977268755435944, 0.11975406855344772, 0.1197354719042778, 0.11971715837717056, 0.11969892680644989, 0.11968080699443817, 0.11966290324926376, 0.11964507400989532, 0.1196274533867836, 0.11960997432470322, 0.11959264427423477, 0.11957547068595886, 0.11955849826335907, 0.11954162269830704, 0.11952488869428635, 0.11950831115245819, 0.11949198693037033, 0.11947564035654068, 0.11945951730012894, 0.11944358050823212, 0.11942774802446365, 0.11941203474998474, 0.11939650774002075, 0.1193811222910881, 0.1193658784031868, 0.119350865483284, 0.11933593451976776, 0.11932110786437988, 0.11930647492408752, 0.11929205805063248, 0.11927764862775803, 0.11926347762346268, 0.11924943327903748, 0.1192355826497078, 0.11922187358140945, 0.11920817941427231, 0.11919476836919785, 0.11918147653341293, 0.11916832625865936, 0.11915531009435654, 0.11914248764514923, 0.11912970244884491, 0.11911720782518387, 0.1191047728061676, 0.11909255385398865, 0.11908038705587387, 0.119068443775177, 0.11905655264854431, 0.11904487013816833, 0.11903335899114609, 0.1190219596028328, 0.11901070177555084, 0.11899964511394501, 0.11898864060640335, 0.1189778670668602, 0.11896722763776779, 0.11895668506622314, 0.11894632875919342, 0.11893603950738907, 0.1189260184764862, 0.11891598999500275, 0.11890622973442078, 0.11889658868312836, 0.1188870370388031, 0.11887773126363754, 0.11886845529079437, 0.1188594326376915, 0.11885048449039459, 0.11884169280529022, 0.11883305758237839, 0.11882460117340088, 0.11881620436906815, 0.11880804598331451, 0.11879989504814148, 0.11879196763038635, 0.11878416687250137, 0.1187765970826149, 0.1187690943479538, 0.11876172572374344, 0.1187545508146286, 0.11874746531248093, 0.11874055862426758, 0.11873379349708557, 0.11872711777687073, 0.11872056871652603, 0.11871419847011566, 0.11870791763067245, 0.11870190501213074, 0.11869592219591141, 0.11869008094072342, 0.11868452280759811, 0.1186789944767952, 0.11867362260818481, 0.11866835504770279, 0.1186632588505745, 0.11865827441215515, 0.11865343898534775, 0.11864887177944183, 0.11864427477121353, 0.11863983422517776, 0.11863560974597931, 0.11863148212432861, 0.11862754821777344, 0.11862372606992722, 0.11861994117498398, 0.11861643195152283, 0.11861297488212585, 0.1186097040772438, 0.11860658973455429, 0.11860361695289612, 0.1186007708311081, 0.11859797686338425, 0.11859539896249771, 0.11859297007322311, 0.11859066039323807, 0.11858851462602615, 0.11858642846345901, 0.118584543466568, 0.11858277022838593, 0.11858122795820236, 0.11857973039150238, 0.11857836693525314, 0.11857718974351883, 0.1185760647058487, 0.11857517808675766, 0.11857441812753677, 0.11857375502586365, 0.11857327818870544, 0.1185748279094696, 0.11857842653989792, 0.11858364939689636, 0.11859039962291718, 0.11859821528196335, 0.11860720813274384, 0.11861701309680939, 0.11862530559301376, 0.11863220483064651, 0.11863774806261063, 0.11864202469587326, 0.11864516884088516, 0.11864716559648514, 0.11864809691905975, 0.11864820122718811, 0.11864745616912842, 0.11864601075649261, 0.1186438500881195, 0.11864340305328369, 0.11864432692527771, 0.11864655464887619, 0.1186499297618866, 0.11865238100290298, 0.11865381896495819, 0.11865437775850296, 0.11865416169166565, 0.11865343898534775, 0.1186521053314209, 0.11865248531103134, 0.1186542734503746, 0.11865745484828949, 0.11865977197885513, 0.11866133660078049, 0.11866222321987152, 0.11866256594657898, 0.11866239458322525, 0.11866192519664764, 0.11866319924592972, 0.1186661645770073, 0.11867058277130127, 0.11867424845695496, 0.11867718398571014, 0.11867961287498474, 0.11868153512477875, 0.11868307739496231, 0.11868630349636078, 0.11869119852781296, 0.11869746446609497, 0.1187029480934143, 0.11870774626731873, 0.11871181428432465, 0.11871521919965744, 0.1187182068824768, 0.11872079223394394, 0.11872516572475433, 0.11873117089271545, 0.1187385842204094, 0.11874519288539886, 0.11875111609697342, 0.11875641345977783, 0.11876104027032852, 0.11876527965068817, 0.11877114325761795, 0.11877857148647308, 0.11878529191017151, 0.11879140138626099, 0.11879689246416092, 0.11880411952733994, 0.11881274729967117, 0.11882055550813675, 0.11882764101028442, 0.11883410811424255, 0.1188400462269783, 0.11884758621454239, 0.11885668337345123, 0.11886494606733322, 0.11887253075838089, 0.11887951195240021, 0.11888805031776428, 0.11889812350273132, 0.11890736222267151, 0.11891567707061768, 0.11892341822385788, 0.11893051117658615, 0.11893933266401291, 0.11894955486059189, 0.11895899474620819, 0.11896780878305435, 0.1189759150147438, 0.11898555606603622, 0.11899685114622116, 0.11900713294744492, 0.11901671439409256, 0.11902550607919693, 0.11903383582830429, 0.1190437376499176, 0.11905520409345627, 0.11906586587429047, 0.11907587945461273, 0.11908742785453796, 0.11909803748130798, 0.11910810321569443, 0.11911964416503906, 0.11913062632083893, 0.11914296448230743, 0.11915449798107147, 0.11916527152061462, 0.11917536705732346, 0.11918717622756958, 0.11920057237148285, 0.11921299248933792, 0.11922468990087509, 0.11923563480377197, 0.11924825608730316, 0.11926236003637314, 0.1192755401134491, 0.1192879006266594, 0.11929953098297119, 0.11931049823760986, 0.11932332068681717, 0.11933755874633789, 0.11935333907604218, 0.1193680465221405, 0.11938189715147018, 0.11939483880996704, 0.11940702795982361, 0.1194186881184578, 0.11943206191062927, 0.11944713443517685, 0.11946344375610352, 0.11947886645793915, 0.11949341744184494, 0.11950715631246567, 0.11952018737792969, 0.11953256279230118, 0.1195468008518219, 0.11956274509429932, 0.11958002299070358, 0.1195964366197586, 0.11961190402507782, 0.11962664127349854, 0.1196429654955864, 0.11965839564800262, 0.11967302858829498, 0.11968944221735, 0.11970727145671844, 0.1197240799665451, 0.11974018067121506, 0.11975535750389099, 0.11977224051952362, 0.11979062110185623, 0.11980802565813065, 0.11982451379299164, 0.11984033137559891, 0.1198553517460823, 0.11987211555242538, 0.11989055573940277, 0.11991044878959656, 0.1199292242527008, 0.11994709819555283, 0.11996404826641083, 0.1199803352355957, 0.11999836564064026, 0.12001782655715942, 0.1200387179851532, 0.12005852907896042, 0.12007725238800049, 0.12009496241807938, 0.12011215090751648, 0.12012846022844315, 0.12014655023813248, 0.12016625702381134, 0.12018752098083496, 0.12020758539438248, 0.12022683769464493, 0.12024521827697754, 0.12026283144950867, 0.12028233706951141, 0.12030329555273056, 0.12032341212034225, 0.1203426867723465, 0.12036362290382385, 0.12038373947143555, 0.12040313333272934, 0.1204240471124649, 0.12044677138328552, 0.12046841531991959, 0.1204892098903656, 0.12050914019346237, 0.1205284595489502, 0.12054955959320068, 0.12057238817214966, 0.12059666216373444, 0.12061990052461624, 0.12064224481582642, 0.12066368758678436, 0.12068428844213486, 0.12070433795452118, 0.12072625011205673, 0.12074979394674301, 0.12077495455741882, 0.12080146372318268, 0.12082672119140625, 0.12085084617137909, 0.12087396532297134, 0.12089616060256958, 0.12091759592294693, 0.12093836069107056, 0.12096087634563446, 0.12098519504070282, 0.12101106345653534, 0.12103588134050369, 0.12105978280305862, 0.1210828498005867, 0.1211075559258461, 0.12113387137651443, 0.12115907669067383, 0.1211833506822586, 0.12120673060417175, 0.12123188376426697, 0.12125612795352936, 0.12128200381994247, 0.12130942940711975, 0.12133567780256271, 0.12136094272136688, 0.1213853508234024, 0.1214088499546051, 0.12143425643444061, 0.12146125733852386, 0.12148966640233994, 0.12151698023080826, 0.12154325097799301, 0.12156838923692703, 0.12159296870231628, 0.12161917984485626, 0.1216469258069992, 0.12167367339134216, 0.12169951945543289, 0.12172717601060867, 0.1217561811208725, 0.12178405374288559, 0.1218109130859375, 0.12183689326047897, 0.12186451256275177, 0.12189118564128876, 0.12191945314407349, 0.12194931507110596, 0.12197791039943695, 0.12200548499822617, 0.12203209847211838, 0.12205798178911209, 0.12208545953035355, 0.12211474776268005, 0.12214556336402893, 0.12217523902654648, 0.1222037598490715, 0.12223125994205475, 0.12226052582263947, 0.12229131907224655, 0.12232088297605515, 0.12234940379858017, 0.12237696349620819, 0.12240639328956604, 0.12243734300136566, 0.12246708571910858, 0.12249588966369629, 0.12252631038427353, 0.12255819886922836, 0.12258891761302948, 0.1226184293627739, 0.12264706194400787, 0.12267480790615082, 0.1227043941617012, 0.12273579835891724, 0.12276863306760788, 0.12280287593603134, 0.1228356584906578, 0.1228671744465828, 0.1228974461555481, 0.1229267343878746, 0.12295514345169067, 0.12298538535833359, 0.12301741540431976, 0.12305085361003876, 0.12308315932750702, 0.12311429530382156, 0.12314444780349731, 0.12317632883787155].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1488 | train: Loss 0.016592 Accuracy 0.3915 | validation: Loss 0.122803 Accuracy -6.7148\n",
      "Epoch 1489 | train: Loss 0.016584 Accuracy 0.3918 | validation: Loss 0.122836 Accuracy -6.7169\n",
      "Epoch 1490 | train: Loss 0.016577 Accuracy 0.3921 | validation: Loss 0.122867 Accuracy -6.7189\n",
      "Epoch 1491 | train: Loss 0.016570 Accuracy 0.3923 | validation: Loss 0.122897 Accuracy -6.7208\n",
      "Epoch 1492 | train: Loss 0.016563 Accuracy 0.3926 | validation: Loss 0.122927 Accuracy -6.7226\n",
      "Epoch 1493 | train: Loss 0.016555 Accuracy 0.3929 | validation: Loss 0.122955 Accuracy -6.7244\n",
      "Epoch 1494 | train: Loss 0.016548 Accuracy 0.3931 | validation: Loss 0.122985 Accuracy -6.7263\n",
      "Epoch 1495 | train: Loss 0.016541 Accuracy 0.3934 | validation: Loss 0.123017 Accuracy -6.7283\n",
      "Epoch 1496 | train: Loss 0.016533 Accuracy 0.3937 | validation: Loss 0.123051 Accuracy -6.7304\n",
      "Epoch 1497 | train: Loss 0.016526 Accuracy 0.3939 | validation: Loss 0.123083 Accuracy -6.7324\n",
      "Epoch 1498 | train: Loss 0.016519 Accuracy 0.3942 | validation: Loss 0.123114 Accuracy -6.7344\n",
      "Epoch 1499 | train: Loss 0.016511 Accuracy 0.3945 | validation: Loss 0.123144 Accuracy -6.7363\n",
      "Epoch 1500 | train: Loss 0.016504 Accuracy 0.3947 | validation: Loss 0.123176 Accuracy -6.7383\n",
      "Epoch 1500 | train: Loss 0.016504 Accuracy 0.3947 | validation: Loss 0.123176 Accuracy -6.7383\n",
      "1 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453beec008724bf8b87d39b4eac9e1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train: Loss 0.957742 Accuracy -34.1236 | validation: Loss 0.330017 Accuracy -19.7326\n",
      "Epoch 2 | train: Loss 0.323409 Accuracy -10.8605 | validation: Loss 1.074363 Accuracy -66.4946\n",
      "Epoch 3 | train: Loss 0.135187 Accuracy -3.9578 | validation: Loss 0.809566 Accuracy -49.8593\n",
      "Epoch 4 | train: Loss 0.079315 Accuracy -1.9087 | validation: Loss 0.281730 Accuracy -16.6991\n",
      "Epoch 5 | train: Loss 0.037723 Accuracy -0.3834 | validation: Loss 0.110277 Accuracy -5.9279\n",
      "Epoch 6 | train: Loss 0.055100 Accuracy -1.0207 | validation: Loss 0.076586 Accuracy -3.8114\n",
      "Epoch 7 | train: Loss 0.061834 Accuracy -1.2677 | validation: Loss 0.101028 Accuracy -5.3469\n",
      "Epoch 8 | train: Loss 0.040200 Accuracy -0.4743 | validation: Loss 0.273497 Accuracy -16.1819\n",
      "Epoch 9 | train: Loss 0.028237 Accuracy -0.0355 | validation: Loss 0.328914 Accuracy -19.6633\n",
      "Epoch 10 | train: Loss 0.035215 Accuracy -0.2915 | validation: Loss 0.156626 Accuracy -8.8397\n",
      "Epoch 11 | train: Loss 0.022894 Accuracy 0.1604 | validation: Loss 0.058479 Accuracy -2.6738\n",
      "Epoch 12 | train: Loss 0.025833 Accuracy 0.0526 | validation: Loss 0.041383 Accuracy -1.5998\n",
      "Epoch 13 | train: Loss 0.030819 Accuracy -0.1303 | validation: Loss 0.049007 Accuracy -2.0788\n",
      "Epoch 14 | train: Loss 0.027215 Accuracy 0.0019 | validation: Loss 0.075906 Accuracy -3.7686\n",
      "Epoch 15 | train: Loss 0.021169 Accuracy 0.2237 | validation: Loss 0.149611 Accuracy -8.3990\n",
      "Epoch 16 | train: Loss 0.020203 Accuracy 0.2591 | validation: Loss 0.207595 Accuracy -12.0417\n",
      "Epoch 17 | train: Loss 0.023527 Accuracy 0.1372 | validation: Loss 0.167244 Accuracy -9.5068\n",
      "Epoch 18 | train: Loss 0.021004 Accuracy 0.2297 | validation: Loss 0.088891 Accuracy -4.5844\n",
      "Epoch 19 | train: Loss 0.019155 Accuracy 0.2975 | validation: Loss 0.055492 Accuracy -2.4862\n",
      "Epoch 20 | train: Loss 0.021191 Accuracy 0.2229 | validation: Loss 0.048431 Accuracy -2.0426\n",
      "Epoch 21 | train: Loss 0.023712 Accuracy 0.1304 | validation: Loss 0.048600 Accuracy -2.0532\n",
      "Epoch 22 | train: Loss 0.023472 Accuracy 0.1392 | validation: Loss 0.054504 Accuracy -2.4241\n",
      "Epoch 23 | train: Loss 0.021064 Accuracy 0.2275 | validation: Loss 0.070655 Accuracy -3.4388\n",
      "Epoch 24 | train: Loss 0.019468 Accuracy 0.2860 | validation: Loss 0.090690 Accuracy -4.6974\n",
      "Epoch 25 | train: Loss 0.019838 Accuracy 0.2725 | validation: Loss 0.095791 Accuracy -5.0179\n",
      "Epoch 26 | train: Loss 0.020081 Accuracy 0.2636 | validation: Loss 0.080761 Accuracy -4.0736\n",
      "Epoch 27 | train: Loss 0.019147 Accuracy 0.2978 | validation: Loss 0.060309 Accuracy -2.7888\n",
      "Epoch 28 | train: Loss 0.018933 Accuracy 0.3057 | validation: Loss 0.048831 Accuracy -2.0677\n",
      "Epoch 29 | train: Loss 0.019937 Accuracy 0.2688 | validation: Loss 0.045590 Accuracy -1.8641\n",
      "Epoch 30 | train: Loss 0.019952 Accuracy 0.2683 | validation: Loss 0.046524 Accuracy -1.9227\n",
      "Epoch 31 | train: Loss 0.018561 Accuracy 0.3193 | validation: Loss 0.051805 Accuracy -2.2545\n",
      "Epoch 32 | train: Loss 0.018486 Accuracy 0.3221 | validation: Loss 0.052137 Accuracy -2.2754\n",
      "Epoch 33 | train: Loss 0.018522 Accuracy 0.3208 | validation: Loss 0.043261 Accuracy -1.7178\n",
      "Epoch 34 | train: Loss 0.018172 Accuracy 0.3336 | validation: Loss 0.040293 Accuracy -1.5313\n",
      "Epoch 35 | train: Loss 0.019519 Accuracy 0.2842 | validation: Loss 0.039624 Accuracy -1.4893\n",
      "Epoch 36 | train: Loss 0.019157 Accuracy 0.2974 | validation: Loss 0.041883 Accuracy -1.6312\n",
      "Epoch 37 | train: Loss 0.017874 Accuracy 0.3445 | validation: Loss 0.046442 Accuracy -1.9176\n",
      "Epoch 38 | train: Loss 0.017910 Accuracy 0.3432 | validation: Loss 0.043634 Accuracy -1.7412\n",
      "Epoch 39 | train: Loss 0.017745 Accuracy 0.3492 | validation: Loss 0.037805 Accuracy -1.3750\n",
      "Epoch 40 | train: Loss 0.017880 Accuracy 0.3443 | validation: Loss 0.035609 Accuracy -1.2370\n",
      "Epoch 41 | train: Loss 0.018586 Accuracy 0.3184 | validation: Loss 0.034825 Accuracy -1.1878\n",
      "Epoch 42 | train: Loss 0.018243 Accuracy 0.3310 | validation: Loss 0.035172 Accuracy -1.2096\n",
      "Epoch 43 | train: Loss 0.017509 Accuracy 0.3579 | validation: Loss 0.035892 Accuracy -1.2549\n",
      "Epoch 44 | train: Loss 0.017549 Accuracy 0.3564 | validation: Loss 0.033327 Accuracy -1.0937\n",
      "Epoch 45 | train: Loss 0.017419 Accuracy 0.3612 | validation: Loss 0.032094 Accuracy -1.0163\n",
      "Epoch 46 | train: Loss 0.017806 Accuracy 0.3470 | validation: Loss 0.032133 Accuracy -1.0187\n",
      "Epoch 47 | train: Loss 0.018007 Accuracy 0.3396 | validation: Loss 0.030442 Accuracy -0.9125\n",
      "Epoch 48 | train: Loss 0.017446 Accuracy 0.3602 | validation: Loss 0.029298 Accuracy -0.8406\n",
      "Epoch 49 | train: Loss 0.017542 Accuracy 0.3567 | validation: Loss 0.030246 Accuracy -0.9001\n",
      "Epoch 50 | train: Loss 0.017500 Accuracy 0.3582 | validation: Loss 0.032869 Accuracy -1.0649\n",
      "Epoch 51 | train: Loss 0.017949 Accuracy 0.3418 | validation: Loss 0.030796 Accuracy -0.9347\n",
      "Epoch 52 | train: Loss 0.017572 Accuracy 0.3556 | validation: Loss 0.029056 Accuracy -0.8254\n",
      "Epoch 53 | train: Loss 0.017593 Accuracy 0.3548 | validation: Loss 0.031956 Accuracy -1.0076\n",
      "Epoch 54 | train: Loss 0.017656 Accuracy 0.3525 | validation: Loss 0.034417 Accuracy -1.1622\n",
      "Epoch 55 | train: Loss 0.017889 Accuracy 0.3440 | validation: Loss 0.031479 Accuracy -0.9776\n",
      "Epoch 56 | train: Loss 0.017647 Accuracy 0.3528 | validation: Loss 0.030760 Accuracy -0.9324\n",
      "Epoch 57 | train: Loss 0.017697 Accuracy 0.3510 | validation: Loss 0.034951 Accuracy -1.1957\n",
      "Epoch 58 | train: Loss 0.017844 Accuracy 0.3456 | validation: Loss 0.035264 Accuracy -1.2154\n",
      "Epoch 59 | train: Loss 0.017861 Accuracy 0.3450 | validation: Loss 0.031766 Accuracy -0.9956\n",
      "Epoch 60 | train: Loss 0.017790 Accuracy 0.3476 | validation: Loss 0.034006 Accuracy -1.1363\n",
      "Epoch 61 | train: Loss 0.017826 Accuracy 0.3463 | validation: Loss 0.037141 Accuracy -1.3333\n",
      "Epoch 62 | train: Loss 0.018046 Accuracy 0.3382 | validation: Loss 0.033707 Accuracy -1.1176\n",
      "Epoch 63 | train: Loss 0.017904 Accuracy 0.3434 | validation: Loss 0.033368 Accuracy -1.0963\n",
      "Epoch 64 | train: Loss 0.017955 Accuracy 0.3415 | validation: Loss 0.037130 Accuracy -1.3326\n",
      "Epoch 65 | train: Loss 0.018161 Accuracy 0.3340 | validation: Loss 0.034817 Accuracy -1.1873\n",
      "Epoch 66 | train: Loss 0.018079 Accuracy 0.3370 | validation: Loss 0.033199 Accuracy -1.0857\n",
      "Epoch 67 | train: Loss 0.018120 Accuracy 0.3355 | validation: Loss 0.036885 Accuracy -1.3172\n",
      "Epoch 68 | train: Loss 0.018295 Accuracy 0.3291 | validation: Loss 0.035333 Accuracy -1.2197\n",
      "Epoch 69 | train: Loss 0.018259 Accuracy 0.3304 | validation: Loss 0.033688 Accuracy -1.1164\n",
      "Epoch 70 | train: Loss 0.018298 Accuracy 0.3289 | validation: Loss 0.037386 Accuracy -1.3487\n",
      "Epoch 71 | train: Loss 0.018478 Accuracy 0.3224 | validation: Loss 0.035471 Accuracy -1.2284\n",
      "Epoch 72 | train: Loss 0.018442 Accuracy 0.3237 | validation: Loss 0.034748 Accuracy -1.1830\n",
      "Epoch 73 | train: Loss 0.018498 Accuracy 0.3216 | validation: Loss 0.038119 Accuracy -1.3947\n",
      "Epoch 74 | train: Loss 0.018684 Accuracy 0.3148 | validation: Loss 0.035466 Accuracy -1.2280\n",
      "Epoch 75 | train: Loss 0.018634 Accuracy 0.3166 | validation: Loss 0.036527 Accuracy -1.2947\n",
      "Epoch 76 | train: Loss 0.018710 Accuracy 0.3139 | validation: Loss 0.038515 Accuracy -1.4196\n",
      "Epoch 77 | train: Loss 0.018839 Accuracy 0.3091 | validation: Loss 0.035718 Accuracy -1.2439\n",
      "Epoch 78 | train: Loss 0.018812 Accuracy 0.3101 | validation: Loss 0.039004 Accuracy -1.4503\n",
      "Epoch 79 | train: Loss 0.018934 Accuracy 0.3056 | validation: Loss 0.037576 Accuracy -1.3606\n",
      "Epoch 80 | train: Loss 0.018912 Accuracy 0.3064 | validation: Loss 0.037741 Accuracy -1.3710\n",
      "Epoch 81 | train: Loss 0.018950 Accuracy 0.3050 | validation: Loss 0.039856 Accuracy -1.5039\n",
      "Epoch 82 | train: Loss 0.019045 Accuracy 0.3016 | validation: Loss 0.037274 Accuracy -1.3416\n",
      "Epoch 83 | train: Loss 0.019015 Accuracy 0.3027 | validation: Loss 0.041336 Accuracy -1.5968\n",
      "Epoch 84 | train: Loss 0.019157 Accuracy 0.2974 | validation: Loss 0.037236 Accuracy -1.3393\n",
      "Epoch 85 | train: Loss 0.019092 Accuracy 0.2998 | validation: Loss 0.042856 Accuracy -1.6924\n",
      "Epoch 86 | train: Loss 0.019286 Accuracy 0.2927 | validation: Loss 0.036507 Accuracy -1.2935\n",
      "Epoch 87 | train: Loss 0.019210 Accuracy 0.2955 | validation: Loss 0.045848 Accuracy -1.8803\n",
      "Epoch 88 | train: Loss 0.019584 Accuracy 0.2818 | validation: Loss 0.034046 Accuracy -1.1389\n",
      "Epoch 89 | train: Loss 0.019548 Accuracy 0.2831 | validation: Loss 0.051179 Accuracy -2.2152\n",
      "Epoch 90 | train: Loss 0.020316 Accuracy 0.2549 | validation: Loss 0.032126 Accuracy -1.0183\n",
      "Epoch 91 | train: Loss 0.020003 Accuracy 0.2664 | validation: Loss 0.049502 Accuracy -2.1098\n",
      "Epoch 92 | train: Loss 0.020092 Accuracy 0.2632 | validation: Loss 0.039331 Accuracy -1.4709\n",
      "Epoch 93 | train: Loss 0.019434 Accuracy 0.2873 | validation: Loss 0.037335 Accuracy -1.3455\n",
      "Epoch 94 | train: Loss 0.019552 Accuracy 0.2829 | validation: Loss 0.050172 Accuracy -2.1519\n",
      "Epoch 95 | train: Loss 0.020270 Accuracy 0.2566 | validation: Loss 0.034844 Accuracy -1.1890\n",
      "Epoch 96 | train: Loss 0.019873 Accuracy 0.2712 | validation: Loss 0.044712 Accuracy -1.8089\n",
      "Epoch 97 | train: Loss 0.019759 Accuracy 0.2754 | validation: Loss 0.044270 Accuracy -1.7812\n",
      "Epoch 98 | train: Loss 0.019759 Accuracy 0.2754 | validation: Loss 0.035878 Accuracy -1.2539\n",
      "Epoch 99 | train: Loss 0.019879 Accuracy 0.2710 | validation: Loss 0.049010 Accuracy -2.0790\n",
      "Epoch 100 | train: Loss 0.020239 Accuracy 0.2578 | validation: Loss 0.038400 Accuracy -1.4124\n",
      "Epoch 101 | train: Loss 0.019756 Accuracy 0.2755 | validation: Loss 0.040783 Accuracy -1.5621\n",
      "Epoch 102 | train: Loss 0.019731 Accuracy 0.2764 | validation: Loss 0.046982 Accuracy -1.9516\n",
      "Epoch 103 | train: Loss 0.020061 Accuracy 0.2643 | validation: Loss 0.036301 Accuracy -1.2805\n",
      "Epoch 104 | train: Loss 0.019960 Accuracy 0.2680 | validation: Loss 0.047133 Accuracy -1.9610\n",
      "Epoch 105 | train: Loss 0.020088 Accuracy 0.2633 | validation: Loss 0.040402 Accuracy -1.5382\n",
      "Epoch 106 | train: Loss 0.019787 Accuracy 0.2743 | validation: Loss 0.040457 Accuracy -1.5416\n",
      "Epoch 107 | train: Loss 0.019797 Accuracy 0.2740 | validation: Loss 0.046901 Accuracy -1.9465\n",
      "Epoch 108 | train: Loss 0.020068 Accuracy 0.2640 | validation: Loss 0.037126 Accuracy -1.3324\n",
      "Epoch 109 | train: Loss 0.019971 Accuracy 0.2676 | validation: Loss 0.048162 Accuracy -2.0257\n",
      "Epoch 110 | train: Loss 0.020175 Accuracy 0.2601 | validation: Loss 0.038917 Accuracy -1.4449\n",
      "Epoch 111 | train: Loss 0.019881 Accuracy 0.2709 | validation: Loss 0.044427 Accuracy -1.7911\n",
      "Epoch 112 | train: Loss 0.019895 Accuracy 0.2704 | validation: Loss 0.043393 Accuracy -1.7261\n",
      "Epoch 113 | train: Loss 0.019855 Accuracy 0.2718 | validation: Loss 0.040451 Accuracy -1.5412\n",
      "Epoch 114 | train: Loss 0.019850 Accuracy 0.2720 | validation: Loss 0.047593 Accuracy -1.9899\n",
      "Epoch 115 | train: Loss 0.020080 Accuracy 0.2636 | validation: Loss 0.037581 Accuracy -1.3609\n",
      "Epoch 116 | train: Loss 0.020025 Accuracy 0.2656 | validation: Loss 0.051565 Accuracy -2.2395\n",
      "Epoch 117 | train: Loss 0.020468 Accuracy 0.2494 | validation: Loss 0.034923 Accuracy -1.1940\n",
      "Epoch 118 | train: Loss 0.020376 Accuracy 0.2527 | validation: Loss 0.055293 Accuracy -2.4736\n",
      "Epoch 119 | train: Loss 0.020971 Accuracy 0.2309 | validation: Loss 0.034033 Accuracy -1.1380\n",
      "Epoch 120 | train: Loss 0.020557 Accuracy 0.2461 | validation: Loss 0.052877 Accuracy -2.3219\n",
      "Epoch 121 | train: Loss 0.020631 Accuracy 0.2434 | validation: Loss 0.039159 Accuracy -1.4601\n",
      "Epoch 122 | train: Loss 0.019984 Accuracy 0.2671 | validation: Loss 0.043117 Accuracy -1.7088\n",
      "Epoch 123 | train: Loss 0.019911 Accuracy 0.2698 | validation: Loss 0.048468 Accuracy -2.0449\n",
      "Epoch 124 | train: Loss 0.020189 Accuracy 0.2596 | validation: Loss 0.036568 Accuracy -1.2973\n",
      "Epoch 125 | train: Loss 0.020259 Accuracy 0.2570 | validation: Loss 0.052980 Accuracy -2.3284\n",
      "Epoch 126 | train: Loss 0.020694 Accuracy 0.2411 | validation: Loss 0.036882 Accuracy -1.3170\n",
      "Epoch 127 | train: Loss 0.020255 Accuracy 0.2572 | validation: Loss 0.048045 Accuracy -2.0183\n",
      "Epoch 128 | train: Loss 0.020193 Accuracy 0.2595 | validation: Loss 0.043412 Accuracy -1.7273\n",
      "Epoch 129 | train: Loss 0.019988 Accuracy 0.2670 | validation: Loss 0.040491 Accuracy -1.5438\n",
      "Epoch 130 | train: Loss 0.020032 Accuracy 0.2654 | validation: Loss 0.050221 Accuracy -2.1550\n",
      "Epoch 131 | train: Loss 0.020391 Accuracy 0.2522 | validation: Loss 0.037186 Accuracy -1.3361\n",
      "Epoch 132 | train: Loss 0.020291 Accuracy 0.2558 | validation: Loss 0.051554 Accuracy -2.2388\n",
      "Epoch 133 | train: Loss 0.020523 Accuracy 0.2473 | validation: Loss 0.038762 Accuracy -1.4351\n",
      "Epoch 134 | train: Loss 0.020164 Accuracy 0.2605 | validation: Loss 0.047370 Accuracy -1.9759\n",
      "Epoch 135 | train: Loss 0.020155 Accuracy 0.2609 | validation: Loss 0.043619 Accuracy -1.7402\n",
      "Epoch 136 | train: Loss 0.020020 Accuracy 0.2658 | validation: Loss 0.042204 Accuracy -1.6514\n",
      "Epoch 137 | train: Loss 0.020028 Accuracy 0.2655 | validation: Loss 0.048555 Accuracy -2.0504\n",
      "Epoch 138 | train: Loss 0.020228 Accuracy 0.2582 | validation: Loss 0.038808 Accuracy -1.4380\n",
      "Epoch 139 | train: Loss 0.020197 Accuracy 0.2593 | validation: Loss 0.052072 Accuracy -2.2713\n",
      "Epoch 140 | train: Loss 0.020545 Accuracy 0.2466 | validation: Loss 0.036849 Accuracy -1.3149\n",
      "Epoch 141 | train: Loss 0.020413 Accuracy 0.2514 | validation: Loss 0.054128 Accuracy -2.4005\n",
      "Epoch 142 | train: Loss 0.020784 Accuracy 0.2378 | validation: Loss 0.036311 Accuracy -1.2812\n",
      "Epoch 143 | train: Loss 0.020501 Accuracy 0.2481 | validation: Loss 0.053464 Accuracy -2.3588\n",
      "Epoch 144 | train: Loss 0.020703 Accuracy 0.2408 | validation: Loss 0.038211 Accuracy -1.4006\n",
      "Epoch 145 | train: Loss 0.020293 Accuracy 0.2558 | validation: Loss 0.049261 Accuracy -2.0947\n",
      "Epoch 146 | train: Loss 0.020291 Accuracy 0.2559 | validation: Loss 0.042556 Accuracy -1.6735\n",
      "Epoch 147 | train: Loss 0.020073 Accuracy 0.2639 | validation: Loss 0.044096 Accuracy -1.7702\n",
      "Epoch 148 | train: Loss 0.020073 Accuracy 0.2639 | validation: Loss 0.047382 Accuracy -1.9767\n",
      "Epoch 149 | train: Loss 0.020179 Accuracy 0.2600 | validation: Loss 0.040246 Accuracy -1.5284\n",
      "Epoch 150 | train: Loss 0.020177 Accuracy 0.2600 | validation: Loss 0.051438 Accuracy -2.2315\n",
      "Epoch 151 | train: Loss 0.020487 Accuracy 0.2487 | validation: Loss 0.037488 Accuracy -1.3551\n",
      "Epoch 152 | train: Loss 0.020435 Accuracy 0.2506 | validation: Loss 0.054977 Accuracy -2.4538\n",
      "Epoch 153 | train: Loss 0.020889 Accuracy 0.2339 | validation: Loss 0.035509 Accuracy -1.2308\n",
      "Epoch 154 | train: Loss 0.020736 Accuracy 0.2396 | validation: Loss 0.056842 Accuracy -2.5710\n",
      "Epoch 155 | train: Loss 0.021150 Accuracy 0.2244 | validation: Loss 0.035844 Accuracy -1.2518\n",
      "Epoch 156 | train: Loss 0.020698 Accuracy 0.2409 | validation: Loss 0.053324 Accuracy -2.3500\n",
      "Epoch 157 | train: Loss 0.020707 Accuracy 0.2406 | validation: Loss 0.040568 Accuracy -1.5486\n",
      "Epoch 158 | train: Loss 0.020226 Accuracy 0.2583 | validation: Loss 0.045460 Accuracy -1.8559\n",
      "Epoch 159 | train: Loss 0.020167 Accuracy 0.2604 | validation: Loss 0.047961 Accuracy -2.0131\n",
      "Epoch 160 | train: Loss 0.020275 Accuracy 0.2564 | validation: Loss 0.039485 Accuracy -1.4806\n",
      "Epoch 161 | train: Loss 0.020335 Accuracy 0.2542 | validation: Loss 0.052984 Accuracy -2.3286\n",
      "Epoch 162 | train: Loss 0.020706 Accuracy 0.2406 | validation: Loss 0.037800 Accuracy -1.3747\n",
      "Epoch 163 | train: Loss 0.020524 Accuracy 0.2473 | validation: Loss 0.052513 Accuracy -2.2990\n",
      "Epoch 164 | train: Loss 0.020663 Accuracy 0.2422 | validation: Loss 0.040320 Accuracy -1.5330\n",
      "Epoch 165 | train: Loss 0.020324 Accuracy 0.2547 | validation: Loss 0.047678 Accuracy -1.9953\n",
      "Epoch 166 | train: Loss 0.020299 Accuracy 0.2556 | validation: Loss 0.045374 Accuracy -1.8505\n",
      "Epoch 167 | train: Loss 0.020227 Accuracy 0.2582 | validation: Loss 0.042720 Accuracy -1.6838\n",
      "Epoch 168 | train: Loss 0.020239 Accuracy 0.2578 | validation: Loss 0.050024 Accuracy -2.1426\n",
      "Epoch 169 | train: Loss 0.020441 Accuracy 0.2504 | validation: Loss 0.039740 Accuracy -1.4966\n",
      "Epoch 170 | train: Loss 0.020401 Accuracy 0.2518 | validation: Loss 0.052784 Accuracy -2.3160\n",
      "Epoch 171 | train: Loss 0.020675 Accuracy 0.2418 | validation: Loss 0.038509 Accuracy -1.4192\n",
      "Epoch 172 | train: Loss 0.020523 Accuracy 0.2474 | validation: Loss 0.053618 Accuracy -2.3685\n",
      "Epoch 173 | train: Loss 0.020755 Accuracy 0.2388 | validation: Loss 0.038629 Accuracy -1.4268\n",
      "Epoch 174 | train: Loss 0.020515 Accuracy 0.2476 | validation: Loss 0.052700 Accuracy -2.3108\n",
      "Epoch 175 | train: Loss 0.020658 Accuracy 0.2424 | validation: Loss 0.039918 Accuracy -1.5078\n",
      "Epoch 176 | train: Loss 0.020403 Accuracy 0.2518 | validation: Loss 0.050546 Accuracy -2.1754\n",
      "Epoch 177 | train: Loss 0.020471 Accuracy 0.2493 | validation: Loss 0.041944 Accuracy -1.6351\n",
      "Epoch 178 | train: Loss 0.020287 Accuracy 0.2560 | validation: Loss 0.048149 Accuracy -2.0248\n",
      "Epoch 179 | train: Loss 0.020323 Accuracy 0.2547 | validation: Loss 0.044004 Accuracy -1.7645\n",
      "Epoch 180 | train: Loss 0.020239 Accuracy 0.2578 | validation: Loss 0.046256 Accuracy -1.9059\n",
      "Epoch 181 | train: Loss 0.020255 Accuracy 0.2572 | validation: Loss 0.045683 Accuracy -1.8699\n",
      "Epoch 182 | train: Loss 0.020243 Accuracy 0.2576 | validation: Loss 0.044876 Accuracy -1.8192\n",
      "Epoch 183 | train: Loss 0.020235 Accuracy 0.2579 | validation: Loss 0.047203 Accuracy -1.9655\n",
      "Epoch 184 | train: Loss 0.020274 Accuracy 0.2565 | validation: Loss 0.043388 Accuracy -1.7258\n",
      "Epoch 185 | train: Loss 0.020246 Accuracy 0.2575 | validation: Loss 0.049525 Accuracy -2.1113\n",
      "Epoch 186 | train: Loss 0.020374 Accuracy 0.2528 | validation: Loss 0.040505 Accuracy -1.5447\n",
      "Epoch 187 | train: Loss 0.020379 Accuracy 0.2526 | validation: Loss 0.055192 Accuracy -2.4673\n",
      "Epoch 188 | train: Loss 0.020878 Accuracy 0.2343 | validation: Loss 0.033968 Accuracy -1.1339\n",
      "Epoch 189 | train: Loss 0.021343 Accuracy 0.2173 | validation: Loss 0.070582 Accuracy -3.4341\n",
      "Epoch 190 | train: Loss 0.023607 Accuracy 0.1343 | validation: Loss 0.025105 Accuracy -0.5772\n",
      "Epoch 191 | train: Loss 0.025105 Accuracy 0.0793 | validation: Loss 0.084380 Accuracy -4.3010\n",
      "Epoch 192 | train: Loss 0.027216 Accuracy 0.0019 | validation: Loss 0.032151 Accuracy -1.0198\n",
      "Epoch 193 | train: Loss 0.021794 Accuracy 0.2007 | validation: Loss 0.038599 Accuracy -1.4249\n",
      "Epoch 194 | train: Loss 0.020669 Accuracy 0.2420 | validation: Loss 0.069999 Accuracy -3.3975\n",
      "Epoch 195 | train: Loss 0.023950 Accuracy 0.1217 | validation: Loss 0.033399 Accuracy -1.0983\n",
      "Epoch 196 | train: Loss 0.021594 Accuracy 0.2081 | validation: Loss 0.039453 Accuracy -1.4786\n",
      "Epoch 197 | train: Loss 0.020780 Accuracy 0.2379 | validation: Loss 0.063547 Accuracy -2.9922\n",
      "Epoch 198 | train: Loss 0.022857 Accuracy 0.1618 | validation: Loss 0.038686 Accuracy -1.4304\n",
      "Epoch 199 | train: Loss 0.020980 Accuracy 0.2306 | validation: Loss 0.036358 Accuracy -1.2841\n",
      "Epoch 200 | train: Loss 0.021329 Accuracy 0.2178 | validation: Loss 0.059911 Accuracy -2.7638\n",
      "Epoch 201 | train: Loss 0.022300 Accuracy 0.1822 | validation: Loss 0.045785 Accuracy -1.8763\n",
      "Epoch 202 | train: Loss 0.020926 Accuracy 0.2326 | validation: Loss 0.034621 Accuracy -1.1750\n",
      "Epoch 203 | train: Loss 0.021845 Accuracy 0.1989 | validation: Loss 0.055085 Accuracy -2.4606\n",
      "Epoch 204 | train: Loss 0.021628 Accuracy 0.2068 | validation: Loss 0.052383 Accuracy -2.2909\n",
      "Epoch 205 | train: Loss 0.021350 Accuracy 0.2170 | validation: Loss 0.035952 Accuracy -1.2586\n",
      "Epoch 206 | train: Loss 0.021756 Accuracy 0.2021 | validation: Loss 0.048832 Accuracy -2.0678\n",
      "Epoch 207 | train: Loss 0.021114 Accuracy 0.2257 | validation: Loss 0.056088 Accuracy -2.5236\n",
      "Epoch 208 | train: Loss 0.021698 Accuracy 0.2043 | validation: Loss 0.039994 Accuracy -1.5125\n",
      "Epoch 209 | train: Loss 0.021280 Accuracy 0.2196 | validation: Loss 0.044164 Accuracy -1.7745\n",
      "Epoch 210 | train: Loss 0.021042 Accuracy 0.2283 | validation: Loss 0.055814 Accuracy -2.5064\n",
      "Epoch 211 | train: Loss 0.021618 Accuracy 0.2072 | validation: Loss 0.037617 Accuracy -1.3632\n",
      "Epoch 212 | train: Loss 0.021637 Accuracy 0.2065 | validation: Loss 0.057897 Accuracy -2.6373\n",
      "Epoch 213 | train: Loss 0.021802 Accuracy 0.2004 | validation: Loss 0.046162 Accuracy -1.9000\n",
      "Epoch 214 | train: Loss 0.020959 Accuracy 0.2314 | validation: Loss 0.038828 Accuracy -1.4393\n",
      "Epoch 215 | train: Loss 0.021323 Accuracy 0.2180 | validation: Loss 0.054389 Accuracy -2.4169\n",
      "Epoch 216 | train: Loss 0.021336 Accuracy 0.2175 | validation: Loss 0.050153 Accuracy -2.1508\n",
      "Epoch 217 | train: Loss 0.020990 Accuracy 0.2302 | validation: Loss 0.038803 Accuracy -1.4377\n",
      "Epoch 218 | train: Loss 0.021231 Accuracy 0.2214 | validation: Loss 0.050977 Accuracy -2.2025\n",
      "Epoch 219 | train: Loss 0.020974 Accuracy 0.2308 | validation: Loss 0.054863 Accuracy -2.4467\n",
      "Epoch 220 | train: Loss 0.021271 Accuracy 0.2199 | validation: Loss 0.032050 Accuracy -1.0135\n",
      "Epoch 221 | train: Loss 0.022563 Accuracy 0.1725 | validation: Loss 0.067030 Accuracy -3.2110\n",
      "Epoch 222 | train: Loss 0.023038 Accuracy 0.1551 | validation: Loss 0.041812 Accuracy -1.6267\n",
      "Epoch 223 | train: Loss 0.020817 Accuracy 0.2366 | validation: Loss 0.038126 Accuracy -1.3952\n",
      "Epoch 224 | train: Loss 0.021144 Accuracy 0.2246 | validation: Loss 0.060056 Accuracy -2.7729\n",
      "Epoch 225 | train: Loss 0.021822 Accuracy 0.1997 | validation: Loss 0.046161 Accuracy -1.8999\n",
      "Epoch 226 | train: Loss 0.020669 Accuracy 0.2420 | validation: Loss 0.036686 Accuracy -1.3047\n",
      "Epoch 227 | train: Loss 0.021308 Accuracy 0.2186 | validation: Loss 0.057914 Accuracy -2.6383\n",
      "Epoch 228 | train: Loss 0.021545 Accuracy 0.2099 | validation: Loss 0.046020 Accuracy -1.8911\n",
      "Epoch 229 | train: Loss 0.020651 Accuracy 0.2427 | validation: Loss 0.038668 Accuracy -1.4293\n",
      "Epoch 230 | train: Loss 0.021005 Accuracy 0.2297 | validation: Loss 0.053318 Accuracy -2.3496\n",
      "Epoch 231 | train: Loss 0.021015 Accuracy 0.2293 | validation: Loss 0.043836 Accuracy -1.7539\n",
      "Epoch 232 | train: Loss 0.020658 Accuracy 0.2424 | validation: Loss 0.045013 Accuracy -1.8279\n",
      "Epoch 233 | train: Loss 0.020642 Accuracy 0.2430 | validation: Loss 0.056975 Accuracy -2.5794\n",
      "Epoch 234 | train: Loss 0.021321 Accuracy 0.2181 | validation: Loss 0.022697 Accuracy -0.4259\n",
      "Epoch 235 | train: Loss 0.028438 Accuracy -0.0429 | validation: Loss 0.168794 Accuracy -9.6041\n",
      "Epoch 236 | train: Loss 0.058779 Accuracy -1.1556 | validation: Loss 0.033393 Accuracy -1.0979\n",
      "Epoch 237 | train: Loss 0.021628 Accuracy 0.2068 | validation: Loss 0.020732 Accuracy -0.3024\n",
      "Epoch 238 | train: Loss 0.048812 Accuracy -0.7901 | validation: Loss 0.089126 Accuracy -4.5992\n",
      "Epoch 239 | train: Loss 0.030386 Accuracy -0.1144 | validation: Loss 0.128593 Accuracy -7.0786\n",
      "Epoch 240 | train: Loss 0.045433 Accuracy -0.6662 | validation: Loss 0.041781 Accuracy -1.6248\n",
      "Epoch 241 | train: Loss 0.021370 Accuracy 0.2163 | validation: Loss 0.021437 Accuracy -0.3467\n",
      "Epoch 242 | train: Loss 0.042060 Accuracy -0.5425 | validation: Loss 0.034904 Accuracy -1.1928\n",
      "Epoch 243 | train: Loss 0.021956 Accuracy 0.1948 | validation: Loss 0.091788 Accuracy -4.7664\n",
      "Epoch 244 | train: Loss 0.034037 Accuracy -0.2483 | validation: Loss 0.090851 Accuracy -4.7075\n",
      "Epoch 245 | train: Loss 0.033903 Accuracy -0.2433 | validation: Loss 0.029534 Accuracy -0.8554\n",
      "Epoch 246 | train: Loss 0.023396 Accuracy 0.1420 | validation: Loss 0.020873 Accuracy -0.3113\n",
      "Epoch 247 | train: Loss 0.034179 Accuracy -0.2535 | validation: Loss 0.035098 Accuracy -1.2050\n",
      "Epoch 248 | train: Loss 0.022484 Accuracy 0.1754 | validation: Loss 0.092711 Accuracy -4.8244\n",
      "Epoch 249 | train: Loss 0.034611 Accuracy -0.2693 | validation: Loss 0.066259 Accuracy -3.1626\n",
      "Epoch 250 | train: Loss 0.025843 Accuracy 0.0523 | validation: Loss 0.035037 Accuracy -1.2011\n",
      "Epoch 251 | train: Loss 0.022769 Accuracy 0.1650 | validation: Loss 0.028507 Accuracy -0.7909\n",
      "Epoch 252 | train: Loss 0.024023 Accuracy 0.1190 | validation: Loss 0.044793 Accuracy -1.8140\n",
      "Epoch 253 | train: Loss 0.022225 Accuracy 0.1849 | validation: Loss 0.048345 Accuracy -2.0372\n",
      "Epoch 254 | train: Loss 0.022318 Accuracy 0.1815 | validation: Loss 0.029912 Accuracy -0.8792\n",
      "Epoch 255 | train: Loss 0.024667 Accuracy 0.0954 | validation: Loss 0.080023 Accuracy -4.0273\n",
      "Epoch 256 | train: Loss 0.027495 Accuracy -0.0083 | validation: Loss 0.031570 Accuracy -0.9833\n",
      "Epoch 257 | train: Loss 0.024788 Accuracy 0.0909 | validation: Loss 0.042345 Accuracy -1.6602\n",
      "Epoch 258 | train: Loss 0.022637 Accuracy 0.1698 | validation: Loss 0.073706 Accuracy -3.6304\n",
      "Epoch 259 | train: Loss 0.024724 Accuracy 0.0933 | validation: Loss 0.070018 Accuracy -3.3987\n",
      "Epoch 260 | train: Loss 0.023930 Accuracy 0.1224 | validation: Loss 0.046935 Accuracy -1.9486\n",
      "Epoch 261 | train: Loss 0.022390 Accuracy 0.1789 | validation: Loss 0.039516 Accuracy -1.4825\n",
      "Epoch 262 | train: Loss 0.023122 Accuracy 0.1520 | validation: Loss 0.046695 Accuracy -1.9335\n",
      "Epoch 263 | train: Loss 0.022312 Accuracy 0.1817 | validation: Loss 0.060848 Accuracy -2.8226\n",
      "Epoch 264 | train: Loss 0.022623 Accuracy 0.1703 | validation: Loss 0.065009 Accuracy -3.0841\n",
      "Epoch 265 | train: Loss 0.023046 Accuracy 0.1548 | validation: Loss 0.054377 Accuracy -2.4161\n",
      "Epoch 266 | train: Loss 0.022150 Accuracy 0.1877 | validation: Loss 0.043894 Accuracy -1.7576\n",
      "Epoch 267 | train: Loss 0.022067 Accuracy 0.1907 | validation: Loss 0.049418 Accuracy -2.1046\n",
      "Epoch 268 | train: Loss 0.021785 Accuracy 0.2011 | validation: Loss 0.054970 Accuracy -2.4534\n",
      "Epoch 269 | train: Loss 0.022118 Accuracy 0.1889 | validation: Loss 0.029633 Accuracy -0.8616\n",
      "Epoch 270 | train: Loss 0.024030 Accuracy 0.1188 | validation: Loss 0.084136 Accuracy -4.2857\n",
      "Epoch 271 | train: Loss 0.028335 Accuracy -0.0391 | validation: Loss 0.019761 Accuracy -0.2414\n",
      "Epoch 272 | train: Loss 0.033203 Accuracy -0.2177 | validation: Loss 0.040922 Accuracy -1.5708\n",
      "Epoch 273 | train: Loss 0.021264 Accuracy 0.2202 | validation: Loss 0.083339 Accuracy -4.2356\n",
      "Epoch 274 | train: Loss 0.027044 Accuracy 0.0082 | validation: Loss 0.068478 Accuracy -3.3020\n",
      "Epoch 275 | train: Loss 0.023215 Accuracy 0.1486 | validation: Loss 0.045189 Accuracy -1.8389\n",
      "Epoch 276 | train: Loss 0.021221 Accuracy 0.2218 | validation: Loss 0.032099 Accuracy -1.0166\n",
      "Epoch 277 | train: Loss 0.022817 Accuracy 0.1632 | validation: Loss 0.036990 Accuracy -1.3238\n",
      "Epoch 278 | train: Loss 0.021706 Accuracy 0.2040 | validation: Loss 0.056867 Accuracy -2.5725\n",
      "Epoch 279 | train: Loss 0.021795 Accuracy 0.2007 | validation: Loss 0.063364 Accuracy -2.9807\n",
      "Epoch 280 | train: Loss 0.022625 Accuracy 0.1703 | validation: Loss 0.047836 Accuracy -2.0052\n",
      "Epoch 281 | train: Loss 0.021119 Accuracy 0.2255 | validation: Loss 0.035649 Accuracy -1.2396\n",
      "Epoch 282 | train: Loss 0.021753 Accuracy 0.2022 | validation: Loss 0.041238 Accuracy -1.5907\n",
      "Epoch 283 | train: Loss 0.021164 Accuracy 0.2238 | validation: Loss 0.055330 Accuracy -2.4760\n",
      "Epoch 284 | train: Loss 0.021711 Accuracy 0.2038 | validation: Loss 0.055201 Accuracy -2.4679\n",
      "Epoch 285 | train: Loss 0.021724 Accuracy 0.2033 | validation: Loss 0.042185 Accuracy -1.6502\n",
      "Epoch 286 | train: Loss 0.021111 Accuracy 0.2258 | validation: Loss 0.036897 Accuracy -1.3180\n",
      "Epoch 287 | train: Loss 0.021492 Accuracy 0.2118 | validation: Loss 0.044565 Accuracy -1.7997\n",
      "Epoch 288 | train: Loss 0.021018 Accuracy 0.2292 | validation: Loss 0.053640 Accuracy -2.3698\n",
      "Epoch 289 | train: Loss 0.021472 Accuracy 0.2126 | validation: Loss 0.050812 Accuracy -2.1921\n",
      "Epoch 290 | train: Loss 0.021166 Accuracy 0.2238 | validation: Loss 0.041420 Accuracy -1.6021\n",
      "Epoch 291 | train: Loss 0.020845 Accuracy 0.2355 | validation: Loss 0.038363 Accuracy -1.4101\n",
      "Epoch 292 | train: Loss 0.020938 Accuracy 0.2321 | validation: Loss 0.044320 Accuracy -1.7843\n",
      "Epoch 293 | train: Loss 0.020651 Accuracy 0.2426 | validation: Loss 0.068004 Accuracy -3.2722\n",
      "Epoch 294 | train: Loss 0.023133 Accuracy 0.1516 | validation: Loss 0.030130 Accuracy -0.8928\n",
      "Epoch 295 | train: Loss 0.022360 Accuracy 0.1800 | validation: Loss 0.042060 Accuracy -1.6423\n",
      "Epoch 296 | train: Loss 0.020508 Accuracy 0.2479 | validation: Loss 0.077848 Accuracy -3.8907\n",
      "Epoch 297 | train: Loss 0.026838 Accuracy 0.0157 | validation: Loss 0.032488 Accuracy -1.0410\n",
      "Epoch 298 | train: Loss 0.021065 Accuracy 0.2275 | validation: Loss 0.074802 Accuracy -3.6992\n",
      "Epoch 299 | train: Loss 0.029345 Accuracy -0.0762 | validation: Loss 0.035411 Accuracy -1.2246\n",
      "Epoch 300 | train: Loss 0.061734 Accuracy -1.2640 | validation: Loss 0.074964 Accuracy -3.7095\n",
      "Epoch 301 | train: Loss 0.028573 Accuracy -0.0479 | validation: Loss 0.107749 Accuracy -5.7691\n",
      "Epoch 302 | train: Loss 0.040520 Accuracy -0.4860 | validation: Loss 0.052472 Accuracy -2.2965\n",
      "Epoch 303 | train: Loss 0.023123 Accuracy 0.1520 | validation: Loss 0.024774 Accuracy -0.5564\n",
      "Epoch 304 | train: Loss 0.027115 Accuracy 0.0056 | validation: Loss 0.025809 Accuracy -0.6214\n",
      "Epoch 305 | train: Loss 0.029987 Accuracy -0.0997 | validation: Loss 0.036838 Accuracy -1.3143\n",
      "Epoch 306 | train: Loss 0.020631 Accuracy 0.2434 | validation: Loss 0.073246 Accuracy -3.6015\n",
      "Epoch 307 | train: Loss 0.029256 Accuracy -0.0729 | validation: Loss 0.070190 Accuracy -3.4095\n",
      "Epoch 308 | train: Loss 0.028118 Accuracy -0.0312 | validation: Loss 0.036887 Accuracy -1.3173\n",
      "Epoch 309 | train: Loss 0.020674 Accuracy 0.2418 | validation: Loss 0.023906 Accuracy -0.5019\n",
      "Epoch 310 | train: Loss 0.025319 Accuracy 0.0715 | validation: Loss 0.024117 Accuracy -0.5151\n",
      "Epoch 311 | train: Loss 0.024394 Accuracy 0.1054 | validation: Loss 0.040094 Accuracy -1.5188\n",
      "Epoch 312 | train: Loss 0.020912 Accuracy 0.2331 | validation: Loss 0.066519 Accuracy -3.1789\n",
      "Epoch 313 | train: Loss 0.025476 Accuracy 0.0657 | validation: Loss 0.067292 Accuracy -3.2275\n",
      "Epoch 314 | train: Loss 0.025292 Accuracy 0.0725 | validation: Loss 0.040370 Accuracy -1.5362\n",
      "Epoch 315 | train: Loss 0.020958 Accuracy 0.2314 | validation: Loss 0.025930 Accuracy -0.6290\n",
      "Epoch 316 | train: Loss 0.023501 Accuracy 0.1381 | validation: Loss 0.027034 Accuracy -0.6983\n",
      "Epoch 317 | train: Loss 0.023163 Accuracy 0.1505 | validation: Loss 0.044892 Accuracy -1.8202\n",
      "Epoch 318 | train: Loss 0.021134 Accuracy 0.2249 | validation: Loss 0.067606 Accuracy -3.2472\n",
      "Epoch 319 | train: Loss 0.023941 Accuracy 0.1220 | validation: Loss 0.060073 Accuracy -2.7740\n",
      "Epoch 320 | train: Loss 0.022360 Accuracy 0.1800 | validation: Loss 0.036966 Accuracy -1.3223\n",
      "Epoch 321 | train: Loss 0.021601 Accuracy 0.2078 | validation: Loss 0.040475 Accuracy -1.5428\n",
      "Epoch 322 | train: Loss 0.021118 Accuracy 0.2255 | validation: Loss 0.054100 Accuracy -2.3987\n",
      "Epoch 323 | train: Loss 0.021412 Accuracy 0.2148 | validation: Loss 0.053319 Accuracy -2.3496\n",
      "Epoch 324 | train: Loss 0.021249 Accuracy 0.2207 | validation: Loss 0.044066 Accuracy -1.7683\n",
      "Epoch 325 | train: Loss 0.021022 Accuracy 0.2290 | validation: Loss 0.040167 Accuracy -1.5234\n",
      "Epoch 326 | train: Loss 0.021126 Accuracy 0.2252 | validation: Loss 0.046478 Accuracy -1.9199\n",
      "Epoch 327 | train: Loss 0.020800 Accuracy 0.2372 | validation: Loss 0.054158 Accuracy -2.4024\n",
      "Epoch 328 | train: Loss 0.020941 Accuracy 0.2320 | validation: Loss 0.048104 Accuracy -2.0220\n",
      "Epoch 329 | train: Loss 0.020585 Accuracy 0.2451 | validation: Loss 0.044674 Accuracy -1.8066\n",
      "Epoch 330 | train: Loss 0.020759 Accuracy 0.2387 | validation: Loss 0.054479 Accuracy -2.4225\n",
      "Epoch 331 | train: Loss 0.020643 Accuracy 0.2429 | validation: Loss 0.054778 Accuracy -2.4413\n",
      "Epoch 332 | train: Loss 0.020716 Accuracy 0.2403 | validation: Loss 0.046842 Accuracy -1.9427\n",
      "Epoch 333 | train: Loss 0.020588 Accuracy 0.2450 | validation: Loss 0.049937 Accuracy -2.1372\n",
      "Epoch 334 | train: Loss 0.020506 Accuracy 0.2480 | validation: Loss 0.052797 Accuracy -2.3169\n",
      "Epoch 335 | train: Loss 0.020579 Accuracy 0.2453 | validation: Loss 0.042412 Accuracy -1.6645\n",
      "Epoch 336 | train: Loss 0.020341 Accuracy 0.2540 | validation: Loss 0.046131 Accuracy -1.8981\n",
      "Epoch 337 | train: Loss 0.020148 Accuracy 0.2611 | validation: Loss 0.049830 Accuracy -2.1305\n",
      "Epoch 338 | train: Loss 0.020204 Accuracy 0.2591 | validation: Loss 0.041826 Accuracy -1.6276\n",
      "Epoch 339 | train: Loss 0.020103 Accuracy 0.2628 | validation: Loss 0.046774 Accuracy -1.9385\n",
      "Epoch 340 | train: Loss 0.020068 Accuracy 0.2640 | validation: Loss 0.052158 Accuracy -2.2767\n",
      "Epoch 341 | train: Loss 0.020362 Accuracy 0.2533 | validation: Loss 0.043796 Accuracy -1.7514\n",
      "Epoch 342 | train: Loss 0.020116 Accuracy 0.2623 | validation: Loss 0.041837 Accuracy -1.6283\n",
      "Epoch 343 | train: Loss 0.020105 Accuracy 0.2627 | validation: Loss 0.049708 Accuracy -2.1228\n",
      "Epoch 344 | train: Loss 0.020215 Accuracy 0.2586 | validation: Loss 0.044207 Accuracy -1.7772\n",
      "Epoch 345 | train: Loss 0.019925 Accuracy 0.2693 | validation: Loss 0.040209 Accuracy -1.5260\n",
      "Epoch 346 | train: Loss 0.019885 Accuracy 0.2708 | validation: Loss 0.048624 Accuracy -2.0547\n",
      "Epoch 347 | train: Loss 0.019994 Accuracy 0.2668 | validation: Loss 0.042691 Accuracy -1.6820\n",
      "Epoch 348 | train: Loss 0.019762 Accuracy 0.2753 | validation: Loss 0.040731 Accuracy -1.5588\n",
      "Epoch 349 | train: Loss 0.019783 Accuracy 0.2745 | validation: Loss 0.048195 Accuracy -2.0278\n",
      "Epoch 350 | train: Loss 0.019992 Accuracy 0.2668 | validation: Loss 0.040322 Accuracy -1.5332\n",
      "Epoch 351 | train: Loss 0.019849 Accuracy 0.2721 | validation: Loss 0.042988 Accuracy -1.7006\n",
      "Epoch 352 | train: Loss 0.019878 Accuracy 0.2710 | validation: Loss 0.046222 Accuracy -1.9038\n",
      "Epoch 353 | train: Loss 0.020029 Accuracy 0.2655 | validation: Loss 0.040024 Accuracy -1.5144\n",
      "Epoch 354 | train: Loss 0.019972 Accuracy 0.2676 | validation: Loss 0.044413 Accuracy -1.7902\n",
      "Epoch 355 | train: Loss 0.020002 Accuracy 0.2664 | validation: Loss 0.045125 Accuracy -1.8349\n",
      "Epoch 356 | train: Loss 0.020025 Accuracy 0.2656 | validation: Loss 0.041284 Accuracy -1.5936\n",
      "Epoch 357 | train: Loss 0.019980 Accuracy 0.2673 | validation: Loss 0.045631 Accuracy -1.8667\n",
      "Epoch 358 | train: Loss 0.020018 Accuracy 0.2659 | validation: Loss 0.045582 Accuracy -1.8636\n",
      "Epoch 359 | train: Loss 0.020007 Accuracy 0.2663 | validation: Loss 0.042375 Accuracy -1.6621\n",
      "Epoch 360 | train: Loss 0.019977 Accuracy 0.2674 | validation: Loss 0.046137 Accuracy -1.8984\n",
      "Epoch 361 | train: Loss 0.020029 Accuracy 0.2655 | validation: Loss 0.045669 Accuracy -1.8691\n",
      "Epoch 362 | train: Loss 0.020012 Accuracy 0.2661 | validation: Loss 0.043622 Accuracy -1.7404\n",
      "Epoch 363 | train: Loss 0.019992 Accuracy 0.2668 | validation: Loss 0.046096 Accuracy -1.8959\n",
      "Epoch 364 | train: Loss 0.020015 Accuracy 0.2660 | validation: Loss 0.045935 Accuracy -1.8858\n",
      "Epoch 365 | train: Loss 0.020010 Accuracy 0.2662 | validation: Loss 0.043811 Accuracy -1.7523\n",
      "Epoch 366 | train: Loss 0.019984 Accuracy 0.2671 | validation: Loss 0.045640 Accuracy -1.8672\n",
      "Epoch 367 | train: Loss 0.019999 Accuracy 0.2666 | validation: Loss 0.046027 Accuracy -1.8916\n",
      "Epoch 368 | train: Loss 0.020002 Accuracy 0.2665 | validation: Loss 0.044162 Accuracy -1.7744\n",
      "Epoch 369 | train: Loss 0.019966 Accuracy 0.2678 | validation: Loss 0.045548 Accuracy -1.8614\n",
      "Epoch 370 | train: Loss 0.019966 Accuracy 0.2678 | validation: Loss 0.046399 Accuracy -1.9149\n",
      "Epoch 371 | train: Loss 0.019968 Accuracy 0.2677 | validation: Loss 0.044712 Accuracy -1.8089\n",
      "Epoch 372 | train: Loss 0.019932 Accuracy 0.2690 | validation: Loss 0.045458 Accuracy -1.8558\n",
      "Epoch 373 | train: Loss 0.019934 Accuracy 0.2689 | validation: Loss 0.046236 Accuracy -1.9047\n",
      "Epoch 374 | train: Loss 0.019952 Accuracy 0.2683 | validation: Loss 0.044636 Accuracy -1.8042\n",
      "Epoch 375 | train: Loss 0.019940 Accuracy 0.2687 | validation: Loss 0.045227 Accuracy -1.8413\n",
      "Epoch 376 | train: Loss 0.019958 Accuracy 0.2681 | validation: Loss 0.045819 Accuracy -1.8785\n",
      "Epoch 377 | train: Loss 0.019981 Accuracy 0.2672 | validation: Loss 0.044667 Accuracy -1.8061\n",
      "Epoch 378 | train: Loss 0.019983 Accuracy 0.2672 | validation: Loss 0.044837 Accuracy -1.8168\n",
      "Epoch 379 | train: Loss 0.019983 Accuracy 0.2671 | validation: Loss 0.045451 Accuracy -1.8554\n",
      "Epoch 380 | train: Loss 0.019986 Accuracy 0.2671 | validation: Loss 0.044606 Accuracy -1.8023\n",
      "Epoch 381 | train: Loss 0.019963 Accuracy 0.2679 | validation: Loss 0.044890 Accuracy -1.8201\n",
      "Epoch 382 | train: Loss 0.019958 Accuracy 0.2681 | validation: Loss 0.045474 Accuracy -1.8568\n",
      "Epoch 383 | train: Loss 0.019964 Accuracy 0.2679 | validation: Loss 0.044778 Accuracy -1.8131\n",
      "Epoch 384 | train: Loss 0.019956 Accuracy 0.2681 | validation: Loss 0.045073 Accuracy -1.8316\n",
      "Epoch 385 | train: Loss 0.019967 Accuracy 0.2677 | validation: Loss 0.045503 Accuracy -1.8586\n",
      "Epoch 386 | train: Loss 0.019983 Accuracy 0.2671 | validation: Loss 0.044840 Accuracy -1.8170\n",
      "Epoch 387 | train: Loss 0.019984 Accuracy 0.2671 | validation: Loss 0.045142 Accuracy -1.8359\n",
      "Epoch 388 | train: Loss 0.019995 Accuracy 0.2667 | validation: Loss 0.045492 Accuracy -1.8579\n",
      "Epoch 389 | train: Loss 0.020005 Accuracy 0.2664 | validation: Loss 0.044953 Accuracy -1.8241\n",
      "Epoch 390 | train: Loss 0.020000 Accuracy 0.2665 | validation: Loss 0.045360 Accuracy -1.8496\n",
      "Epoch 391 | train: Loss 0.020006 Accuracy 0.2663 | validation: Loss 0.045595 Accuracy -1.8644\n",
      "Epoch 392 | train: Loss 0.020011 Accuracy 0.2661 | validation: Loss 0.045185 Accuracy -1.8387\n",
      "Epoch 393 | train: Loss 0.020008 Accuracy 0.2662 | validation: Loss 0.045647 Accuracy -1.8677\n",
      "Epoch 394 | train: Loss 0.020018 Accuracy 0.2659 | validation: Loss 0.045711 Accuracy -1.8717\n",
      "Epoch 395 | train: Loss 0.020025 Accuracy 0.2656 | validation: Loss 0.045391 Accuracy -1.8516\n",
      "Epoch 396 | train: Loss 0.020027 Accuracy 0.2655 | validation: Loss 0.045822 Accuracy -1.8787\n",
      "Epoch 397 | train: Loss 0.020039 Accuracy 0.2651 | validation: Loss 0.045711 Accuracy -1.8717\n",
      "Epoch 398 | train: Loss 0.020044 Accuracy 0.2649 | validation: Loss 0.045522 Accuracy -1.8598\n",
      "Epoch 399 | train: Loss 0.020048 Accuracy 0.2648 | validation: Loss 0.045918 Accuracy -1.8847\n",
      "Epoch 400 | train: Loss 0.020058 Accuracy 0.2644 | validation: Loss 0.045716 Accuracy -1.8720\n",
      "Epoch 401 | train: Loss 0.020061 Accuracy 0.2643 | validation: Loss 0.045696 Accuracy -1.8708\n",
      "Epoch 402 | train: Loss 0.020065 Accuracy 0.2641 | validation: Loss 0.046010 Accuracy -1.8905\n",
      "Epoch 403 | train: Loss 0.020074 Accuracy 0.2638 | validation: Loss 0.045760 Accuracy -1.8748\n",
      "Epoch 404 | train: Loss 0.020075 Accuracy 0.2638 | validation: Loss 0.045861 Accuracy -1.8811\n",
      "Epoch 405 | train: Loss 0.020082 Accuracy 0.2635 | validation: Loss 0.046027 Accuracy -1.8916\n",
      "Epoch 406 | train: Loss 0.020090 Accuracy 0.2632 | validation: Loss 0.045766 Accuracy -1.8752\n",
      "Epoch 407 | train: Loss 0.020094 Accuracy 0.2631 | validation: Loss 0.045932 Accuracy -1.8856\n",
      "Epoch 408 | train: Loss 0.020105 Accuracy 0.2627 | validation: Loss 0.045938 Accuracy -1.8860\n",
      "Epoch 409 | train: Loss 0.020113 Accuracy 0.2624 | validation: Loss 0.045741 Accuracy -1.8736\n",
      "Epoch 410 | train: Loss 0.020119 Accuracy 0.2622 | validation: Loss 0.045939 Accuracy -1.8860\n",
      "Epoch 411 | train: Loss 0.020128 Accuracy 0.2618 | validation: Loss 0.045846 Accuracy -1.8802\n",
      "Epoch 412 | train: Loss 0.020132 Accuracy 0.2617 | validation: Loss 0.045779 Accuracy -1.8760\n",
      "Epoch 413 | train: Loss 0.020135 Accuracy 0.2616 | validation: Loss 0.045968 Accuracy -1.8878\n",
      "Epoch 414 | train: Loss 0.020141 Accuracy 0.2614 | validation: Loss 0.045835 Accuracy -1.8795\n",
      "Epoch 415 | train: Loss 0.020142 Accuracy 0.2613 | validation: Loss 0.045888 Accuracy -1.8828\n",
      "Epoch 416 | train: Loss 0.020146 Accuracy 0.2612 | validation: Loss 0.046004 Accuracy -1.8901\n",
      "Epoch 417 | train: Loss 0.020152 Accuracy 0.2610 | validation: Loss 0.045871 Accuracy -1.8818\n",
      "Epoch 418 | train: Loss 0.020155 Accuracy 0.2609 | validation: Loss 0.045996 Accuracy -1.8896\n",
      "Epoch 419 | train: Loss 0.020161 Accuracy 0.2606 | validation: Loss 0.046012 Accuracy -1.8906\n",
      "Epoch 420 | train: Loss 0.020167 Accuracy 0.2604 | validation: Loss 0.045935 Accuracy -1.8858\n",
      "Epoch 421 | train: Loss 0.020170 Accuracy 0.2603 | validation: Loss 0.046080 Accuracy -1.8949\n",
      "Epoch 422 | train: Loss 0.020176 Accuracy 0.2601 | validation: Loss 0.046026 Accuracy -1.8915\n",
      "Epoch 423 | train: Loss 0.020179 Accuracy 0.2600 | validation: Loss 0.046038 Accuracy -1.8923\n",
      "Epoch 424 | train: Loss 0.020183 Accuracy 0.2598 | validation: Loss 0.046150 Accuracy -1.8993\n",
      "Epoch 425 | train: Loss 0.020188 Accuracy 0.2596 | validation: Loss 0.046076 Accuracy -1.8946\n",
      "Epoch 426 | train: Loss 0.020191 Accuracy 0.2595 | validation: Loss 0.046162 Accuracy -1.9000\n",
      "Epoch 427 | train: Loss 0.020196 Accuracy 0.2593 | validation: Loss 0.046202 Accuracy -1.9026\n",
      "Epoch 428 | train: Loss 0.020200 Accuracy 0.2592 | validation: Loss 0.046155 Accuracy -1.8996\n",
      "Epoch 429 | train: Loss 0.020204 Accuracy 0.2591 | validation: Loss 0.046259 Accuracy -1.9062\n",
      "Epoch 430 | train: Loss 0.020209 Accuracy 0.2589 | validation: Loss 0.046229 Accuracy -1.9042\n",
      "Epoch 431 | train: Loss 0.020212 Accuracy 0.2588 | validation: Loss 0.046236 Accuracy -1.9047\n",
      "Epoch 432 | train: Loss 0.020216 Accuracy 0.2586 | validation: Loss 0.046307 Accuracy -1.9091\n",
      "Epoch 433 | train: Loss 0.020220 Accuracy 0.2585 | validation: Loss 0.046250 Accuracy -1.9056\n",
      "Epoch 434 | train: Loss 0.020223 Accuracy 0.2584 | validation: Loss 0.046308 Accuracy -1.9092\n",
      "Epoch 435 | train: Loss 0.020228 Accuracy 0.2582 | validation: Loss 0.046323 Accuracy -1.9101\n",
      "Epoch 436 | train: Loss 0.020232 Accuracy 0.2580 | validation: Loss 0.046296 Accuracy -1.9084\n",
      "Epoch 437 | train: Loss 0.020235 Accuracy 0.2579 | validation: Loss 0.046369 Accuracy -1.9130\n",
      "Epoch 438 | train: Loss 0.020240 Accuracy 0.2577 | validation: Loss 0.046341 Accuracy -1.9113\n",
      "Epoch 439 | train: Loss 0.020243 Accuracy 0.2576 | validation: Loss 0.046366 Accuracy -1.9128\n",
      "Epoch 440 | train: Loss 0.020247 Accuracy 0.2575 | validation: Loss 0.046406 Accuracy -1.9154\n",
      "Epoch 441 | train: Loss 0.020251 Accuracy 0.2573 | validation: Loss 0.046375 Accuracy -1.9134\n",
      "Epoch 442 | train: Loss 0.020254 Accuracy 0.2572 | validation: Loss 0.046428 Accuracy -1.9167\n",
      "Epoch 443 | train: Loss 0.020259 Accuracy 0.2570 | validation: Loss 0.046421 Accuracy -1.9163\n",
      "Epoch 444 | train: Loss 0.020263 Accuracy 0.2569 | validation: Loss 0.046420 Accuracy -1.9162\n",
      "Epoch 445 | train: Loss 0.020267 Accuracy 0.2567 | validation: Loss 0.046462 Accuracy -1.9189\n",
      "Epoch 446 | train: Loss 0.020272 Accuracy 0.2566 | validation: Loss 0.046435 Accuracy -1.9172\n",
      "Epoch 447 | train: Loss 0.020275 Accuracy 0.2564 | validation: Loss 0.046470 Accuracy -1.9194\n",
      "Epoch 448 | train: Loss 0.020279 Accuracy 0.2563 | validation: Loss 0.046479 Accuracy -1.9200\n",
      "Epoch 449 | train: Loss 0.020283 Accuracy 0.2562 | validation: Loss 0.046473 Accuracy -1.9196\n",
      "Epoch 450 | train: Loss 0.020286 Accuracy 0.2561 | validation: Loss 0.046515 Accuracy -1.9222\n",
      "Epoch 451 | train: Loss 0.020289 Accuracy 0.2559 | validation: Loss 0.046501 Accuracy -1.9213\n",
      "Epoch 452 | train: Loss 0.020292 Accuracy 0.2558 | validation: Loss 0.046528 Accuracy -1.9230\n",
      "Epoch 453 | train: Loss 0.020296 Accuracy 0.2557 | validation: Loss 0.046545 Accuracy -1.9241\n",
      "Epoch 454 | train: Loss 0.020299 Accuracy 0.2556 | validation: Loss 0.046539 Accuracy -1.9237\n",
      "Epoch 455 | train: Loss 0.020303 Accuracy 0.2554 | validation: Loss 0.046576 Accuracy -1.9260\n",
      "Epoch 456 | train: Loss 0.020307 Accuracy 0.2553 | validation: Loss 0.046568 Accuracy -1.9256\n",
      "Epoch 457 | train: Loss 0.020310 Accuracy 0.2552 | validation: Loss 0.046590 Accuracy -1.9269\n",
      "Epoch 458 | train: Loss 0.020314 Accuracy 0.2550 | validation: Loss 0.046607 Accuracy -1.9280\n",
      "Epoch 459 | train: Loss 0.020317 Accuracy 0.2549 | validation: Loss 0.046605 Accuracy -1.9279\n",
      "Epoch 460 | train: Loss 0.020321 Accuracy 0.2548 | validation: Loss 0.046637 Accuracy -1.9299\n",
      "Epoch 461 | train: Loss 0.020324 Accuracy 0.2546 | validation: Loss 0.046635 Accuracy -1.9297\n",
      "Epoch 462 | train: Loss 0.020327 Accuracy 0.2545 | validation: Loss 0.046655 Accuracy -1.9310\n",
      "Epoch 463 | train: Loss 0.020330 Accuracy 0.2544 | validation: Loss 0.046671 Accuracy -1.9320\n",
      "Epoch 464 | train: Loss 0.020334 Accuracy 0.2543 | validation: Loss 0.046672 Accuracy -1.9321\n",
      "Epoch 465 | train: Loss 0.020337 Accuracy 0.2542 | validation: Loss 0.046699 Accuracy -1.9338\n",
      "Epoch 466 | train: Loss 0.020340 Accuracy 0.2541 | validation: Loss 0.046697 Accuracy -1.9337\n",
      "Epoch 467 | train: Loss 0.020343 Accuracy 0.2539 | validation: Loss 0.046717 Accuracy -1.9349\n",
      "Epoch 468 | train: Loss 0.020347 Accuracy 0.2538 | validation: Loss 0.046729 Accuracy -1.9356\n",
      "Epoch 469 | train: Loss 0.020350 Accuracy 0.2537 | validation: Loss 0.046734 Accuracy -1.9360\n",
      "Epoch 470 | train: Loss 0.020353 Accuracy 0.2536 | validation: Loss 0.046757 Accuracy -1.9374\n",
      "Epoch 471 | train: Loss 0.020356 Accuracy 0.2535 | validation: Loss 0.046757 Accuracy -1.9374\n",
      "Epoch 472 | train: Loss 0.020359 Accuracy 0.2534 | validation: Loss 0.046777 Accuracy -1.9387\n",
      "Epoch 473 | train: Loss 0.020362 Accuracy 0.2532 | validation: Loss 0.046784 Accuracy -1.9391\n",
      "Epoch 474 | train: Loss 0.020365 Accuracy 0.2531 | validation: Loss 0.046794 Accuracy -1.9398\n",
      "Epoch 475 | train: Loss 0.020369 Accuracy 0.2530 | validation: Loss 0.046811 Accuracy -1.9408\n",
      "Epoch 476 | train: Loss 0.020372 Accuracy 0.2529 | validation: Loss 0.046813 Accuracy -1.9409\n",
      "Epoch 477 | train: Loss 0.020375 Accuracy 0.2528 | validation: Loss 0.046832 Accuracy -1.9421\n",
      "Epoch 478 | train: Loss 0.020378 Accuracy 0.2527 | validation: Loss 0.046835 Accuracy -1.9423\n",
      "Epoch 479 | train: Loss 0.020381 Accuracy 0.2525 | validation: Loss 0.046850 Accuracy -1.9432\n",
      "Epoch 480 | train: Loss 0.020385 Accuracy 0.2524 | validation: Loss 0.046859 Accuracy -1.9438\n",
      "Epoch 481 | train: Loss 0.020388 Accuracy 0.2523 | validation: Loss 0.046866 Accuracy -1.9443\n",
      "Epoch 482 | train: Loss 0.020391 Accuracy 0.2522 | validation: Loss 0.046882 Accuracy -1.9453\n",
      "Epoch 483 | train: Loss 0.020394 Accuracy 0.2521 | validation: Loss 0.046885 Accuracy -1.9455\n",
      "Epoch 484 | train: Loss 0.020396 Accuracy 0.2520 | validation: Loss 0.046902 Accuracy -1.9465\n",
      "Epoch 485 | train: Loss 0.020400 Accuracy 0.2519 | validation: Loss 0.046906 Accuracy -1.9468\n",
      "Epoch 486 | train: Loss 0.020403 Accuracy 0.2518 | validation: Loss 0.046920 Accuracy -1.9476\n",
      "Epoch 487 | train: Loss 0.020406 Accuracy 0.2517 | validation: Loss 0.046929 Accuracy -1.9482\n",
      "Epoch 488 | train: Loss 0.020409 Accuracy 0.2516 | validation: Loss 0.046938 Accuracy -1.9488\n",
      "Epoch 489 | train: Loss 0.020411 Accuracy 0.2514 | validation: Loss 0.046951 Accuracy -1.9496\n",
      "Epoch 490 | train: Loss 0.020414 Accuracy 0.2513 | validation: Loss 0.046957 Accuracy -1.9500\n",
      "Epoch 491 | train: Loss 0.020417 Accuracy 0.2512 | validation: Loss 0.046972 Accuracy -1.9509\n",
      "Epoch 492 | train: Loss 0.020420 Accuracy 0.2511 | validation: Loss 0.046977 Accuracy -1.9513\n",
      "Epoch 493 | train: Loss 0.020423 Accuracy 0.2510 | validation: Loss 0.046991 Accuracy -1.9521\n",
      "Epoch 494 | train: Loss 0.020426 Accuracy 0.2509 | validation: Loss 0.046998 Accuracy -1.9526\n",
      "Epoch 495 | train: Loss 0.020429 Accuracy 0.2508 | validation: Loss 0.047009 Accuracy -1.9533\n",
      "Epoch 496 | train: Loss 0.020432 Accuracy 0.2507 | validation: Loss 0.047019 Accuracy -1.9539\n",
      "Epoch 497 | train: Loss 0.020435 Accuracy 0.2506 | validation: Loss 0.047027 Accuracy -1.9544\n",
      "Epoch 498 | train: Loss 0.020438 Accuracy 0.2505 | validation: Loss 0.047039 Accuracy -1.9552\n",
      "Epoch 499 | train: Loss 0.020441 Accuracy 0.2504 | validation: Loss 0.047046 Accuracy -1.9556\n",
      "Epoch 500 | train: Loss 0.020443 Accuracy 0.2503 | validation: Loss 0.047059 Accuracy -1.9564\n",
      "Epoch 501 | train: Loss 0.020446 Accuracy 0.2502 | validation: Loss 0.047065 Accuracy -1.9567\n",
      "Epoch 502 | train: Loss 0.020449 Accuracy 0.2501 | validation: Loss 0.047078 Accuracy -1.9576\n",
      "Epoch 503 | train: Loss 0.020452 Accuracy 0.2500 | validation: Loss 0.047084 Accuracy -1.9579\n",
      "Epoch 504 | train: Loss 0.020455 Accuracy 0.2499 | validation: Loss 0.047097 Accuracy -1.9587\n",
      "Epoch 505 | train: Loss 0.020457 Accuracy 0.2498 | validation: Loss 0.047102 Accuracy -1.9591\n",
      "Epoch 506 | train: Loss 0.020460 Accuracy 0.2497 | validation: Loss 0.047115 Accuracy -1.9599\n",
      "Epoch 507 | train: Loss 0.020463 Accuracy 0.2496 | validation: Loss 0.047121 Accuracy -1.9603\n",
      "Epoch 508 | train: Loss 0.020466 Accuracy 0.2495 | validation: Loss 0.047132 Accuracy -1.9610\n",
      "Epoch 509 | train: Loss 0.020468 Accuracy 0.2494 | validation: Loss 0.047139 Accuracy -1.9614\n",
      "Epoch 510 | train: Loss 0.020471 Accuracy 0.2493 | validation: Loss 0.047150 Accuracy -1.9621\n",
      "Epoch 511 | train: Loss 0.020474 Accuracy 0.2492 | validation: Loss 0.047157 Accuracy -1.9625\n",
      "Epoch 512 | train: Loss 0.020477 Accuracy 0.2491 | validation: Loss 0.047167 Accuracy -1.9632\n",
      "Epoch 513 | train: Loss 0.020479 Accuracy 0.2490 | validation: Loss 0.047174 Accuracy -1.9636\n",
      "Epoch 514 | train: Loss 0.020482 Accuracy 0.2489 | validation: Loss 0.047185 Accuracy -1.9643\n",
      "Epoch 515 | train: Loss 0.020485 Accuracy 0.2488 | validation: Loss 0.047191 Accuracy -1.9647\n",
      "Epoch 516 | train: Loss 0.020487 Accuracy 0.2487 | validation: Loss 0.047202 Accuracy -1.9654\n",
      "Epoch 517 | train: Loss 0.020490 Accuracy 0.2486 | validation: Loss 0.047207 Accuracy -1.9657\n",
      "Epoch 518 | train: Loss 0.020493 Accuracy 0.2485 | validation: Loss 0.047221 Accuracy -1.9665\n",
      "Epoch 519 | train: Loss 0.020495 Accuracy 0.2484 | validation: Loss 0.047222 Accuracy -1.9666\n",
      "Epoch 520 | train: Loss 0.020498 Accuracy 0.2483 | validation: Loss 0.047239 Accuracy -1.9677\n",
      "Epoch 521 | train: Loss 0.020501 Accuracy 0.2482 | validation: Loss 0.047236 Accuracy -1.9675\n",
      "Epoch 522 | train: Loss 0.020503 Accuracy 0.2481 | validation: Loss 0.047260 Accuracy -1.9690\n",
      "Epoch 523 | train: Loss 0.020506 Accuracy 0.2480 | validation: Loss 0.047248 Accuracy -1.9682\n",
      "Epoch 524 | train: Loss 0.020508 Accuracy 0.2479 | validation: Loss 0.047283 Accuracy -1.9705\n",
      "Epoch 525 | train: Loss 0.020511 Accuracy 0.2478 | validation: Loss 0.047254 Accuracy -1.9687\n",
      "Epoch 526 | train: Loss 0.020513 Accuracy 0.2477 | validation: Loss 0.047314 Accuracy -1.9724\n",
      "Epoch 527 | train: Loss 0.020516 Accuracy 0.2476 | validation: Loss 0.047250 Accuracy -1.9684\n",
      "Epoch 528 | train: Loss 0.020518 Accuracy 0.2475 | validation: Loss 0.047359 Accuracy -1.9752\n",
      "Epoch 529 | train: Loss 0.020522 Accuracy 0.2474 | validation: Loss 0.047224 Accuracy -1.9667\n",
      "Epoch 530 | train: Loss 0.020523 Accuracy 0.2474 | validation: Loss 0.047439 Accuracy -1.9803\n",
      "Epoch 531 | train: Loss 0.020527 Accuracy 0.2472 | validation: Loss 0.047145 Accuracy -1.9618\n",
      "Epoch 532 | train: Loss 0.020527 Accuracy 0.2472 | validation: Loss 0.047599 Accuracy -1.9903\n",
      "Epoch 533 | train: Loss 0.020534 Accuracy 0.2469 | validation: Loss 0.046944 Accuracy -1.9491\n",
      "Epoch 534 | train: Loss 0.020531 Accuracy 0.2471 | validation: Loss 0.047952 Accuracy -2.0125\n",
      "Epoch 535 | train: Loss 0.020544 Accuracy 0.2466 | validation: Loss 0.046448 Accuracy -1.9180\n",
      "Epoch 536 | train: Loss 0.020535 Accuracy 0.2469 | validation: Loss 0.048792 Accuracy -2.0652\n",
      "Epoch 537 | train: Loss 0.020568 Accuracy 0.2457 | validation: Loss 0.045231 Accuracy -1.8416\n",
      "Epoch 538 | train: Loss 0.020552 Accuracy 0.2463 | validation: Loss 0.050904 Accuracy -2.1979\n",
      "Epoch 539 | train: Loss 0.020655 Accuracy 0.2425 | validation: Loss 0.042318 Accuracy -1.6585\n",
      "Epoch 540 | train: Loss 0.020674 Accuracy 0.2418 | validation: Loss 0.056373 Accuracy -2.5415\n",
      "Epoch 541 | train: Loss 0.021084 Accuracy 0.2268 | validation: Loss 0.036344 Accuracy -1.2832\n",
      "Epoch 542 | train: Loss 0.021385 Accuracy 0.2157 | validation: Loss 0.068097 Accuracy -3.2781\n",
      "Epoch 543 | train: Loss 0.022833 Accuracy 0.1626 | validation: Loss 0.031048 Accuracy -0.9505\n",
      "Epoch 544 | train: Loss 0.022744 Accuracy 0.1659 | validation: Loss 0.065889 Accuracy -3.1393\n",
      "Epoch 545 | train: Loss 0.022672 Accuracy 0.1686 | validation: Loss 0.043818 Accuracy -1.7528\n",
      "Epoch 546 | train: Loss 0.020807 Accuracy 0.2370 | validation: Loss 0.035600 Accuracy -1.2365\n",
      "Epoch 547 | train: Loss 0.021650 Accuracy 0.2060 | validation: Loss 0.060179 Accuracy -2.7806\n",
      "Epoch 548 | train: Loss 0.022048 Accuracy 0.1914 | validation: Loss 0.048197 Accuracy -2.0279\n",
      "Epoch 549 | train: Loss 0.020937 Accuracy 0.2322 | validation: Loss 0.035116 Accuracy -1.2061\n",
      "Epoch 550 | train: Loss 0.021795 Accuracy 0.2007 | validation: Loss 0.052443 Accuracy -2.2946\n",
      "Epoch 551 | train: Loss 0.021161 Accuracy 0.2240 | validation: Loss 0.055581 Accuracy -2.4918\n",
      "Epoch 552 | train: Loss 0.021406 Accuracy 0.2150 | validation: Loss 0.037971 Accuracy -1.3855\n",
      "Epoch 553 | train: Loss 0.021357 Accuracy 0.2168 | validation: Loss 0.044996 Accuracy -1.8268\n",
      "Epoch 554 | train: Loss 0.020865 Accuracy 0.2348 | validation: Loss 0.058521 Accuracy -2.6765\n",
      "Epoch 555 | train: Loss 0.021597 Accuracy 0.2080 | validation: Loss 0.045141 Accuracy -1.8359\n",
      "Epoch 556 | train: Loss 0.020918 Accuracy 0.2329 | validation: Loss 0.040596 Accuracy -1.5504\n",
      "Epoch 557 | train: Loss 0.021206 Accuracy 0.2223 | validation: Loss 0.054619 Accuracy -2.4313\n",
      "Epoch 558 | train: Loss 0.021237 Accuracy 0.2212 | validation: Loss 0.052632 Accuracy -2.3065\n",
      "Epoch 559 | train: Loss 0.021109 Accuracy 0.2259 | validation: Loss 0.041122 Accuracy -1.5834\n",
      "Epoch 560 | train: Loss 0.021232 Accuracy 0.2213 | validation: Loss 0.048124 Accuracy -2.0233\n",
      "Epoch 561 | train: Loss 0.020970 Accuracy 0.2309 | validation: Loss 0.055508 Accuracy -2.4872\n",
      "Epoch 562 | train: Loss 0.021314 Accuracy 0.2183 | validation: Loss 0.045244 Accuracy -1.8424\n",
      "Epoch 563 | train: Loss 0.021003 Accuracy 0.2297 | validation: Loss 0.044633 Accuracy -1.8040\n",
      "Epoch 564 | train: Loss 0.021017 Accuracy 0.2292 | validation: Loss 0.054248 Accuracy -2.4080\n",
      "Epoch 565 | train: Loss 0.021202 Accuracy 0.2224 | validation: Loss 0.048537 Accuracy -2.0492\n",
      "Epoch 566 | train: Loss 0.020948 Accuracy 0.2318 | validation: Loss 0.043256 Accuracy -1.7175\n",
      "Epoch 567 | train: Loss 0.021044 Accuracy 0.2282 | validation: Loss 0.051187 Accuracy -2.2157\n",
      "Epoch 568 | train: Loss 0.021004 Accuracy 0.2297 | validation: Loss 0.050997 Accuracy -2.2038\n",
      "Epoch 569 | train: Loss 0.020992 Accuracy 0.2302 | validation: Loss 0.043919 Accuracy -1.7591\n",
      "Epoch 570 | train: Loss 0.020970 Accuracy 0.2310 | validation: Loss 0.047745 Accuracy -1.9995\n",
      "Epoch 571 | train: Loss 0.020897 Accuracy 0.2336 | validation: Loss 0.051957 Accuracy -2.2641\n",
      "Epoch 572 | train: Loss 0.021028 Accuracy 0.2288 | validation: Loss 0.045886 Accuracy -1.8827\n",
      "Epoch 573 | train: Loss 0.020897 Accuracy 0.2336 | validation: Loss 0.045525 Accuracy -1.8600\n",
      "Epoch 574 | train: Loss 0.020898 Accuracy 0.2336 | validation: Loss 0.051127 Accuracy -2.2120\n",
      "Epoch 575 | train: Loss 0.020978 Accuracy 0.2307 | validation: Loss 0.047783 Accuracy -2.0018\n",
      "Epoch 576 | train: Loss 0.020871 Accuracy 0.2346 | validation: Loss 0.044801 Accuracy -1.8146\n",
      "Epoch 577 | train: Loss 0.020883 Accuracy 0.2341 | validation: Loss 0.049841 Accuracy -2.1311\n",
      "Epoch 578 | train: Loss 0.020892 Accuracy 0.2338 | validation: Loss 0.048949 Accuracy -2.0751\n",
      "Epoch 579 | train: Loss 0.020851 Accuracy 0.2353 | validation: Loss 0.044975 Accuracy -1.8255\n",
      "Epoch 580 | train: Loss 0.020835 Accuracy 0.2359 | validation: Loss 0.048868 Accuracy -2.0700\n",
      "Epoch 581 | train: Loss 0.020816 Accuracy 0.2366 | validation: Loss 0.049540 Accuracy -2.1122\n",
      "Epoch 582 | train: Loss 0.020817 Accuracy 0.2366 | validation: Loss 0.045539 Accuracy -1.8609\n",
      "Epoch 583 | train: Loss 0.020779 Accuracy 0.2380 | validation: Loss 0.048166 Accuracy -2.0259\n",
      "Epoch 584 | train: Loss 0.020761 Accuracy 0.2386 | validation: Loss 0.049638 Accuracy -2.1184\n",
      "Epoch 585 | train: Loss 0.020785 Accuracy 0.2378 | validation: Loss 0.046022 Accuracy -1.8912\n",
      "Epoch 586 | train: Loss 0.020742 Accuracy 0.2393 | validation: Loss 0.047697 Accuracy -1.9965\n",
      "Epoch 587 | train: Loss 0.020734 Accuracy 0.2396 | validation: Loss 0.049480 Accuracy -2.1085\n",
      "Epoch 588 | train: Loss 0.020762 Accuracy 0.2386 | validation: Loss 0.046369 Accuracy -1.9131\n",
      "Epoch 589 | train: Loss 0.020720 Accuracy 0.2401 | validation: Loss 0.047466 Accuracy -1.9820\n",
      "Epoch 590 | train: Loss 0.020711 Accuracy 0.2404 | validation: Loss 0.049253 Accuracy -2.0942\n",
      "Epoch 591 | train: Loss 0.020735 Accuracy 0.2396 | validation: Loss 0.046604 Accuracy -1.9278\n",
      "Epoch 592 | train: Loss 0.020698 Accuracy 0.2410 | validation: Loss 0.047415 Accuracy -1.9787\n",
      "Epoch 593 | train: Loss 0.020694 Accuracy 0.2411 | validation: Loss 0.049044 Accuracy -2.0811\n",
      "Epoch 594 | train: Loss 0.020717 Accuracy 0.2402 | validation: Loss 0.046745 Accuracy -1.9366\n",
      "Epoch 595 | train: Loss 0.020688 Accuracy 0.2413 | validation: Loss 0.047487 Accuracy -1.9833\n",
      "Epoch 596 | train: Loss 0.020687 Accuracy 0.2413 | validation: Loss 0.048830 Accuracy -2.0677\n",
      "Epoch 597 | train: Loss 0.020702 Accuracy 0.2408 | validation: Loss 0.046831 Accuracy -1.9421\n",
      "Epoch 598 | train: Loss 0.020674 Accuracy 0.2418 | validation: Loss 0.047633 Accuracy -1.9925\n",
      "Epoch 599 | train: Loss 0.020671 Accuracy 0.2419 | validation: Loss 0.048626 Accuracy -2.0548\n",
      "Epoch 600 | train: Loss 0.020679 Accuracy 0.2416 | validation: Loss 0.046886 Accuracy -1.9455\n",
      "Epoch 601 | train: Loss 0.020658 Accuracy 0.2424 | validation: Loss 0.047798 Accuracy -2.0028\n",
      "Epoch 602 | train: Loss 0.020660 Accuracy 0.2423 | validation: Loss 0.048421 Accuracy -2.0420\n",
      "Epoch 603 | train: Loss 0.020668 Accuracy 0.2421 | validation: Loss 0.046942 Accuracy -1.9491\n",
      "Epoch 604 | train: Loss 0.020655 Accuracy 0.2425 | validation: Loss 0.047948 Accuracy -2.0122\n",
      "Epoch 605 | train: Loss 0.020661 Accuracy 0.2423 | validation: Loss 0.048197 Accuracy -2.0279\n",
      "Epoch 606 | train: Loss 0.020664 Accuracy 0.2422 | validation: Loss 0.047012 Accuracy -1.9535\n",
      "Epoch 607 | train: Loss 0.020654 Accuracy 0.2426 | validation: Loss 0.048076 Accuracy -2.0203\n",
      "Epoch 608 | train: Loss 0.020660 Accuracy 0.2423 | validation: Loss 0.047973 Accuracy -2.0138\n",
      "Epoch 609 | train: Loss 0.020658 Accuracy 0.2424 | validation: Loss 0.047123 Accuracy -1.9604\n",
      "Epoch 610 | train: Loss 0.020650 Accuracy 0.2427 | validation: Loss 0.048167 Accuracy -2.0260\n",
      "Epoch 611 | train: Loss 0.020658 Accuracy 0.2424 | validation: Loss 0.047768 Accuracy -2.0009\n",
      "Epoch 612 | train: Loss 0.020652 Accuracy 0.2426 | validation: Loss 0.047284 Accuracy -1.9705\n",
      "Epoch 613 | train: Loss 0.020649 Accuracy 0.2427 | validation: Loss 0.048217 Accuracy -2.0291\n",
      "Epoch 614 | train: Loss 0.020659 Accuracy 0.2424 | validation: Loss 0.047610 Accuracy -1.9910\n",
      "Epoch 615 | train: Loss 0.020654 Accuracy 0.2426 | validation: Loss 0.047495 Accuracy -1.9838\n",
      "Epoch 616 | train: Loss 0.020655 Accuracy 0.2425 | validation: Loss 0.048203 Accuracy -2.0282\n",
      "Epoch 617 | train: Loss 0.020665 Accuracy 0.2421 | validation: Loss 0.047510 Accuracy -1.9847\n",
      "Epoch 618 | train: Loss 0.020661 Accuracy 0.2423 | validation: Loss 0.047709 Accuracy -1.9972\n",
      "Epoch 619 | train: Loss 0.020664 Accuracy 0.2422 | validation: Loss 0.048122 Accuracy -2.0231\n",
      "Epoch 620 | train: Loss 0.020671 Accuracy 0.2419 | validation: Loss 0.047472 Accuracy -1.9823\n",
      "Epoch 621 | train: Loss 0.020667 Accuracy 0.2421 | validation: Loss 0.047890 Accuracy -2.0086\n",
      "Epoch 622 | train: Loss 0.020672 Accuracy 0.2419 | validation: Loss 0.047986 Accuracy -2.0146\n",
      "Epoch 623 | train: Loss 0.020675 Accuracy 0.2418 | validation: Loss 0.047505 Accuracy -1.9844\n",
      "Epoch 624 | train: Loss 0.020674 Accuracy 0.2418 | validation: Loss 0.048018 Accuracy -2.0166\n",
      "Epoch 625 | train: Loss 0.020681 Accuracy 0.2416 | validation: Loss 0.047842 Accuracy -2.0056\n",
      "Epoch 626 | train: Loss 0.020681 Accuracy 0.2415 | validation: Loss 0.047613 Accuracy -1.9912\n",
      "Epoch 627 | train: Loss 0.020682 Accuracy 0.2415 | validation: Loss 0.048076 Accuracy -2.0203\n",
      "Epoch 628 | train: Loss 0.020688 Accuracy 0.2413 | validation: Loss 0.047727 Accuracy -1.9984\n",
      "Epoch 629 | train: Loss 0.020686 Accuracy 0.2414 | validation: Loss 0.047762 Accuracy -2.0005\n",
      "Epoch 630 | train: Loss 0.020687 Accuracy 0.2413 | validation: Loss 0.048048 Accuracy -2.0185\n",
      "Epoch 631 | train: Loss 0.020692 Accuracy 0.2412 | validation: Loss 0.047667 Accuracy -1.9946\n",
      "Epoch 632 | train: Loss 0.020691 Accuracy 0.2412 | validation: Loss 0.047902 Accuracy -2.0093\n",
      "Epoch 633 | train: Loss 0.020695 Accuracy 0.2410 | validation: Loss 0.047954 Accuracy -2.0126\n",
      "Epoch 634 | train: Loss 0.020698 Accuracy 0.2409 | validation: Loss 0.047684 Accuracy -1.9957\n",
      "Epoch 635 | train: Loss 0.020699 Accuracy 0.2409 | validation: Loss 0.047998 Accuracy -2.0154\n",
      "Epoch 636 | train: Loss 0.020704 Accuracy 0.2407 | validation: Loss 0.047850 Accuracy -2.0061\n",
      "Epoch 637 | train: Loss 0.020705 Accuracy 0.2407 | validation: Loss 0.047780 Accuracy -2.0017\n",
      "Epoch 638 | train: Loss 0.020707 Accuracy 0.2406 | validation: Loss 0.048030 Accuracy -2.0174\n",
      "Epoch 639 | train: Loss 0.020712 Accuracy 0.2404 | validation: Loss 0.047789 Accuracy -2.0022\n",
      "Epoch 640 | train: Loss 0.020712 Accuracy 0.2404 | validation: Loss 0.047908 Accuracy -2.0097\n",
      "Epoch 641 | train: Loss 0.020715 Accuracy 0.2403 | validation: Loss 0.047991 Accuracy -2.0149\n",
      "Epoch 642 | train: Loss 0.020718 Accuracy 0.2402 | validation: Loss 0.047795 Accuracy -2.0026\n",
      "Epoch 643 | train: Loss 0.020718 Accuracy 0.2402 | validation: Loss 0.048005 Accuracy -2.0158\n",
      "Epoch 644 | train: Loss 0.020722 Accuracy 0.2401 | validation: Loss 0.047917 Accuracy -2.0103\n",
      "Epoch 645 | train: Loss 0.020724 Accuracy 0.2400 | validation: Loss 0.047867 Accuracy -2.0071\n",
      "Epoch 646 | train: Loss 0.020725 Accuracy 0.2399 | validation: Loss 0.048035 Accuracy -2.0177\n",
      "Epoch 647 | train: Loss 0.020729 Accuracy 0.2398 | validation: Loss 0.047868 Accuracy -2.0072\n",
      "Epoch 648 | train: Loss 0.020730 Accuracy 0.2398 | validation: Loss 0.047966 Accuracy -2.0134\n",
      "Epoch 649 | train: Loss 0.020733 Accuracy 0.2396 | validation: Loss 0.048000 Accuracy -2.0155\n",
      "Epoch 650 | train: Loss 0.020736 Accuracy 0.2396 | validation: Loss 0.047883 Accuracy -2.0081\n",
      "Epoch 651 | train: Loss 0.020736 Accuracy 0.2395 | validation: Loss 0.048034 Accuracy -2.0177\n",
      "Epoch 652 | train: Loss 0.020740 Accuracy 0.2394 | validation: Loss 0.047942 Accuracy -2.0119\n",
      "Epoch 653 | train: Loss 0.020741 Accuracy 0.2394 | validation: Loss 0.047951 Accuracy -2.0124\n",
      "Epoch 654 | train: Loss 0.020743 Accuracy 0.2393 | validation: Loss 0.048036 Accuracy -2.0178\n",
      "Epoch 655 | train: Loss 0.020746 Accuracy 0.2392 | validation: Loss 0.047921 Accuracy -2.0105\n",
      "Epoch 656 | train: Loss 0.020747 Accuracy 0.2392 | validation: Loss 0.048026 Accuracy -2.0171\n",
      "Epoch 657 | train: Loss 0.020749 Accuracy 0.2390 | validation: Loss 0.047993 Accuracy -2.0151\n",
      "Epoch 658 | train: Loss 0.020751 Accuracy 0.2390 | validation: Loss 0.047962 Accuracy -2.0131\n",
      "Epoch 659 | train: Loss 0.020752 Accuracy 0.2389 | validation: Loss 0.048053 Accuracy -2.0188\n",
      "Epoch 660 | train: Loss 0.020755 Accuracy 0.2388 | validation: Loss 0.047962 Accuracy -2.0131\n",
      "Epoch 661 | train: Loss 0.020756 Accuracy 0.2388 | validation: Loss 0.048030 Accuracy -2.0174\n",
      "Epoch 662 | train: Loss 0.020758 Accuracy 0.2387 | validation: Loss 0.048026 Accuracy -2.0172\n",
      "Epoch 663 | train: Loss 0.020760 Accuracy 0.2387 | validation: Loss 0.047985 Accuracy -2.0146\n",
      "Epoch 664 | train: Loss 0.020761 Accuracy 0.2386 | validation: Loss 0.048065 Accuracy -2.0196\n",
      "Epoch 665 | train: Loss 0.020764 Accuracy 0.2385 | validation: Loss 0.047996 Accuracy -2.0153\n",
      "Epoch 666 | train: Loss 0.020765 Accuracy 0.2385 | validation: Loss 0.048043 Accuracy -2.0182\n",
      "Epoch 667 | train: Loss 0.020767 Accuracy 0.2384 | validation: Loss 0.048049 Accuracy -2.0186\n",
      "Epoch 668 | train: Loss 0.020769 Accuracy 0.2383 | validation: Loss 0.048014 Accuracy -2.0164\n",
      "Epoch 669 | train: Loss 0.020770 Accuracy 0.2383 | validation: Loss 0.048079 Accuracy -2.0205\n",
      "Epoch 670 | train: Loss 0.020773 Accuracy 0.2382 | validation: Loss 0.048025 Accuracy -2.0171\n",
      "Epoch 671 | train: Loss 0.020774 Accuracy 0.2382 | validation: Loss 0.048067 Accuracy -2.0197\n",
      "Epoch 672 | train: Loss 0.020776 Accuracy 0.2381 | validation: Loss 0.048067 Accuracy -2.0197\n",
      "Epoch 673 | train: Loss 0.020777 Accuracy 0.2380 | validation: Loss 0.048046 Accuracy -2.0184\n",
      "Epoch 674 | train: Loss 0.020779 Accuracy 0.2380 | validation: Loss 0.048096 Accuracy -2.0215\n",
      "Epoch 675 | train: Loss 0.020781 Accuracy 0.2379 | validation: Loss 0.048052 Accuracy -2.0188\n",
      "Epoch 676 | train: Loss 0.020782 Accuracy 0.2379 | validation: Loss 0.048094 Accuracy -2.0214\n",
      "Epoch 677 | train: Loss 0.020784 Accuracy 0.2378 | validation: Loss 0.048083 Accuracy -2.0207\n",
      "Epoch 678 | train: Loss 0.020785 Accuracy 0.2377 | validation: Loss 0.048079 Accuracy -2.0204\n",
      "Epoch 679 | train: Loss 0.020787 Accuracy 0.2377 | validation: Loss 0.048111 Accuracy -2.0225\n",
      "Epoch 680 | train: Loss 0.020789 Accuracy 0.2376 | validation: Loss 0.048078 Accuracy -2.0204\n",
      "Epoch 681 | train: Loss 0.020790 Accuracy 0.2376 | validation: Loss 0.048118 Accuracy -2.0229\n",
      "Epoch 682 | train: Loss 0.020792 Accuracy 0.2375 | validation: Loss 0.048096 Accuracy -2.0215\n",
      "Epoch 683 | train: Loss 0.020793 Accuracy 0.2374 | validation: Loss 0.048110 Accuracy -2.0224\n",
      "Epoch 684 | train: Loss 0.020795 Accuracy 0.2374 | validation: Loss 0.048121 Accuracy -2.0231\n",
      "Epoch 685 | train: Loss 0.020797 Accuracy 0.2373 | validation: Loss 0.048104 Accuracy -2.0221\n",
      "Epoch 686 | train: Loss 0.020798 Accuracy 0.2373 | validation: Loss 0.048137 Accuracy -2.0241\n",
      "Epoch 687 | train: Loss 0.020800 Accuracy 0.2372 | validation: Loss 0.048111 Accuracy -2.0225\n",
      "Epoch 688 | train: Loss 0.020801 Accuracy 0.2372 | validation: Loss 0.048140 Accuracy -2.0243\n",
      "Epoch 689 | train: Loss 0.020803 Accuracy 0.2371 | validation: Loss 0.048129 Accuracy -2.0236\n",
      "Epoch 690 | train: Loss 0.020804 Accuracy 0.2370 | validation: Loss 0.048136 Accuracy -2.0241\n",
      "Epoch 691 | train: Loss 0.020806 Accuracy 0.2370 | validation: Loss 0.048147 Accuracy -2.0248\n",
      "Epoch 692 | train: Loss 0.020807 Accuracy 0.2369 | validation: Loss 0.048136 Accuracy -2.0240\n",
      "Epoch 693 | train: Loss 0.020809 Accuracy 0.2369 | validation: Loss 0.048161 Accuracy -2.0256\n",
      "Epoch 694 | train: Loss 0.020811 Accuracy 0.2368 | validation: Loss 0.048142 Accuracy -2.0244\n",
      "Epoch 695 | train: Loss 0.020812 Accuracy 0.2368 | validation: Loss 0.048167 Accuracy -2.0260\n",
      "Epoch 696 | train: Loss 0.020814 Accuracy 0.2367 | validation: Loss 0.048155 Accuracy -2.0253\n",
      "Epoch 697 | train: Loss 0.020815 Accuracy 0.2366 | validation: Loss 0.048169 Accuracy -2.0261\n",
      "Epoch 698 | train: Loss 0.020817 Accuracy 0.2366 | validation: Loss 0.048170 Accuracy -2.0262\n",
      "Epoch 699 | train: Loss 0.020818 Accuracy 0.2365 | validation: Loss 0.048171 Accuracy -2.0263\n",
      "Epoch 700 | train: Loss 0.020820 Accuracy 0.2365 | validation: Loss 0.048184 Accuracy -2.0271\n",
      "Epoch 701 | train: Loss 0.020821 Accuracy 0.2364 | validation: Loss 0.048175 Accuracy -2.0265\n",
      "Epoch 702 | train: Loss 0.020823 Accuracy 0.2364 | validation: Loss 0.048195 Accuracy -2.0278\n",
      "Epoch 703 | train: Loss 0.020824 Accuracy 0.2363 | validation: Loss 0.048182 Accuracy -2.0270\n",
      "Epoch 704 | train: Loss 0.020826 Accuracy 0.2363 | validation: Loss 0.048203 Accuracy -2.0283\n",
      "Epoch 705 | train: Loss 0.020827 Accuracy 0.2362 | validation: Loss 0.048191 Accuracy -2.0275\n",
      "Epoch 706 | train: Loss 0.020829 Accuracy 0.2361 | validation: Loss 0.048209 Accuracy -2.0286\n",
      "Epoch 707 | train: Loss 0.020830 Accuracy 0.2361 | validation: Loss 0.048201 Accuracy -2.0282\n",
      "Epoch 708 | train: Loss 0.020832 Accuracy 0.2360 | validation: Loss 0.048215 Accuracy -2.0290\n",
      "Epoch 709 | train: Loss 0.020833 Accuracy 0.2360 | validation: Loss 0.048211 Accuracy -2.0288\n",
      "Epoch 710 | train: Loss 0.020835 Accuracy 0.2359 | validation: Loss 0.048220 Accuracy -2.0293\n",
      "Epoch 711 | train: Loss 0.020836 Accuracy 0.2359 | validation: Loss 0.048221 Accuracy -2.0294\n",
      "Epoch 712 | train: Loss 0.020838 Accuracy 0.2358 | validation: Loss 0.048225 Accuracy -2.0297\n",
      "Epoch 713 | train: Loss 0.020839 Accuracy 0.2358 | validation: Loss 0.048231 Accuracy -2.0300\n",
      "Epoch 714 | train: Loss 0.020841 Accuracy 0.2357 | validation: Loss 0.048231 Accuracy -2.0300\n",
      "Epoch 715 | train: Loss 0.020842 Accuracy 0.2356 | validation: Loss 0.048240 Accuracy -2.0306\n",
      "Epoch 716 | train: Loss 0.020844 Accuracy 0.2356 | validation: Loss 0.048237 Accuracy -2.0304\n",
      "Epoch 717 | train: Loss 0.020845 Accuracy 0.2355 | validation: Loss 0.048249 Accuracy -2.0312\n",
      "Epoch 718 | train: Loss 0.020847 Accuracy 0.2355 | validation: Loss 0.048242 Accuracy -2.0307\n",
      "Epoch 719 | train: Loss 0.020848 Accuracy 0.2354 | validation: Loss 0.048260 Accuracy -2.0318\n",
      "Epoch 720 | train: Loss 0.020850 Accuracy 0.2354 | validation: Loss 0.048247 Accuracy -2.0310\n",
      "Epoch 721 | train: Loss 0.020851 Accuracy 0.2353 | validation: Loss 0.048272 Accuracy -2.0326\n",
      "Epoch 722 | train: Loss 0.020853 Accuracy 0.2353 | validation: Loss 0.048248 Accuracy -2.0311\n",
      "Epoch 723 | train: Loss 0.020854 Accuracy 0.2352 | validation: Loss 0.048287 Accuracy -2.0335\n",
      "Epoch 724 | train: Loss 0.020856 Accuracy 0.2352 | validation: Loss 0.048244 Accuracy -2.0309\n",
      "Epoch 725 | train: Loss 0.020857 Accuracy 0.2351 | validation: Loss 0.048310 Accuracy -2.0350\n",
      "Epoch 726 | train: Loss 0.020859 Accuracy 0.2350 | validation: Loss 0.048231 Accuracy -2.0300\n",
      "Epoch 727 | train: Loss 0.020859 Accuracy 0.2350 | validation: Loss 0.048348 Accuracy -2.0373\n",
      "Epoch 728 | train: Loss 0.020862 Accuracy 0.2349 | validation: Loss 0.048195 Accuracy -2.0277\n",
      "Epoch 729 | train: Loss 0.020862 Accuracy 0.2349 | validation: Loss 0.048418 Accuracy -2.0418\n",
      "Epoch 730 | train: Loss 0.020865 Accuracy 0.2348 | validation: Loss 0.048110 Accuracy -2.0224\n",
      "Epoch 731 | train: Loss 0.020864 Accuracy 0.2348 | validation: Loss 0.048562 Accuracy -2.0508\n",
      "Epoch 732 | train: Loss 0.020869 Accuracy 0.2346 | validation: Loss 0.047916 Accuracy -2.0102\n",
      "Epoch 733 | train: Loss 0.020866 Accuracy 0.2348 | validation: Loss 0.048876 Accuracy -2.0705\n",
      "Epoch 734 | train: Loss 0.020876 Accuracy 0.2344 | validation: Loss 0.047466 Accuracy -1.9819\n",
      "Epoch 735 | train: Loss 0.020869 Accuracy 0.2347 | validation: Loss 0.049604 Accuracy -2.1163\n",
      "Epoch 736 | train: Loss 0.020893 Accuracy 0.2338 | validation: Loss 0.046403 Accuracy -1.9152\n",
      "Epoch 737 | train: Loss 0.020882 Accuracy 0.2342 | validation: Loss 0.051383 Accuracy -2.2280\n",
      "Epoch 738 | train: Loss 0.020956 Accuracy 0.2315 | validation: Loss 0.043887 Accuracy -1.7571\n",
      "Epoch 739 | train: Loss 0.020975 Accuracy 0.2308 | validation: Loss 0.055963 Accuracy -2.5158\n",
      "Epoch 740 | train: Loss 0.021268 Accuracy 0.2200 | validation: Loss 0.038250 Accuracy -1.4030\n",
      "Epoch 741 | train: Loss 0.021564 Accuracy 0.2092 | validation: Loss 0.067935 Accuracy -3.2679\n",
      "Epoch 742 | train: Loss 0.022918 Accuracy 0.1595 | validation: Loss 0.028813 Accuracy -0.8101\n",
      "Epoch 743 | train: Loss 0.024363 Accuracy 0.1065 | validation: Loss 0.089940 Accuracy -4.6503\n",
      "Epoch 744 | train: Loss 0.028164 Accuracy -0.0329 | validation: Loss 0.024154 Accuracy -0.5175\n",
      "Epoch 745 | train: Loss 0.027284 Accuracy -0.0006 | validation: Loss 0.079146 Accuracy -3.9722\n",
      "Epoch 746 | train: Loss 0.025701 Accuracy 0.0575 | validation: Loss 0.040976 Accuracy -1.5742\n",
      "Epoch 747 | train: Loss 0.021402 Accuracy 0.2151 | validation: Loss 0.034770 Accuracy -1.1844\n",
      "Epoch 748 | train: Loss 0.022405 Accuracy 0.1783 | validation: Loss 0.075060 Accuracy -3.7155\n",
      "Epoch 749 | train: Loss 0.025224 Accuracy 0.0750 | validation: Loss 0.036538 Accuracy -1.2954\n",
      "Epoch 750 | train: Loss 0.022213 Accuracy 0.1854 | validation: Loss 0.039515 Accuracy -1.4824\n",
      "Epoch 751 | train: Loss 0.021894 Accuracy 0.1971 | validation: Loss 0.068282 Accuracy -3.2897\n",
      "Epoch 752 | train: Loss 0.024039 Accuracy 0.1184 | validation: Loss 0.039597 Accuracy -1.4876\n",
      "Epoch 753 | train: Loss 0.022099 Accuracy 0.1896 | validation: Loss 0.039570 Accuracy -1.4859\n",
      "Epoch 754 | train: Loss 0.022212 Accuracy 0.1854 | validation: Loss 0.065131 Accuracy -3.0917\n",
      "Epoch 755 | train: Loss 0.023533 Accuracy 0.1370 | validation: Loss 0.044893 Accuracy -1.8203\n",
      "Epoch 756 | train: Loss 0.021994 Accuracy 0.1934 | validation: Loss 0.038485 Accuracy -1.4178\n",
      "Epoch 757 | train: Loss 0.022665 Accuracy 0.1688 | validation: Loss 0.062729 Accuracy -2.9408\n",
      "Epoch 758 | train: Loss 0.023141 Accuracy 0.1513 | validation: Loss 0.050696 Accuracy -2.1849\n",
      "Epoch 759 | train: Loss 0.022110 Accuracy 0.1892 | validation: Loss 0.038072 Accuracy -1.3918\n",
      "Epoch 760 | train: Loss 0.022949 Accuracy 0.1584 | validation: Loss 0.059405 Accuracy -2.7320\n",
      "Epoch 761 | train: Loss 0.022705 Accuracy 0.1673 | validation: Loss 0.054804 Accuracy -2.4430\n",
      "Epoch 762 | train: Loss 0.022315 Accuracy 0.1816 | validation: Loss 0.039205 Accuracy -1.4629\n",
      "Epoch 763 | train: Loss 0.022849 Accuracy 0.1620 | validation: Loss 0.056273 Accuracy -2.5353\n",
      "Epoch 764 | train: Loss 0.022391 Accuracy 0.1788 | validation: Loss 0.056205 Accuracy -2.5309\n",
      "Epoch 765 | train: Loss 0.022371 Accuracy 0.1796 | validation: Loss 0.040862 Accuracy -1.5670\n",
      "Epoch 766 | train: Loss 0.022570 Accuracy 0.1723 | validation: Loss 0.053083 Accuracy -2.3348\n",
      "Epoch 767 | train: Loss 0.022158 Accuracy 0.1874 | validation: Loss 0.056471 Accuracy -2.5477\n",
      "Epoch 768 | train: Loss 0.022350 Accuracy 0.1804 | validation: Loss 0.042376 Accuracy -1.6622\n",
      "Epoch 769 | train: Loss 0.022306 Accuracy 0.1820 | validation: Loss 0.050648 Accuracy -2.1819\n",
      "Epoch 770 | train: Loss 0.022004 Accuracy 0.1930 | validation: Loss 0.055791 Accuracy -2.5049\n",
      "Epoch 771 | train: Loss 0.022236 Accuracy 0.1845 | validation: Loss 0.043900 Accuracy -1.7579\n",
      "Epoch 772 | train: Loss 0.022066 Accuracy 0.1908 | validation: Loss 0.048989 Accuracy -2.0776\n",
      "Epoch 773 | train: Loss 0.021878 Accuracy 0.1977 | validation: Loss 0.054674 Accuracy -2.4348\n",
      "Epoch 774 | train: Loss 0.022052 Accuracy 0.1913 | validation: Loss 0.044432 Accuracy -1.7913\n",
      "Epoch 775 | train: Loss 0.021911 Accuracy 0.1964 | validation: Loss 0.050462 Accuracy -2.1702\n",
      "Epoch 776 | train: Loss 0.021781 Accuracy 0.2012 | validation: Loss 0.052779 Accuracy -2.3157\n",
      "Epoch 777 | train: Loss 0.021822 Accuracy 0.1997 | validation: Loss 0.044703 Accuracy -1.8084\n",
      "Epoch 778 | train: Loss 0.021749 Accuracy 0.2024 | validation: Loss 0.050422 Accuracy -2.1676\n",
      "Epoch 779 | train: Loss 0.021660 Accuracy 0.2057 | validation: Loss 0.051661 Accuracy -2.2455\n",
      "Epoch 780 | train: Loss 0.021663 Accuracy 0.2055 | validation: Loss 0.045267 Accuracy -1.8438\n",
      "Epoch 781 | train: Loss 0.021608 Accuracy 0.2076 | validation: Loss 0.050272 Accuracy -2.1583\n",
      "Epoch 782 | train: Loss 0.021544 Accuracy 0.2099 | validation: Loss 0.050832 Accuracy -2.1934\n",
      "Epoch 783 | train: Loss 0.021524 Accuracy 0.2106 | validation: Loss 0.045532 Accuracy -1.8604\n",
      "Epoch 784 | train: Loss 0.021480 Accuracy 0.2123 | validation: Loss 0.050512 Accuracy -2.1733\n",
      "Epoch 785 | train: Loss 0.021440 Accuracy 0.2137 | validation: Loss 0.049839 Accuracy -2.1310\n",
      "Epoch 786 | train: Loss 0.021385 Accuracy 0.2157 | validation: Loss 0.045928 Accuracy -1.8853\n",
      "Epoch 787 | train: Loss 0.021349 Accuracy 0.2171 | validation: Loss 0.050934 Accuracy -2.1999\n",
      "Epoch 788 | train: Loss 0.021342 Accuracy 0.2173 | validation: Loss 0.048078 Accuracy -2.0204\n",
      "Epoch 789 | train: Loss 0.021250 Accuracy 0.2207 | validation: Loss 0.047041 Accuracy -1.9553\n",
      "Epoch 790 | train: Loss 0.021221 Accuracy 0.2217 | validation: Loss 0.051034 Accuracy -2.2061\n",
      "Epoch 791 | train: Loss 0.021246 Accuracy 0.2209 | validation: Loss 0.047134 Accuracy -1.9611\n",
      "Epoch 792 | train: Loss 0.021167 Accuracy 0.2237 | validation: Loss 0.048527 Accuracy -2.0486\n",
      "Epoch 793 | train: Loss 0.021140 Accuracy 0.2247 | validation: Loss 0.050202 Accuracy -2.1539\n",
      "Epoch 794 | train: Loss 0.021143 Accuracy 0.2246 | validation: Loss 0.046877 Accuracy -1.9450\n",
      "Epoch 795 | train: Loss 0.021098 Accuracy 0.2263 | validation: Loss 0.049593 Accuracy -2.1156\n",
      "Epoch 796 | train: Loss 0.021085 Accuracy 0.2268 | validation: Loss 0.049051 Accuracy -2.0815\n",
      "Epoch 797 | train: Loss 0.021054 Accuracy 0.2279 | validation: Loss 0.047237 Accuracy -1.9676\n",
      "Epoch 798 | train: Loss 0.021029 Accuracy 0.2288 | validation: Loss 0.050102 Accuracy -2.1475\n",
      "Epoch 799 | train: Loss 0.021035 Accuracy 0.2286 | validation: Loss 0.047793 Accuracy -2.0025\n",
      "Epoch 800 | train: Loss 0.020989 Accuracy 0.2303 | validation: Loss 0.048307 Accuracy -2.0348\n",
      "Epoch 801 | train: Loss 0.020972 Accuracy 0.2309 | validation: Loss 0.049625 Accuracy -2.1176\n",
      "Epoch 802 | train: Loss 0.020973 Accuracy 0.2309 | validation: Loss 0.047187 Accuracy -1.9644\n",
      "Epoch 803 | train: Loss 0.020940 Accuracy 0.2321 | validation: Loss 0.049354 Accuracy -2.1005\n",
      "Epoch 804 | train: Loss 0.020939 Accuracy 0.2321 | validation: Loss 0.048294 Accuracy -2.0340\n",
      "Epoch 805 | train: Loss 0.020913 Accuracy 0.2331 | validation: Loss 0.047740 Accuracy -1.9992\n",
      "Epoch 806 | train: Loss 0.020900 Accuracy 0.2335 | validation: Loss 0.049378 Accuracy -2.1021\n",
      "Epoch 807 | train: Loss 0.020909 Accuracy 0.2332 | validation: Loss 0.047310 Accuracy -1.9722\n",
      "Epoch 808 | train: Loss 0.020883 Accuracy 0.2341 | validation: Loss 0.048797 Accuracy -2.0655\n",
      "Epoch 809 | train: Loss 0.020885 Accuracy 0.2341 | validation: Loss 0.048406 Accuracy -2.0410\n",
      "Epoch 810 | train: Loss 0.020874 Accuracy 0.2345 | validation: Loss 0.047600 Accuracy -1.9903\n",
      "Epoch 811 | train: Loss 0.020866 Accuracy 0.2348 | validation: Loss 0.049149 Accuracy -2.0877\n",
      "Epoch 812 | train: Loss 0.020877 Accuracy 0.2344 | validation: Loss 0.047573 Accuracy -1.9887\n",
      "Epoch 813 | train: Loss 0.020859 Accuracy 0.2350 | validation: Loss 0.048570 Accuracy -2.0513\n",
      "Epoch 814 | train: Loss 0.020862 Accuracy 0.2349 | validation: Loss 0.048523 Accuracy -2.0484\n",
      "Epoch 815 | train: Loss 0.020860 Accuracy 0.2350 | validation: Loss 0.047754 Accuracy -2.0000\n",
      "Epoch 816 | train: Loss 0.020854 Accuracy 0.2352 | validation: Loss 0.049022 Accuracy -2.0797\n",
      "Epoch 817 | train: Loss 0.020864 Accuracy 0.2349 | validation: Loss 0.047791 Accuracy -2.0023\n",
      "Epoch 818 | train: Loss 0.020853 Accuracy 0.2353 | validation: Loss 0.048606 Accuracy -2.0536\n",
      "Epoch 819 | train: Loss 0.020858 Accuracy 0.2351 | validation: Loss 0.048491 Accuracy -2.0464\n",
      "Epoch 820 | train: Loss 0.020858 Accuracy 0.2351 | validation: Loss 0.048019 Accuracy -2.0167\n",
      "Epoch 821 | train: Loss 0.020857 Accuracy 0.2351 | validation: Loss 0.048906 Accuracy -2.0724\n",
      "Epoch 822 | train: Loss 0.020867 Accuracy 0.2348 | validation: Loss 0.047960 Accuracy -2.0130\n",
      "Epoch 823 | train: Loss 0.020861 Accuracy 0.2350 | validation: Loss 0.048667 Accuracy -2.0574\n",
      "Epoch 824 | train: Loss 0.020868 Accuracy 0.2347 | validation: Loss 0.048414 Accuracy -2.0415\n",
      "Epoch 825 | train: Loss 0.020868 Accuracy 0.2347 | validation: Loss 0.048183 Accuracy -2.0270\n",
      "Epoch 826 | train: Loss 0.020869 Accuracy 0.2347 | validation: Loss 0.048784 Accuracy -2.0648\n",
      "Epoch 827 | train: Loss 0.020877 Accuracy 0.2344 | validation: Loss 0.048036 Accuracy -2.0177\n",
      "Epoch 828 | train: Loss 0.020874 Accuracy 0.2345 | validation: Loss 0.048688 Accuracy -2.0587\n",
      "Epoch 829 | train: Loss 0.020882 Accuracy 0.2342 | validation: Loss 0.048322 Accuracy -2.0358\n",
      "Epoch 830 | train: Loss 0.020882 Accuracy 0.2342 | validation: Loss 0.048333 Accuracy -2.0364\n",
      "Epoch 831 | train: Loss 0.020885 Accuracy 0.2341 | validation: Loss 0.048656 Accuracy -2.0567\n",
      "Epoch 832 | train: Loss 0.020891 Accuracy 0.2338 | validation: Loss 0.048128 Accuracy -2.0235\n",
      "Epoch 833 | train: Loss 0.020891 Accuracy 0.2339 | validation: Loss 0.048707 Accuracy -2.0599\n",
      "Epoch 834 | train: Loss 0.020899 Accuracy 0.2336 | validation: Loss 0.048230 Accuracy -2.0299\n",
      "Epoch 835 | train: Loss 0.020899 Accuracy 0.2336 | validation: Loss 0.048495 Accuracy -2.0466\n",
      "Epoch 836 | train: Loss 0.020904 Accuracy 0.2334 | validation: Loss 0.048480 Accuracy -2.0457\n",
      "Epoch 837 | train: Loss 0.020908 Accuracy 0.2332 | validation: Loss 0.048268 Accuracy -2.0324\n",
      "Epoch 838 | train: Loss 0.020910 Accuracy 0.2332 | validation: Loss 0.048648 Accuracy -2.0562\n",
      "Epoch 839 | train: Loss 0.020916 Accuracy 0.2329 | validation: Loss 0.048218 Accuracy -2.0292\n",
      "Epoch 840 | train: Loss 0.020916 Accuracy 0.2329 | validation: Loss 0.048626 Accuracy -2.0549\n",
      "Epoch 841 | train: Loss 0.020922 Accuracy 0.2327 | validation: Loss 0.048350 Accuracy -2.0375\n",
      "Epoch 842 | train: Loss 0.020923 Accuracy 0.2327 | validation: Loss 0.048482 Accuracy -2.0458\n",
      "Epoch 843 | train: Loss 0.020928 Accuracy 0.2325 | validation: Loss 0.048540 Accuracy -2.0494\n",
      "Epoch 844 | train: Loss 0.020931 Accuracy 0.2324 | validation: Loss 0.048352 Accuracy -2.0376\n",
      "Epoch 845 | train: Loss 0.020932 Accuracy 0.2323 | validation: Loss 0.048652 Accuracy -2.0564\n",
      "Epoch 846 | train: Loss 0.020938 Accuracy 0.2321 | validation: Loss 0.048324 Accuracy -2.0359\n",
      "Epoch 847 | train: Loss 0.020938 Accuracy 0.2321 | validation: Loss 0.048645 Accuracy -2.0560\n",
      "Epoch 848 | train: Loss 0.020943 Accuracy 0.2320 | validation: Loss 0.048398 Accuracy -2.0405\n",
      "Epoch 849 | train: Loss 0.020944 Accuracy 0.2319 | validation: Loss 0.048558 Accuracy -2.0506\n",
      "Epoch 850 | train: Loss 0.020947 Accuracy 0.2318 | validation: Loss 0.048515 Accuracy -2.0478\n",
      "Epoch 851 | train: Loss 0.020949 Accuracy 0.2317 | validation: Loss 0.048463 Accuracy -2.0446\n",
      "Epoch 852 | train: Loss 0.020951 Accuracy 0.2316 | validation: Loss 0.048614 Accuracy -2.0540\n",
      "Epoch 853 | train: Loss 0.020955 Accuracy 0.2315 | validation: Loss 0.048415 Accuracy -2.0415\n",
      "Epoch 854 | train: Loss 0.020955 Accuracy 0.2315 | validation: Loss 0.048658 Accuracy -2.0569\n",
      "Epoch 855 | train: Loss 0.020959 Accuracy 0.2314 | validation: Loss 0.048424 Accuracy -2.0421\n",
      "Epoch 856 | train: Loss 0.020960 Accuracy 0.2313 | validation: Loss 0.048646 Accuracy -2.0561\n",
      "Epoch 857 | train: Loss 0.020963 Accuracy 0.2312 | validation: Loss 0.048472 Accuracy -2.0452\n",
      "Epoch 858 | train: Loss 0.020964 Accuracy 0.2312 | validation: Loss 0.048598 Accuracy -2.0531\n",
      "Epoch 859 | train: Loss 0.020967 Accuracy 0.2311 | validation: Loss 0.048533 Accuracy -2.0490\n",
      "Epoch 860 | train: Loss 0.020968 Accuracy 0.2310 | validation: Loss 0.048541 Accuracy -2.0495\n",
      "Epoch 861 | train: Loss 0.020970 Accuracy 0.2310 | validation: Loss 0.048585 Accuracy -2.0523\n",
      "Epoch 862 | train: Loss 0.020972 Accuracy 0.2309 | validation: Loss 0.048495 Accuracy -2.0466\n",
      "Epoch 863 | train: Loss 0.020972 Accuracy 0.2309 | validation: Loss 0.048620 Accuracy -2.0544\n",
      "Epoch 864 | train: Loss 0.020975 Accuracy 0.2308 | validation: Loss 0.048470 Accuracy -2.0451\n",
      "Epoch 865 | train: Loss 0.020975 Accuracy 0.2308 | validation: Loss 0.048638 Accuracy -2.0556\n",
      "Epoch 866 | train: Loss 0.020977 Accuracy 0.2307 | validation: Loss 0.048468 Accuracy -2.0449\n",
      "Epoch 867 | train: Loss 0.020977 Accuracy 0.2307 | validation: Loss 0.048645 Accuracy -2.0560\n",
      "Epoch 868 | train: Loss 0.020980 Accuracy 0.2306 | validation: Loss 0.048481 Accuracy -2.0458\n",
      "Epoch 869 | train: Loss 0.020980 Accuracy 0.2306 | validation: Loss 0.048646 Accuracy -2.0561\n",
      "Epoch 870 | train: Loss 0.020982 Accuracy 0.2305 | validation: Loss 0.048501 Accuracy -2.0470\n",
      "Epoch 871 | train: Loss 0.020982 Accuracy 0.2305 | validation: Loss 0.048645 Accuracy -2.0560\n",
      "Epoch 872 | train: Loss 0.020985 Accuracy 0.2304 | validation: Loss 0.048521 Accuracy -2.0482\n",
      "Epoch 873 | train: Loss 0.020985 Accuracy 0.2304 | validation: Loss 0.048643 Accuracy -2.0559\n",
      "Epoch 874 | train: Loss 0.020987 Accuracy 0.2303 | validation: Loss 0.048534 Accuracy -2.0491\n",
      "Epoch 875 | train: Loss 0.020987 Accuracy 0.2303 | validation: Loss 0.048644 Accuracy -2.0559\n",
      "Epoch 876 | train: Loss 0.020989 Accuracy 0.2303 | validation: Loss 0.048541 Accuracy -2.0495\n",
      "Epoch 877 | train: Loss 0.020989 Accuracy 0.2302 | validation: Loss 0.048649 Accuracy -2.0563\n",
      "Epoch 878 | train: Loss 0.020991 Accuracy 0.2302 | validation: Loss 0.048542 Accuracy -2.0495\n",
      "Epoch 879 | train: Loss 0.020991 Accuracy 0.2302 | validation: Loss 0.048662 Accuracy -2.0571\n",
      "Epoch 880 | train: Loss 0.020993 Accuracy 0.2301 | validation: Loss 0.048534 Accuracy -2.0490\n",
      "Epoch 881 | train: Loss 0.020993 Accuracy 0.2301 | validation: Loss 0.048684 Accuracy -2.0585\n",
      "Epoch 882 | train: Loss 0.020995 Accuracy 0.2300 | validation: Loss 0.048514 Accuracy -2.0478\n",
      "Epoch 883 | train: Loss 0.020994 Accuracy 0.2301 | validation: Loss 0.048723 Accuracy -2.0609\n",
      "Epoch 884 | train: Loss 0.020997 Accuracy 0.2300 | validation: Loss 0.048474 Accuracy -2.0453\n",
      "Epoch 885 | train: Loss 0.020996 Accuracy 0.2300 | validation: Loss 0.048786 Accuracy -2.0649\n",
      "Epoch 886 | train: Loss 0.020999 Accuracy 0.2299 | validation: Loss 0.048400 Accuracy -2.0406\n",
      "Epoch 887 | train: Loss 0.020998 Accuracy 0.2299 | validation: Loss 0.048895 Accuracy -2.0717\n",
      "Epoch 888 | train: Loss 0.021003 Accuracy 0.2298 | validation: Loss 0.048263 Accuracy -2.0321\n",
      "Epoch 889 | train: Loss 0.020999 Accuracy 0.2299 | validation: Loss 0.049091 Accuracy -2.0840\n",
      "Epoch 890 | train: Loss 0.021007 Accuracy 0.2296 | validation: Loss 0.048010 Accuracy -2.0161\n",
      "Epoch 891 | train: Loss 0.021000 Accuracy 0.2298 | validation: Loss 0.049455 Accuracy -2.1069\n",
      "Epoch 892 | train: Loss 0.021014 Accuracy 0.2293 | validation: Loss 0.047530 Accuracy -1.9860\n",
      "Epoch 893 | train: Loss 0.021004 Accuracy 0.2297 | validation: Loss 0.050154 Accuracy -2.1508\n",
      "Epoch 894 | train: Loss 0.021031 Accuracy 0.2287 | validation: Loss 0.046615 Accuracy -1.9285\n",
      "Epoch 895 | train: Loss 0.021017 Accuracy 0.2292 | validation: Loss 0.051524 Accuracy -2.2369\n",
      "Epoch 896 | train: Loss 0.021079 Accuracy 0.2270 | validation: Loss 0.044890 Accuracy -1.8201\n",
      "Epoch 897 | train: Loss 0.021072 Accuracy 0.2272 | validation: Loss 0.054199 Accuracy -2.4050\n",
      "Epoch 898 | train: Loss 0.021228 Accuracy 0.2215 | validation: Loss 0.041864 Accuracy -1.6300\n",
      "Epoch 899 | train: Loss 0.021275 Accuracy 0.2198 | validation: Loss 0.059244 Accuracy -2.7219\n",
      "Epoch 900 | train: Loss 0.021693 Accuracy 0.2044 | validation: Loss 0.036941 Accuracy -1.3208\n",
      "Epoch 901 | train: Loss 0.021963 Accuracy 0.1946 | validation: Loss 0.068421 Accuracy -3.2984\n",
      "Epoch 902 | train: Loss 0.023046 Accuracy 0.1548 | validation: Loss 0.031224 Accuracy -0.9616\n",
      "Epoch 903 | train: Loss 0.023527 Accuracy 0.1372 | validation: Loss 0.078229 Accuracy -3.9146\n",
      "Epoch 904 | train: Loss 0.025120 Accuracy 0.0788 | validation: Loss 0.029101 Accuracy -0.8282\n",
      "Epoch 905 | train: Loss 0.024411 Accuracy 0.1048 | validation: Loss 0.072535 Accuracy -3.5569\n",
      "Epoch 906 | train: Loss 0.024112 Accuracy 0.1157 | validation: Loss 0.037618 Accuracy -1.3633\n",
      "Epoch 907 | train: Loss 0.021971 Accuracy 0.1943 | validation: Loss 0.048004 Accuracy -2.0158\n",
      "Epoch 908 | train: Loss 0.021329 Accuracy 0.2178 | validation: Loss 0.057866 Accuracy -2.6353\n",
      "Epoch 909 | train: Loss 0.022034 Accuracy 0.1919 | validation: Loss 0.032825 Accuracy -1.0622\n",
      "Epoch 910 | train: Loss 0.023343 Accuracy 0.1439 | validation: Loss 0.078320 Accuracy -3.9203\n",
      "Epoch 911 | train: Loss 0.025752 Accuracy 0.0556 | validation: Loss 0.032292 Accuracy -1.0287\n",
      "Epoch 912 | train: Loss 0.023615 Accuracy 0.1340 | validation: Loss 0.052343 Accuracy -2.2884\n",
      "Epoch 913 | train: Loss 0.021821 Accuracy 0.1998 | validation: Loss 0.060574 Accuracy -2.8054\n",
      "Epoch 914 | train: Loss 0.022642 Accuracy 0.1696 | validation: Loss 0.033675 Accuracy -1.1155\n",
      "Epoch 915 | train: Loss 0.023496 Accuracy 0.1383 | validation: Loss 0.062529 Accuracy -2.9282\n",
      "Epoch 916 | train: Loss 0.022966 Accuracy 0.1577 | validation: Loss 0.050119 Accuracy -2.1486\n",
      "Epoch 917 | train: Loss 0.021943 Accuracy 0.1953 | validation: Loss 0.041131 Accuracy -1.5840\n",
      "Epoch 918 | train: Loss 0.022307 Accuracy 0.1819 | validation: Loss 0.110686 Accuracy -5.9536\n",
      "Epoch 919 | train: Loss 0.037045 Accuracy -0.3586 | validation: Loss 0.067458 Accuracy -3.2379\n",
      "Epoch 920 | train: Loss 0.146831 Accuracy -4.3848 | validation: Loss 0.346402 Accuracy -20.7620\n",
      "Epoch 921 | train: Loss 0.139663 Accuracy -4.1219 | validation: Loss 0.390062 Accuracy -23.5049\n",
      "Epoch 922 | train: Loss 0.171714 Accuracy -5.2973 | validation: Loss 0.085274 Accuracy -4.3572\n",
      "Epoch 923 | train: Loss 0.031288 Accuracy -0.1474 | validation: Loss 0.097601 Accuracy -5.1316\n",
      "Epoch 924 | train: Loss 0.151866 Accuracy -4.5694 | validation: Loss 0.031730 Accuracy -0.9934\n",
      "Epoch 925 | train: Loss 0.025374 Accuracy 0.0695 | validation: Loss 0.190758 Accuracy -10.9840\n",
      "Epoch 926 | train: Loss 0.085178 Accuracy -2.1238 | validation: Loss 0.204154 Accuracy -11.8255\n",
      "Epoch 927 | train: Loss 0.090797 Accuracy -2.3298 | validation: Loss 0.076612 Accuracy -3.8130\n",
      "Epoch 928 | train: Loss 0.031380 Accuracy -0.1508 | validation: Loss 0.019953 Accuracy -0.2535\n",
      "Epoch 929 | train: Loss 0.053040 Accuracy -0.9451 | validation: Loss 0.019859 Accuracy -0.2476\n",
      "Epoch 930 | train: Loss 0.063213 Accuracy -1.3182 | validation: Loss 0.043146 Accuracy -1.7106\n",
      "Epoch 931 | train: Loss 0.026442 Accuracy 0.0303 | validation: Loss 0.113179 Accuracy -6.1103\n",
      "Epoch 932 | train: Loss 0.037266 Accuracy -0.3667 | validation: Loss 0.125605 Accuracy -6.8908\n",
      "Epoch 933 | train: Loss 0.033945 Accuracy -0.2449 | validation: Loss 1.670352 Accuracy -103.9364\n",
      "Epoch 934 | train: Loss 1.422777 Accuracy -51.1780 | validation: Loss 0.406198 Accuracy -24.5186\n",
      "Epoch 935 | train: Loss 0.202294 Accuracy -6.4188 | validation: Loss 0.325562 Accuracy -19.4527\n",
      "Epoch 936 | train: Loss 0.159615 Accuracy -4.8536 | validation: Loss 0.339596 Accuracy -20.3344\n",
      "Epoch 937 | train: Loss 0.168845 Accuracy -5.1921 | validation: Loss 0.318951 Accuracy -19.0375\n",
      "Epoch 938 | train: Loss 0.155899 Accuracy -4.7173 | validation: Loss 0.272139 Accuracy -16.0966\n",
      "Epoch 939 | train: Loss 0.155282 Accuracy -4.6947 | validation: Loss 0.179533 Accuracy -10.2788\n",
      "Epoch 940 | train: Loss 0.073891 Accuracy -1.7098 | validation: Loss 0.145002 Accuracy -8.1095\n",
      "Epoch 941 | train: Loss 0.200488 Accuracy -6.3526 | validation: Loss 0.141243 Accuracy -7.8733\n",
      "Epoch 942 | train: Loss 0.052804 Accuracy -0.9365 | validation: Loss 0.090518 Accuracy -4.6866\n",
      "Epoch 943 | train: Loss 0.037593 Accuracy -0.3787 | validation: Loss 0.151210 Accuracy -8.4995\n",
      "Epoch 944 | train: Loss 0.030856 Accuracy -0.1316 | validation: Loss 0.054558 Accuracy -2.4275\n",
      "Epoch 945 | train: Loss 0.027652 Accuracy -0.0141 | validation: Loss 0.914843 Accuracy -56.4731\n",
      "Epoch 946 | train: Loss 0.027532 Accuracy -0.0097 | validation: Loss 1.208101 Accuracy -74.8964\n",
      "Epoch 947 | train: Loss 0.029261 Accuracy -0.0731 | validation: Loss 0.783967 Accuracy -48.2511\n",
      "Epoch 948 | train: Loss 0.028419 Accuracy -0.0422 | validation: Loss 0.237866 Accuracy -13.9434\n",
      "Epoch 949 | train: Loss 0.026941 Accuracy 0.0120 | validation: Loss 1.097738 Accuracy -67.9631\n",
      "Epoch 950 | train: Loss 0.023553 Accuracy 0.1362 | validation: Loss 1.752545 Accuracy -109.1000\n",
      "Epoch 951 | train: Loss 0.035314 Accuracy -0.2951 | validation: Loss 5.693780 Accuracy -356.7000\n",
      "Epoch 952 | train: Loss 0.145136 Accuracy -4.3226 | validation: Loss 0.085444 Accuracy -4.3678\n",
      "Epoch 953 | train: Loss 0.039080 Accuracy -0.4332 | validation: Loss 0.062583 Accuracy -2.9317\n",
      "Epoch 954 | train: Loss 0.038292 Accuracy -0.4043 | validation: Loss 0.046091 Accuracy -1.8956\n",
      "Epoch 955 | train: Loss 0.030246 Accuracy -0.1092 | validation: Loss 0.092000 Accuracy -4.7797\n",
      "Epoch 956 | train: Loss 0.040787 Accuracy -0.4958 | validation: Loss 0.035527 Accuracy -1.2319\n",
      "Epoch 957 | train: Loss 0.036225 Accuracy -0.3285 | validation: Loss 0.025691 Accuracy -0.6140\n",
      "Epoch 958 | train: Loss 0.034078 Accuracy -0.2497 | validation: Loss 0.032665 Accuracy -1.0521\n",
      "Epoch 959 | train: Loss 0.031265 Accuracy -0.1466 | validation: Loss 0.039546 Accuracy -1.4844\n",
      "Epoch 960 | train: Loss 0.028785 Accuracy -0.0556 | validation: Loss 0.048523 Accuracy -2.0484\n",
      "Epoch 961 | train: Loss 0.027414 Accuracy -0.0054 | validation: Loss 0.059044 Accuracy -2.7093\n",
      "Epoch 962 | train: Loss 0.027493 Accuracy -0.0083 | validation: Loss 0.070148 Accuracy -3.4069\n",
      "Epoch 963 | train: Loss 0.028885 Accuracy -0.0593 | validation: Loss 0.066311 Accuracy -3.1659\n",
      "Epoch 964 | train: Loss 0.031080 Accuracy -0.1398 | validation: Loss 0.204218 Accuracy -11.8296\n",
      "Epoch 965 | train: Loss 0.022515 Accuracy 0.1743 | validation: Loss 0.029022 Accuracy -0.8233\n",
      "Epoch 966 | train: Loss 0.241580 Accuracy -7.8596 | validation: Loss 16.868473 Accuracy -1058.7272\n",
      "Epoch 967 | train: Loss 6.163744 Accuracy -225.0452 | validation: Loss 1.467986 Accuracy -91.2232\n",
      "Epoch 968 | train: Loss 0.242984 Accuracy -7.9110 | validation: Loss 0.038551 Accuracy -1.4219\n",
      "Epoch 969 | train: Loss 0.029516 Accuracy -0.0825 | validation: Loss 0.019044 Accuracy -0.1964\n",
      "Epoch 970 | train: Loss 0.061110 Accuracy -1.2411 | validation: Loss 0.055721 Accuracy -2.5006\n",
      "Epoch 971 | train: Loss 0.064674 Accuracy -1.3718 | validation: Loss 0.100172 Accuracy -5.2931\n",
      "Epoch 972 | train: Loss 0.085763 Accuracy -2.1452 | validation: Loss 0.155972 Accuracy -8.7986\n",
      "Epoch 973 | train: Loss 0.073680 Accuracy -1.7021 | validation: Loss 0.550231 Accuracy -33.5671\n",
      "Epoch 974 | train: Loss 0.049030 Accuracy -0.7981 | validation: Loss 0.238071 Accuracy -13.9563\n",
      "Epoch 975 | train: Loss 0.038277 Accuracy -0.4038 | validation: Loss 0.140114 Accuracy -7.8023\n",
      "Epoch 976 | train: Loss 0.041504 Accuracy -0.5221 | validation: Loss 0.211028 Accuracy -12.2574\n",
      "Epoch 977 | train: Loss 0.033090 Accuracy -0.2135 | validation: Loss 0.145686 Accuracy -8.1524\n",
      "Epoch 978 | train: Loss 0.029233 Accuracy -0.0721 | validation: Loss 0.461310 Accuracy -27.9809\n",
      "Epoch 979 | train: Loss 0.086383 Accuracy -2.1680 | validation: Loss 0.088779 Accuracy -4.5773\n",
      "Epoch 980 | train: Loss 0.214861 Accuracy -6.8797 | validation: Loss 0.040605 Accuracy -1.5510\n",
      "Epoch 981 | train: Loss 0.147663 Accuracy -4.4153 | validation: Loss 0.018804 Accuracy -0.1813\n",
      "Epoch 982 | train: Loss 0.074167 Accuracy -1.7200 | validation: Loss 0.026003 Accuracy -0.6336\n",
      "Epoch 983 | train: Loss 0.040704 Accuracy -0.4928 | validation: Loss 0.043813 Accuracy -1.7525\n",
      "Epoch 984 | train: Loss 0.027652 Accuracy -0.0141 | validation: Loss 0.052066 Accuracy -2.2709\n",
      "Epoch 985 | train: Loss 0.042475 Accuracy -0.5577 | validation: Loss 0.016976 Accuracy -0.0665\n",
      "Epoch 986 | train: Loss 0.039740 Accuracy -0.4574 | validation: Loss 0.020150 Accuracy -0.2659\n",
      "Epoch 987 | train: Loss 0.040343 Accuracy -0.4795 | validation: Loss 0.026175 Accuracy -0.6444\n",
      "Epoch 988 | train: Loss 0.033539 Accuracy -0.2300 | validation: Loss 0.035298 Accuracy -1.2176\n",
      "Epoch 989 | train: Loss 0.027751 Accuracy -0.0177 | validation: Loss 0.048942 Accuracy -2.0747\n",
      "Epoch 990 | train: Loss 0.026037 Accuracy 0.0451 | validation: Loss 0.070920 Accuracy -3.4554\n",
      "Epoch 991 | train: Loss 0.034495 Accuracy -0.2650 | validation: Loss 0.059736 Accuracy -2.7528\n",
      "Epoch 992 | train: Loss 0.027499 Accuracy -0.0085 | validation: Loss 0.045798 Accuracy -1.8772\n",
      "Epoch 993 | train: Loss 0.027995 Accuracy -0.0267 | validation: Loss 0.037570 Accuracy -1.3602\n",
      "Epoch 994 | train: Loss 0.028080 Accuracy -0.0298 | validation: Loss 0.060355 Accuracy -2.7917\n",
      "Epoch 995 | train: Loss 0.027253 Accuracy 0.0006 | validation: Loss 0.042676 Accuracy -1.6810\n",
      "Epoch 996 | train: Loss 0.025556 Accuracy 0.0628 | validation: Loss 0.053868 Accuracy -2.3841\n",
      "Epoch 997 | train: Loss 0.024399 Accuracy 0.1052 | validation: Loss 0.055594 Accuracy -2.4926\n",
      "Epoch 998 | train: Loss 0.023625 Accuracy 0.1336 | validation: Loss 0.048136 Accuracy -2.0240\n",
      "Epoch 999 | train: Loss 0.023101 Accuracy 0.1528 | validation: Loss 0.070270 Accuracy -3.4146\n",
      "Epoch 1000 | train: Loss 0.023709 Accuracy 0.1305 | validation: Loss 0.040697 Accuracy -1.5567\n",
      "Epoch 1001 | train: Loss 0.023625 Accuracy 0.1336 | validation: Loss 0.077142 Accuracy -3.8463\n",
      "Epoch 1002 | train: Loss 0.024267 Accuracy 0.1100 | validation: Loss 0.022426 Accuracy -0.4088\n",
      "Epoch 1003 | train: Loss 0.034551 Accuracy -0.2671 | validation: Loss 0.267461 Accuracy -15.8027\n",
      "Epoch 1004 | train: Loss 0.134757 Accuracy -3.9420 | validation: Loss 0.219992 Accuracy -12.8205\n",
      "Epoch 1005 | train: Loss 0.093910 Accuracy -2.4440 | validation: Loss 0.088869 Accuracy -4.5830\n",
      "Epoch 1006 | train: Loss 0.034817 Accuracy -0.2769 | validation: Loss 0.198456 Accuracy -11.4676\n",
      "Epoch 1007 | train: Loss 0.266818 Accuracy -8.7851 | validation: Loss 0.197041 Accuracy -11.3787\n",
      "Epoch 1008 | train: Loss 0.058380 Accuracy -1.1410 | validation: Loss 0.233222 Accuracy -13.6517\n",
      "Epoch 1009 | train: Loss 0.102069 Accuracy -2.7432 | validation: Loss 0.225175 Accuracy -13.1462\n",
      "Epoch 1010 | train: Loss 0.097380 Accuracy -2.5712 | validation: Loss 0.209110 Accuracy -12.1369\n",
      "Epoch 1011 | train: Loss 0.087585 Accuracy -2.2120 | validation: Loss 0.185741 Accuracy -10.6688\n",
      "Epoch 1012 | train: Loss 0.074753 Accuracy -1.7415 | validation: Loss 0.158312 Accuracy -8.9456\n",
      "Epoch 1013 | train: Loss 0.061088 Accuracy -1.2403 | validation: Loss 0.129964 Accuracy -7.1647\n",
      "Epoch 1014 | train: Loss 0.048527 Accuracy -0.7796 | validation: Loss 0.103213 Accuracy -5.4841\n",
      "Epoch 1015 | train: Loss 0.038323 Accuracy -0.4054 | validation: Loss 0.079605 Accuracy -4.0010\n",
      "Epoch 1016 | train: Loss 0.031159 Accuracy -0.1427 | validation: Loss 0.059846 Accuracy -2.7597\n",
      "Epoch 1017 | train: Loss 0.027511 Accuracy -0.0089 | validation: Loss 0.044581 Accuracy -1.8007\n",
      "Epoch 1018 | train: Loss 0.027306 Accuracy -0.0014 | validation: Loss 0.034310 Accuracy -1.1555\n",
      "Epoch 1019 | train: Loss 0.029651 Accuracy -0.0874 | validation: Loss 0.028856 Accuracy -0.8128\n",
      "Epoch 1020 | train: Loss 0.033151 Accuracy -0.2157 | validation: Loss 0.027312 Accuracy -0.7158\n",
      "Epoch 1021 | train: Loss 0.034232 Accuracy -0.2554 | validation: Loss 0.028189 Accuracy -0.7709\n",
      "Epoch 1022 | train: Loss 0.033985 Accuracy -0.2464 | validation: Loss 0.028604 Accuracy -0.7970\n",
      "Epoch 1023 | train: Loss 0.033673 Accuracy -0.2349 | validation: Loss 0.029353 Accuracy -0.8440\n",
      "Epoch 1024 | train: Loss 0.033159 Accuracy -0.2160 | validation: Loss 0.031220 Accuracy -0.9613\n",
      "Epoch 1025 | train: Loss 0.032023 Accuracy -0.1744 | validation: Loss 0.034379 Accuracy -1.1598\n",
      "Epoch 1026 | train: Loss 0.030421 Accuracy -0.1156 | validation: Loss 0.039806 Accuracy -1.5007\n",
      "Epoch 1027 | train: Loss 0.028709 Accuracy -0.0529 | validation: Loss 0.045371 Accuracy -1.8503\n",
      "Epoch 1028 | train: Loss 0.027708 Accuracy -0.0162 | validation: Loss 0.053531 Accuracy -2.3630\n",
      "Epoch 1029 | train: Loss 0.027206 Accuracy 0.0023 | validation: Loss 0.063206 Accuracy -2.9708\n",
      "Epoch 1030 | train: Loss 0.027699 Accuracy -0.0158 | validation: Loss 0.067026 Accuracy -3.2108\n",
      "Epoch 1031 | train: Loss 0.028293 Accuracy -0.0376 | validation: Loss 0.062544 Accuracy -2.9292\n",
      "Epoch 1032 | train: Loss 0.027691 Accuracy -0.0155 | validation: Loss 0.055688 Accuracy -2.4985\n",
      "Epoch 1033 | train: Loss 0.027035 Accuracy 0.0085 | validation: Loss 0.059721 Accuracy -2.7519\n",
      "Epoch 1034 | train: Loss 0.026779 Accuracy 0.0179 | validation: Loss 0.067006 Accuracy -3.2095\n",
      "Epoch 1035 | train: Loss 0.026121 Accuracy 0.0421 | validation: Loss 0.068574 Accuracy -3.3080\n",
      "Epoch 1036 | train: Loss 0.025095 Accuracy 0.0797 | validation: Loss 0.061418 Accuracy -2.8585\n",
      "Epoch 1037 | train: Loss 0.023759 Accuracy 0.1287 | validation: Loss 0.058036 Accuracy -2.6460\n",
      "Epoch 1038 | train: Loss 0.022421 Accuracy 0.1778 | validation: Loss 0.067300 Accuracy -3.2280\n",
      "Epoch 1039 | train: Loss 0.021296 Accuracy 0.2190 | validation: Loss 0.042415 Accuracy -1.6647\n",
      "Epoch 1040 | train: Loss 0.020153 Accuracy 0.2609 | validation: Loss 0.056023 Accuracy -2.5196\n",
      "Epoch 1041 | train: Loss 0.019859 Accuracy 0.2717 | validation: Loss 0.033304 Accuracy -1.0923\n",
      "Epoch 1042 | train: Loss 0.019475 Accuracy 0.2858 | validation: Loss 0.055646 Accuracy -2.4958\n",
      "Epoch 1043 | train: Loss 0.020834 Accuracy 0.2360 | validation: Loss 0.022442 Accuracy -0.4099\n",
      "Epoch 1044 | train: Loss 0.023815 Accuracy 0.1266 | validation: Loss 0.133363 Accuracy -7.3782\n",
      "Epoch 1045 | train: Loss 0.041072 Accuracy -0.5062 | validation: Loss 0.021131 Accuracy -0.3275\n",
      "Epoch 1046 | train: Loss 0.040329 Accuracy -0.4790 | validation: Loss 0.077011 Accuracy -3.8381\n",
      "Epoch 1047 | train: Loss 0.027406 Accuracy -0.0051 | validation: Loss 0.114688 Accuracy -6.2051\n",
      "Epoch 1048 | train: Loss 0.038273 Accuracy -0.4036 | validation: Loss 0.077528 Accuracy -3.8706\n",
      "Epoch 1049 | train: Loss 0.029708 Accuracy -0.0895 | validation: Loss 0.026165 Accuracy -0.6438\n",
      "Epoch 1050 | train: Loss 0.028271 Accuracy -0.0368 | validation: Loss 0.027340 Accuracy -0.7176\n",
      "Epoch 1051 | train: Loss 0.027893 Accuracy -0.0229 | validation: Loss 0.073544 Accuracy -3.6203\n",
      "Epoch 1052 | train: Loss 0.030495 Accuracy -0.1183 | validation: Loss 0.078129 Accuracy -3.9083\n",
      "Epoch 1053 | train: Loss 0.031174 Accuracy -0.1433 | validation: Loss 0.037491 Accuracy -1.3553\n",
      "Epoch 1054 | train: Loss 0.025835 Accuracy 0.0525 | validation: Loss 0.028512 Accuracy -0.7912\n",
      "Epoch 1055 | train: Loss 0.029104 Accuracy -0.0673 | validation: Loss 0.069995 Accuracy -3.3973\n",
      "Epoch 1056 | train: Loss 0.026057 Accuracy 0.0444 | validation: Loss 0.100915 Accuracy -5.3398\n",
      "Epoch 1057 | train: Loss 0.030095 Accuracy -0.1037 | validation: Loss 0.074821 Accuracy -3.7005\n",
      "Epoch 1058 | train: Loss 0.025450 Accuracy 0.0667 | validation: Loss 0.033903 Accuracy -1.1299\n",
      "Epoch 1059 | train: Loss 0.027308 Accuracy -0.0015 | validation: Loss 0.046840 Accuracy -1.9426\n",
      "Epoch 1060 | train: Loss 0.024025 Accuracy 0.1189 | validation: Loss 0.085105 Accuracy -4.3466\n",
      "Epoch 1061 | train: Loss 0.026234 Accuracy 0.0379 | validation: Loss 0.064643 Accuracy -3.0611\n",
      "Epoch 1062 | train: Loss 0.023757 Accuracy 0.1287 | validation: Loss 0.030315 Accuracy -0.9044\n",
      "Epoch 1063 | train: Loss 0.024021 Accuracy 0.1191 | validation: Loss 0.034600 Accuracy -1.1736\n",
      "Epoch 1064 | train: Loss 0.022391 Accuracy 0.1788 | validation: Loss 0.062371 Accuracy -2.9183\n",
      "Epoch 1065 | train: Loss 0.024070 Accuracy 0.1173 | validation: Loss 0.044450 Accuracy -1.7925\n",
      "Epoch 1066 | train: Loss 0.021535 Accuracy 0.2102 | validation: Loss 0.029814 Accuracy -0.8730\n",
      "Epoch 1067 | train: Loss 0.022482 Accuracy 0.1755 | validation: Loss 0.055931 Accuracy -2.5138\n",
      "Epoch 1068 | train: Loss 0.022151 Accuracy 0.1877 | validation: Loss 0.059202 Accuracy -2.7192\n",
      "Epoch 1069 | train: Loss 0.022527 Accuracy 0.1738 | validation: Loss 0.033004 Accuracy -1.0734\n",
      "Epoch 1070 | train: Loss 0.022741 Accuracy 0.1660 | validation: Loss 0.049899 Accuracy -2.1348\n",
      "Epoch 1071 | train: Loss 0.022095 Accuracy 0.1897 | validation: Loss 0.061506 Accuracy -2.8640\n",
      "Epoch 1072 | train: Loss 0.023152 Accuracy 0.1509 | validation: Loss 0.036194 Accuracy -1.2738\n",
      "Epoch 1073 | train: Loss 0.022458 Accuracy 0.1764 | validation: Loss 0.045362 Accuracy -1.8498\n",
      "Epoch 1074 | train: Loss 0.021780 Accuracy 0.2013 | validation: Loss 0.057592 Accuracy -2.6181\n",
      "Epoch 1075 | train: Loss 0.022305 Accuracy 0.1820 | validation: Loss 0.036177 Accuracy -1.2727\n",
      "Epoch 1076 | train: Loss 0.021316 Accuracy 0.2183 | validation: Loss 0.044003 Accuracy -1.7644\n",
      "Epoch 1077 | train: Loss 0.020667 Accuracy 0.2421 | validation: Loss 0.053037 Accuracy -2.3319\n",
      "Epoch 1078 | train: Loss 0.020952 Accuracy 0.2316 | validation: Loss 0.035954 Accuracy -1.2587\n",
      "Epoch 1079 | train: Loss 0.020513 Accuracy 0.2477 | validation: Loss 0.047874 Accuracy -2.0076\n",
      "Epoch 1080 | train: Loss 0.020396 Accuracy 0.2520 | validation: Loss 0.049918 Accuracy -2.1360\n",
      "Epoch 1081 | train: Loss 0.020543 Accuracy 0.2466 | validation: Loss 0.037726 Accuracy -1.3701\n",
      "Epoch 1082 | train: Loss 0.020657 Accuracy 0.2424 | validation: Loss 0.055313 Accuracy -2.4749\n",
      "Epoch 1083 | train: Loss 0.021014 Accuracy 0.2293 | validation: Loss 0.042815 Accuracy -1.6898\n",
      "Epoch 1084 | train: Loss 0.020545 Accuracy 0.2465 | validation: Loss 0.043594 Accuracy -1.7387\n",
      "Epoch 1085 | train: Loss 0.020496 Accuracy 0.2483 | validation: Loss 0.052510 Accuracy -2.2988\n",
      "Epoch 1086 | train: Loss 0.020838 Accuracy 0.2358 | validation: Loss 0.033726 Accuracy -1.1187\n",
      "Epoch 1087 | train: Loss 0.020916 Accuracy 0.2329 | validation: Loss 0.060652 Accuracy -2.8104\n",
      "Epoch 1088 | train: Loss 0.021697 Accuracy 0.2043 | validation: Loss 0.033640 Accuracy -1.1134\n",
      "Epoch 1089 | train: Loss 0.020651 Accuracy 0.2426 | validation: Loss 0.048676 Accuracy -2.0580\n",
      "Epoch 1090 | train: Loss 0.020447 Accuracy 0.2501 | validation: Loss 0.044340 Accuracy -1.7856\n",
      "Epoch 1091 | train: Loss 0.020165 Accuracy 0.2605 | validation: Loss 0.036774 Accuracy -1.3102\n",
      "Epoch 1092 | train: Loss 0.020233 Accuracy 0.2580 | validation: Loss 0.055309 Accuracy -2.4747\n",
      "Epoch 1093 | train: Loss 0.020915 Accuracy 0.2330 | validation: Loss 0.035017 Accuracy -1.1999\n",
      "Epoch 1094 | train: Loss 0.020488 Accuracy 0.2486 | validation: Loss 0.051708 Accuracy -2.2484\n",
      "Epoch 1095 | train: Loss 0.020619 Accuracy 0.2438 | validation: Loss 0.041709 Accuracy -1.6203\n",
      "Epoch 1096 | train: Loss 0.020246 Accuracy 0.2575 | validation: Loss 0.041452 Accuracy -1.6041\n",
      "Epoch 1097 | train: Loss 0.020276 Accuracy 0.2564 | validation: Loss 0.050613 Accuracy -2.1797\n",
      "Epoch 1098 | train: Loss 0.020635 Accuracy 0.2432 | validation: Loss 0.036369 Accuracy -1.2848\n",
      "Epoch 1099 | train: Loss 0.020452 Accuracy 0.2500 | validation: Loss 0.052731 Accuracy -2.3127\n",
      "Epoch 1100 | train: Loss 0.020756 Accuracy 0.2388 | validation: Loss 0.037617 Accuracy -1.3632\n",
      "Epoch 1101 | train: Loss 0.020255 Accuracy 0.2572 | validation: Loss 0.047561 Accuracy -1.9879\n",
      "Epoch 1102 | train: Loss 0.020272 Accuracy 0.2566 | validation: Loss 0.042803 Accuracy -1.6890\n",
      "Epoch 1103 | train: Loss 0.020071 Accuracy 0.2639 | validation: Loss 0.041395 Accuracy -1.6006\n",
      "Epoch 1104 | train: Loss 0.020051 Accuracy 0.2647 | validation: Loss 0.048242 Accuracy -2.0307\n",
      "Epoch 1105 | train: Loss 0.020270 Accuracy 0.2566 | validation: Loss 0.037724 Accuracy -1.3699\n",
      "Epoch 1106 | train: Loss 0.020158 Accuracy 0.2607 | validation: Loss 0.051599 Accuracy -2.2416\n",
      "Epoch 1107 | train: Loss 0.020525 Accuracy 0.2473 | validation: Loss 0.036096 Accuracy -1.2677\n",
      "Epoch 1108 | train: Loss 0.020259 Accuracy 0.2570 | validation: Loss 0.053616 Accuracy -2.3683\n",
      "Epoch 1109 | train: Loss 0.020660 Accuracy 0.2423 | validation: Loss 0.034980 Accuracy -1.1975\n",
      "Epoch 1110 | train: Loss 0.020308 Accuracy 0.2552 | validation: Loss 0.055996 Accuracy -2.5178\n",
      "Epoch 1111 | train: Loss 0.020833 Accuracy 0.2360 | validation: Loss 0.033051 Accuracy -1.0764\n",
      "Epoch 1112 | train: Loss 0.020482 Accuracy 0.2488 | validation: Loss 0.059945 Accuracy -2.7659\n",
      "Epoch 1113 | train: Loss 0.021337 Accuracy 0.2175 | validation: Loss 0.030098 Accuracy -0.8909\n",
      "Epoch 1114 | train: Loss 0.021003 Accuracy 0.2298 | validation: Loss 0.065370 Accuracy -3.1068\n",
      "Epoch 1115 | train: Loss 0.022332 Accuracy 0.1810 | validation: Loss 0.027714 Accuracy -0.7411\n",
      "Epoch 1116 | train: Loss 0.021708 Accuracy 0.2039 | validation: Loss 0.066961 Accuracy -3.2067\n",
      "Epoch 1117 | train: Loss 0.022894 Accuracy 0.1604 | validation: Loss 0.029039 Accuracy -0.8243\n",
      "Epoch 1118 | train: Loss 0.021341 Accuracy 0.2174 | validation: Loss 0.054544 Accuracy -2.4266\n",
      "Epoch 1119 | train: Loss 0.021206 Accuracy 0.2223 | validation: Loss 0.038850 Accuracy -1.4406\n",
      "Epoch 1120 | train: Loss 0.020224 Accuracy 0.2583 | validation: Loss 0.037789 Accuracy -1.3740\n",
      "Epoch 1121 | train: Loss 0.020295 Accuracy 0.2557 | validation: Loss 0.054694 Accuracy -2.4360\n",
      "Epoch 1122 | train: Loss 0.021209 Accuracy 0.2222 | validation: Loss 0.032824 Accuracy -1.0621\n",
      "Epoch 1123 | train: Loss 0.020864 Accuracy 0.2348 | validation: Loss 0.054140 Accuracy -2.4013\n",
      "Epoch 1124 | train: Loss 0.021101 Accuracy 0.2261 | validation: Loss 0.039975 Accuracy -1.5114\n",
      "Epoch 1125 | train: Loss 0.020374 Accuracy 0.2528 | validation: Loss 0.041073 Accuracy -1.5804\n",
      "Epoch 1126 | train: Loss 0.020357 Accuracy 0.2535 | validation: Loss 0.052710 Accuracy -2.3114\n",
      "Epoch 1127 | train: Loss 0.020861 Accuracy 0.2349 | validation: Loss 0.035355 Accuracy -1.2211\n",
      "Epoch 1128 | train: Loss 0.020612 Accuracy 0.2441 | validation: Loss 0.054364 Accuracy -2.4153\n",
      "Epoch 1129 | train: Loss 0.020875 Accuracy 0.2345 | validation: Loss 0.038914 Accuracy -1.4447\n",
      "Epoch 1130 | train: Loss 0.020213 Accuracy 0.2587 | validation: Loss 0.045446 Accuracy -1.8551\n",
      "Epoch 1131 | train: Loss 0.020113 Accuracy 0.2624 | validation: Loss 0.047219 Accuracy -1.9664\n",
      "Epoch 1132 | train: Loss 0.020120 Accuracy 0.2621 | validation: Loss 0.038033 Accuracy -1.3894\n",
      "Epoch 1133 | train: Loss 0.020041 Accuracy 0.2650 | validation: Loss 0.053064 Accuracy -2.3336\n",
      "Epoch 1134 | train: Loss 0.020480 Accuracy 0.2489 | validation: Loss 0.035033 Accuracy -1.2009\n",
      "Epoch 1135 | train: Loss 0.020170 Accuracy 0.2603 | validation: Loss 0.053716 Accuracy -2.3746\n",
      "Epoch 1136 | train: Loss 0.020573 Accuracy 0.2455 | validation: Loss 0.034629 Accuracy -1.1755\n",
      "Epoch 1137 | train: Loss 0.020120 Accuracy 0.2621 | validation: Loss 0.051965 Accuracy -2.2646\n",
      "Epoch 1138 | train: Loss 0.020424 Accuracy 0.2510 | validation: Loss 0.035074 Accuracy -1.2035\n",
      "Epoch 1139 | train: Loss 0.019992 Accuracy 0.2668 | validation: Loss 0.050719 Accuracy -2.1863\n",
      "Epoch 1140 | train: Loss 0.020266 Accuracy 0.2568 | validation: Loss 0.035247 Accuracy -1.2143\n",
      "Epoch 1141 | train: Loss 0.019902 Accuracy 0.2701 | validation: Loss 0.051287 Accuracy -2.2220\n",
      "Epoch 1142 | train: Loss 0.020243 Accuracy 0.2576 | validation: Loss 0.034325 Accuracy -1.1564\n",
      "Epoch 1143 | train: Loss 0.019939 Accuracy 0.2688 | validation: Loss 0.054401 Accuracy -2.4176\n",
      "Epoch 1144 | train: Loss 0.020518 Accuracy 0.2475 | validation: Loss 0.031715 Accuracy -0.9924\n",
      "Epoch 1145 | train: Loss 0.020299 Accuracy 0.2556 | validation: Loss 0.061472 Accuracy -2.8618\n",
      "Epoch 1146 | train: Loss 0.021469 Accuracy 0.2127 | validation: Loss 0.027618 Accuracy -0.7351\n",
      "Epoch 1147 | train: Loss 0.021388 Accuracy 0.2156 | validation: Loss 0.073369 Accuracy -3.6093\n",
      "Epoch 1148 | train: Loss 0.023666 Accuracy 0.1321 | validation: Loss 0.024377 Accuracy -0.5315\n",
      "Epoch 1149 | train: Loss 0.023131 Accuracy 0.1517 | validation: Loss 0.079303 Accuracy -3.9820\n",
      "Epoch 1150 | train: Loss 0.025294 Accuracy 0.0724 | validation: Loss 0.025952 Accuracy -0.6304\n",
      "Epoch 1151 | train: Loss 0.022245 Accuracy 0.1842 | validation: Loss 0.054771 Accuracy -2.4409\n",
      "Epoch 1152 | train: Loss 0.021283 Accuracy 0.2195 | validation: Loss 0.041569 Accuracy -1.6115\n",
      "Epoch 1153 | train: Loss 0.020184 Accuracy 0.2598 | validation: Loss 0.031328 Accuracy -0.9681\n",
      "Epoch 1154 | train: Loss 0.020858 Accuracy 0.2351 | validation: Loss 0.063015 Accuracy -2.9588\n",
      "Epoch 1155 | train: Loss 0.022700 Accuracy 0.1675 | validation: Loss 0.031656 Accuracy -0.9887\n",
      "Epoch 1156 | train: Loss 0.021076 Accuracy 0.2271 | validation: Loss 0.045157 Accuracy -1.8369\n",
      "Epoch 1157 | train: Loss 0.020585 Accuracy 0.2451 | validation: Loss 0.052137 Accuracy -2.2754\n",
      "Epoch 1158 | train: Loss 0.021099 Accuracy 0.2262 | validation: Loss 0.032607 Accuracy -1.0484\n",
      "Epoch 1159 | train: Loss 0.021233 Accuracy 0.2213 | validation: Loss 0.057774 Accuracy -2.6295\n",
      "Epoch 1160 | train: Loss 0.021586 Accuracy 0.2084 | validation: Loss 0.041007 Accuracy -1.5762\n",
      "Epoch 1161 | train: Loss 0.020514 Accuracy 0.2477 | validation: Loss 0.039453 Accuracy -1.4785\n",
      "Epoch 1162 | train: Loss 0.020518 Accuracy 0.2475 | validation: Loss 0.058067 Accuracy -2.6480\n",
      "Epoch 1163 | train: Loss 0.021349 Accuracy 0.2171 | validation: Loss 0.035283 Accuracy -1.2166\n",
      "Epoch 1164 | train: Loss 0.020722 Accuracy 0.2401 | validation: Loss 0.051164 Accuracy -2.2143\n",
      "Epoch 1165 | train: Loss 0.020550 Accuracy 0.2464 | validation: Loss 0.045755 Accuracy -1.8745\n",
      "Epoch 1166 | train: Loss 0.020205 Accuracy 0.2590 | validation: Loss 0.037824 Accuracy -1.3762\n",
      "Epoch 1167 | train: Loss 0.020245 Accuracy 0.2576 | validation: Loss 0.055561 Accuracy -2.4905\n",
      "Epoch 1168 | train: Loss 0.020833 Accuracy 0.2360 | validation: Loss 0.035440 Accuracy -1.2265\n",
      "Epoch 1169 | train: Loss 0.020302 Accuracy 0.2554 | validation: Loss 0.050125 Accuracy -2.1490\n",
      "Epoch 1170 | train: Loss 0.020308 Accuracy 0.2552 | validation: Loss 0.041413 Accuracy -1.6017\n",
      "Epoch 1171 | train: Loss 0.019906 Accuracy 0.2700 | validation: Loss 0.040258 Accuracy -1.5291\n",
      "Epoch 1172 | train: Loss 0.019860 Accuracy 0.2716 | validation: Loss 0.049227 Accuracy -2.0926\n",
      "Epoch 1173 | train: Loss 0.020161 Accuracy 0.2606 | validation: Loss 0.034935 Accuracy -1.1947\n",
      "Epoch 1174 | train: Loss 0.019994 Accuracy 0.2668 | validation: Loss 0.053026 Accuracy -2.3312\n",
      "Epoch 1175 | train: Loss 0.020442 Accuracy 0.2503 | validation: Loss 0.033589 Accuracy -1.1102\n",
      "Epoch 1176 | train: Loss 0.020013 Accuracy 0.2661 | validation: Loss 0.052846 Accuracy -2.3199\n",
      "Epoch 1177 | train: Loss 0.020356 Accuracy 0.2535 | validation: Loss 0.034092 Accuracy -1.1418\n",
      "Epoch 1178 | train: Loss 0.019875 Accuracy 0.2711 | validation: Loss 0.051400 Accuracy -2.2291\n",
      "Epoch 1179 | train: Loss 0.020152 Accuracy 0.2609 | validation: Loss 0.035066 Accuracy -1.2030\n",
      "Epoch 1180 | train: Loss 0.019750 Accuracy 0.2757 | validation: Loss 0.050466 Accuracy -2.1704\n",
      "Epoch 1181 | train: Loss 0.020020 Accuracy 0.2658 | validation: Loss 0.035733 Accuracy -1.2448\n",
      "Epoch 1182 | train: Loss 0.019703 Accuracy 0.2774 | validation: Loss 0.050624 Accuracy -2.1803\n",
      "Epoch 1183 | train: Loss 0.020004 Accuracy 0.2664 | validation: Loss 0.035621 Accuracy -1.2378\n",
      "Epoch 1184 | train: Loss 0.019732 Accuracy 0.2763 | validation: Loss 0.051964 Accuracy -2.2645\n",
      "Epoch 1185 | train: Loss 0.020116 Accuracy 0.2623 | validation: Loss 0.034498 Accuracy -1.1673\n",
      "Epoch 1186 | train: Loss 0.019858 Accuracy 0.2717 | validation: Loss 0.054538 Accuracy -2.4262\n",
      "Epoch 1187 | train: Loss 0.020404 Accuracy 0.2517 | validation: Loss 0.032484 Accuracy -1.0407\n",
      "Epoch 1188 | train: Loss 0.020131 Accuracy 0.2617 | validation: Loss 0.058225 Accuracy -2.6578\n",
      "Epoch 1189 | train: Loss 0.020922 Accuracy 0.2327 | validation: Loss 0.030261 Accuracy -0.9011\n",
      "Epoch 1190 | train: Loss 0.020561 Accuracy 0.2460 | validation: Loss 0.061608 Accuracy -2.8704\n",
      "Epoch 1191 | train: Loss 0.021526 Accuracy 0.2106 | validation: Loss 0.029196 Accuracy -0.8342\n",
      "Epoch 1192 | train: Loss 0.020853 Accuracy 0.2352 | validation: Loss 0.060500 Accuracy -2.8008\n",
      "Epoch 1193 | train: Loss 0.021510 Accuracy 0.2112 | validation: Loss 0.031264 Accuracy -0.9641\n",
      "Epoch 1194 | train: Loss 0.020454 Accuracy 0.2499 | validation: Loss 0.051602 Accuracy -2.2418\n",
      "Epoch 1195 | train: Loss 0.020471 Accuracy 0.2492 | validation: Loss 0.038574 Accuracy -1.4233\n",
      "Epoch 1196 | train: Loss 0.019869 Accuracy 0.2714 | validation: Loss 0.040552 Accuracy -1.5476\n",
      "Epoch 1197 | train: Loss 0.019890 Accuracy 0.2706 | validation: Loss 0.049228 Accuracy -2.0926\n",
      "Epoch 1198 | train: Loss 0.020297 Accuracy 0.2556 | validation: Loss 0.034946 Accuracy -1.1954\n",
      "Epoch 1199 | train: Loss 0.020236 Accuracy 0.2579 | validation: Loss 0.053949 Accuracy -2.3893\n",
      "Epoch 1200 | train: Loss 0.020714 Accuracy 0.2404 | validation: Loss 0.036195 Accuracy -1.2739\n",
      "Epoch 1201 | train: Loss 0.020181 Accuracy 0.2599 | validation: Loss 0.048683 Accuracy -2.0584\n",
      "Epoch 1202 | train: Loss 0.020209 Accuracy 0.2589 | validation: Loss 0.042747 Accuracy -1.6855\n",
      "Epoch 1203 | train: Loss 0.019954 Accuracy 0.2682 | validation: Loss 0.041056 Accuracy -1.5793\n",
      "Epoch 1204 | train: Loss 0.019932 Accuracy 0.2690 | validation: Loss 0.049858 Accuracy -2.1323\n",
      "Epoch 1205 | train: Loss 0.020200 Accuracy 0.2592 | validation: Loss 0.037180 Accuracy -1.3357\n",
      "Epoch 1206 | train: Loss 0.020027 Accuracy 0.2656 | validation: Loss 0.052006 Accuracy -2.2672\n",
      "Epoch 1207 | train: Loss 0.020316 Accuracy 0.2549 | validation: Loss 0.037428 Accuracy -1.3513\n",
      "Epoch 1208 | train: Loss 0.019944 Accuracy 0.2686 | validation: Loss 0.048983 Accuracy -2.0773\n",
      "Epoch 1209 | train: Loss 0.020050 Accuracy 0.2647 | validation: Loss 0.040227 Accuracy -1.5272\n",
      "Epoch 1210 | train: Loss 0.019777 Accuracy 0.2747 | validation: Loss 0.044348 Accuracy -1.7861\n",
      "Epoch 1211 | train: Loss 0.019791 Accuracy 0.2742 | validation: Loss 0.043765 Accuracy -1.7495\n",
      "Epoch 1212 | train: Loss 0.019757 Accuracy 0.2754 | validation: Loss 0.040555 Accuracy -1.5478\n",
      "Epoch 1213 | train: Loss 0.019704 Accuracy 0.2774 | validation: Loss 0.046908 Accuracy -1.9469\n",
      "Epoch 1214 | train: Loss 0.019853 Accuracy 0.2719 | validation: Loss 0.037856 Accuracy -1.3783\n",
      "Epoch 1215 | train: Loss 0.019721 Accuracy 0.2768 | validation: Loss 0.049792 Accuracy -2.1281\n",
      "Epoch 1216 | train: Loss 0.020011 Accuracy 0.2661 | validation: Loss 0.035443 Accuracy -1.2266\n",
      "Epoch 1217 | train: Loss 0.019823 Accuracy 0.2730 | validation: Loss 0.053610 Accuracy -2.3679\n",
      "Epoch 1218 | train: Loss 0.020336 Accuracy 0.2542 | validation: Loss 0.032351 Accuracy -1.0324\n",
      "Epoch 1219 | train: Loss 0.020161 Accuracy 0.2606 | validation: Loss 0.060295 Accuracy -2.7879\n",
      "Epoch 1220 | train: Loss 0.021180 Accuracy 0.2233 | validation: Loss 0.028209 Accuracy -0.7722\n",
      "Epoch 1221 | train: Loss 0.021127 Accuracy 0.2252 | validation: Loss 0.071496 Accuracy -3.4916\n",
      "Epoch 1222 | train: Loss 0.023157 Accuracy 0.1508 | validation: Loss 0.024531 Accuracy -0.5411\n",
      "Epoch 1223 | train: Loss 0.022916 Accuracy 0.1596 | validation: Loss 0.080546 Accuracy -4.0601\n",
      "Epoch 1224 | train: Loss 0.025365 Accuracy 0.0698 | validation: Loss 0.024678 Accuracy -0.5503\n",
      "Epoch 1225 | train: Loss 0.022921 Accuracy 0.1594 | validation: Loss 0.062913 Accuracy -2.9524\n",
      "Epoch 1226 | train: Loss 0.022375 Accuracy 0.1794 | validation: Loss 0.035891 Accuracy -1.2548\n",
      "Epoch 1227 | train: Loss 0.020115 Accuracy 0.2623 | validation: Loss 0.034647 Accuracy -1.1767\n",
      "Epoch 1228 | train: Loss 0.020322 Accuracy 0.2547 | validation: Loss 0.060692 Accuracy -2.8128\n",
      "Epoch 1229 | train: Loss 0.022238 Accuracy 0.1845 | validation: Loss 0.029791 Accuracy -0.8715\n",
      "Epoch 1230 | train: Loss 0.021401 Accuracy 0.2152 | validation: Loss 0.051551 Accuracy -2.2386\n",
      "Epoch 1231 | train: Loss 0.020989 Accuracy 0.2303 | validation: Loss 0.046143 Accuracy -1.8988\n",
      "Epoch 1232 | train: Loss 0.020574 Accuracy 0.2455 | validation: Loss 0.033773 Accuracy -1.1217\n",
      "Epoch 1233 | train: Loss 0.021015 Accuracy 0.2293 | validation: Loss 0.060438 Accuracy -2.7969\n",
      "Epoch 1234 | train: Loss 0.021895 Accuracy 0.1970 | validation: Loss 0.037862 Accuracy -1.3786\n",
      "Epoch 1235 | train: Loss 0.020664 Accuracy 0.2422 | validation: Loss 0.042597 Accuracy -1.6761\n",
      "Epoch 1236 | train: Loss 0.020470 Accuracy 0.2493 | validation: Loss 0.056280 Accuracy -2.5357\n",
      "Epoch 1237 | train: Loss 0.021177 Accuracy 0.2234 | validation: Loss 0.035287 Accuracy -1.2169\n",
      "Epoch 1238 | train: Loss 0.020837 Accuracy 0.2358 | validation: Loss 0.052942 Accuracy -2.3260\n",
      "Epoch 1239 | train: Loss 0.020740 Accuracy 0.2394 | validation: Loss 0.045230 Accuracy -1.8415\n",
      "Epoch 1240 | train: Loss 0.020274 Accuracy 0.2565 | validation: Loss 0.038591 Accuracy -1.4244\n",
      "Epoch 1241 | train: Loss 0.020335 Accuracy 0.2543 | validation: Loss 0.055836 Accuracy -2.5078\n",
      "Epoch 1242 | train: Loss 0.020891 Accuracy 0.2339 | validation: Loss 0.036918 Accuracy -1.3193\n",
      "Epoch 1243 | train: Loss 0.020335 Accuracy 0.2543 | validation: Loss 0.048178 Accuracy -2.0267\n",
      "Epoch 1244 | train: Loss 0.020227 Accuracy 0.2582 | validation: Loss 0.045501 Accuracy -1.8585\n",
      "Epoch 1245 | train: Loss 0.020076 Accuracy 0.2638 | validation: Loss 0.038088 Accuracy -1.3928\n",
      "Epoch 1246 | train: Loss 0.020071 Accuracy 0.2639 | validation: Loss 0.052410 Accuracy -2.2926\n",
      "Epoch 1247 | train: Loss 0.020476 Accuracy 0.2491 | validation: Loss 0.035953 Accuracy -1.2587\n",
      "Epoch 1248 | train: Loss 0.020087 Accuracy 0.2633 | validation: Loss 0.049317 Accuracy -2.0982\n",
      "Epoch 1249 | train: Loss 0.020146 Accuracy 0.2612 | validation: Loss 0.039938 Accuracy -1.5090\n",
      "Epoch 1250 | train: Loss 0.019769 Accuracy 0.2750 | validation: Loss 0.042324 Accuracy -1.6589\n",
      "Epoch 1251 | train: Loss 0.019719 Accuracy 0.2769 | validation: Loss 0.045974 Accuracy -1.8882\n",
      "Epoch 1252 | train: Loss 0.019799 Accuracy 0.2739 | validation: Loss 0.037574 Accuracy -1.3605\n",
      "Epoch 1253 | train: Loss 0.019710 Accuracy 0.2772 | validation: Loss 0.050404 Accuracy -2.1665\n",
      "Epoch 1254 | train: Loss 0.020027 Accuracy 0.2656 | validation: Loss 0.035628 Accuracy -1.2382\n",
      "Epoch 1255 | train: Loss 0.019799 Accuracy 0.2739 | validation: Loss 0.052338 Accuracy -2.2880\n",
      "Epoch 1256 | train: Loss 0.020139 Accuracy 0.2614 | validation: Loss 0.035213 Accuracy -1.2122\n",
      "Epoch 1257 | train: Loss 0.019821 Accuracy 0.2731 | validation: Loss 0.052729 Accuracy -2.3126\n",
      "Epoch 1258 | train: Loss 0.020141 Accuracy 0.2614 | validation: Loss 0.035302 Accuracy -1.2177\n",
      "Epoch 1259 | train: Loss 0.019803 Accuracy 0.2737 | validation: Loss 0.052506 Accuracy -2.2986\n",
      "Epoch 1260 | train: Loss 0.020113 Accuracy 0.2624 | validation: Loss 0.035434 Accuracy -1.2261\n",
      "Epoch 1261 | train: Loss 0.019785 Accuracy 0.2744 | validation: Loss 0.052028 Accuracy -2.2686\n",
      "Epoch 1262 | train: Loss 0.020089 Accuracy 0.2633 | validation: Loss 0.035610 Accuracy -1.2371\n",
      "Epoch 1263 | train: Loss 0.019774 Accuracy 0.2748 | validation: Loss 0.051251 Accuracy -2.2198\n",
      "Epoch 1264 | train: Loss 0.020058 Accuracy 0.2644 | validation: Loss 0.036080 Accuracy -1.2667\n",
      "Epoch 1265 | train: Loss 0.019756 Accuracy 0.2755 | validation: Loss 0.049992 Accuracy -2.1406\n",
      "Epoch 1266 | train: Loss 0.019994 Accuracy 0.2668 | validation: Loss 0.037129 Accuracy -1.3326\n",
      "Epoch 1267 | train: Loss 0.019720 Accuracy 0.2768 | validation: Loss 0.048173 Accuracy -2.0263\n",
      "Epoch 1268 | train: Loss 0.019889 Accuracy 0.2706 | validation: Loss 0.038879 Accuracy -1.4425\n",
      "Epoch 1269 | train: Loss 0.019679 Accuracy 0.2783 | validation: Loss 0.045987 Accuracy -1.8890\n",
      "Epoch 1270 | train: Loss 0.019778 Accuracy 0.2747 | validation: Loss 0.041160 Accuracy -1.5858\n",
      "Epoch 1271 | train: Loss 0.019668 Accuracy 0.2787 | validation: Loss 0.043831 Accuracy -1.7536\n",
      "Epoch 1272 | train: Loss 0.019705 Accuracy 0.2773 | validation: Loss 0.043532 Accuracy -1.7348\n",
      "Epoch 1273 | train: Loss 0.019700 Accuracy 0.2775 | validation: Loss 0.042066 Accuracy -1.6427\n",
      "Epoch 1274 | train: Loss 0.019681 Accuracy 0.2782 | validation: Loss 0.045518 Accuracy -1.8596\n",
      "Epoch 1275 | train: Loss 0.019755 Accuracy 0.2755 | validation: Loss 0.040835 Accuracy -1.5654\n",
      "Epoch 1276 | train: Loss 0.019685 Accuracy 0.2781 | validation: Loss 0.046860 Accuracy -1.9439\n",
      "Epoch 1277 | train: Loss 0.019803 Accuracy 0.2738 | validation: Loss 0.040064 Accuracy -1.5170\n",
      "Epoch 1278 | train: Loss 0.019691 Accuracy 0.2779 | validation: Loss 0.047603 Accuracy -1.9906\n",
      "Epoch 1279 | train: Loss 0.019832 Accuracy 0.2727 | validation: Loss 0.039558 Accuracy -1.4852\n",
      "Epoch 1280 | train: Loss 0.019694 Accuracy 0.2778 | validation: Loss 0.047993 Accuracy -2.0151\n",
      "Epoch 1281 | train: Loss 0.019850 Accuracy 0.2720 | validation: Loss 0.039094 Accuracy -1.4560\n",
      "Epoch 1282 | train: Loss 0.019698 Accuracy 0.2776 | validation: Loss 0.048351 Accuracy -2.0376\n",
      "Epoch 1283 | train: Loss 0.019875 Accuracy 0.2711 | validation: Loss 0.038473 Accuracy -1.4170\n",
      "Epoch 1284 | train: Loss 0.019714 Accuracy 0.2770 | validation: Loss 0.049016 Accuracy -2.0794\n",
      "Epoch 1285 | train: Loss 0.019929 Accuracy 0.2692 | validation: Loss 0.037527 Accuracy -1.3576\n",
      "Epoch 1286 | train: Loss 0.019755 Accuracy 0.2755 | validation: Loss 0.050356 Accuracy -2.1635\n",
      "Epoch 1287 | train: Loss 0.020041 Accuracy 0.2650 | validation: Loss 0.036092 Accuracy -1.2674\n",
      "Epoch 1288 | train: Loss 0.019852 Accuracy 0.2720 | validation: Loss 0.052790 Accuracy -2.3164\n",
      "Epoch 1289 | train: Loss 0.020271 Accuracy 0.2566 | validation: Loss 0.034039 Accuracy -1.1385\n",
      "Epoch 1290 | train: Loss 0.020070 Accuracy 0.2640 | validation: Loss 0.056722 Accuracy -2.5634\n",
      "Epoch 1291 | train: Loss 0.020727 Accuracy 0.2399 | validation: Loss 0.031461 Accuracy -0.9764\n",
      "Epoch 1292 | train: Loss 0.020505 Accuracy 0.2480 | validation: Loss 0.061965 Accuracy -2.8928\n",
      "Epoch 1293 | train: Loss 0.021493 Accuracy 0.2118 | validation: Loss 0.029094 Accuracy -0.8278\n",
      "Epoch 1294 | train: Loss 0.021120 Accuracy 0.2255 | validation: Loss 0.065828 Accuracy -3.1355\n",
      "Epoch 1295 | train: Loss 0.022221 Accuracy 0.1851 | validation: Loss 0.028656 Accuracy -0.8003\n",
      "Epoch 1296 | train: Loss 0.021315 Accuracy 0.2183 | validation: Loss 0.061768 Accuracy -2.8805\n",
      "Epoch 1297 | train: Loss 0.021734 Accuracy 0.2030 | validation: Loss 0.032835 Accuracy -1.0628\n",
      "Epoch 1298 | train: Loss 0.020461 Accuracy 0.2496 | validation: Loss 0.048406 Accuracy -2.0410\n",
      "Epoch 1299 | train: Loss 0.020262 Accuracy 0.2569 | validation: Loss 0.043856 Accuracy -1.7552\n",
      "Epoch 1300 | train: Loss 0.020055 Accuracy 0.2645 | validation: Loss 0.036955 Accuracy -1.3216\n",
      "Epoch 1301 | train: Loss 0.020203 Accuracy 0.2591 | validation: Loss 0.054987 Accuracy -2.4545\n",
      "Epoch 1302 | train: Loss 0.020925 Accuracy 0.2326 | validation: Loss 0.034400 Accuracy -1.1611\n",
      "Epoch 1303 | train: Loss 0.020563 Accuracy 0.2459 | validation: Loss 0.053193 Accuracy -2.3417\n",
      "Epoch 1304 | train: Loss 0.020709 Accuracy 0.2405 | validation: Loss 0.040405 Accuracy -1.5383\n",
      "Epoch 1305 | train: Loss 0.020170 Accuracy 0.2603 | validation: Loss 0.043137 Accuracy -1.7100\n",
      "Epoch 1306 | train: Loss 0.020138 Accuracy 0.2615 | validation: Loss 0.050261 Accuracy -2.1576\n",
      "Epoch 1307 | train: Loss 0.020401 Accuracy 0.2518 | validation: Loss 0.037261 Accuracy -1.3408\n",
      "Epoch 1308 | train: Loss 0.020309 Accuracy 0.2552 | validation: Loss 0.053692 Accuracy -2.3731\n",
      "Epoch 1309 | train: Loss 0.020619 Accuracy 0.2438 | validation: Loss 0.038321 Accuracy -1.4074\n",
      "Epoch 1310 | train: Loss 0.020172 Accuracy 0.2602 | validation: Loss 0.048321 Accuracy -2.0357\n",
      "Epoch 1311 | train: Loss 0.020163 Accuracy 0.2606 | validation: Loss 0.043989 Accuracy -1.7635\n",
      "Epoch 1312 | train: Loss 0.019985 Accuracy 0.2671 | validation: Loss 0.041547 Accuracy -1.6101\n",
      "Epoch 1313 | train: Loss 0.019958 Accuracy 0.2681 | validation: Loss 0.049647 Accuracy -2.1190\n",
      "Epoch 1314 | train: Loss 0.020178 Accuracy 0.2600 | validation: Loss 0.037984 Accuracy -1.3863\n",
      "Epoch 1315 | train: Loss 0.020034 Accuracy 0.2653 | validation: Loss 0.051550 Accuracy -2.2385\n",
      "Epoch 1316 | train: Loss 0.020293 Accuracy 0.2558 | validation: Loss 0.037585 Accuracy -1.3612\n",
      "Epoch 1317 | train: Loss 0.019996 Accuracy 0.2667 | validation: Loss 0.049926 Accuracy -2.1365\n",
      "Epoch 1318 | train: Loss 0.020138 Accuracy 0.2615 | validation: Loss 0.039012 Accuracy -1.4508\n",
      "Epoch 1319 | train: Loss 0.019853 Accuracy 0.2719 | validation: Loss 0.047041 Accuracy -1.9553\n",
      "Epoch 1320 | train: Loss 0.019913 Accuracy 0.2697 | validation: Loss 0.041073 Accuracy -1.5803\n",
      "Epoch 1321 | train: Loss 0.019743 Accuracy 0.2760 | validation: Loss 0.044487 Accuracy -1.7948\n",
      "Epoch 1322 | train: Loss 0.019755 Accuracy 0.2755 | validation: Loss 0.043082 Accuracy -1.7065\n",
      "Epoch 1323 | train: Loss 0.019700 Accuracy 0.2775 | validation: Loss 0.042633 Accuracy -1.6783\n",
      "Epoch 1324 | train: Loss 0.019674 Accuracy 0.2785 | validation: Loss 0.044932 Accuracy -1.8228\n",
      "Epoch 1325 | train: Loss 0.019706 Accuracy 0.2773 | validation: Loss 0.041112 Accuracy -1.5828\n",
      "Epoch 1326 | train: Loss 0.019644 Accuracy 0.2796 | validation: Loss 0.047002 Accuracy -1.9528\n",
      "Epoch 1327 | train: Loss 0.019761 Accuracy 0.2753 | validation: Loss 0.039221 Accuracy -1.4640\n",
      "Epoch 1328 | train: Loss 0.019669 Accuracy 0.2787 | validation: Loss 0.050179 Accuracy -2.1524\n",
      "Epoch 1329 | train: Loss 0.019938 Accuracy 0.2688 | validation: Loss 0.036061 Accuracy -1.2654\n",
      "Epoch 1330 | train: Loss 0.019854 Accuracy 0.2719 | validation: Loss 0.056337 Accuracy -2.5392\n",
      "Epoch 1331 | train: Loss 0.020529 Accuracy 0.2471 | validation: Loss 0.030824 Accuracy -0.9364\n",
      "Epoch 1332 | train: Loss 0.020643 Accuracy 0.2430 | validation: Loss 0.069229 Accuracy -3.3492\n",
      "Epoch 1333 | train: Loss 0.022519 Accuracy 0.1741 | validation: Loss 0.024504 Accuracy -0.5394\n",
      "Epoch 1334 | train: Loss 0.023174 Accuracy 0.1501 | validation: Loss 0.090580 Accuracy -4.6905\n",
      "Epoch 1335 | train: Loss 0.027405 Accuracy -0.0050 | validation: Loss 0.021895 Accuracy -0.3755\n",
      "Epoch 1336 | train: Loss 0.026186 Accuracy 0.0397 | validation: Loss 0.086909 Accuracy -4.4599\n",
      "Epoch 1337 | train: Loss 0.027524 Accuracy -0.0094 | validation: Loss 0.028375 Accuracy -0.7826\n",
      "Epoch 1338 | train: Loss 0.021482 Accuracy 0.2122 | validation: Loss 0.037069 Accuracy -1.3288\n",
      "Epoch 1339 | train: Loss 0.020323 Accuracy 0.2547 | validation: Loss 0.063778 Accuracy -3.0067\n",
      "Epoch 1340 | train: Loss 0.023301 Accuracy 0.1455 | validation: Loss 0.027156 Accuracy -0.7060\n",
      "Epoch 1341 | train: Loss 0.022514 Accuracy 0.1743 | validation: Loss 0.051249 Accuracy -2.2196\n",
      "Epoch 1342 | train: Loss 0.021421 Accuracy 0.2144 | validation: Loss 0.050562 Accuracy -2.1764\n",
      "Epoch 1343 | train: Loss 0.021356 Accuracy 0.2168 | validation: Loss 0.030392 Accuracy -0.9093\n",
      "Epoch 1344 | train: Loss 0.022200 Accuracy 0.1858 | validation: Loss 0.060358 Accuracy -2.7919\n",
      "Epoch 1345 | train: Loss 0.022401 Accuracy 0.1785 | validation: Loss 0.046336 Accuracy -1.9110\n",
      "Epoch 1346 | train: Loss 0.021098 Accuracy 0.2263 | validation: Loss 0.034126 Accuracy -1.1439\n",
      "Epoch 1347 | train: Loss 0.021861 Accuracy 0.1983 | validation: Loss 0.065143 Accuracy -3.0925\n",
      "Epoch 1348 | train: Loss 0.022696 Accuracy 0.1677 | validation: Loss 0.043725 Accuracy -1.7469\n",
      "Epoch 1349 | train: Loss 0.021019 Accuracy 0.2292 | validation: Loss 0.037242 Accuracy -1.3396\n",
      "Epoch 1350 | train: Loss 0.021379 Accuracy 0.2159 | validation: Loss 0.065541 Accuracy -3.1175\n",
      "Epoch 1351 | train: Loss 0.022477 Accuracy 0.1757 | validation: Loss 0.040561 Accuracy -1.5482\n",
      "Epoch 1352 | train: Loss 0.020922 Accuracy 0.2327 | validation: Loss 0.040432 Accuracy -1.5401\n",
      "Epoch 1353 | train: Loss 0.020841 Accuracy 0.2357 | validation: Loss 0.062112 Accuracy -2.9021\n",
      "Epoch 1354 | train: Loss 0.021911 Accuracy 0.1965 | validation: Loss 0.037322 Accuracy -1.3447\n",
      "Epoch 1355 | train: Loss 0.020912 Accuracy 0.2331 | validation: Loss 0.045135 Accuracy -1.8355\n",
      "Epoch 1356 | train: Loss 0.020536 Accuracy 0.2469 | validation: Loss 0.055138 Accuracy -2.4640\n",
      "Epoch 1357 | train: Loss 0.021087 Accuracy 0.2267 | validation: Loss 0.035422 Accuracy -1.2253\n",
      "Epoch 1358 | train: Loss 0.020862 Accuracy 0.2349 | validation: Loss 0.051360 Accuracy -2.2266\n",
      "Epoch 1359 | train: Loss 0.020707 Accuracy 0.2406 | validation: Loss 0.045540 Accuracy -1.8610\n",
      "Epoch 1360 | train: Loss 0.020338 Accuracy 0.2541 | validation: Loss 0.037082 Accuracy -1.3296\n",
      "Epoch 1361 | train: Loss 0.020419 Accuracy 0.2512 | validation: Loss 0.054417 Accuracy -2.4186\n",
      "Epoch 1362 | train: Loss 0.020863 Accuracy 0.2349 | validation: Loss 0.037369 Accuracy -1.3476\n",
      "Epoch 1363 | train: Loss 0.020224 Accuracy 0.2583 | validation: Loss 0.044684 Accuracy -1.8072\n",
      "Epoch 1364 | train: Loss 0.020048 Accuracy 0.2648 | validation: Loss 0.047628 Accuracy -1.9922\n",
      "Epoch 1365 | train: Loss 0.020120 Accuracy 0.2621 | validation: Loss 0.036580 Accuracy -1.2980\n",
      "Epoch 1366 | train: Loss 0.020084 Accuracy 0.2635 | validation: Loss 0.052479 Accuracy -2.2969\n",
      "Epoch 1367 | train: Loss 0.020374 Accuracy 0.2528 | validation: Loss 0.037871 Accuracy -1.3791\n",
      "Epoch 1368 | train: Loss 0.019920 Accuracy 0.2695 | validation: Loss 0.046615 Accuracy -1.9285\n",
      "Epoch 1369 | train: Loss 0.019861 Accuracy 0.2716 | validation: Loss 0.045130 Accuracy -1.8352\n",
      "Epoch 1370 | train: Loss 0.019772 Accuracy 0.2749 | validation: Loss 0.039942 Accuracy -1.5093\n",
      "Epoch 1371 | train: Loss 0.019757 Accuracy 0.2754 | validation: Loss 0.051315 Accuracy -2.2238\n",
      "Epoch 1372 | train: Loss 0.020038 Accuracy 0.2651 | validation: Loss 0.037679 Accuracy -1.3671\n",
      "Epoch 1373 | train: Loss 0.019841 Accuracy 0.2723 | validation: Loss 0.051365 Accuracy -2.2269\n",
      "Epoch 1374 | train: Loss 0.020004 Accuracy 0.2664 | validation: Loss 0.039396 Accuracy -1.4750\n",
      "Epoch 1375 | train: Loss 0.019700 Accuracy 0.2775 | validation: Loss 0.046850 Accuracy -1.9432\n",
      "Epoch 1376 | train: Loss 0.019720 Accuracy 0.2768 | validation: Loss 0.043238 Accuracy -1.7163\n",
      "Epoch 1377 | train: Loss 0.019614 Accuracy 0.2807 | validation: Loss 0.041937 Accuracy -1.6346\n",
      "Epoch 1378 | train: Loss 0.019603 Accuracy 0.2811 | validation: Loss 0.046913 Accuracy -1.9472\n",
      "Epoch 1379 | train: Loss 0.019730 Accuracy 0.2764 | validation: Loss 0.039046 Accuracy -1.4530\n",
      "Epoch 1380 | train: Loss 0.019654 Accuracy 0.2792 | validation: Loss 0.048580 Accuracy -2.0519\n",
      "Epoch 1381 | train: Loss 0.019845 Accuracy 0.2722 | validation: Loss 0.038605 Accuracy -1.4253\n",
      "Epoch 1382 | train: Loss 0.019683 Accuracy 0.2781 | validation: Loss 0.047777 Accuracy -2.0015\n",
      "Epoch 1383 | train: Loss 0.019813 Accuracy 0.2734 | validation: Loss 0.040176 Accuracy -1.5240\n",
      "Epoch 1384 | train: Loss 0.019656 Accuracy 0.2791 | validation: Loss 0.045401 Accuracy -1.8522\n",
      "Epoch 1385 | train: Loss 0.019714 Accuracy 0.2770 | validation: Loss 0.042895 Accuracy -1.6948\n",
      "Epoch 1386 | train: Loss 0.019663 Accuracy 0.2789 | validation: Loss 0.042864 Accuracy -1.6928\n",
      "Epoch 1387 | train: Loss 0.019669 Accuracy 0.2787 | validation: Loss 0.045558 Accuracy -1.8621\n",
      "Epoch 1388 | train: Loss 0.019733 Accuracy 0.2763 | validation: Loss 0.041231 Accuracy -1.5902\n",
      "Epoch 1389 | train: Loss 0.019685 Accuracy 0.2781 | validation: Loss 0.047044 Accuracy -1.9554\n",
      "Epoch 1390 | train: Loss 0.019795 Accuracy 0.2740 | validation: Loss 0.040914 Accuracy -1.5704\n",
      "Epoch 1391 | train: Loss 0.019702 Accuracy 0.2775 | validation: Loss 0.046904 Accuracy -1.9467\n",
      "Epoch 1392 | train: Loss 0.019794 Accuracy 0.2741 | validation: Loss 0.041786 Accuracy -1.6251\n",
      "Epoch 1393 | train: Loss 0.019702 Accuracy 0.2775 | validation: Loss 0.045549 Accuracy -1.8615\n",
      "Epoch 1394 | train: Loss 0.019754 Accuracy 0.2755 | validation: Loss 0.043335 Accuracy -1.7224\n",
      "Epoch 1395 | train: Loss 0.019717 Accuracy 0.2769 | validation: Loss 0.043836 Accuracy -1.7539\n",
      "Epoch 1396 | train: Loss 0.019731 Accuracy 0.2764 | validation: Loss 0.044858 Accuracy -1.8181\n",
      "Epoch 1397 | train: Loss 0.019759 Accuracy 0.2754 | validation: Loss 0.042512 Accuracy -1.6708\n",
      "Epoch 1398 | train: Loss 0.019735 Accuracy 0.2763 | validation: Loss 0.045765 Accuracy -1.8751\n",
      "Epoch 1399 | train: Loss 0.019799 Accuracy 0.2739 | validation: Loss 0.041932 Accuracy -1.6343\n",
      "Epoch 1400 | train: Loss 0.019744 Accuracy 0.2759 | validation: Loss 0.045847 Accuracy -1.8802\n",
      "Epoch 1401 | train: Loss 0.019808 Accuracy 0.2736 | validation: Loss 0.042091 Accuracy -1.6443\n",
      "Epoch 1402 | train: Loss 0.019744 Accuracy 0.2759 | validation: Loss 0.045294 Accuracy -1.8455\n",
      "Epoch 1403 | train: Loss 0.019789 Accuracy 0.2743 | validation: Loss 0.042769 Accuracy -1.6869\n",
      "Epoch 1404 | train: Loss 0.019742 Accuracy 0.2760 | validation: Loss 0.044482 Accuracy -1.7945\n",
      "Epoch 1405 | train: Loss 0.019764 Accuracy 0.2752 | validation: Loss 0.043667 Accuracy -1.7433\n",
      "Epoch 1406 | train: Loss 0.019747 Accuracy 0.2758 | validation: Loss 0.043731 Accuracy -1.7473\n",
      "Epoch 1407 | train: Loss 0.019746 Accuracy 0.2758 | validation: Loss 0.044510 Accuracy -1.7962\n",
      "Epoch 1408 | train: Loss 0.019759 Accuracy 0.2754 | validation: Loss 0.043194 Accuracy -1.7136\n",
      "Epoch 1409 | train: Loss 0.019739 Accuracy 0.2761 | validation: Loss 0.045123 Accuracy -1.8348\n",
      "Epoch 1410 | train: Loss 0.019771 Accuracy 0.2749 | validation: Loss 0.042875 Accuracy -1.6935\n",
      "Epoch 1411 | train: Loss 0.019737 Accuracy 0.2762 | validation: Loss 0.045454 Accuracy -1.8556\n",
      "Epoch 1412 | train: Loss 0.019779 Accuracy 0.2747 | validation: Loss 0.042704 Accuracy -1.6828\n",
      "Epoch 1413 | train: Loss 0.019735 Accuracy 0.2762 | validation: Loss 0.045556 Accuracy -1.8620\n",
      "Epoch 1414 | train: Loss 0.019780 Accuracy 0.2746 | validation: Loss 0.042604 Accuracy -1.6765\n",
      "Epoch 1415 | train: Loss 0.019732 Accuracy 0.2764 | validation: Loss 0.045537 Accuracy -1.8608\n",
      "Epoch 1416 | train: Loss 0.019777 Accuracy 0.2747 | validation: Loss 0.042520 Accuracy -1.6712\n",
      "Epoch 1417 | train: Loss 0.019728 Accuracy 0.2765 | validation: Loss 0.045515 Accuracy -1.8594\n",
      "Epoch 1418 | train: Loss 0.019775 Accuracy 0.2748 | validation: Loss 0.042413 Accuracy -1.6645\n",
      "Epoch 1419 | train: Loss 0.019726 Accuracy 0.2766 | validation: Loss 0.045589 Accuracy -1.8640\n",
      "Epoch 1420 | train: Loss 0.019777 Accuracy 0.2747 | validation: Loss 0.042242 Accuracy -1.6538\n",
      "Epoch 1421 | train: Loss 0.019725 Accuracy 0.2766 | validation: Loss 0.045832 Accuracy -1.8793\n",
      "Epoch 1422 | train: Loss 0.019785 Accuracy 0.2744 | validation: Loss 0.041944 Accuracy -1.6350\n",
      "Epoch 1423 | train: Loss 0.019728 Accuracy 0.2765 | validation: Loss 0.046318 Accuracy -1.9099\n",
      "Epoch 1424 | train: Loss 0.019803 Accuracy 0.2738 | validation: Loss 0.041422 Accuracy -1.6023\n",
      "Epoch 1425 | train: Loss 0.019735 Accuracy 0.2763 | validation: Loss 0.047155 Accuracy -1.9624\n",
      "Epoch 1426 | train: Loss 0.019840 Accuracy 0.2724 | validation: Loss 0.040543 Accuracy -1.5470\n",
      "Epoch 1427 | train: Loss 0.019755 Accuracy 0.2755 | validation: Loss 0.048539 Accuracy -2.0494\n",
      "Epoch 1428 | train: Loss 0.019914 Accuracy 0.2697 | validation: Loss 0.039117 Accuracy -1.4574\n",
      "Epoch 1429 | train: Loss 0.019812 Accuracy 0.2734 | validation: Loss 0.050830 Accuracy -2.1933\n",
      "Epoch 1430 | train: Loss 0.020075 Accuracy 0.2638 | validation: Loss 0.036917 Accuracy -1.3193\n",
      "Epoch 1431 | train: Loss 0.019962 Accuracy 0.2679 | validation: Loss 0.054607 Accuracy -2.4306\n",
      "Epoch 1432 | train: Loss 0.020433 Accuracy 0.2506 | validation: Loss 0.033806 Accuracy -1.1238\n",
      "Epoch 1433 | train: Loss 0.020336 Accuracy 0.2542 | validation: Loss 0.060512 Accuracy -2.8015\n",
      "Epoch 1434 | train: Loss 0.021192 Accuracy 0.2228 | validation: Loss 0.030156 Accuracy -0.8945\n",
      "Epoch 1435 | train: Loss 0.021100 Accuracy 0.2262 | validation: Loss 0.067825 Accuracy -3.2609\n",
      "Epoch 1436 | train: Loss 0.022439 Accuracy 0.1771 | validation: Loss 0.027543 Accuracy -0.7303\n",
      "Epoch 1437 | train: Loss 0.021988 Accuracy 0.1936 | validation: Loss 0.070163 Accuracy -3.4079\n",
      "Epoch 1438 | train: Loss 0.023081 Accuracy 0.1535 | validation: Loss 0.028893 Accuracy -0.8152\n",
      "Epoch 1439 | train: Loss 0.021573 Accuracy 0.2089 | validation: Loss 0.057570 Accuracy -2.6167\n",
      "Epoch 1440 | train: Loss 0.021284 Accuracy 0.2195 | validation: Loss 0.038649 Accuracy -1.4280\n",
      "Epoch 1441 | train: Loss 0.020193 Accuracy 0.2594 | validation: Loss 0.039659 Accuracy -1.4915\n",
      "Epoch 1442 | train: Loss 0.020228 Accuracy 0.2582 | validation: Loss 0.054972 Accuracy -2.4535\n",
      "Epoch 1443 | train: Loss 0.021077 Accuracy 0.2270 | validation: Loss 0.033245 Accuracy -1.0885\n",
      "Epoch 1444 | train: Loss 0.020984 Accuracy 0.2304 | validation: Loss 0.056369 Accuracy -2.5413\n",
      "Epoch 1445 | train: Loss 0.021222 Accuracy 0.2217 | validation: Loss 0.040028 Accuracy -1.5147\n",
      "Epoch 1446 | train: Loss 0.020448 Accuracy 0.2501 | validation: Loss 0.042821 Accuracy -1.6902\n",
      "Epoch 1447 | train: Loss 0.020407 Accuracy 0.2516 | validation: Loss 0.053765 Accuracy -2.3776\n",
      "Epoch 1448 | train: Loss 0.020895 Accuracy 0.2337 | validation: Loss 0.036653 Accuracy -1.3027\n",
      "Epoch 1449 | train: Loss 0.020768 Accuracy 0.2384 | validation: Loss 0.054559 Accuracy -2.4275\n",
      "Epoch 1450 | train: Loss 0.020906 Accuracy 0.2333 | validation: Loss 0.042142 Accuracy -1.6475\n",
      "Epoch 1451 | train: Loss 0.020384 Accuracy 0.2524 | validation: Loss 0.043729 Accuracy -1.7472\n",
      "Epoch 1452 | train: Loss 0.020332 Accuracy 0.2544 | validation: Loss 0.052184 Accuracy -2.2783\n",
      "Epoch 1453 | train: Loss 0.020619 Accuracy 0.2438 | validation: Loss 0.038250 Accuracy -1.4030\n",
      "Epoch 1454 | train: Loss 0.020484 Accuracy 0.2488 | validation: Loss 0.053068 Accuracy -2.3339\n",
      "Epoch 1455 | train: Loss 0.020639 Accuracy 0.2431 | validation: Loss 0.041320 Accuracy -1.5959\n",
      "Epoch 1456 | train: Loss 0.020246 Accuracy 0.2575 | validation: Loss 0.045420 Accuracy -1.8534\n",
      "Epoch 1457 | train: Loss 0.020197 Accuracy 0.2593 | validation: Loss 0.048319 Accuracy -2.0355\n",
      "Epoch 1458 | train: Loss 0.020271 Accuracy 0.2566 | validation: Loss 0.039588 Accuracy -1.4870\n",
      "Epoch 1459 | train: Loss 0.020213 Accuracy 0.2587 | validation: Loss 0.051766 Accuracy -2.2521\n",
      "Epoch 1460 | train: Loss 0.020446 Accuracy 0.2502 | validation: Loss 0.038880 Accuracy -1.4426\n",
      "Epoch 1461 | train: Loss 0.020170 Accuracy 0.2603 | validation: Loss 0.049265 Accuracy -2.0950\n",
      "Epoch 1462 | train: Loss 0.020221 Accuracy 0.2584 | validation: Loss 0.041806 Accuracy -1.6264\n",
      "Epoch 1463 | train: Loss 0.019987 Accuracy 0.2670 | validation: Loss 0.044558 Accuracy -1.7993\n",
      "Epoch 1464 | train: Loss 0.019962 Accuracy 0.2679 | validation: Loss 0.045848 Accuracy -1.8803\n",
      "Epoch 1465 | train: Loss 0.019965 Accuracy 0.2678 | validation: Loss 0.040934 Accuracy -1.5716\n",
      "Epoch 1466 | train: Loss 0.019903 Accuracy 0.2701 | validation: Loss 0.049058 Accuracy -2.0820\n",
      "Epoch 1467 | train: Loss 0.020062 Accuracy 0.2643 | validation: Loss 0.039003 Accuracy -1.4503\n",
      "Epoch 1468 | train: Loss 0.019931 Accuracy 0.2691 | validation: Loss 0.050963 Accuracy -2.2017\n",
      "Epoch 1469 | train: Loss 0.020144 Accuracy 0.2613 | validation: Loss 0.038102 Accuracy -1.3937\n",
      "Epoch 1470 | train: Loss 0.019958 Accuracy 0.2681 | validation: Loss 0.052094 Accuracy -2.2727\n",
      "Epoch 1471 | train: Loss 0.020198 Accuracy 0.2593 | validation: Loss 0.037454 Accuracy -1.3530\n",
      "Epoch 1472 | train: Loss 0.019988 Accuracy 0.2670 | validation: Loss 0.053123 Accuracy -2.3373\n",
      "Epoch 1473 | train: Loss 0.020270 Accuracy 0.2566 | validation: Loss 0.036565 Accuracy -1.2971\n",
      "Epoch 1474 | train: Loss 0.020052 Accuracy 0.2646 | validation: Loss 0.054488 Accuracy -2.4231\n",
      "Epoch 1475 | train: Loss 0.020409 Accuracy 0.2515 | validation: Loss 0.035315 Accuracy -1.2186\n",
      "Epoch 1476 | train: Loss 0.020180 Accuracy 0.2599 | validation: Loss 0.056252 Accuracy -2.5339\n",
      "Epoch 1477 | train: Loss 0.020635 Accuracy 0.2433 | validation: Loss 0.033982 Accuracy -1.1348\n",
      "Epoch 1478 | train: Loss 0.020365 Accuracy 0.2531 | validation: Loss 0.057823 Accuracy -2.6326\n",
      "Epoch 1479 | train: Loss 0.020883 Accuracy 0.2341 | validation: Loss 0.033255 Accuracy -1.0892\n",
      "Epoch 1480 | train: Loss 0.020508 Accuracy 0.2479 | validation: Loss 0.057668 Accuracy -2.6228\n",
      "Epoch 1481 | train: Loss 0.020939 Accuracy 0.2321 | validation: Loss 0.034170 Accuracy -1.1467\n",
      "Epoch 1482 | train: Loss 0.020421 Accuracy 0.2511 | validation: Loss 0.054122 Accuracy -2.4001\n",
      "Epoch 1483 | train: Loss 0.020600 Accuracy 0.2445 | validation: Loss 0.037728 Accuracy -1.3702\n",
      "Epoch 1484 | train: Loss 0.020122 Accuracy 0.2620 | validation: Loss 0.047779 Accuracy -2.0016\n",
      "Epoch 1485 | train: Loss 0.020139 Accuracy 0.2614 | validation: Loss 0.043805 Accuracy -1.7519\n",
      "Epoch 1486 | train: Loss 0.020025 Accuracy 0.2656 | validation: Loss 0.041821 Accuracy -1.6273\n",
      "Epoch 1487 | train: Loss 0.020046 Accuracy 0.2648 | validation: Loss 0.049808 Accuracy -2.1291\n",
      "Epoch 1488 | train: Loss 0.020282 Accuracy 0.2562 | validation: Loss 0.038842 Accuracy -1.4402\n",
      "Epoch 1489 | train: Loss 0.020194 Accuracy 0.2594 | validation: Loss 0.052053 Accuracy -2.2701\n",
      "Epoch 1490 | train: Loss 0.020433 Accuracy 0.2507 | validation: Loss 0.039396 Accuracy -1.4750\n",
      "Epoch 1491 | train: Loss 0.020179 Accuracy 0.2600 | validation: Loss 0.049667 Accuracy -2.1202\n",
      "Epoch 1492 | train: Loss 0.020254 Accuracy 0.2572 | validation: Loss 0.042654 Accuracy -1.6797\n",
      "Epoch 1493 | train: Loss 0.020064 Accuracy 0.2642 | validation: Loss 0.045298 Accuracy -1.8457\n",
      "Epoch 1494 | train: Loss 0.020064 Accuracy 0.2642 | validation: Loss 0.046728 Accuracy -1.9356\n",
      "Epoch 1495 | train: Loss 0.020090 Accuracy 0.2632 | validation: Loss 0.041829 Accuracy -1.6278\n",
      "Epoch 1496 | train: Loss 0.020045 Accuracy 0.2649 | validation: Loss 0.049467 Accuracy -2.1077\n",
      "Epoch 1497 | train: Loss 0.020195 Accuracy 0.2594 | validation: Loss 0.040322 Accuracy -1.5331\n",
      "Epoch 1498 | train: Loss 0.020067 Accuracy 0.2641 | validation: Loss 0.049913 Accuracy -2.1357\n",
      "Epoch 1499 | train: Loss 0.020208 Accuracy 0.2589 | validation: Loss 0.040511 Accuracy -1.5450\n",
      "Epoch 1500 | train: Loss 0.020033 Accuracy 0.2653 | validation: Loss 0.048617 Accuracy -2.0543\n",
      "Epoch 1500 | train: Loss 0.020033 Accuracy 0.2653 | validation: Loss 0.048617 Accuracy -2.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-19 23:54:54,598] Trial 2 failed with parameters: {'hidden_units': 12, 'depth': 4, 'learning_rate': 0.06119843831573892, 'weight_decay': 0.0837148242711029} because of the following error: The number of the values 1500 did not match the number of the objectives 1.\n",
      "[W 2024-05-19 23:54:54,598] Trial 2 failed with value [0.3300171494483948, 1.0743626356124878, 0.8095661401748657, 0.28172969818115234, 0.11027657985687256, 0.07658614963293076, 0.1010279580950737, 0.27349749207496643, 0.3289135992527008, 0.15662553906440735, 0.05847928300499916, 0.041382960975170135, 0.049007054418325424, 0.07590553909540176, 0.14961124956607819, 0.20759476721286774, 0.16724413633346558, 0.08889106661081314, 0.05549231171607971, 0.04843101277947426, 0.04859960824251175, 0.05450427159667015, 0.07065536826848984, 0.09068950265645981, 0.09579076617956161, 0.08076054602861404, 0.060309138149023056, 0.04883083328604698, 0.04559030383825302, 0.046523574739694595, 0.05180468037724495, 0.0521368533372879, 0.043261073529720306, 0.04029278829693794, 0.03962364047765732, 0.041882872581481934, 0.04644199460744858, 0.043634187430143356, 0.03780536353588104, 0.03560857102274895, 0.0348249115049839, 0.03517245873808861, 0.035892270505428314, 0.033326584845781326, 0.03209443762898445, 0.03213304281234741, 0.030442073941230774, 0.029298316687345505, 0.030245553702116013, 0.03286900371313095, 0.030795561149716377, 0.029055707156658173, 0.03195615112781525, 0.034417182207107544, 0.031478531658649445, 0.030759550631046295, 0.03495107218623161, 0.03526445850729942, 0.031766217201948166, 0.03400565683841705, 0.03714122995734215, 0.03370721638202667, 0.033368270844221115, 0.03713008016347885, 0.03481653705239296, 0.0331990085542202, 0.03688492625951767, 0.03533286973834038, 0.03368818759918213, 0.03738641366362572, 0.03547133877873421, 0.03474792093038559, 0.0381188839673996, 0.0354655459523201, 0.036526940762996674, 0.038514696061611176, 0.03571781516075134, 0.03900398686528206, 0.03757606819272041, 0.03774091228842735, 0.03985631465911865, 0.03727365657687187, 0.041335709393024445, 0.0372358001768589, 0.04285644739866257, 0.03650684654712677, 0.04584772512316704, 0.03404569625854492, 0.05117865651845932, 0.03212607279419899, 0.049501702189445496, 0.039330869913101196, 0.037335194647312164, 0.05017191544175148, 0.03484421595931053, 0.044711802154779434, 0.044269952923059464, 0.03587751090526581, 0.04901033267378807, 0.03839968144893646, 0.040782880038022995, 0.046982284635305405, 0.03630094602704048, 0.047132693231105804, 0.040401969105005264, 0.040456853806972504, 0.04690131917595863, 0.037126291543245316, 0.04816177859902382, 0.03891712799668312, 0.044427357614040375, 0.043393250554800034, 0.04045073688030243, 0.04759315401315689, 0.03758089616894722, 0.051564883440732956, 0.034923043102025986, 0.055292557924985886, 0.03403270244598389, 0.052877143025398254, 0.03915883228182793, 0.04311738908290863, 0.04846811294555664, 0.036567818373441696, 0.052980195730924606, 0.03688213601708412, 0.048045091331005096, 0.043412063270807266, 0.04049144685268402, 0.05022061616182327, 0.037185944616794586, 0.05155444145202637, 0.038761600852012634, 0.04737009108066559, 0.04361854866147041, 0.04220364987850189, 0.048554789274930954, 0.038807615637779236, 0.05207230895757675, 0.036848798394203186, 0.05412830039858818, 0.0363108292222023, 0.05346410721540451, 0.03821147233247757, 0.049260981380939484, 0.04255646467208862, 0.04409557208418846, 0.047382038086652756, 0.04024595394730568, 0.051438070833683014, 0.03748809173703194, 0.05497661605477333, 0.03550867736339569, 0.05684177204966545, 0.03584389016032219, 0.05332385376095772, 0.04056825116276741, 0.04545961692929268, 0.04796142503619194, 0.03948502242565155, 0.05298387631773949, 0.03780018165707588, 0.05251322686672211, 0.040319714695215225, 0.0476783849298954, 0.045373912900686264, 0.04272026941180229, 0.050023648887872696, 0.03973989188671112, 0.052783604711294174, 0.03850874677300453, 0.0536181665956974, 0.03862907737493515, 0.05270010605454445, 0.03991849347949028, 0.05054600536823273, 0.041944470256567, 0.04814863204956055, 0.044004376977682114, 0.0462556891143322, 0.04568302258849144, 0.04487564042210579, 0.04720339551568031, 0.04338807240128517, 0.049524616450071335, 0.04050547629594803, 0.05519154667854309, 0.03396761417388916, 0.0705815851688385, 0.02510504052042961, 0.0843803882598877, 0.03215092793107033, 0.03859870880842209, 0.06999856978654861, 0.03339949622750282, 0.03945337235927582, 0.06354738771915436, 0.03868608921766281, 0.0363583117723465, 0.059911344200372696, 0.04578492417931557, 0.034620534628629684, 0.05508539825677872, 0.05238332226872444, 0.035952333360910416, 0.048831772059202194, 0.05608806386590004, 0.03999384865164757, 0.044163648039102554, 0.0558142215013504, 0.03761735185980797, 0.05789685249328613, 0.046161990612745285, 0.03882816806435585, 0.05438873916864395, 0.0501529797911644, 0.03880319371819496, 0.0509767159819603, 0.05486293137073517, 0.032050345093011856, 0.06703025847673416, 0.0418117493391037, 0.038125764578580856, 0.06005581468343735, 0.046160586178302765, 0.03668612614274025, 0.05791350454092026, 0.04601970314979553, 0.03866831585764885, 0.053317684680223465, 0.0438358448445797, 0.04501327499747276, 0.05697546899318695, 0.022696541622281075, 0.16879352927207947, 0.03339345380663872, 0.020731577649712563, 0.08912625908851624, 0.12859313189983368, 0.041780803352594376, 0.021436991170048714, 0.034903980791568756, 0.09178822487592697, 0.09085116535425186, 0.02953369915485382, 0.020872674882411957, 0.03509833291172981, 0.09271057695150375, 0.06625945121049881, 0.035036519169807434, 0.02850661426782608, 0.044792965054512024, 0.048344653099775314, 0.02991233579814434, 0.08002281934022903, 0.03157030791044235, 0.04234454780817032, 0.07370616495609283, 0.07001763582229614, 0.046934593468904495, 0.03951587527990341, 0.04669451713562012, 0.06084766238927841, 0.06500906497240067, 0.05437741056084633, 0.04389422386884689, 0.04941762238740921, 0.054970309138298035, 0.02963321842253208, 0.08413571864366531, 0.019760798662900925, 0.04092175513505936, 0.08333854377269745, 0.06847824156284332, 0.045188695192337036, 0.03209926560521126, 0.036989662796258926, 0.05686677619814873, 0.06336407363414764, 0.04783608764410019, 0.035648759454488754, 0.041238341480493546, 0.05532975494861603, 0.055201224982738495, 0.042185381054878235, 0.036897409707307816, 0.04456467926502228, 0.05363984405994415, 0.050811562687158585, 0.04141979664564133, 0.03836262971162796, 0.044319998472929, 0.06800399720668793, 0.03012988530099392, 0.04205998405814171, 0.07784834504127502, 0.032487839460372925, 0.07480151206254959, 0.035410698503255844, 0.07496387511491776, 0.10774871706962585, 0.05247209221124649, 0.02477410063147545, 0.02580948919057846, 0.036838263273239136, 0.07324599474668503, 0.07018954306840897, 0.03688681125640869, 0.023906247690320015, 0.024117283523082733, 0.04009394347667694, 0.06651913374662399, 0.06729205697774887, 0.040369912981987, 0.025929829105734825, 0.02703382633626461, 0.044891856610774994, 0.06760627776384354, 0.060073040425777435, 0.03696556016802788, 0.040475282818078995, 0.05409996211528778, 0.05331854522228241, 0.04406557232141495, 0.0401669479906559, 0.04647762328386307, 0.05415827035903931, 0.04810416325926781, 0.04467437416315079, 0.05447915568947792, 0.05477778613567352, 0.04684189334511757, 0.04993702843785286, 0.05279680714011192, 0.04241206869482994, 0.04613129049539566, 0.049829721450805664, 0.04182606562972069, 0.04677444323897362, 0.05215822160243988, 0.043795567005872726, 0.041836682707071304, 0.049707602709531784, 0.04420717805624008, 0.04020881652832031, 0.04862403869628906, 0.042690787464380264, 0.040730562061071396, 0.048195142298936844, 0.040322430431842804, 0.04298808425664902, 0.046221595257520676, 0.04002394527196884, 0.0444130040705204, 0.04512501880526543, 0.041283637285232544, 0.04563063755631447, 0.045582037419080734, 0.0423753596842289, 0.046136654913425446, 0.0456693060696125, 0.04362170770764351, 0.04609609395265579, 0.04593503102660179, 0.04381059110164642, 0.04564010724425316, 0.04602719470858574, 0.044162310659885406, 0.045547693967819214, 0.046399109065532684, 0.044711802154779434, 0.045458391308784485, 0.046236179769039154, 0.04463617131114006, 0.045226916670799255, 0.04581949859857559, 0.04466710612177849, 0.04483674466609955, 0.045451268553733826, 0.04460581764578819, 0.04488954693078995, 0.045474015176296234, 0.04477783292531967, 0.04507342353463173, 0.04550300911068916, 0.04484044015407562, 0.04514157399535179, 0.04549185559153557, 0.04495261237025261, 0.0453597716987133, 0.0455947108566761, 0.045185498893260956, 0.04564725607633591, 0.045710865408182144, 0.04539056122303009, 0.04582170397043228, 0.04571142792701721, 0.04552217945456505, 0.04591784253716469, 0.045716118067502975, 0.045696329325437546, 0.046010229736566544, 0.04575998708605766, 0.04586132615804672, 0.046027228236198425, 0.0457659550011158, 0.04593207687139511, 0.04593849182128906, 0.04574074596166611, 0.045939281582832336, 0.04584624990820885, 0.0457787849009037, 0.04596785083413124, 0.04583510383963585, 0.045887526124715805, 0.04600358009338379, 0.045871175825595856, 0.0459955558180809, 0.046012211591005325, 0.04593522846698761, 0.046080082654953, 0.04602571576833725, 0.04603823274374008, 0.04615043103694916, 0.04607626795768738, 0.04616190120577812, 0.046202484518289566, 0.046154510229825974, 0.046259429305791855, 0.04622893035411835, 0.04623560607433319, 0.04630661755800247, 0.04625030979514122, 0.046308375895023346, 0.04632274806499481, 0.04629583656787872, 0.046368516981601715, 0.04634099453687668, 0.04636567458510399, 0.04640629515051842, 0.04637463018298149, 0.046427734196186066, 0.04642055928707123, 0.04641958326101303, 0.0464617945253849, 0.046434950083494186, 0.04647029563784599, 0.046479083597660065, 0.04647314175963402, 0.046515367925167084, 0.046501290053129196, 0.04652814939618111, 0.0465448834002018, 0.04653923213481903, 0.046575792133808136, 0.04656821861863136, 0.046589598059654236, 0.04660704731941223, 0.046605054289102554, 0.046637315303087234, 0.046634625643491745, 0.04665495082736015, 0.046670522540807724, 0.04667186737060547, 0.04669881612062454, 0.046697333455085754, 0.046716995537281036, 0.046728551387786865, 0.04673383757472038, 0.04675659164786339, 0.04675663262605667, 0.04677744209766388, 0.04678429290652275, 0.04679431766271591, 0.04681076109409332, 0.04681280255317688, 0.046832095831632614, 0.0468350388109684, 0.0468495637178421, 0.046859193593263626, 0.04686638340353966, 0.04688206687569618, 0.04688514024019241, 0.04690208286046982, 0.046906255185604095, 0.04691978916525841, 0.04692879319190979, 0.04693751037120819, 0.04695095494389534, 0.04695669189095497, 0.04697170853614807, 0.04697727411985397, 0.04699096456170082, 0.04699837788939476, 0.047009170055389404, 0.047019295394420624, 0.047027215361595154, 0.047039471566677094, 0.047045860439538956, 0.0470590703189373, 0.047064755111932755, 0.047078173607587814, 0.047083720564842224, 0.047096628695726395, 0.04710245877504349, 0.0471145324409008, 0.04712081328034401, 0.047132112085819244, 0.04713873192667961, 0.04714972525835037, 0.04715653881430626, 0.047167208045721054, 0.047173891216516495, 0.047184769064188004, 0.04719062149524689, 0.047202497720718384, 0.04720667004585266, 0.04722050204873085, 0.04722200334072113, 0.04723929986357689, 0.047236114740371704, 0.04725972190499306, 0.047247741371393204, 0.04728338122367859, 0.04725434631109238, 0.047313809394836426, 0.04725046083331108, 0.04735927656292915, 0.047223709523677826, 0.04743896424770355, 0.047144826501607895, 0.04759882763028145, 0.04694372043013573, 0.04795213043689728, 0.0464477501809597, 0.04879172146320343, 0.04523122310638428, 0.050903547555208206, 0.04231756553053856, 0.05637315288186073, 0.03634394705295563, 0.06809722632169724, 0.031047776341438293, 0.06588853150606155, 0.04381849244236946, 0.03559985011816025, 0.060179006308317184, 0.04819699749350548, 0.035115644335746765, 0.052443210035562515, 0.05558125302195549, 0.03797115758061409, 0.04499643296003342, 0.058521345257759094, 0.045140884816646576, 0.04059630259871483, 0.05461914837360382, 0.05263220891356468, 0.0411222018301487, 0.04812376946210861, 0.0555080808699131, 0.04524421691894531, 0.04463336616754532, 0.05424841493368149, 0.0485367514193058, 0.04325591027736664, 0.0511871874332428, 0.050996873527765274, 0.043918509036302567, 0.04774508252739906, 0.05195680633187294, 0.04588630050420761, 0.04552508518099785, 0.051127366721630096, 0.047782547771930695, 0.044801387935876846, 0.04984087124466896, 0.048948775976896286, 0.04497497156262398, 0.048867568373680115, 0.04953984543681145, 0.04553874954581261, 0.048165980726480484, 0.04963812977075577, 0.04602205753326416, 0.04769686236977577, 0.049479734152555466, 0.04636925458908081, 0.047466471791267395, 0.04925290122628212, 0.04660358279943466, 0.047414880245923996, 0.04904358834028244, 0.04674467816948891, 0.047487322241067886, 0.04883040115237236, 0.04683109372854233, 0.04763316735625267, 0.048626337200403214, 0.04688563942909241, 0.04779763147234917, 0.04842134937644005, 0.04694240912795067, 0.047948043793439865, 0.04819708690047264, 0.04701243340969086, 0.04807567596435547, 0.04797302559018135, 0.047122642397880554, 0.04816727340221405, 0.047767527401447296, 0.04728390648961067, 0.048216696828603745, 0.047609757632017136, 0.047494880855083466, 0.04820263013243675, 0.04751038923859596, 0.04770901799201965, 0.04812166839838028, 0.04747208207845688, 0.04789019748568535, 0.04798608273267746, 0.047505270689725876, 0.048018116503953934, 0.04784195125102997, 0.047613486647605896, 0.048075996339321136, 0.04772699624300003, 0.04776200279593468, 0.04804779216647148, 0.047666870057582855, 0.04790160804986954, 0.047953855246305466, 0.047684330493211746, 0.04799820855259895, 0.0478503555059433, 0.04777982085943222, 0.04802964627742767, 0.047788988798856735, 0.04790777340531349, 0.047990959137678146, 0.047794945538043976, 0.04800532013177872, 0.04791718348860741, 0.04786655679345131, 0.04803513363003731, 0.04786849021911621, 0.04796641692519188, 0.04799974709749222, 0.047882888466119766, 0.04803428798913956, 0.047941889613866806, 0.047951433807611465, 0.04803607240319252, 0.047920648008584976, 0.04802577942609787, 0.04799289628863335, 0.047961555421352386, 0.04805302619934082, 0.04796219617128372, 0.04802950844168663, 0.048026323318481445, 0.047985054552555084, 0.04806475341320038, 0.04799628257751465, 0.04804340377449989, 0.048048775643110275, 0.04801367595791817, 0.048079293221235275, 0.04802548512816429, 0.04806653782725334, 0.048067349940538406, 0.048045601695775986, 0.04809608310461044, 0.048052236437797546, 0.04809350147843361, 0.048083338886499405, 0.04807870090007782, 0.04811111092567444, 0.048077646642923355, 0.04811815917491913, 0.04809614643454552, 0.04811020940542221, 0.0481211356818676, 0.048104479908943176, 0.048137109726667404, 0.04811149463057518, 0.048140015453100204, 0.048128776252269745, 0.048136480152606964, 0.048147451132535934, 0.048135679215192795, 0.04816058650612831, 0.048142366111278534, 0.04816678166389465, 0.048155270516872406, 0.048169124871492386, 0.04817046597599983, 0.048171158879995346, 0.04818427190184593, 0.04817529767751694, 0.04819510132074356, 0.04818226024508476, 0.048202965408563614, 0.048191413283348083, 0.04820916801691055, 0.048201341181993484, 0.048214640468358994, 0.04821132495999336, 0.048219963908195496, 0.04822120442986488, 0.048225369304418564, 0.0482306033372879, 0.04823112115263939, 0.048239897936582565, 0.048237018287181854, 0.04824928939342499, 0.048242393881082535, 0.048259612172842026, 0.048246510326862335, 0.04827166721224785, 0.04824801906943321, 0.0482872799038887, 0.04824449494481087, 0.048310019075870514, 0.048230648040771484, 0.048347752541303635, 0.04819464311003685, 0.048418283462524414, 0.04811021313071251, 0.04856181889772415, 0.047916095703840256, 0.04887599125504494, 0.04746558144688606, 0.0496038943529129, 0.046403124928474426, 0.05138251930475235, 0.0438871756196022, 0.0559629388153553, 0.038249704986810684, 0.06793460249900818, 0.028812717646360397, 0.08993957936763763, 0.024154402315616608, 0.07914599031209946, 0.040975604206323624, 0.03477016091346741, 0.07506010681390762, 0.03653827682137489, 0.03951467201113701, 0.06828166544437408, 0.039596863090991974, 0.039569683372974396, 0.06513053923845291, 0.044893309473991394, 0.03848528489470482, 0.06272915750741959, 0.05069608986377716, 0.038071878254413605, 0.05940530449151993, 0.05480431020259857, 0.03920451179146767, 0.05627325549721718, 0.05620457977056503, 0.0408615842461586, 0.05308253690600395, 0.05647081509232521, 0.04237585887312889, 0.050648223608732224, 0.05579053983092308, 0.0439000129699707, 0.04898920655250549, 0.054674241691827774, 0.04443180561065674, 0.0504622682929039, 0.05277906358242035, 0.04470321908593178, 0.050421666353940964, 0.051660992205142975, 0.045266833156347275, 0.05027246102690697, 0.05083208531141281, 0.045531757175922394, 0.050512347370386124, 0.04983885586261749, 0.04592794552445412, 0.05093446373939514, 0.04807836934924126, 0.0470414012670517, 0.0510338619351387, 0.047133564949035645, 0.04852651059627533, 0.05020223557949066, 0.04687744379043579, 0.04959262162446976, 0.04905112832784653, 0.047237079590559006, 0.05010183900594711, 0.04779261723160744, 0.048307113349437714, 0.0496254488825798, 0.04718731343746185, 0.04935358837246895, 0.048293665051460266, 0.047740448266267776, 0.04937770217657089, 0.04731030389666557, 0.04879666492342949, 0.048405811190605164, 0.047599587589502335, 0.04914911091327667, 0.04757342487573624, 0.04857003316283226, 0.0485234372317791, 0.047753844410181046, 0.04902219772338867, 0.047790613025426865, 0.048606038093566895, 0.04849148914217949, 0.04801860824227333, 0.04890575259923935, 0.04796021431684494, 0.04866749420762062, 0.048414383083581924, 0.04818335175514221, 0.048784080892801285, 0.04803568497300148, 0.04868796467781067, 0.04832232743501663, 0.04833278805017471, 0.04865620285272598, 0.048127952963113785, 0.04870688542723656, 0.04822968319058418, 0.0484948605298996, 0.04848024621605873, 0.04826822876930237, 0.04864849895238876, 0.048217933624982834, 0.04862638935446739, 0.048350073397159576, 0.0484815388917923, 0.048539549112319946, 0.04835224896669388, 0.04865178093314171, 0.048323940485715866, 0.04864511266350746, 0.04839762672781944, 0.04855801910161972, 0.04851459339261055, 0.048463210463523865, 0.04861358180642128, 0.04841458797454834, 0.04865844175219536, 0.04842386022210121, 0.04864634945988655, 0.048472486436367035, 0.04859812185168266, 0.04853333905339241, 0.04854079708456993, 0.04858529567718506, 0.04849481210112572, 0.04861968010663986, 0.04847048595547676, 0.04863816127181053, 0.04846834018826485, 0.04864531382918358, 0.048481497913599014, 0.04864632710814476, 0.048501379787921906, 0.048644717782735825, 0.04852061718702316, 0.04864301532506943, 0.0485343262553215, 0.048643533140420914, 0.04854130744934082, 0.04864894971251488, 0.04854152351617813, 0.048661597073078156, 0.048533666878938675, 0.048684366047382355, 0.0485139824450016, 0.04872256889939308, 0.04847436770796776, 0.0487859807908535, 0.048400115221738815, 0.04889514297246933, 0.04826349392533302, 0.04909097030758858, 0.04800967872142792, 0.04945513233542442, 0.04753030464053154, 0.050153881311416626, 0.0466151162981987, 0.0515240877866745, 0.0448901504278183, 0.05419943109154701, 0.04186365380883217, 0.059243977069854736, 0.03694126754999161, 0.06842131167650223, 0.031223610043525696, 0.0782293975353241, 0.029100751504302025, 0.07253534346818924, 0.03761841729283333, 0.048004280775785446, 0.0578656904399395, 0.03282482549548149, 0.07832042872905731, 0.03229159489274025, 0.05234331265091896, 0.06057382747530937, 0.03367455303668976, 0.06252880394458771, 0.050119150429964066, 0.04113100841641426, 0.11068575829267502, 0.0674581453204155, 0.3464023172855377, 0.390062153339386, 0.08527401089668274, 0.09760069847106934, 0.031729958951473236, 0.1907583475112915, 0.2041538655757904, 0.07661204040050507, 0.01995270699262619, 0.01985863223671913, 0.04314591735601425, 0.11317941546440125, 0.12560458481311798, 1.6703521013259888, 0.40619832277297974, 0.32556161284446716, 0.33959588408470154, 0.318951278924942, 0.2721389830112457, 0.17953315377235413, 0.14500245451927185, 0.14124324917793274, 0.09051782637834549, 0.15121009945869446, 0.0545576810836792, 0.9148429036140442, 1.2081005573272705, 0.7839670777320862, 0.2378660887479782, 1.0977377891540527, 1.7525453567504883, 5.693780422210693, 0.08544392883777618, 0.06258343905210495, 0.046091478317976, 0.09199971705675125, 0.035527173429727554, 0.025691010057926178, 0.0326654277741909, 0.039545729756355286, 0.04852347820997238, 0.05904423072934151, 0.07014818489551544, 0.06631115078926086, 0.20421789586544037, 0.029022205621004105, 16.868473052978516, 1.4679863452911377, 0.038551412522792816, 0.019043954089283943, 0.055721040815114975, 0.10017214715480804, 0.15597152709960938, 0.5502309203147888, 0.23807096481323242, 0.14011357724666595, 0.2110283076763153, 0.14568641781806946, 0.4613102674484253, 0.08877856284379959, 0.04060543328523636, 0.018804265186190605, 0.02600286528468132, 0.043813083320856094, 0.052065879106521606, 0.016975868493318558, 0.020149558782577515, 0.026175063103437424, 0.03529848903417587, 0.04894197732210159, 0.0709199458360672, 0.05973570793867111, 0.045798156410455704, 0.037569642066955566, 0.06035535782575607, 0.042675577104091644, 0.053867749869823456, 0.05559421703219414, 0.04813604801893234, 0.07027049362659454, 0.04069741815328598, 0.07714175432920456, 0.022425567731261253, 0.26746147871017456, 0.21999163925647736, 0.0888691321015358, 0.1984557956457138, 0.1970408409833908, 0.23322205245494843, 0.22517536580562592, 0.20911043882369995, 0.18574146926403046, 0.15831221640110016, 0.12996424734592438, 0.10321308672428131, 0.07960466295480728, 0.05984614044427872, 0.04458111152052879, 0.03431003913283348, 0.028856486082077026, 0.027312109246850014, 0.028189359232783318, 0.02860420197248459, 0.029352577403187752, 0.03121970221400261, 0.034379273653030396, 0.0398058257997036, 0.04537111893296242, 0.05353104695677757, 0.06320632994174957, 0.06702584773302078, 0.06254415214061737, 0.055688440799713135, 0.059721313416957855, 0.06700552999973297, 0.06857390701770782, 0.061418380588293076, 0.05803585797548294, 0.0672997459769249, 0.04241524636745453, 0.05602341145277023, 0.03330421447753906, 0.055646080523729324, 0.022442227229475975, 0.13336284458637238, 0.02113097906112671, 0.07701094448566437, 0.11468836665153503, 0.07752827554941177, 0.02616487815976143, 0.027340248227119446, 0.07354441285133362, 0.07812855392694473, 0.03749118745326996, 0.028511637821793556, 0.06999455392360687, 0.1009146049618721, 0.0748208537697792, 0.03390311077237129, 0.04684020206332207, 0.08510519564151764, 0.06464292109012604, 0.03031454235315323, 0.034599561244249344, 0.06237077713012695, 0.044449590146541595, 0.029814112931489944, 0.055931273847818375, 0.05920201912522316, 0.03300362452864647, 0.049898602068424225, 0.06150636821985245, 0.0361938439309597, 0.045362167060375214, 0.05759218707680702, 0.03617662936449051, 0.04400281608104706, 0.05303693562746048, 0.03595362976193428, 0.0478738434612751, 0.049917664378881454, 0.037726473063230515, 0.05531315505504608, 0.042815156280994415, 0.04359431564807892, 0.0525100976228714, 0.033725716173648834, 0.06065230071544647, 0.03364018350839615, 0.04867600277066231, 0.044340260326862335, 0.036773648113012314, 0.05530886352062225, 0.03501731902360916, 0.05170796066522598, 0.041709236800670624, 0.04145204275846481, 0.0506131537258625, 0.03636925667524338, 0.052730560302734375, 0.0376167893409729, 0.04756076633930206, 0.042802680283784866, 0.04139500483870506, 0.048241615295410156, 0.037724122405052185, 0.05159906670451164, 0.036096252501010895, 0.053615763783454895, 0.034979503601789474, 0.05599615350365639, 0.03305112197995186, 0.05994541943073273, 0.030098432675004005, 0.06537036597728729, 0.02771417610347271, 0.06696100533008575, 0.02903934381902218, 0.05454447120428085, 0.038849543780088425, 0.03778928145766258, 0.05469405651092529, 0.032824352383613586, 0.05414045602083206, 0.039975229650735855, 0.04107339680194855, 0.05270950496196747, 0.03535514324903488, 0.054364267736673355, 0.03891438990831375, 0.045446060597896576, 0.047218792140483856, 0.0380333736538887, 0.053063973784446716, 0.0350327342748642, 0.05371609330177307, 0.03462890908122063, 0.05196494236588478, 0.0350741483271122, 0.05071861669421196, 0.03524673730134964, 0.05128711834549904, 0.034325119107961655, 0.05440107360482216, 0.031714797019958496, 0.0614718422293663, 0.027618413791060448, 0.07336921244859695, 0.024377357214689255, 0.07930268347263336, 0.02595183439552784, 0.05477112904191017, 0.041569121181964874, 0.0313277542591095, 0.06301503628492355, 0.03165612369775772, 0.0451568141579628, 0.052136752754449844, 0.03260650113224983, 0.05777371674776077, 0.04100707173347473, 0.03945259004831314, 0.058067381381988525, 0.035282738506793976, 0.05116374418139458, 0.04575538635253906, 0.03782448545098305, 0.05556095391511917, 0.0354403592646122, 0.05012453719973564, 0.04141293466091156, 0.04025755822658539, 0.049227192997932434, 0.03493514657020569, 0.053025972098112106, 0.03358940780162811, 0.052845846861600876, 0.034092016518116, 0.051400333642959595, 0.035066183656454086, 0.05046587809920311, 0.035732660442590714, 0.050623588263988495, 0.035621415823698044, 0.05196419730782509, 0.03449816256761551, 0.05453803017735481, 0.03248356282711029, 0.05822456628084183, 0.030260568484663963, 0.061608098447322845, 0.029195785522460938, 0.06050001457333565, 0.03126427158713341, 0.051601968705654144, 0.03857385739684105, 0.04055177792906761, 0.049227841198444366, 0.034946415573358536, 0.053949300199747086, 0.03619501367211342, 0.048682745546102524, 0.0427471399307251, 0.04105589538812637, 0.049858417361974716, 0.03717966377735138, 0.052006326615810394, 0.037427838891744614, 0.04898323491215706, 0.04022708535194397, 0.0443483330309391, 0.043765295296907425, 0.0405549630522728, 0.04690810292959213, 0.03785644471645355, 0.049791574478149414, 0.03544270247220993, 0.05360998585820198, 0.032351285219192505, 0.06029542535543442, 0.02820941060781479, 0.0714959055185318, 0.0245314072817564, 0.08054592460393906, 0.02467782236635685, 0.06291258335113525, 0.03589129447937012, 0.034647468477487564, 0.06069183349609375, 0.029790503904223442, 0.05155101418495178, 0.046142514795064926, 0.03377324342727661, 0.06043769046664238, 0.037861790508031845, 0.042597196996212006, 0.05628049001097679, 0.03528732433915138, 0.05294184386730194, 0.04522976651787758, 0.038591016083955765, 0.05583590641617775, 0.03691786155104637, 0.04817787930369377, 0.04550071060657501, 0.03808801993727684, 0.05241049453616142, 0.03595287725329399, 0.049316998571157455, 0.039937861263751984, 0.0423235148191452, 0.04597382992506027, 0.037574149668216705, 0.0504036583006382, 0.035627659410238266, 0.05233784019947052, 0.035213250666856766, 0.05272936820983887, 0.035301562398672104, 0.05250643566250801, 0.03543419763445854, 0.05202815681695938, 0.03561009466648102, 0.05125134810805321, 0.03608023002743721, 0.049991510808467865, 0.03712906315922737, 0.04817264527082443, 0.038879312574863434, 0.04598680138587952, 0.04115976020693779, 0.04383125901222229, 0.04353208839893341, 0.042066168040037155, 0.04551779851317406, 0.04083525761961937, 0.046860065311193466, 0.0400644913315773, 0.047603409737348557, 0.039558105170726776, 0.04799327626824379, 0.03909383714199066, 0.04835100471973419, 0.03847341239452362, 0.04901646450161934, 0.03752700611948967, 0.05035635456442833, 0.036092475056648254, 0.052790023386478424, 0.03403947502374649, 0.05672159790992737, 0.03146056830883026, 0.061964910477399826, 0.029094332829117775, 0.0658283606171608, 0.0286561306566, 0.06176823750138283, 0.03283456340432167, 0.048405762761831284, 0.0438561737537384, 0.03695474565029144, 0.05498727783560753, 0.03439974784851074, 0.053192637860774994, 0.04040466621518135, 0.04313662275671959, 0.050261300057172775, 0.03726092725992203, 0.053691621869802475, 0.03832095116376877, 0.0483214408159256, 0.04398859664797783, 0.04154748097062111, 0.04964720830321312, 0.03798403590917587, 0.05154956132173538, 0.03758455067873001, 0.04992637410759926, 0.03901195153594017, 0.04704095795750618, 0.04107258468866348, 0.044486649334430695, 0.043081626296043396, 0.042633164674043655, 0.04493248090147972, 0.041112225502729416, 0.047002293169498444, 0.039220862090587616, 0.05017949268221855, 0.036060746759176254, 0.05633661523461342, 0.030823642387986183, 0.06922903656959534, 0.02450419031083584, 0.0905795693397522, 0.02189479023218155, 0.08690910786390305, 0.028375018388032913, 0.037068676203489304, 0.06377837806940079, 0.02715589851140976, 0.0512489378452301, 0.050561804324388504, 0.03039185330271721, 0.060357820242643356, 0.04633616283535957, 0.034125544130802155, 0.06514299660921097, 0.043725237250328064, 0.0372416190803051, 0.06554070115089417, 0.04056129232048988, 0.04043188318610191, 0.06211192533373833, 0.03732160106301308, 0.045134566724300385, 0.05513843148946762, 0.035422034561634064, 0.05135959014296532, 0.045539937913417816, 0.037081893533468246, 0.054416999220848083, 0.03736930340528488, 0.044683728367090225, 0.047628436237573624, 0.036579545587301254, 0.05247890576720238, 0.037870511412620544, 0.046614669263362885, 0.045130226761102676, 0.03994181379675865, 0.05131541192531586, 0.037678830325603485, 0.05136537551879883, 0.03939614072442055, 0.0468498133122921, 0.04323779046535492, 0.04193708673119545, 0.04691307246685028, 0.03904595971107483, 0.04857982322573662, 0.038604650646448135, 0.04777718707919121, 0.04017600789666176, 0.04540093243122101, 0.0428948812186718, 0.04286380112171173, 0.04555832967162132, 0.041230667382478714, 0.04704369977116585, 0.04091436415910721, 0.04690447449684143, 0.04178585484623909, 0.0455486923456192, 0.043334685266017914, 0.04383591189980507, 0.04485807567834854, 0.042512379586696625, 0.04576502740383148, 0.04193207249045372, 0.0458466000854969, 0.04209074005484581, 0.045293696224689484, 0.04276895523071289, 0.04448183253407478, 0.04366665706038475, 0.04373111203312874, 0.04450972005724907, 0.04319436848163605, 0.04512287676334381, 0.0428752601146698, 0.04545443877577782, 0.04270409420132637, 0.04555623233318329, 0.042603954672813416, 0.04553713649511337, 0.04251992329955101, 0.04551521688699722, 0.042413439601659775, 0.04558870196342468, 0.04224233701825142, 0.04583236575126648, 0.04194354638457298, 0.046318378299474716, 0.04142238572239876, 0.04715467616915703, 0.04054267331957817, 0.04853920266032219, 0.039116621017456055, 0.05082978680729866, 0.03691737726330757, 0.05460720509290695, 0.03380552679300308, 0.06051213666796684, 0.030156491324305534, 0.06782460957765579, 0.027542613446712494, 0.0701630711555481, 0.028893426060676575, 0.05757007375359535, 0.03864864259958267, 0.039658818393945694, 0.054972127079963684, 0.03324461728334427, 0.05636943131685257, 0.04002775624394417, 0.04282144084572792, 0.05376455932855606, 0.036653291434049606, 0.05455882474780083, 0.04214200749993324, 0.043728914111852646, 0.05218374356627464, 0.038250237703323364, 0.05306750908493996, 0.04132039099931717, 0.04541976377367973, 0.04831855744123459, 0.039587996900081635, 0.05176633968949318, 0.0388801172375679, 0.04926527291536331, 0.04180586338043213, 0.04455791786313057, 0.04584839195013046, 0.04093359783291817, 0.04905810207128525, 0.03900253027677536, 0.050963472574949265, 0.03810211271047592, 0.05209381878376007, 0.03745430335402489, 0.05312291905283928, 0.0365653932094574, 0.05448811501264572, 0.03531547635793686, 0.05625186860561371, 0.03398166596889496, 0.0578228235244751, 0.03325524181127548, 0.057667527347803116, 0.03416995704174042, 0.054121777415275574, 0.037728145718574524, 0.047779057174921036, 0.04380461946129799, 0.041820939630270004, 0.0498078428208828, 0.03884227201342583, 0.05205332860350609, 0.03939644992351532, 0.04966728016734123, 0.042654260993003845, 0.04529758542776108, 0.04672791436314583, 0.04182877019047737, 0.04946693032979965, 0.04032161086797714, 0.049912795424461365, 0.04051116481423378, 0.048617035150527954].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4361a46366e74f2aa193f00440640454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train: Loss 0.410965 Accuracy -14.0715 | validation: Loss 0.209319 Accuracy -12.1500\n",
      "Epoch 2 | train: Loss 0.119602 Accuracy -3.3862 | validation: Loss 0.394740 Accuracy -23.7987\n",
      "Epoch 3 | train: Loss 0.100344 Accuracy -2.6799 | validation: Loss 0.333001 Accuracy -19.9201\n",
      "Epoch 4 | train: Loss 0.052734 Accuracy -0.9339 | validation: Loss 0.172502 Accuracy -9.8371\n",
      "Epoch 5 | train: Loss 0.031716 Accuracy -0.1631 | validation: Loss 0.113265 Accuracy -6.1157\n",
      "Epoch 6 | train: Loss 0.036523 Accuracy -0.3394 | validation: Loss 0.094786 Accuracy -4.9548\n",
      "Epoch 7 | train: Loss 0.036634 Accuracy -0.3435 | validation: Loss 0.082606 Accuracy -4.1895\n",
      "Epoch 8 | train: Loss 0.029673 Accuracy -0.0882 | validation: Loss 0.088605 Accuracy -4.5664\n",
      "Epoch 9 | train: Loss 0.021702 Accuracy 0.2041 | validation: Loss 0.122480 Accuracy -6.6945\n",
      "Epoch 10 | train: Loss 0.020882 Accuracy 0.2342 | validation: Loss 0.141682 Accuracy -7.9009\n",
      "Epoch 11 | train: Loss 0.023222 Accuracy 0.1484 | validation: Loss 0.120960 Accuracy -6.5990\n",
      "Epoch 12 | train: Loss 0.021414 Accuracy 0.2147 | validation: Loss 0.081200 Accuracy -4.1012\n",
      "Epoch 13 | train: Loss 0.017615 Accuracy 0.3540 | validation: Loss 0.051713 Accuracy -2.2488\n",
      "Epoch 14 | train: Loss 0.016955 Accuracy 0.3782 | validation: Loss 0.042891 Accuracy -1.6946\n",
      "Epoch 15 | train: Loss 0.018375 Accuracy 0.3261 | validation: Loss 0.041782 Accuracy -1.6248\n",
      "Epoch 16 | train: Loss 0.017165 Accuracy 0.3705 | validation: Loss 0.046010 Accuracy -1.8905\n",
      "Epoch 17 | train: Loss 0.015268 Accuracy 0.4401 | validation: Loss 0.056077 Accuracy -2.5229\n",
      "Epoch 18 | train: Loss 0.014952 Accuracy 0.4517 | validation: Loss 0.064619 Accuracy -3.0595\n",
      "Epoch 19 | train: Loss 0.015388 Accuracy 0.4357 | validation: Loss 0.064791 Accuracy -3.0704\n",
      "Epoch 20 | train: Loss 0.014873 Accuracy 0.4545 | validation: Loss 0.058575 Accuracy -2.6799\n",
      "Epoch 21 | train: Loss 0.013654 Accuracy 0.4993 | validation: Loss 0.052534 Accuracy -2.3004\n",
      "Epoch 22 | train: Loss 0.012776 Accuracy 0.5315 | validation: Loss 0.049025 Accuracy -2.0799\n",
      "Epoch 23 | train: Loss 0.012731 Accuracy 0.5331 | validation: Loss 0.049032 Accuracy -2.0803\n",
      "Epoch 24 | train: Loss 0.012882 Accuracy 0.5276 | validation: Loss 0.053102 Accuracy -2.3360\n",
      "Epoch 25 | train: Loss 0.012354 Accuracy 0.5470 | validation: Loss 0.058430 Accuracy -2.6708\n",
      "Epoch 26 | train: Loss 0.011620 Accuracy 0.5739 | validation: Loss 0.065433 Accuracy -3.1107\n",
      "Epoch 27 | train: Loss 0.011462 Accuracy 0.5796 | validation: Loss 0.072421 Accuracy -3.5497\n",
      "Epoch 28 | train: Loss 0.011663 Accuracy 0.5723 | validation: Loss 0.074859 Accuracy -3.7028\n",
      "Epoch 29 | train: Loss 0.011475 Accuracy 0.5792 | validation: Loss 0.071843 Accuracy -3.5134\n",
      "Epoch 30 | train: Loss 0.010884 Accuracy 0.6008 | validation: Loss 0.067090 Accuracy -3.2148\n",
      "Epoch 31 | train: Loss 0.010581 Accuracy 0.6120 | validation: Loss 0.064493 Accuracy -3.0517\n",
      "Epoch 32 | train: Loss 0.010699 Accuracy 0.6076 | validation: Loss 0.065598 Accuracy -3.1211\n",
      "Epoch 33 | train: Loss 0.010556 Accuracy 0.6129 | validation: Loss 0.070386 Accuracy -3.4219\n",
      "Epoch 34 | train: Loss 0.010061 Accuracy 0.6310 | validation: Loss 0.077693 Accuracy -3.8809\n",
      "Epoch 35 | train: Loss 0.009850 Accuracy 0.6388 | validation: Loss 0.082182 Accuracy -4.1629\n",
      "Epoch 36 | train: Loss 0.009838 Accuracy 0.6392 | validation: Loss 0.080782 Accuracy -4.0750\n",
      "Epoch 37 | train: Loss 0.009495 Accuracy 0.6518 | validation: Loss 0.075034 Accuracy -3.7139\n",
      "Epoch 38 | train: Loss 0.009044 Accuracy 0.6683 | validation: Loss 0.069510 Accuracy -3.3668\n",
      "Epoch 39 | train: Loss 0.008957 Accuracy 0.6715 | validation: Loss 0.067990 Accuracy -3.2713\n",
      "Epoch 40 | train: Loss 0.008807 Accuracy 0.6770 | validation: Loss 0.070106 Accuracy -3.4042\n",
      "Epoch 41 | train: Loss 0.008434 Accuracy 0.6907 | validation: Loss 0.073652 Accuracy -3.6271\n",
      "Epoch 42 | train: Loss 0.008298 Accuracy 0.6957 | validation: Loss 0.075401 Accuracy -3.7369\n",
      "Epoch 43 | train: Loss 0.008253 Accuracy 0.6973 | validation: Loss 0.073462 Accuracy -3.6151\n",
      "Epoch 44 | train: Loss 0.007983 Accuracy 0.7073 | validation: Loss 0.070039 Accuracy -3.4000\n",
      "Epoch 45 | train: Loss 0.007737 Accuracy 0.7163 | validation: Loss 0.067805 Accuracy -3.2597\n",
      "Epoch 46 | train: Loss 0.007574 Accuracy 0.7222 | validation: Loss 0.067545 Accuracy -3.2434\n",
      "Epoch 47 | train: Loss 0.007449 Accuracy 0.7268 | validation: Loss 0.068815 Accuracy -3.3231\n",
      "Epoch 48 | train: Loss 0.007406 Accuracy 0.7284 | validation: Loss 0.070171 Accuracy -3.4083\n",
      "Epoch 49 | train: Loss 0.007383 Accuracy 0.7293 | validation: Loss 0.070611 Accuracy -3.4360\n",
      "Epoch 50 | train: Loss 0.007308 Accuracy 0.7320 | validation: Loss 0.070927 Accuracy -3.4558\n",
      "Epoch 51 | train: Loss 0.007254 Accuracy 0.7340 | validation: Loss 0.072707 Accuracy -3.5677\n",
      "Epoch 52 | train: Loss 0.007211 Accuracy 0.7355 | validation: Loss 0.076168 Accuracy -3.7851\n",
      "Epoch 53 | train: Loss 0.007133 Accuracy 0.7384 | validation: Loss 0.080634 Accuracy -4.0657\n",
      "Epoch 54 | train: Loss 0.007092 Accuracy 0.7399 | validation: Loss 0.083822 Accuracy -4.2659\n",
      "Epoch 55 | train: Loss 0.007040 Accuracy 0.7418 | validation: Loss 0.085414 Accuracy -4.3660\n",
      "Epoch 56 | train: Loss 0.006995 Accuracy 0.7435 | validation: Loss 0.090248 Accuracy -4.6696\n",
      "Epoch 57 | train: Loss 0.006954 Accuracy 0.7450 | validation: Loss 0.099096 Accuracy -5.2255\n",
      "Epoch 58 | train: Loss 0.006916 Accuracy 0.7464 | validation: Loss 0.102206 Accuracy -5.4209\n",
      "Epoch 59 | train: Loss 0.006870 Accuracy 0.7481 | validation: Loss 0.106124 Accuracy -5.6670\n",
      "Epoch 60 | train: Loss 0.006838 Accuracy 0.7492 | validation: Loss 0.125117 Accuracy -6.8602\n",
      "Epoch 61 | train: Loss 0.006852 Accuracy 0.7487 | validation: Loss 0.110212 Accuracy -5.9238\n",
      "Epoch 62 | train: Loss 0.006922 Accuracy 0.7462 | validation: Loss 0.126815 Accuracy -6.9669\n",
      "Epoch 63 | train: Loss 0.006755 Accuracy 0.7523 | validation: Loss 0.133030 Accuracy -7.3573\n",
      "Epoch 64 | train: Loss 0.006780 Accuracy 0.7513 | validation: Loss 0.120841 Accuracy -6.5916\n",
      "Epoch 65 | train: Loss 0.006807 Accuracy 0.7504 | validation: Loss 0.128315 Accuracy -7.0611\n",
      "Epoch 66 | train: Loss 0.006683 Accuracy 0.7549 | validation: Loss 0.140861 Accuracy -7.8493\n",
      "Epoch 67 | train: Loss 0.006789 Accuracy 0.7510 | validation: Loss 0.128748 Accuracy -7.0884\n",
      "Epoch 68 | train: Loss 0.006670 Accuracy 0.7554 | validation: Loss 0.126245 Accuracy -6.9311\n",
      "Epoch 69 | train: Loss 0.006692 Accuracy 0.7546 | validation: Loss 0.136419 Accuracy -7.5703\n",
      "Epoch 70 | train: Loss 0.006692 Accuracy 0.7546 | validation: Loss 0.132434 Accuracy -7.3199\n",
      "Epoch 71 | train: Loss 0.006641 Accuracy 0.7565 | validation: Loss 0.123803 Accuracy -6.7777\n",
      "Epoch 72 | train: Loss 0.006697 Accuracy 0.7544 | validation: Loss 0.133894 Accuracy -7.4116\n",
      "Epoch 73 | train: Loss 0.006614 Accuracy 0.7574 | validation: Loss 0.142281 Accuracy -7.9385\n",
      "Epoch 74 | train: Loss 0.006655 Accuracy 0.7559 | validation: Loss 0.135611 Accuracy -7.5195\n",
      "Epoch 75 | train: Loss 0.006612 Accuracy 0.7575 | validation: Loss 0.137642 Accuracy -7.6471\n",
      "Epoch 76 | train: Loss 0.006599 Accuracy 0.7580 | validation: Loss 0.147939 Accuracy -8.2939\n",
      "Epoch 77 | train: Loss 0.006606 Accuracy 0.7577 | validation: Loss 0.142591 Accuracy -7.9580\n",
      "Epoch 78 | train: Loss 0.006550 Accuracy 0.7598 | validation: Loss 0.136918 Accuracy -7.6016\n",
      "Epoch 79 | train: Loss 0.006572 Accuracy 0.7590 | validation: Loss 0.143178 Accuracy -7.9948\n",
      "Epoch 80 | train: Loss 0.006532 Accuracy 0.7605 | validation: Loss 0.142341 Accuracy -7.9423\n",
      "Epoch 81 | train: Loss 0.006519 Accuracy 0.7609 | validation: Loss 0.131875 Accuracy -7.2847\n",
      "Epoch 82 | train: Loss 0.006534 Accuracy 0.7604 | validation: Loss 0.140883 Accuracy -7.8507\n",
      "Epoch 83 | train: Loss 0.006485 Accuracy 0.7622 | validation: Loss 0.143107 Accuracy -7.9904\n",
      "Epoch 84 | train: Loss 0.006471 Accuracy 0.7627 | validation: Loss 0.136529 Accuracy -7.5772\n",
      "Epoch 85 | train: Loss 0.006478 Accuracy 0.7624 | validation: Loss 0.143239 Accuracy -7.9987\n",
      "Epoch 86 | train: Loss 0.006435 Accuracy 0.7640 | validation: Loss 0.141533 Accuracy -7.8915\n",
      "Epoch 87 | train: Loss 0.006417 Accuracy 0.7647 | validation: Loss 0.132135 Accuracy -7.3011\n",
      "Epoch 88 | train: Loss 0.006422 Accuracy 0.7645 | validation: Loss 0.138324 Accuracy -7.6899\n",
      "Epoch 89 | train: Loss 0.006394 Accuracy 0.7655 | validation: Loss 0.129541 Accuracy -7.1382\n",
      "Epoch 90 | train: Loss 0.006361 Accuracy 0.7667 | validation: Loss 0.129632 Accuracy -7.1439\n",
      "Epoch 91 | train: Loss 0.006338 Accuracy 0.7676 | validation: Loss 0.133093 Accuracy -7.3613\n",
      "Epoch 92 | train: Loss 0.006328 Accuracy 0.7679 | validation: Loss 0.124467 Accuracy -6.8194\n",
      "Epoch 93 | train: Loss 0.006343 Accuracy 0.7674 | validation: Loss 0.142713 Accuracy -7.9656\n",
      "Epoch 94 | train: Loss 0.006453 Accuracy 0.7633 | validation: Loss 0.107923 Accuracy -5.7800\n",
      "Epoch 95 | train: Loss 0.006885 Accuracy 0.7475 | validation: Loss 0.142597 Accuracy -7.9584\n",
      "Epoch 96 | train: Loss 0.006923 Accuracy 0.7461 | validation: Loss 0.114150 Accuracy -6.1712\n",
      "Epoch 97 | train: Loss 0.006316 Accuracy 0.7684 | validation: Loss 0.109109 Accuracy -5.8546\n",
      "Epoch 98 | train: Loss 0.006455 Accuracy 0.7633 | validation: Loss 0.133278 Accuracy -7.3729\n",
      "Epoch 99 | train: Loss 0.006658 Accuracy 0.7558 | validation: Loss 0.115331 Accuracy -6.2454\n",
      "Epoch 100 | train: Loss 0.006235 Accuracy 0.7713 | validation: Loss 0.108982 Accuracy -5.8466\n",
      "Epoch 101 | train: Loss 0.006439 Accuracy 0.7638 | validation: Loss 0.130024 Accuracy -7.1685\n",
      "Epoch 102 | train: Loss 0.006425 Accuracy 0.7644 | validation: Loss 0.121220 Accuracy -6.6154\n",
      "Epoch 103 | train: Loss 0.006175 Accuracy 0.7735 | validation: Loss 0.111202 Accuracy -5.9861\n",
      "Epoch 104 | train: Loss 0.006392 Accuracy 0.7656 | validation: Loss 0.128723 Accuracy -7.0867\n",
      "Epoch 105 | train: Loss 0.006236 Accuracy 0.7713 | validation: Loss 0.126539 Accuracy -6.9496\n",
      "Epoch 106 | train: Loss 0.006148 Accuracy 0.7745 | validation: Loss 0.114290 Accuracy -6.1800\n",
      "Epoch 107 | train: Loss 0.006292 Accuracy 0.7692 | validation: Loss 0.128911 Accuracy -7.0985\n",
      "Epoch 108 | train: Loss 0.006113 Accuracy 0.7758 | validation: Loss 0.129529 Accuracy -7.1374\n",
      "Epoch 109 | train: Loss 0.006083 Accuracy 0.7769 | validation: Loss 0.117078 Accuracy -6.3552\n",
      "Epoch 110 | train: Loss 0.006174 Accuracy 0.7736 | validation: Loss 0.130965 Accuracy -7.2276\n",
      "Epoch 111 | train: Loss 0.006031 Accuracy 0.7788 | validation: Loss 0.128031 Accuracy -7.0433\n",
      "Epoch 112 | train: Loss 0.005941 Accuracy 0.7821 | validation: Loss 0.117776 Accuracy -6.3991\n",
      "Epoch 113 | train: Loss 0.006028 Accuracy 0.7789 | validation: Loss 0.141099 Accuracy -7.8643\n",
      "Epoch 114 | train: Loss 0.006020 Accuracy 0.7792 | validation: Loss 0.128680 Accuracy -7.0841\n",
      "Epoch 115 | train: Loss 0.005813 Accuracy 0.7868 | validation: Loss 0.133942 Accuracy -7.4146\n",
      "Epoch 116 | train: Loss 0.005638 Accuracy 0.7932 | validation: Loss 0.142684 Accuracy -7.9639\n",
      "Epoch 117 | train: Loss 0.005608 Accuracy 0.7943 | validation: Loss 0.114849 Accuracy -6.2151\n",
      "Epoch 118 | train: Loss 0.006136 Accuracy 0.7750 | validation: Loss 0.174125 Accuracy -9.9391\n",
      "Epoch 119 | train: Loss 0.008779 Accuracy 0.6780 | validation: Loss 0.089297 Accuracy -4.6099\n",
      "Epoch 120 | train: Loss 0.010257 Accuracy 0.6238 | validation: Loss 0.131294 Accuracy -7.2482\n",
      "Epoch 121 | train: Loss 0.006139 Accuracy 0.7749 | validation: Loss 0.129887 Accuracy -7.1599\n",
      "Epoch 122 | train: Loss 0.005911 Accuracy 0.7832 | validation: Loss 0.095343 Accuracy -4.9897\n",
      "Epoch 123 | train: Loss 0.008662 Accuracy 0.6823 | validation: Loss 0.140685 Accuracy -7.8383\n",
      "Epoch 124 | train: Loss 0.006052 Accuracy 0.7781 | validation: Loss 0.151580 Accuracy -8.5227\n",
      "Epoch 125 | train: Loss 0.006868 Accuracy 0.7481 | validation: Loss 0.107666 Accuracy -5.7639\n",
      "Epoch 126 | train: Loss 0.006629 Accuracy 0.7569 | validation: Loss 0.114524 Accuracy -6.1947\n",
      "Epoch 127 | train: Loss 0.006026 Accuracy 0.7790 | validation: Loss 0.149544 Accuracy -8.3948\n",
      "Epoch 128 | train: Loss 0.006479 Accuracy 0.7624 | validation: Loss 0.141590 Accuracy -7.8951\n",
      "Epoch 129 | train: Loss 0.005953 Accuracy 0.7817 | validation: Loss 0.113234 Accuracy -6.1137\n",
      "Epoch 130 | train: Loss 0.006191 Accuracy 0.7730 | validation: Loss 0.117309 Accuracy -6.3697\n",
      "Epoch 131 | train: Loss 0.005969 Accuracy 0.7811 | validation: Loss 0.146118 Accuracy -8.1796\n",
      "Epoch 132 | train: Loss 0.005927 Accuracy 0.7826 | validation: Loss 0.148598 Accuracy -8.3354\n",
      "Epoch 133 | train: Loss 0.005893 Accuracy 0.7839 | validation: Loss 0.125754 Accuracy -6.9002\n",
      "Epoch 134 | train: Loss 0.005681 Accuracy 0.7916 | validation: Loss 0.125185 Accuracy -6.8645\n",
      "Epoch 135 | train: Loss 0.005722 Accuracy 0.7901 | validation: Loss 0.148634 Accuracy -8.3376\n",
      "Epoch 136 | train: Loss 0.005393 Accuracy 0.8022 | validation: Loss 0.153229 Accuracy -8.6263\n",
      "Epoch 137 | train: Loss 0.005418 Accuracy 0.8013 | validation: Loss 0.132126 Accuracy -7.3005\n",
      "Epoch 138 | train: Loss 0.005145 Accuracy 0.8113 | validation: Loss 0.131877 Accuracy -7.2849\n",
      "Epoch 139 | train: Loss 0.004957 Accuracy 0.8182 | validation: Loss 0.152197 Accuracy -8.5615\n",
      "Epoch 140 | train: Loss 0.004963 Accuracy 0.8180 | validation: Loss 0.139206 Accuracy -7.7453\n",
      "Epoch 141 | train: Loss 0.004368 Accuracy 0.8398 | validation: Loss 0.129576 Accuracy -7.1404\n",
      "Epoch 142 | train: Loss 0.004605 Accuracy 0.8311 | validation: Loss 0.159693 Accuracy -9.0324\n",
      "Epoch 143 | train: Loss 0.005357 Accuracy 0.8036 | validation: Loss 0.118892 Accuracy -6.4692\n",
      "Epoch 144 | train: Loss 0.006416 Accuracy 0.7647 | validation: Loss 0.168971 Accuracy -9.6153\n",
      "Epoch 145 | train: Loss 0.006107 Accuracy 0.7761 | validation: Loss 0.132451 Accuracy -7.3209\n",
      "Epoch 146 | train: Loss 0.003915 Accuracy 0.8564 | validation: Loss 0.120841 Accuracy -6.5916\n",
      "Epoch 147 | train: Loss 0.005044 Accuracy 0.8150 | validation: Loss 0.153404 Accuracy -8.6373\n",
      "Epoch 148 | train: Loss 0.004197 Accuracy 0.8461 | validation: Loss 0.161554 Accuracy -9.1493\n",
      "Epoch 149 | train: Loss 0.004577 Accuracy 0.8321 | validation: Loss 0.132209 Accuracy -7.3057\n",
      "Epoch 150 | train: Loss 0.004239 Accuracy 0.8445 | validation: Loss 0.132253 Accuracy -7.3085\n",
      "Epoch 151 | train: Loss 0.004081 Accuracy 0.8503 | validation: Loss 0.163235 Accuracy -9.2549\n",
      "Epoch 152 | train: Loss 0.004072 Accuracy 0.8507 | validation: Loss 0.156833 Accuracy -8.8527\n",
      "Epoch 153 | train: Loss 0.003425 Accuracy 0.8744 | validation: Loss 0.136318 Accuracy -7.5639\n",
      "Epoch 154 | train: Loss 0.003936 Accuracy 0.8557 | validation: Loss 0.163255 Accuracy -9.2562\n",
      "Epoch 155 | train: Loss 0.003019 Accuracy 0.8893 | validation: Loss 0.182784 Accuracy -10.4830\n",
      "Epoch 156 | train: Loss 0.003426 Accuracy 0.8743 | validation: Loss 0.152245 Accuracy -8.5645\n",
      "Epoch 157 | train: Loss 0.003988 Accuracy 0.8538 | validation: Loss 0.189020 Accuracy -10.8748\n",
      "Epoch 158 | train: Loss 0.003260 Accuracy 0.8805 | validation: Loss 0.177369 Accuracy -10.1429\n",
      "Epoch 159 | train: Loss 0.002819 Accuracy 0.8966 | validation: Loss 0.160974 Accuracy -9.1129\n",
      "Epoch 160 | train: Loss 0.003168 Accuracy 0.8838 | validation: Loss 0.186516 Accuracy -10.7175\n",
      "Epoch 161 | train: Loss 0.003084 Accuracy 0.8869 | validation: Loss 0.168305 Accuracy -9.5734\n",
      "Epoch 162 | train: Loss 0.002699 Accuracy 0.9010 | validation: Loss 0.158556 Accuracy -8.9609\n",
      "Epoch 163 | train: Loss 0.002941 Accuracy 0.8922 | validation: Loss 0.179114 Accuracy -10.2525\n",
      "Epoch 164 | train: Loss 0.002829 Accuracy 0.8962 | validation: Loss 0.176456 Accuracy -10.0855\n",
      "Epoch 165 | train: Loss 0.002683 Accuracy 0.9016 | validation: Loss 0.164229 Accuracy -9.3174\n",
      "Epoch 166 | train: Loss 0.002802 Accuracy 0.8972 | validation: Loss 0.182478 Accuracy -10.4638\n",
      "Epoch 167 | train: Loss 0.002540 Accuracy 0.9069 | validation: Loss 0.192647 Accuracy -11.1026\n",
      "Epoch 168 | train: Loss 0.002586 Accuracy 0.9051 | validation: Loss 0.178913 Accuracy -10.2399\n",
      "Epoch 169 | train: Loss 0.002629 Accuracy 0.9036 | validation: Loss 0.193723 Accuracy -11.1702\n",
      "Epoch 170 | train: Loss 0.002443 Accuracy 0.9104 | validation: Loss 0.200874 Accuracy -11.6195\n",
      "Epoch 171 | train: Loss 0.002506 Accuracy 0.9081 | validation: Loss 0.183595 Accuracy -10.5340\n",
      "Epoch 172 | train: Loss 0.002528 Accuracy 0.9073 | validation: Loss 0.193072 Accuracy -11.1294\n",
      "Epoch 173 | train: Loss 0.002375 Accuracy 0.9129 | validation: Loss 0.193954 Accuracy -11.1848\n",
      "Epoch 174 | train: Loss 0.002411 Accuracy 0.9116 | validation: Loss 0.177763 Accuracy -10.1676\n",
      "Epoch 175 | train: Loss 0.002428 Accuracy 0.9110 | validation: Loss 0.184666 Accuracy -10.6013\n",
      "Epoch 176 | train: Loss 0.002295 Accuracy 0.9158 | validation: Loss 0.186712 Accuracy -10.7298\n",
      "Epoch 177 | train: Loss 0.002303 Accuracy 0.9155 | validation: Loss 0.170846 Accuracy -9.7330\n",
      "Epoch 178 | train: Loss 0.002357 Accuracy 0.9135 | validation: Loss 0.184546 Accuracy -10.5937\n",
      "Epoch 179 | train: Loss 0.002324 Accuracy 0.9148 | validation: Loss 0.166092 Accuracy -9.4344\n",
      "Epoch 180 | train: Loss 0.002298 Accuracy 0.9157 | validation: Loss 0.177823 Accuracy -10.1714\n",
      "Epoch 181 | train: Loss 0.002190 Accuracy 0.9197 | validation: Loss 0.168094 Accuracy -9.5602\n",
      "Epoch 182 | train: Loss 0.002098 Accuracy 0.9230 | validation: Loss 0.169066 Accuracy -9.6212\n",
      "Epoch 183 | train: Loss 0.002046 Accuracy 0.9250 | validation: Loss 0.176906 Accuracy -10.1137\n",
      "Epoch 184 | train: Loss 0.002024 Accuracy 0.9258 | validation: Loss 0.166860 Accuracy -9.4826\n",
      "Epoch 185 | train: Loss 0.002072 Accuracy 0.9240 | validation: Loss 0.184371 Accuracy -10.5827\n",
      "Epoch 186 | train: Loss 0.002154 Accuracy 0.9210 | validation: Loss 0.157382 Accuracy -8.8872\n",
      "Epoch 187 | train: Loss 0.002463 Accuracy 0.9097 | validation: Loss 0.186693 Accuracy -10.7286\n",
      "Epoch 188 | train: Loss 0.002282 Accuracy 0.9163 | validation: Loss 0.166087 Accuracy -9.4341\n",
      "Epoch 189 | train: Loss 0.001850 Accuracy 0.9321 | validation: Loss 0.172929 Accuracy -9.8639\n",
      "Epoch 190 | train: Loss 0.001723 Accuracy 0.9368 | validation: Loss 0.186920 Accuracy -10.7429\n",
      "Epoch 191 | train: Loss 0.001942 Accuracy 0.9288 | validation: Loss 0.160421 Accuracy -9.0781\n",
      "Epoch 192 | train: Loss 0.002322 Accuracy 0.9148 | validation: Loss 0.190521 Accuracy -10.9691\n",
      "Epoch 193 | train: Loss 0.002149 Accuracy 0.9212 | validation: Loss 0.170874 Accuracy -9.7348\n",
      "Epoch 194 | train: Loss 0.001636 Accuracy 0.9400 | validation: Loss 0.167943 Accuracy -9.5507\n",
      "Epoch 195 | train: Loss 0.001742 Accuracy 0.9361 | validation: Loss 0.175530 Accuracy -10.0273\n",
      "Epoch 196 | train: Loss 0.001478 Accuracy 0.9458 | validation: Loss 0.167890 Accuracy -9.5473\n",
      "Epoch 197 | train: Loss 0.001794 Accuracy 0.9342 | validation: Loss 0.166761 Accuracy -9.4764\n",
      "Epoch 198 | train: Loss 0.001568 Accuracy 0.9425 | validation: Loss 0.152943 Accuracy -8.6083\n",
      "Epoch 199 | train: Loss 0.001819 Accuracy 0.9333 | validation: Loss 0.163604 Accuracy -9.2781\n",
      "Epoch 200 | train: Loss 0.001452 Accuracy 0.9468 | validation: Loss 0.160339 Accuracy -9.0730\n",
      "Epoch 201 | train: Loss 0.001787 Accuracy 0.9345 | validation: Loss 0.167394 Accuracy -9.5162\n",
      "Epoch 202 | train: Loss 0.001875 Accuracy 0.9312 | validation: Loss 0.135273 Accuracy -7.4983\n",
      "Epoch 203 | train: Loss 0.002490 Accuracy 0.9087 | validation: Loss 0.158809 Accuracy -8.9768\n",
      "Epoch 204 | train: Loss 0.001610 Accuracy 0.9410 | validation: Loss 0.164263 Accuracy -9.3195\n",
      "Epoch 205 | train: Loss 0.001593 Accuracy 0.9416 | validation: Loss 0.147172 Accuracy -8.2458\n",
      "Epoch 206 | train: Loss 0.002279 Accuracy 0.9164 | validation: Loss 0.175781 Accuracy -10.0431\n",
      "Epoch 207 | train: Loss 0.001944 Accuracy 0.9287 | validation: Loss 0.167949 Accuracy -9.5510\n",
      "Epoch 208 | train: Loss 0.001540 Accuracy 0.9435 | validation: Loss 0.162634 Accuracy -9.2172\n",
      "Epoch 209 | train: Loss 0.001609 Accuracy 0.9410 | validation: Loss 0.190768 Accuracy -10.9846\n",
      "Epoch 210 | train: Loss 0.002108 Accuracy 0.9227 | validation: Loss 0.164667 Accuracy -9.3448\n",
      "Epoch 211 | train: Loss 0.001673 Accuracy 0.9386 | validation: Loss 0.174179 Accuracy -9.9425\n",
      "Epoch 212 | train: Loss 0.001858 Accuracy 0.9319 | validation: Loss 0.172951 Accuracy -9.8653\n",
      "Epoch 213 | train: Loss 0.001565 Accuracy 0.9426 | validation: Loss 0.163490 Accuracy -9.2709\n",
      "Epoch 214 | train: Loss 0.001427 Accuracy 0.9477 | validation: Loss 0.174655 Accuracy -9.9723\n",
      "Epoch 215 | train: Loss 0.001623 Accuracy 0.9405 | validation: Loss 0.172112 Accuracy -9.8126\n",
      "Epoch 216 | train: Loss 0.001460 Accuracy 0.9465 | validation: Loss 0.161716 Accuracy -9.1595\n",
      "Epoch 217 | train: Loss 0.001405 Accuracy 0.9485 | validation: Loss 0.174877 Accuracy -9.9863\n",
      "Epoch 218 | train: Loss 0.001384 Accuracy 0.9493 | validation: Loss 0.179856 Accuracy -10.2991\n",
      "Epoch 219 | train: Loss 0.001371 Accuracy 0.9497 | validation: Loss 0.171819 Accuracy -9.7942\n",
      "Epoch 220 | train: Loss 0.001413 Accuracy 0.9482 | validation: Loss 0.184957 Accuracy -10.6195\n",
      "Epoch 221 | train: Loss 0.001264 Accuracy 0.9536 | validation: Loss 0.178738 Accuracy -10.2288\n",
      "Epoch 222 | train: Loss 0.001132 Accuracy 0.9585 | validation: Loss 0.168414 Accuracy -9.5802\n",
      "Epoch 223 | train: Loss 0.001267 Accuracy 0.9535 | validation: Loss 0.181404 Accuracy -10.3963\n",
      "Epoch 224 | train: Loss 0.001201 Accuracy 0.9560 | validation: Loss 0.180432 Accuracy -10.3352\n",
      "Epoch 225 | train: Loss 0.001093 Accuracy 0.9599 | validation: Loss 0.171636 Accuracy -9.7827\n",
      "Epoch 226 | train: Loss 0.001167 Accuracy 0.9572 | validation: Loss 0.184494 Accuracy -10.5904\n",
      "Epoch 227 | train: Loss 0.001092 Accuracy 0.9600 | validation: Loss 0.177322 Accuracy -10.1399\n",
      "Epoch 228 | train: Loss 0.001018 Accuracy 0.9627 | validation: Loss 0.170939 Accuracy -9.7389\n",
      "Epoch 229 | train: Loss 0.001027 Accuracy 0.9623 | validation: Loss 0.173147 Accuracy -9.8776\n",
      "Epoch 230 | train: Loss 0.001100 Accuracy 0.9597 | validation: Loss 0.157351 Accuracy -8.8853\n",
      "Epoch 231 | train: Loss 0.001091 Accuracy 0.9600 | validation: Loss 0.165666 Accuracy -9.4076\n",
      "Epoch 232 | train: Loss 0.001018 Accuracy 0.9627 | validation: Loss 0.159674 Accuracy -9.0312\n",
      "Epoch 233 | train: Loss 0.000963 Accuracy 0.9647 | validation: Loss 0.155620 Accuracy -8.7765\n",
      "Epoch 234 | train: Loss 0.000995 Accuracy 0.9635 | validation: Loss 0.166322 Accuracy -9.4488\n",
      "Epoch 235 | train: Loss 0.000987 Accuracy 0.9638 | validation: Loss 0.160693 Accuracy -9.0952\n",
      "Epoch 236 | train: Loss 0.000926 Accuracy 0.9660 | validation: Loss 0.166392 Accuracy -9.4532\n",
      "Epoch 237 | train: Loss 0.000890 Accuracy 0.9674 | validation: Loss 0.170962 Accuracy -9.7404\n",
      "Epoch 238 | train: Loss 0.000904 Accuracy 0.9668 | validation: Loss 0.163675 Accuracy -9.2825\n",
      "Epoch 239 | train: Loss 0.000932 Accuracy 0.9658 | validation: Loss 0.171800 Accuracy -9.7930\n",
      "Epoch 240 | train: Loss 0.000919 Accuracy 0.9663 | validation: Loss 0.161658 Accuracy -9.1558\n",
      "Epoch 241 | train: Loss 0.000875 Accuracy 0.9679 | validation: Loss 0.163535 Accuracy -9.2737\n",
      "Epoch 242 | train: Loss 0.000839 Accuracy 0.9692 | validation: Loss 0.163343 Accuracy -9.2617\n",
      "Epoch 243 | train: Loss 0.000834 Accuracy 0.9694 | validation: Loss 0.158380 Accuracy -8.9499\n",
      "Epoch 244 | train: Loss 0.000851 Accuracy 0.9688 | validation: Loss 0.167813 Accuracy -9.5425\n",
      "Epoch 245 | train: Loss 0.000894 Accuracy 0.9672 | validation: Loss 0.155012 Accuracy -8.7383\n",
      "Epoch 246 | train: Loss 0.000998 Accuracy 0.9634 | validation: Loss 0.172284 Accuracy -9.8234\n",
      "Epoch 247 | train: Loss 0.001200 Accuracy 0.9560 | validation: Loss 0.144117 Accuracy -8.0538\n",
      "Epoch 248 | train: Loss 0.001634 Accuracy 0.9401 | validation: Loss 0.176101 Accuracy -10.0632\n",
      "Epoch 249 | train: Loss 0.001893 Accuracy 0.9306 | validation: Loss 0.139800 Accuracy -7.7827\n",
      "Epoch 250 | train: Loss 0.001647 Accuracy 0.9396 | validation: Loss 0.157573 Accuracy -8.8992\n",
      "Epoch 251 | train: Loss 0.000910 Accuracy 0.9666 | validation: Loss 0.164678 Accuracy -9.3456\n",
      "Epoch 252 | train: Loss 0.001118 Accuracy 0.9590 | validation: Loss 0.139897 Accuracy -7.7887\n",
      "Epoch 253 | train: Loss 0.001603 Accuracy 0.9412 | validation: Loss 0.165432 Accuracy -9.3929\n",
      "Epoch 254 | train: Loss 0.001176 Accuracy 0.9569 | validation: Loss 0.152183 Accuracy -8.5606\n",
      "Epoch 255 | train: Loss 0.000790 Accuracy 0.9710 | validation: Loss 0.142325 Accuracy -7.9413\n",
      "Epoch 256 | train: Loss 0.001003 Accuracy 0.9632 | validation: Loss 0.158065 Accuracy -8.9301\n",
      "Epoch 257 | train: Loss 0.001119 Accuracy 0.9590 | validation: Loss 0.139813 Accuracy -7.7835\n",
      "Epoch 258 | train: Loss 0.000843 Accuracy 0.9691 | validation: Loss 0.136216 Accuracy -7.5575\n",
      "Epoch 259 | train: Loss 0.000846 Accuracy 0.9690 | validation: Loss 0.148549 Accuracy -8.3323\n",
      "Epoch 260 | train: Loss 0.001004 Accuracy 0.9632 | validation: Loss 0.134080 Accuracy -7.4233\n",
      "Epoch 261 | train: Loss 0.000814 Accuracy 0.9701 | validation: Loss 0.137638 Accuracy -7.6469\n",
      "Epoch 262 | train: Loss 0.000747 Accuracy 0.9726 | validation: Loss 0.146789 Accuracy -8.2217\n",
      "Epoch 263 | train: Loss 0.000867 Accuracy 0.9682 | validation: Loss 0.132764 Accuracy -7.3406\n",
      "Epoch 264 | train: Loss 0.000910 Accuracy 0.9666 | validation: Loss 0.140438 Accuracy -7.8227\n",
      "Epoch 265 | train: Loss 0.000762 Accuracy 0.9720 | validation: Loss 0.138463 Accuracy -7.6987\n",
      "Epoch 266 | train: Loss 0.000728 Accuracy 0.9733 | validation: Loss 0.130293 Accuracy -7.1854\n",
      "Epoch 267 | train: Loss 0.000825 Accuracy 0.9697 | validation: Loss 0.139959 Accuracy -7.7926\n",
      "Epoch 268 | train: Loss 0.000724 Accuracy 0.9734 | validation: Loss 0.135580 Accuracy -7.5175\n",
      "Epoch 269 | train: Loss 0.000680 Accuracy 0.9751 | validation: Loss 0.131428 Accuracy -7.2567\n",
      "Epoch 270 | train: Loss 0.000688 Accuracy 0.9748 | validation: Loss 0.138778 Accuracy -7.7185\n",
      "Epoch 271 | train: Loss 0.000720 Accuracy 0.9736 | validation: Loss 0.131762 Accuracy -7.2777\n",
      "Epoch 272 | train: Loss 0.000629 Accuracy 0.9769 | validation: Loss 0.130785 Accuracy -7.2163\n",
      "Epoch 273 | train: Loss 0.000653 Accuracy 0.9760 | validation: Loss 0.136693 Accuracy -7.5874\n",
      "Epoch 274 | train: Loss 0.000648 Accuracy 0.9762 | validation: Loss 0.129444 Accuracy -7.1320\n",
      "Epoch 275 | train: Loss 0.000650 Accuracy 0.9762 | validation: Loss 0.131763 Accuracy -7.2777\n",
      "Epoch 276 | train: Loss 0.000591 Accuracy 0.9783 | validation: Loss 0.131551 Accuracy -7.2644\n",
      "Epoch 277 | train: Loss 0.000593 Accuracy 0.9783 | validation: Loss 0.125287 Accuracy -6.8709\n",
      "Epoch 278 | train: Loss 0.000616 Accuracy 0.9774 | validation: Loss 0.130503 Accuracy -7.1986\n",
      "Epoch 279 | train: Loss 0.000588 Accuracy 0.9784 | validation: Loss 0.125238 Accuracy -6.8678\n",
      "Epoch 280 | train: Loss 0.000563 Accuracy 0.9793 | validation: Loss 0.126078 Accuracy -6.9206\n",
      "Epoch 281 | train: Loss 0.000537 Accuracy 0.9803 | validation: Loss 0.129530 Accuracy -7.1374\n",
      "Epoch 282 | train: Loss 0.000557 Accuracy 0.9796 | validation: Loss 0.123085 Accuracy -6.7326\n",
      "Epoch 283 | train: Loss 0.000557 Accuracy 0.9796 | validation: Loss 0.128436 Accuracy -7.0687\n",
      "Epoch 284 | train: Loss 0.000531 Accuracy 0.9805 | validation: Loss 0.124929 Accuracy -6.8484\n",
      "Epoch 285 | train: Loss 0.000516 Accuracy 0.9811 | validation: Loss 0.125520 Accuracy -6.8855\n",
      "Epoch 286 | train: Loss 0.000494 Accuracy 0.9819 | validation: Loss 0.127459 Accuracy -7.0074\n",
      "Epoch 287 | train: Loss 0.000490 Accuracy 0.9820 | validation: Loss 0.124195 Accuracy -6.8023\n",
      "Epoch 288 | train: Loss 0.000504 Accuracy 0.9815 | validation: Loss 0.128429 Accuracy -7.0683\n",
      "Epoch 289 | train: Loss 0.000517 Accuracy 0.9811 | validation: Loss 0.121227 Accuracy -6.6158\n",
      "Epoch 290 | train: Loss 0.000512 Accuracy 0.9812 | validation: Loss 0.129134 Accuracy -7.1125\n",
      "Epoch 291 | train: Loss 0.000510 Accuracy 0.9813 | validation: Loss 0.120972 Accuracy -6.5998\n",
      "Epoch 292 | train: Loss 0.000512 Accuracy 0.9812 | validation: Loss 0.129753 Accuracy -7.1515\n",
      "Epoch 293 | train: Loss 0.000497 Accuracy 0.9818 | validation: Loss 0.121527 Accuracy -6.6347\n",
      "Epoch 294 | train: Loss 0.000485 Accuracy 0.9822 | validation: Loss 0.128646 Accuracy -7.0819\n",
      "Epoch 295 | train: Loss 0.000474 Accuracy 0.9826 | validation: Loss 0.122257 Accuracy -6.6806\n",
      "Epoch 296 | train: Loss 0.000469 Accuracy 0.9828 | validation: Loss 0.126426 Accuracy -6.9425\n",
      "Epoch 297 | train: Loss 0.000472 Accuracy 0.9827 | validation: Loss 0.121968 Accuracy -6.6624\n",
      "Epoch 298 | train: Loss 0.000475 Accuracy 0.9826 | validation: Loss 0.122866 Accuracy -6.7188\n",
      "Epoch 299 | train: Loss 0.000489 Accuracy 0.9821 | validation: Loss 0.122303 Accuracy -6.6835\n",
      "Epoch 300 | train: Loss 0.000469 Accuracy 0.9828 | validation: Loss 0.120386 Accuracy -6.5630\n",
      "Epoch 301 | train: Loss 0.000431 Accuracy 0.9842 | validation: Loss 0.123703 Accuracy -6.7714\n",
      "Epoch 302 | train: Loss 0.000397 Accuracy 0.9854 | validation: Loss 0.120482 Accuracy -6.5690\n",
      "Epoch 303 | train: Loss 0.000410 Accuracy 0.9850 | validation: Loss 0.124605 Accuracy -6.8280\n",
      "Epoch 304 | train: Loss 0.000449 Accuracy 0.9835 | validation: Loss 0.120697 Accuracy -6.5825\n",
      "Epoch 305 | train: Loss 0.000440 Accuracy 0.9839 | validation: Loss 0.125883 Accuracy -6.9084\n",
      "Epoch 306 | train: Loss 0.000414 Accuracy 0.9848 | validation: Loss 0.119191 Accuracy -6.4879\n",
      "Epoch 307 | train: Loss 0.000401 Accuracy 0.9853 | validation: Loss 0.126273 Accuracy -6.9328\n",
      "Epoch 308 | train: Loss 0.000403 Accuracy 0.9852 | validation: Loss 0.118477 Accuracy -6.4430\n",
      "Epoch 309 | train: Loss 0.000394 Accuracy 0.9855 | validation: Loss 0.124871 Accuracy -6.8448\n",
      "Epoch 310 | train: Loss 0.000365 Accuracy 0.9866 | validation: Loss 0.120771 Accuracy -6.5872\n",
      "Epoch 311 | train: Loss 0.000342 Accuracy 0.9875 | validation: Loss 0.123375 Accuracy -6.7508\n",
      "Epoch 312 | train: Loss 0.000342 Accuracy 0.9875 | validation: Loss 0.123555 Accuracy -6.7621\n",
      "Epoch 313 | train: Loss 0.000366 Accuracy 0.9866 | validation: Loss 0.122478 Accuracy -6.6944\n",
      "Epoch 314 | train: Loss 0.000413 Accuracy 0.9848 | validation: Loss 0.121017 Accuracy -6.6026\n",
      "Epoch 315 | train: Loss 0.000473 Accuracy 0.9827 | validation: Loss 0.123676 Accuracy -6.7697\n",
      "Epoch 316 | train: Loss 0.000582 Accuracy 0.9786 | validation: Loss 0.114298 Accuracy -6.1805\n",
      "Epoch 317 | train: Loss 0.000547 Accuracy 0.9800 | validation: Loss 0.129569 Accuracy -7.1399\n",
      "Epoch 318 | train: Loss 0.000585 Accuracy 0.9786 | validation: Loss 0.110891 Accuracy -5.9665\n",
      "Epoch 319 | train: Loss 0.000696 Accuracy 0.9745 | validation: Loss 0.134893 Accuracy -7.4744\n",
      "Epoch 320 | train: Loss 0.000680 Accuracy 0.9751 | validation: Loss 0.115872 Accuracy -6.2794\n",
      "Epoch 321 | train: Loss 0.000442 Accuracy 0.9838 | validation: Loss 0.121235 Accuracy -6.6163\n",
      "Epoch 322 | train: Loss 0.000302 Accuracy 0.9889 | validation: Loss 0.125525 Accuracy -6.8859\n",
      "Epoch 323 | train: Loss 0.000392 Accuracy 0.9856 | validation: Loss 0.112491 Accuracy -6.0670\n",
      "Epoch 324 | train: Loss 0.000435 Accuracy 0.9841 | validation: Loss 0.125641 Accuracy -6.8931\n",
      "Epoch 325 | train: Loss 0.000375 Accuracy 0.9863 | validation: Loss 0.118285 Accuracy -6.4310\n",
      "Epoch 326 | train: Loss 0.000396 Accuracy 0.9855 | validation: Loss 0.119469 Accuracy -6.5054\n",
      "Epoch 327 | train: Loss 0.000528 Accuracy 0.9806 | validation: Loss 0.120087 Accuracy -6.5442\n",
      "Epoch 328 | train: Loss 0.000381 Accuracy 0.9860 | validation: Loss 0.117195 Accuracy -6.3625\n",
      "Epoch 329 | train: Loss 0.000300 Accuracy 0.9890 | validation: Loss 0.119371 Accuracy -6.4992\n",
      "Epoch 330 | train: Loss 0.000414 Accuracy 0.9848 | validation: Loss 0.120420 Accuracy -6.5651\n",
      "Epoch 331 | train: Loss 0.000369 Accuracy 0.9865 | validation: Loss 0.125721 Accuracy -6.8982\n",
      "Epoch 332 | train: Loss 0.000315 Accuracy 0.9884 | validation: Loss 0.119100 Accuracy -6.4822\n",
      "Epoch 333 | train: Loss 0.000323 Accuracy 0.9882 | validation: Loss 0.127193 Accuracy -6.9906\n",
      "Epoch 334 | train: Loss 0.000329 Accuracy 0.9879 | validation: Loss 0.124506 Accuracy -6.8218\n",
      "Epoch 335 | train: Loss 0.000285 Accuracy 0.9896 | validation: Loss 0.117457 Accuracy -6.3790\n",
      "Epoch 336 | train: Loss 0.000312 Accuracy 0.9885 | validation: Loss 0.128490 Accuracy -7.0721\n",
      "Epoch 337 | train: Loss 0.000333 Accuracy 0.9878 | validation: Loss 0.117310 Accuracy -6.3697\n",
      "Epoch 338 | train: Loss 0.000294 Accuracy 0.9892 | validation: Loss 0.125329 Accuracy -6.8735\n",
      "Epoch 339 | train: Loss 0.000245 Accuracy 0.9910 | validation: Loss 0.126926 Accuracy -6.9738\n",
      "Epoch 340 | train: Loss 0.000258 Accuracy 0.9905 | validation: Loss 0.121985 Accuracy -6.6635\n",
      "Epoch 341 | train: Loss 0.000280 Accuracy 0.9897 | validation: Loss 0.127475 Accuracy -7.0084\n",
      "Epoch 342 | train: Loss 0.000266 Accuracy 0.9903 | validation: Loss 0.123594 Accuracy -6.7645\n",
      "Epoch 343 | train: Loss 0.000239 Accuracy 0.9913 | validation: Loss 0.120009 Accuracy -6.5393\n",
      "Epoch 344 | train: Loss 0.000244 Accuracy 0.9911 | validation: Loss 0.130361 Accuracy -7.1896\n",
      "Epoch 345 | train: Loss 0.000268 Accuracy 0.9902 | validation: Loss 0.120623 Accuracy -6.5779\n",
      "Epoch 346 | train: Loss 0.000271 Accuracy 0.9900 | validation: Loss 0.130879 Accuracy -7.2222\n",
      "Epoch 347 | train: Loss 0.000250 Accuracy 0.9908 | validation: Loss 0.123464 Accuracy -6.7564\n",
      "Epoch 348 | train: Loss 0.000266 Accuracy 0.9902 | validation: Loss 0.123362 Accuracy -6.7500\n",
      "Epoch 349 | train: Loss 0.000429 Accuracy 0.9843 | validation: Loss 0.122773 Accuracy -6.7129\n",
      "Epoch 350 | train: Loss 0.000433 Accuracy 0.9841 | validation: Loss 0.120608 Accuracy -6.5770\n",
      "Epoch 351 | train: Loss 0.000344 Accuracy 0.9874 | validation: Loss 0.122394 Accuracy -6.6891\n",
      "Epoch 352 | train: Loss 0.000191 Accuracy 0.9930 | validation: Loss 0.126613 Accuracy -6.9542\n",
      "Epoch 353 | train: Loss 0.000411 Accuracy 0.9849 | validation: Loss 0.126387 Accuracy -6.9400\n",
      "Epoch 354 | train: Loss 0.000542 Accuracy 0.9801 | validation: Loss 0.123781 Accuracy -6.7763\n",
      "Epoch 355 | train: Loss 0.000266 Accuracy 0.9903 | validation: Loss 0.122956 Accuracy -6.7245\n",
      "Epoch 356 | train: Loss 0.000444 Accuracy 0.9837 | validation: Loss 0.122776 Accuracy -6.7132\n",
      "Epoch 357 | train: Loss 0.000510 Accuracy 0.9813 | validation: Loss 0.117769 Accuracy -6.3986\n",
      "Epoch 358 | train: Loss 0.000331 Accuracy 0.9878 | validation: Loss 0.125636 Accuracy -6.8928\n",
      "Epoch 359 | train: Loss 0.000595 Accuracy 0.9782 | validation: Loss 0.126156 Accuracy -6.9255\n",
      "Epoch 360 | train: Loss 0.000204 Accuracy 0.9925 | validation: Loss 0.117522 Accuracy -6.3831\n",
      "Epoch 361 | train: Loss 0.000492 Accuracy 0.9819 | validation: Loss 0.131925 Accuracy -7.2879\n",
      "Epoch 362 | train: Loss 0.000365 Accuracy 0.9866 | validation: Loss 0.114044 Accuracy -6.1646\n",
      "Epoch 363 | train: Loss 0.000521 Accuracy 0.9809 | validation: Loss 0.123606 Accuracy -6.7653\n",
      "Epoch 364 | train: Loss 0.000374 Accuracy 0.9863 | validation: Loss 0.115088 Accuracy -6.2302\n",
      "Epoch 365 | train: Loss 0.000368 Accuracy 0.9865 | validation: Loss 0.119147 Accuracy -6.4852\n",
      "Epoch 366 | train: Loss 0.000222 Accuracy 0.9919 | validation: Loss 0.127330 Accuracy -6.9992\n",
      "Epoch 367 | train: Loss 0.000323 Accuracy 0.9881 | validation: Loss 0.116812 Accuracy -6.3385\n",
      "Epoch 368 | train: Loss 0.000268 Accuracy 0.9902 | validation: Loss 0.123925 Accuracy -6.7853\n",
      "Epoch 369 | train: Loss 0.000285 Accuracy 0.9895 | validation: Loss 0.122850 Accuracy -6.7178\n",
      "Epoch 370 | train: Loss 0.000163 Accuracy 0.9940 | validation: Loss 0.119575 Accuracy -6.5120\n",
      "Epoch 371 | train: Loss 0.000253 Accuracy 0.9907 | validation: Loss 0.120829 Accuracy -6.5909\n",
      "Epoch 372 | train: Loss 0.000209 Accuracy 0.9923 | validation: Loss 0.110848 Accuracy -5.9638\n",
      "Epoch 373 | train: Loss 0.000276 Accuracy 0.9899 | validation: Loss 0.121498 Accuracy -6.6328\n",
      "Epoch 374 | train: Loss 0.000166 Accuracy 0.9939 | validation: Loss 0.117277 Accuracy -6.3677\n",
      "Epoch 375 | train: Loss 0.000237 Accuracy 0.9913 | validation: Loss 0.118644 Accuracy -6.4536\n",
      "Epoch 376 | train: Loss 0.000153 Accuracy 0.9944 | validation: Loss 0.116197 Accuracy -6.2998\n",
      "Epoch 377 | train: Loss 0.000186 Accuracy 0.9932 | validation: Loss 0.119180 Accuracy -6.4872\n",
      "Epoch 378 | train: Loss 0.000103 Accuracy 0.9962 | validation: Loss 0.121368 Accuracy -6.6247\n",
      "Epoch 379 | train: Loss 0.000162 Accuracy 0.9940 | validation: Loss 0.115427 Accuracy -6.2515\n",
      "Epoch 380 | train: Loss 0.000118 Accuracy 0.9957 | validation: Loss 0.117697 Accuracy -6.3941\n",
      "Epoch 381 | train: Loss 0.000135 Accuracy 0.9951 | validation: Loss 0.118293 Accuracy -6.4315\n",
      "Epoch 382 | train: Loss 0.000088 Accuracy 0.9968 | validation: Loss 0.119461 Accuracy -6.5049\n",
      "Epoch 383 | train: Loss 0.000100 Accuracy 0.9963 | validation: Loss 0.118474 Accuracy -6.4429\n",
      "Epoch 384 | train: Loss 0.000097 Accuracy 0.9964 | validation: Loss 0.114563 Accuracy -6.1972\n",
      "Epoch 385 | train: Loss 0.000082 Accuracy 0.9970 | validation: Loss 0.119974 Accuracy -6.5371\n",
      "Epoch 386 | train: Loss 0.000101 Accuracy 0.9963 | validation: Loss 0.114166 Accuracy -6.1722\n",
      "Epoch 387 | train: Loss 0.000062 Accuracy 0.9977 | validation: Loss 0.118023 Accuracy -6.4145\n",
      "Epoch 388 | train: Loss 0.000087 Accuracy 0.9968 | validation: Loss 0.115651 Accuracy -6.2655\n",
      "Epoch 389 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.121312 Accuracy -6.6212\n",
      "Epoch 390 | train: Loss 0.000072 Accuracy 0.9974 | validation: Loss 0.114164 Accuracy -6.1721\n",
      "Epoch 391 | train: Loss 0.000068 Accuracy 0.9975 | validation: Loss 0.120401 Accuracy -6.5639\n",
      "Epoch 392 | train: Loss 0.000046 Accuracy 0.9983 | validation: Loss 0.115989 Accuracy -6.2868\n",
      "Epoch 393 | train: Loss 0.000078 Accuracy 0.9971 | validation: Loss 0.120566 Accuracy -6.5743\n",
      "Epoch 394 | train: Loss 0.000056 Accuracy 0.9980 | validation: Loss 0.111800 Accuracy -6.0236\n",
      "Epoch 395 | train: Loss 0.000096 Accuracy 0.9965 | validation: Loss 0.124402 Accuracy -6.8153\n",
      "Epoch 396 | train: Loss 0.000147 Accuracy 0.9946 | validation: Loss 0.107423 Accuracy -5.7487\n",
      "Epoch 397 | train: Loss 0.000257 Accuracy 0.9906 | validation: Loss 0.126906 Accuracy -6.9726\n",
      "Epoch 398 | train: Loss 0.000415 Accuracy 0.9848 | validation: Loss 0.101306 Accuracy -5.3643\n",
      "Epoch 399 | train: Loss 0.000792 Accuracy 0.9709 | validation: Loss 0.137987 Accuracy -7.6687\n",
      "Epoch 400 | train: Loss 0.001039 Accuracy 0.9619 | validation: Loss 0.093868 Accuracy -4.8970\n",
      "Epoch 401 | train: Loss 0.001203 Accuracy 0.9559 | validation: Loss 0.129345 Accuracy -7.1258\n",
      "Epoch 402 | train: Loss 0.000634 Accuracy 0.9767 | validation: Loss 0.107190 Accuracy -5.7340\n",
      "Epoch 403 | train: Loss 0.000112 Accuracy 0.9959 | validation: Loss 0.105749 Accuracy -5.6435\n",
      "Epoch 404 | train: Loss 0.000147 Accuracy 0.9946 | validation: Loss 0.124639 Accuracy -6.8302\n",
      "Epoch 405 | train: Loss 0.000473 Accuracy 0.9827 | validation: Loss 0.097079 Accuracy -5.0988\n",
      "Epoch 406 | train: Loss 0.000507 Accuracy 0.9814 | validation: Loss 0.115157 Accuracy -6.2345\n",
      "Epoch 407 | train: Loss 0.000119 Accuracy 0.9956 | validation: Loss 0.114692 Accuracy -6.2053\n",
      "Epoch 408 | train: Loss 0.000085 Accuracy 0.9969 | validation: Loss 0.100433 Accuracy -5.3095\n",
      "Epoch 409 | train: Loss 0.000336 Accuracy 0.9877 | validation: Loss 0.120539 Accuracy -6.5726\n",
      "Epoch 410 | train: Loss 0.000260 Accuracy 0.9905 | validation: Loss 0.105740 Accuracy -5.6429\n",
      "Epoch 411 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.104580 Accuracy -5.5700\n",
      "Epoch 412 | train: Loss 0.000085 Accuracy 0.9969 | validation: Loss 0.119584 Accuracy -6.5126\n",
      "Epoch 413 | train: Loss 0.000211 Accuracy 0.9923 | validation: Loss 0.103107 Accuracy -5.4775\n",
      "Epoch 414 | train: Loss 0.000149 Accuracy 0.9945 | validation: Loss 0.109787 Accuracy -5.8972\n",
      "Epoch 415 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.114296 Accuracy -6.1804\n",
      "Epoch 416 | train: Loss 0.000087 Accuracy 0.9968 | validation: Loss 0.102061 Accuracy -5.4118\n",
      "Epoch 417 | train: Loss 0.000153 Accuracy 0.9944 | validation: Loss 0.114776 Accuracy -6.2106\n",
      "Epoch 418 | train: Loss 0.000054 Accuracy 0.9980 | validation: Loss 0.112180 Accuracy -6.0475\n",
      "Epoch 419 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.105579 Accuracy -5.6328\n",
      "Epoch 420 | train: Loss 0.000085 Accuracy 0.9969 | validation: Loss 0.116371 Accuracy -6.3108\n",
      "Epoch 421 | train: Loss 0.000084 Accuracy 0.9969 | validation: Loss 0.108063 Accuracy -5.7888\n",
      "Epoch 422 | train: Loss 0.000034 Accuracy 0.9988 | validation: Loss 0.108306 Accuracy -5.8041\n",
      "Epoch 423 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.115862 Accuracy -6.2788\n",
      "Epoch 424 | train: Loss 0.000063 Accuracy 0.9977 | validation: Loss 0.106354 Accuracy -5.6815\n",
      "Epoch 425 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.110199 Accuracy -5.9230\n",
      "Epoch 426 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.110992 Accuracy -5.9729\n",
      "Epoch 427 | train: Loss 0.000028 Accuracy 0.9990 | validation: Loss 0.103789 Accuracy -5.5204\n",
      "Epoch 428 | train: Loss 0.000055 Accuracy 0.9980 | validation: Loss 0.110591 Accuracy -5.9476\n",
      "Epoch 429 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.107764 Accuracy -5.7701\n",
      "Epoch 430 | train: Loss 0.000013 Accuracy 0.9995 | validation: Loss 0.105342 Accuracy -5.6179\n",
      "Epoch 431 | train: Loss 0.000023 Accuracy 0.9992 | validation: Loss 0.111391 Accuracy -5.9979\n",
      "Epoch 432 | train: Loss 0.000028 Accuracy 0.9990 | validation: Loss 0.106482 Accuracy -5.6895\n",
      "Epoch 433 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.107803 Accuracy -5.7725\n",
      "Epoch 434 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.109924 Accuracy -5.9057\n",
      "Epoch 435 | train: Loss 0.000015 Accuracy 0.9994 | validation: Loss 0.105096 Accuracy -5.6024\n",
      "Epoch 436 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.108702 Accuracy -5.8290\n",
      "Epoch 437 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.107393 Accuracy -5.7468\n",
      "Epoch 438 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.105146 Accuracy -5.6056\n",
      "Epoch 439 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.108577 Accuracy -5.8211\n",
      "Epoch 440 | train: Loss 0.000016 Accuracy 0.9994 | validation: Loss 0.104483 Accuracy -5.5639\n",
      "Epoch 441 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.105481 Accuracy -5.6266\n",
      "Epoch 442 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.107324 Accuracy -5.7424\n",
      "Epoch 443 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.103943 Accuracy -5.5300\n",
      "Epoch 444 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.106630 Accuracy -5.6988\n",
      "Epoch 445 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.105155 Accuracy -5.6061\n",
      "Epoch 446 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.103819 Accuracy -5.5222\n",
      "Epoch 447 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.105939 Accuracy -5.6554\n",
      "Epoch 448 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.102951 Accuracy -5.4677\n",
      "Epoch 449 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.104405 Accuracy -5.5590\n",
      "Epoch 450 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.104341 Accuracy -5.5550\n",
      "Epoch 451 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.102426 Accuracy -5.4347\n",
      "Epoch 452 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.104528 Accuracy -5.5667\n",
      "Epoch 453 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.102405 Accuracy -5.4334\n",
      "Epoch 454 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.102625 Accuracy -5.4472\n",
      "Epoch 455 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.103169 Accuracy -5.4814\n",
      "Epoch 456 | train: Loss 0.000004 Accuracy 0.9998 | validation: Loss 0.101992 Accuracy -5.4074\n",
      "Epoch 457 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.102507 Accuracy -5.4398\n",
      "Epoch 458 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.101884 Accuracy -5.4006\n",
      "Epoch 459 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.101924 Accuracy -5.4032\n",
      "Epoch 460 | train: Loss 0.000004 Accuracy 0.9998 | validation: Loss 0.101437 Accuracy -5.3726\n",
      "Epoch 461 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.101688 Accuracy -5.3883\n",
      "Epoch 462 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.101084 Accuracy -5.3504\n",
      "Epoch 463 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.100886 Accuracy -5.3380\n",
      "Epoch 464 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.101157 Accuracy -5.3550\n",
      "Epoch 465 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.100101 Accuracy -5.2886\n",
      "Epoch 466 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.101256 Accuracy -5.3612\n",
      "Epoch 467 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.099286 Accuracy -5.2374\n",
      "Epoch 468 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.101136 Accuracy -5.3537\n",
      "Epoch 469 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.098917 Accuracy -5.2143\n",
      "Epoch 470 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.100447 Accuracy -5.3104\n",
      "Epoch 471 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.098997 Accuracy -5.2193\n",
      "Epoch 472 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.099965 Accuracy -5.2801\n",
      "Epoch 473 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.098938 Accuracy -5.2156\n",
      "Epoch 474 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.100022 Accuracy -5.2837\n",
      "Epoch 475 | train: Loss 0.000007 Accuracy 0.9998 | validation: Loss 0.098796 Accuracy -5.2067\n",
      "Epoch 476 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.100086 Accuracy -5.2877\n",
      "Epoch 477 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.099245 Accuracy -5.2349\n",
      "Epoch 478 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.099798 Accuracy -5.2696\n",
      "Epoch 479 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.099805 Accuracy -5.2700\n",
      "Epoch 480 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.099378 Accuracy -5.2432\n",
      "Epoch 481 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.100520 Accuracy -5.3150\n",
      "Epoch 482 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.098679 Accuracy -5.1993\n",
      "Epoch 483 | train: Loss 0.000016 Accuracy 0.9994 | validation: Loss 0.101348 Accuracy -5.3670\n",
      "Epoch 484 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.097105 Accuracy -5.1004\n",
      "Epoch 485 | train: Loss 0.000080 Accuracy 0.9971 | validation: Loss 0.102635 Accuracy -5.4478\n",
      "Epoch 486 | train: Loss 0.000198 Accuracy 0.9927 | validation: Loss 0.093105 Accuracy -4.8491\n",
      "Epoch 487 | train: Loss 0.000314 Accuracy 0.9885 | validation: Loss 0.104436 Accuracy -5.5609\n",
      "Epoch 488 | train: Loss 0.000410 Accuracy 0.9850 | validation: Loss 0.089321 Accuracy -4.6114\n",
      "Epoch 489 | train: Loss 0.000167 Accuracy 0.9939 | validation: Loss 0.101461 Accuracy -5.3741\n",
      "Epoch 490 | train: Loss 0.000226 Accuracy 0.9917 | validation: Loss 0.098154 Accuracy -5.1663\n",
      "Epoch 491 | train: Loss 0.000103 Accuracy 0.9962 | validation: Loss 0.099613 Accuracy -5.2580\n",
      "Epoch 492 | train: Loss 0.000134 Accuracy 0.9951 | validation: Loss 0.101274 Accuracy -5.3624\n",
      "Epoch 493 | train: Loss 0.000061 Accuracy 0.9977 | validation: Loss 0.096748 Accuracy -5.0780\n",
      "Epoch 494 | train: Loss 0.000150 Accuracy 0.9945 | validation: Loss 0.100951 Accuracy -5.3420\n",
      "Epoch 495 | train: Loss 0.000081 Accuracy 0.9970 | validation: Loss 0.100745 Accuracy -5.3291\n",
      "Epoch 496 | train: Loss 0.000103 Accuracy 0.9962 | validation: Loss 0.101538 Accuracy -5.3789\n",
      "Epoch 497 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.105749 Accuracy -5.6435\n",
      "Epoch 498 | train: Loss 0.000088 Accuracy 0.9968 | validation: Loss 0.103197 Accuracy -5.4832\n",
      "Epoch 499 | train: Loss 0.000053 Accuracy 0.9981 | validation: Loss 0.106998 Accuracy -5.7219\n",
      "Epoch 500 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.103388 Accuracy -5.4951\n",
      "Epoch 501 | train: Loss 0.000054 Accuracy 0.9980 | validation: Loss 0.104246 Accuracy -5.5490\n",
      "Epoch 502 | train: Loss 0.000033 Accuracy 0.9988 | validation: Loss 0.107430 Accuracy -5.7491\n",
      "Epoch 503 | train: Loss 0.000056 Accuracy 0.9980 | validation: Loss 0.105787 Accuracy -5.6458\n",
      "Epoch 504 | train: Loss 0.000038 Accuracy 0.9986 | validation: Loss 0.107700 Accuracy -5.7660\n",
      "Epoch 505 | train: Loss 0.000027 Accuracy 0.9990 | validation: Loss 0.104802 Accuracy -5.5840\n",
      "Epoch 506 | train: Loss 0.000049 Accuracy 0.9982 | validation: Loss 0.106379 Accuracy -5.6830\n",
      "Epoch 507 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.104609 Accuracy -5.5718\n",
      "Epoch 508 | train: Loss 0.000028 Accuracy 0.9990 | validation: Loss 0.106459 Accuracy -5.6881\n",
      "Epoch 509 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.104465 Accuracy -5.5628\n",
      "Epoch 510 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.104472 Accuracy -5.5632\n",
      "Epoch 511 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.104345 Accuracy -5.5553\n",
      "Epoch 512 | train: Loss 0.000015 Accuracy 0.9995 | validation: Loss 0.103688 Accuracy -5.5140\n",
      "Epoch 513 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.104721 Accuracy -5.5789\n",
      "Epoch 514 | train: Loss 0.000012 Accuracy 0.9995 | validation: Loss 0.103978 Accuracy -5.5322\n",
      "Epoch 515 | train: Loss 0.000013 Accuracy 0.9995 | validation: Loss 0.104847 Accuracy -5.5868\n",
      "Epoch 516 | train: Loss 0.000018 Accuracy 0.9993 | validation: Loss 0.104486 Accuracy -5.5641\n",
      "Epoch 517 | train: Loss 0.000007 Accuracy 0.9998 | validation: Loss 0.105768 Accuracy -5.6447\n",
      "Epoch 518 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.104598 Accuracy -5.5711\n",
      "Epoch 519 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.105657 Accuracy -5.6377\n",
      "Epoch 520 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.104007 Accuracy -5.5340\n",
      "Epoch 521 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.105487 Accuracy -5.6270\n",
      "Epoch 522 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.104118 Accuracy -5.5410\n",
      "Epoch 523 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.105653 Accuracy -5.6375\n",
      "Epoch 524 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.104271 Accuracy -5.5506\n",
      "Epoch 525 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.106060 Accuracy -5.6630\n",
      "Epoch 526 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.104303 Accuracy -5.5526\n",
      "Epoch 527 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.105964 Accuracy -5.6570\n",
      "Epoch 528 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.103816 Accuracy -5.5220\n",
      "Epoch 529 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.106276 Accuracy -5.6766\n",
      "Epoch 530 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.103639 Accuracy -5.5109\n",
      "Epoch 531 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.106948 Accuracy -5.7188\n",
      "Epoch 532 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.102873 Accuracy -5.4628\n",
      "Epoch 533 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.107543 Accuracy -5.7562\n",
      "Epoch 534 | train: Loss 0.000019 Accuracy 0.9993 | validation: Loss 0.101915 Accuracy -5.4026\n",
      "Epoch 535 | train: Loss 0.000028 Accuracy 0.9990 | validation: Loss 0.109810 Accuracy -5.8986\n",
      "Epoch 536 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.100338 Accuracy -5.3035\n",
      "Epoch 537 | train: Loss 0.000066 Accuracy 0.9976 | validation: Loss 0.112646 Accuracy -6.0768\n",
      "Epoch 538 | train: Loss 0.000105 Accuracy 0.9961 | validation: Loss 0.096216 Accuracy -5.0446\n",
      "Epoch 539 | train: Loss 0.000191 Accuracy 0.9930 | validation: Loss 0.118425 Accuracy -6.4398\n",
      "Epoch 540 | train: Loss 0.000320 Accuracy 0.9883 | validation: Loss 0.090089 Accuracy -4.6596\n",
      "Epoch 541 | train: Loss 0.000594 Accuracy 0.9782 | validation: Loss 0.128205 Accuracy -7.0542\n",
      "Epoch 542 | train: Loss 0.000880 Accuracy 0.9677 | validation: Loss 0.081678 Accuracy -4.1312\n",
      "Epoch 543 | train: Loss 0.001387 Accuracy 0.9491 | validation: Loss 0.131729 Accuracy -7.2756\n",
      "Epoch 544 | train: Loss 0.001297 Accuracy 0.9525 | validation: Loss 0.082370 Accuracy -4.1747\n",
      "Epoch 545 | train: Loss 0.000930 Accuracy 0.9659 | validation: Loss 0.109149 Accuracy -5.8571\n",
      "Epoch 546 | train: Loss 0.000188 Accuracy 0.9931 | validation: Loss 0.104738 Accuracy -5.5800\n",
      "Epoch 547 | train: Loss 0.000092 Accuracy 0.9966 | validation: Loss 0.084019 Accuracy -4.2783\n",
      "Epoch 548 | train: Loss 0.000528 Accuracy 0.9806 | validation: Loss 0.113361 Accuracy -6.1217\n",
      "Epoch 549 | train: Loss 0.000588 Accuracy 0.9784 | validation: Loss 0.085802 Accuracy -4.3903\n",
      "Epoch 550 | train: Loss 0.000203 Accuracy 0.9926 | validation: Loss 0.090560 Accuracy -4.6892\n",
      "Epoch 551 | train: Loss 0.000048 Accuracy 0.9983 | validation: Loss 0.105677 Accuracy -5.6389\n",
      "Epoch 552 | train: Loss 0.000343 Accuracy 0.9874 | validation: Loss 0.081599 Accuracy -4.1263\n",
      "Epoch 553 | train: Loss 0.000354 Accuracy 0.9870 | validation: Loss 0.095562 Accuracy -5.0035\n",
      "Epoch 554 | train: Loss 0.000082 Accuracy 0.9970 | validation: Loss 0.095972 Accuracy -5.0292\n",
      "Epoch 555 | train: Loss 0.000098 Accuracy 0.9964 | validation: Loss 0.081100 Accuracy -4.0949\n",
      "Epoch 556 | train: Loss 0.000248 Accuracy 0.9909 | validation: Loss 0.097020 Accuracy -5.0951\n",
      "Epoch 557 | train: Loss 0.000132 Accuracy 0.9952 | validation: Loss 0.091026 Accuracy -4.7185\n",
      "Epoch 558 | train: Loss 0.000031 Accuracy 0.9989 | validation: Loss 0.083484 Accuracy -4.2447\n",
      "Epoch 559 | train: Loss 0.000126 Accuracy 0.9954 | validation: Loss 0.096937 Accuracy -5.0899\n",
      "Epoch 560 | train: Loss 0.000132 Accuracy 0.9952 | validation: Loss 0.089000 Accuracy -4.5912\n",
      "Epoch 561 | train: Loss 0.000046 Accuracy 0.9983 | validation: Loss 0.085139 Accuracy -4.3487\n",
      "Epoch 562 | train: Loss 0.000054 Accuracy 0.9980 | validation: Loss 0.094381 Accuracy -4.9293\n",
      "Epoch 563 | train: Loss 0.000104 Accuracy 0.9962 | validation: Loss 0.087173 Accuracy -4.4765\n",
      "Epoch 564 | train: Loss 0.000049 Accuracy 0.9982 | validation: Loss 0.088103 Accuracy -4.5349\n",
      "Epoch 565 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.095472 Accuracy -4.9978\n",
      "Epoch 566 | train: Loss 0.000077 Accuracy 0.9972 | validation: Loss 0.087566 Accuracy -4.5011\n",
      "Epoch 567 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.088910 Accuracy -4.5856\n",
      "Epoch 568 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.093693 Accuracy -4.8860\n",
      "Epoch 569 | train: Loss 0.000054 Accuracy 0.9980 | validation: Loss 0.088175 Accuracy -4.5394\n",
      "Epoch 570 | train: Loss 0.000037 Accuracy 0.9987 | validation: Loss 0.092820 Accuracy -4.8312\n",
      "Epoch 571 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.094422 Accuracy -4.9319\n",
      "Epoch 572 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.088222 Accuracy -4.5424\n",
      "Epoch 573 | train: Loss 0.000033 Accuracy 0.9988 | validation: Loss 0.093595 Accuracy -4.8799\n",
      "Epoch 574 | train: Loss 0.000015 Accuracy 0.9995 | validation: Loss 0.093683 Accuracy -4.8854\n",
      "Epoch 575 | train: Loss 0.000012 Accuracy 0.9995 | validation: Loss 0.089765 Accuracy -4.6393\n",
      "Epoch 576 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.094842 Accuracy -4.9583\n",
      "Epoch 577 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.092227 Accuracy -4.7940\n",
      "Epoch 578 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.090281 Accuracy -4.6717\n",
      "Epoch 579 | train: Loss 0.000015 Accuracy 0.9995 | validation: Loss 0.095084 Accuracy -4.9734\n",
      "Epoch 580 | train: Loss 0.000021 Accuracy 0.9992 | validation: Loss 0.091170 Accuracy -4.7276\n",
      "Epoch 581 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.090945 Accuracy -4.7134\n",
      "Epoch 582 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.094667 Accuracy -4.9473\n",
      "Epoch 583 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.091061 Accuracy -4.7207\n",
      "Epoch 584 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.092526 Accuracy -4.8127\n",
      "Epoch 585 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.093854 Accuracy -4.8962\n",
      "Epoch 586 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.090734 Accuracy -4.7002\n",
      "Epoch 587 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.094106 Accuracy -4.9120\n",
      "Epoch 588 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.093859 Accuracy -4.8965\n",
      "Epoch 589 | train: Loss 0.000004 Accuracy 0.9998 | validation: Loss 0.091799 Accuracy -4.7671\n",
      "Epoch 590 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.094836 Accuracy -4.9579\n",
      "Epoch 591 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.093334 Accuracy -4.8635\n",
      "Epoch 592 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.093591 Accuracy -4.8797\n",
      "Epoch 593 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.095583 Accuracy -5.0048\n",
      "Epoch 594 | train: Loss 0.000007 Accuracy 0.9998 | validation: Loss 0.093071 Accuracy -4.8470\n",
      "Epoch 595 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.095066 Accuracy -4.9723\n",
      "Epoch 596 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.095265 Accuracy -4.9848\n",
      "Epoch 597 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.093668 Accuracy -4.8845\n",
      "Epoch 598 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.095949 Accuracy -5.0278\n",
      "Epoch 599 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.094317 Accuracy -4.9253\n",
      "Epoch 600 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.095166 Accuracy -4.9786\n",
      "Epoch 601 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.095655 Accuracy -5.0093\n",
      "Epoch 602 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.094330 Accuracy -4.9261\n",
      "Epoch 603 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.096239 Accuracy -5.0460\n",
      "Epoch 604 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.094923 Accuracy -4.9634\n",
      "Epoch 605 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.095289 Accuracy -4.9863\n",
      "Epoch 606 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.095880 Accuracy -5.0234\n",
      "Epoch 607 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.095129 Accuracy -4.9763\n",
      "Epoch 608 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.096053 Accuracy -5.0343\n",
      "Epoch 609 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.095231 Accuracy -4.9827\n",
      "Epoch 610 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.095876 Accuracy -5.0232\n",
      "Epoch 611 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.095296 Accuracy -4.9868\n",
      "Epoch 612 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.095956 Accuracy -5.0282\n",
      "Epoch 613 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.094947 Accuracy -4.9649\n",
      "Epoch 614 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.096181 Accuracy -5.0424\n",
      "Epoch 615 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.094304 Accuracy -4.9245\n",
      "Epoch 616 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.096299 Accuracy -5.0498\n",
      "Epoch 617 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.094129 Accuracy -4.9135\n",
      "Epoch 618 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.096010 Accuracy -5.0316\n",
      "Epoch 619 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.093717 Accuracy -4.8876\n",
      "Epoch 620 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.096120 Accuracy -5.0386\n",
      "Epoch 621 | train: Loss 0.000035 Accuracy 0.9987 | validation: Loss 0.092667 Accuracy -4.8216\n",
      "Epoch 622 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.097075 Accuracy -5.0985\n",
      "Epoch 623 | train: Loss 0.000127 Accuracy 0.9953 | validation: Loss 0.089886 Accuracy -4.6469\n",
      "Epoch 624 | train: Loss 0.000167 Accuracy 0.9939 | validation: Loss 0.098756 Accuracy -5.2042\n",
      "Epoch 625 | train: Loss 0.000251 Accuracy 0.9908 | validation: Loss 0.086901 Accuracy -4.4594\n",
      "Epoch 626 | train: Loss 0.000143 Accuracy 0.9948 | validation: Loss 0.097041 Accuracy -5.0964\n",
      "Epoch 627 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.090916 Accuracy -4.7116\n",
      "Epoch 628 | train: Loss 0.000056 Accuracy 0.9979 | validation: Loss 0.091216 Accuracy -4.7304\n",
      "Epoch 629 | train: Loss 0.000068 Accuracy 0.9975 | validation: Loss 0.095395 Accuracy -4.9930\n",
      "Epoch 630 | train: Loss 0.000034 Accuracy 0.9988 | validation: Loss 0.091710 Accuracy -4.7615\n",
      "Epoch 631 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.095450 Accuracy -4.9964\n",
      "Epoch 632 | train: Loss 0.000072 Accuracy 0.9973 | validation: Loss 0.090054 Accuracy -4.6574\n",
      "Epoch 633 | train: Loss 0.000063 Accuracy 0.9977 | validation: Loss 0.096133 Accuracy -5.0394\n",
      "Epoch 634 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.090724 Accuracy -4.6995\n",
      "Epoch 635 | train: Loss 0.000047 Accuracy 0.9983 | validation: Loss 0.091730 Accuracy -4.7628\n",
      "Epoch 636 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.094262 Accuracy -4.9218\n",
      "Epoch 637 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.090176 Accuracy -4.6651\n",
      "Epoch 638 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.095540 Accuracy -5.0021\n",
      "Epoch 639 | train: Loss 0.000050 Accuracy 0.9982 | validation: Loss 0.089765 Accuracy -4.6393\n",
      "Epoch 640 | train: Loss 0.000045 Accuracy 0.9984 | validation: Loss 0.096045 Accuracy -5.0338\n",
      "Epoch 641 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.091446 Accuracy -4.7449\n",
      "Epoch 642 | train: Loss 0.000021 Accuracy 0.9992 | validation: Loss 0.093445 Accuracy -4.8705\n",
      "Epoch 643 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.093040 Accuracy -4.8450\n",
      "Epoch 644 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.093536 Accuracy -4.8762\n",
      "Epoch 645 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.094507 Accuracy -4.9372\n",
      "Epoch 646 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.088617 Accuracy -4.5672\n",
      "Epoch 647 | train: Loss 0.000033 Accuracy 0.9988 | validation: Loss 0.096841 Accuracy -5.0839\n",
      "Epoch 648 | train: Loss 0.000031 Accuracy 0.9988 | validation: Loss 0.091592 Accuracy -4.7541\n",
      "Epoch 649 | train: Loss 0.000022 Accuracy 0.9992 | validation: Loss 0.094601 Accuracy -4.9431\n",
      "Epoch 650 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.092756 Accuracy -4.8272\n",
      "Epoch 651 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.095164 Accuracy -4.9785\n",
      "Epoch 652 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.093631 Accuracy -4.8822\n",
      "Epoch 653 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.091755 Accuracy -4.7643\n",
      "Epoch 654 | train: Loss 0.000013 Accuracy 0.9995 | validation: Loss 0.096022 Accuracy -5.0324\n",
      "Epoch 655 | train: Loss 0.000013 Accuracy 0.9995 | validation: Loss 0.091348 Accuracy -4.7387\n",
      "Epoch 656 | train: Loss 0.000020 Accuracy 0.9992 | validation: Loss 0.095203 Accuracy -4.9809\n",
      "Epoch 657 | train: Loss 0.000021 Accuracy 0.9992 | validation: Loss 0.090787 Accuracy -4.7035\n",
      "Epoch 658 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.095488 Accuracy -4.9988\n",
      "Epoch 659 | train: Loss 0.000019 Accuracy 0.9993 | validation: Loss 0.091771 Accuracy -4.7653\n",
      "Epoch 660 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.093377 Accuracy -4.8662\n",
      "Epoch 661 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.092234 Accuracy -4.7944\n",
      "Epoch 662 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.093790 Accuracy -4.8922\n",
      "Epoch 663 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.092200 Accuracy -4.7923\n",
      "Epoch 664 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.091363 Accuracy -4.7397\n",
      "Epoch 665 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.093720 Accuracy -4.8877\n",
      "Epoch 666 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.090833 Accuracy -4.7064\n",
      "Epoch 667 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.093554 Accuracy -4.8774\n",
      "Epoch 668 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.091064 Accuracy -4.7209\n",
      "Epoch 669 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.093732 Accuracy -4.8885\n",
      "Epoch 670 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.089725 Accuracy -4.6368\n",
      "Epoch 671 | train: Loss 0.000018 Accuracy 0.9993 | validation: Loss 0.094540 Accuracy -4.9393\n",
      "Epoch 672 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.088903 Accuracy -4.5852\n",
      "Epoch 673 | train: Loss 0.000029 Accuracy 0.9990 | validation: Loss 0.094914 Accuracy -4.9628\n",
      "Epoch 674 | train: Loss 0.000039 Accuracy 0.9986 | validation: Loss 0.086777 Accuracy -4.4516\n",
      "Epoch 675 | train: Loss 0.000062 Accuracy 0.9977 | validation: Loss 0.097634 Accuracy -5.1337\n",
      "Epoch 676 | train: Loss 0.000089 Accuracy 0.9967 | validation: Loss 0.083790 Accuracy -4.2639\n",
      "Epoch 677 | train: Loss 0.000146 Accuracy 0.9947 | validation: Loss 0.100165 Accuracy -5.2927\n",
      "Epoch 678 | train: Loss 0.000202 Accuracy 0.9926 | validation: Loss 0.080644 Accuracy -4.0663\n",
      "Epoch 679 | train: Loss 0.000322 Accuracy 0.9882 | validation: Loss 0.103339 Accuracy -5.4921\n",
      "Epoch 680 | train: Loss 0.000427 Accuracy 0.9843 | validation: Loss 0.075375 Accuracy -3.7353\n",
      "Epoch 681 | train: Loss 0.000574 Accuracy 0.9790 | validation: Loss 0.104451 Accuracy -5.5619\n",
      "Epoch 682 | train: Loss 0.000522 Accuracy 0.9809 | validation: Loss 0.074784 Accuracy -3.6981\n",
      "Epoch 683 | train: Loss 0.000420 Accuracy 0.9846 | validation: Loss 0.094832 Accuracy -4.9576\n",
      "Epoch 684 | train: Loss 0.000207 Accuracy 0.9924 | validation: Loss 0.084090 Accuracy -4.2828\n",
      "Epoch 685 | train: Loss 0.000094 Accuracy 0.9966 | validation: Loss 0.081989 Accuracy -4.1508\n",
      "Epoch 686 | train: Loss 0.000095 Accuracy 0.9965 | validation: Loss 0.091166 Accuracy -4.7273\n",
      "Epoch 687 | train: Loss 0.000108 Accuracy 0.9960 | validation: Loss 0.079374 Accuracy -3.9865\n",
      "Epoch 688 | train: Loss 0.000172 Accuracy 0.9937 | validation: Loss 0.092322 Accuracy -4.8000\n",
      "Epoch 689 | train: Loss 0.000192 Accuracy 0.9930 | validation: Loss 0.079245 Accuracy -3.9784\n",
      "Epoch 690 | train: Loss 0.000129 Accuracy 0.9953 | validation: Loss 0.085486 Accuracy -4.3705\n",
      "Epoch 691 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.087674 Accuracy -4.5079\n",
      "Epoch 692 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.077632 Accuracy -3.8771\n",
      "Epoch 693 | train: Loss 0.000121 Accuracy 0.9956 | validation: Loss 0.088933 Accuracy -4.5870\n",
      "Epoch 694 | train: Loss 0.000091 Accuracy 0.9967 | validation: Loss 0.081502 Accuracy -4.1202\n",
      "Epoch 695 | train: Loss 0.000052 Accuracy 0.9981 | validation: Loss 0.084106 Accuracy -4.2838\n",
      "Epoch 696 | train: Loss 0.000037 Accuracy 0.9986 | validation: Loss 0.084466 Accuracy -4.3064\n",
      "Epoch 697 | train: Loss 0.000050 Accuracy 0.9982 | validation: Loss 0.080279 Accuracy -4.0434\n",
      "Epoch 698 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.086125 Accuracy -4.4106\n",
      "Epoch 699 | train: Loss 0.000056 Accuracy 0.9979 | validation: Loss 0.079908 Accuracy -4.0201\n",
      "Epoch 700 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.083303 Accuracy -4.2333\n",
      "Epoch 701 | train: Loss 0.000012 Accuracy 0.9995 | validation: Loss 0.082861 Accuracy -4.2056\n",
      "Epoch 702 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.081182 Accuracy -4.1001\n",
      "Epoch 703 | train: Loss 0.000042 Accuracy 0.9985 | validation: Loss 0.084240 Accuracy -4.2922\n",
      "Epoch 704 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.080043 Accuracy -4.0285\n",
      "Epoch 705 | train: Loss 0.000036 Accuracy 0.9987 | validation: Loss 0.084625 Accuracy -4.3164\n",
      "Epoch 706 | train: Loss 0.000036 Accuracy 0.9987 | validation: Loss 0.082076 Accuracy -4.1563\n",
      "Epoch 707 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.080710 Accuracy -4.0704\n",
      "Epoch 708 | train: Loss 0.000023 Accuracy 0.9992 | validation: Loss 0.084490 Accuracy -4.3079\n",
      "Epoch 709 | train: Loss 0.000033 Accuracy 0.9988 | validation: Loss 0.080445 Accuracy -4.0538\n",
      "Epoch 710 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.082546 Accuracy -4.1858\n",
      "Epoch 711 | train: Loss 0.000019 Accuracy 0.9993 | validation: Loss 0.082839 Accuracy -4.2042\n",
      "Epoch 712 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.082733 Accuracy -4.1975\n",
      "Epoch 713 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.082780 Accuracy -4.2005\n",
      "Epoch 714 | train: Loss 0.000015 Accuracy 0.9994 | validation: Loss 0.081530 Accuracy -4.1219\n",
      "Epoch 715 | train: Loss 0.000015 Accuracy 0.9994 | validation: Loss 0.083480 Accuracy -4.2444\n",
      "Epoch 716 | train: Loss 0.000007 Accuracy 0.9998 | validation: Loss 0.080904 Accuracy -4.0826\n",
      "Epoch 717 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.081800 Accuracy -4.1389\n",
      "Epoch 718 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.081968 Accuracy -4.1494\n",
      "Epoch 719 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.080490 Accuracy -4.0566\n",
      "Epoch 720 | train: Loss 0.000012 Accuracy 0.9995 | validation: Loss 0.082513 Accuracy -4.1837\n",
      "Epoch 721 | train: Loss 0.000012 Accuracy 0.9995 | validation: Loss 0.080328 Accuracy -4.0464\n",
      "Epoch 722 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.081626 Accuracy -4.1280\n",
      "Epoch 723 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.080578 Accuracy -4.0622\n",
      "Epoch 724 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.080389 Accuracy -4.0503\n",
      "Epoch 725 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.080970 Accuracy -4.0867\n",
      "Epoch 726 | train: Loss 0.000004 Accuracy 0.9999 | validation: Loss 0.080272 Accuracy -4.0429\n",
      "Epoch 727 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.080778 Accuracy -4.0747\n",
      "Epoch 728 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.079715 Accuracy -4.0079\n",
      "Epoch 729 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.081287 Accuracy -4.1067\n",
      "Epoch 730 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.079459 Accuracy -3.9918\n",
      "Epoch 731 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.079957 Accuracy -4.0231\n",
      "Epoch 732 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.080230 Accuracy -4.0403\n",
      "Epoch 733 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.079163 Accuracy -3.9733\n",
      "Epoch 734 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.079857 Accuracy -4.0169\n",
      "Epoch 735 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.078965 Accuracy -3.9608\n",
      "Epoch 736 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.079738 Accuracy -4.0094\n",
      "Epoch 737 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.078552 Accuracy -3.9348\n",
      "Epoch 738 | train: Loss 0.000005 Accuracy 0.9998 | validation: Loss 0.079084 Accuracy -3.9683\n",
      "Epoch 739 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.078547 Accuracy -3.9346\n",
      "Epoch 740 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.078264 Accuracy -3.9167\n",
      "Epoch 741 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.078365 Accuracy -3.9231\n",
      "Epoch 742 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.078121 Accuracy -3.9078\n",
      "Epoch 743 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.078158 Accuracy -3.9101\n",
      "Epoch 744 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.077296 Accuracy -3.8560\n",
      "Epoch 745 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.078327 Accuracy -3.9207\n",
      "Epoch 746 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.077104 Accuracy -3.8439\n",
      "Epoch 747 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.077539 Accuracy -3.8712\n",
      "Epoch 748 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076951 Accuracy -3.8343\n",
      "Epoch 749 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.077268 Accuracy -3.8542\n",
      "Epoch 750 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076628 Accuracy -3.8140\n",
      "Epoch 751 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.076984 Accuracy -3.8363\n",
      "Epoch 752 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076688 Accuracy -3.8178\n",
      "Epoch 753 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076382 Accuracy -3.7985\n",
      "Epoch 754 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076441 Accuracy -3.8023\n",
      "Epoch 755 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076184 Accuracy -3.7861\n",
      "Epoch 756 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.076151 Accuracy -3.7840\n",
      "Epoch 757 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.075792 Accuracy -3.7615\n",
      "Epoch 758 | train: Loss 0.000001 Accuracy 0.9999 | validation: Loss 0.076015 Accuracy -3.7755\n",
      "Epoch 759 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.075355 Accuracy -3.7340\n",
      "Epoch 760 | train: Loss 0.000001 Accuracy 0.9999 | validation: Loss 0.075739 Accuracy -3.7581\n",
      "Epoch 761 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.075202 Accuracy -3.7244\n",
      "Epoch 762 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.075517 Accuracy -3.7442\n",
      "Epoch 763 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.074909 Accuracy -3.7060\n",
      "Epoch 764 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.075437 Accuracy -3.7392\n",
      "Epoch 765 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.074546 Accuracy -3.6832\n",
      "Epoch 766 | train: Loss 0.000001 Accuracy 1.0000 | validation: Loss 0.075438 Accuracy -3.7392\n",
      "Epoch 767 | train: Loss 0.000002 Accuracy 0.9999 | validation: Loss 0.074007 Accuracy -3.6493\n",
      "Epoch 768 | train: Loss 0.000003 Accuracy 0.9999 | validation: Loss 0.075592 Accuracy -3.7489\n",
      "Epoch 769 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.073285 Accuracy -3.6040\n",
      "Epoch 770 | train: Loss 0.000013 Accuracy 0.9995 | validation: Loss 0.076135 Accuracy -3.7830\n",
      "Epoch 771 | train: Loss 0.000030 Accuracy 0.9989 | validation: Loss 0.071529 Accuracy -3.4937\n",
      "Epoch 772 | train: Loss 0.000074 Accuracy 0.9973 | validation: Loss 0.078026 Accuracy -3.9018\n",
      "Epoch 773 | train: Loss 0.000192 Accuracy 0.9930 | validation: Loss 0.068278 Accuracy -3.2894\n",
      "Epoch 774 | train: Loss 0.000413 Accuracy 0.9849 | validation: Loss 0.082429 Accuracy -4.1784\n",
      "Epoch 775 | train: Loss 0.001430 Accuracy 0.9476 | validation: Loss 0.072157 Accuracy -3.5331\n",
      "Epoch 776 | train: Loss 0.009153 Accuracy 0.6643 | validation: Loss 0.091707 Accuracy -4.7613\n",
      "Epoch 777 | train: Loss 0.017789 Accuracy 0.3476 | validation: Loss 0.050090 Accuracy -2.1468\n",
      "Epoch 778 | train: Loss 0.037433 Accuracy -0.3728 | validation: Loss 0.100954 Accuracy -5.3422\n",
      "Epoch 779 | train: Loss 0.017678 Accuracy 0.3517 | validation: Loss 0.109625 Accuracy -5.8870\n",
      "Epoch 780 | train: Loss 0.017812 Accuracy 0.3468 | validation: Loss 0.037576 Accuracy -1.3607\n",
      "Epoch 781 | train: Loss 0.015668 Accuracy 0.4254 | validation: Loss 0.041864 Accuracy -1.6300\n",
      "Epoch 782 | train: Loss 0.014627 Accuracy 0.4636 | validation: Loss 0.090424 Accuracy -4.6807\n",
      "Epoch 783 | train: Loss 0.008273 Accuracy 0.6966 | validation: Loss 0.115196 Accuracy -6.2370\n",
      "Epoch 784 | train: Loss 0.016291 Accuracy 0.4026 | validation: Loss 0.228181 Accuracy -13.3350\n",
      "Epoch 785 | train: Loss 0.016803 Accuracy 0.3838 | validation: Loss 0.126813 Accuracy -6.9668\n",
      "Epoch 786 | train: Loss 0.007038 Accuracy 0.7419 | validation: Loss 0.070562 Accuracy -3.4329\n",
      "Epoch 787 | train: Loss 0.011880 Accuracy 0.5643 | validation: Loss 0.081066 Accuracy -4.0928\n",
      "Epoch 788 | train: Loss 0.010146 Accuracy 0.6279 | validation: Loss 0.154081 Accuracy -8.6798\n",
      "Epoch 789 | train: Loss 0.007975 Accuracy 0.7075 | validation: Loss 0.224074 Accuracy -13.0770\n",
      "Epoch 790 | train: Loss 0.008517 Accuracy 0.6876 | validation: Loss 0.194358 Accuracy -11.2101\n",
      "Epoch 791 | train: Loss 0.005108 Accuracy 0.8127 | validation: Loss 0.179742 Accuracy -10.2919\n",
      "Epoch 792 | train: Loss 0.008526 Accuracy 0.6873 | validation: Loss 0.220992 Accuracy -12.8834\n",
      "Epoch 793 | train: Loss 0.005971 Accuracy 0.7810 | validation: Loss 0.217079 Accuracy -12.6375\n",
      "Epoch 794 | train: Loss 0.006279 Accuracy 0.7697 | validation: Loss 0.167947 Accuracy -9.5509\n",
      "Epoch 795 | train: Loss 0.008540 Accuracy 0.6868 | validation: Loss 0.167337 Accuracy -9.5126\n",
      "Epoch 796 | train: Loss 0.008894 Accuracy 0.6738 | validation: Loss 0.164659 Accuracy -9.3444\n",
      "Epoch 797 | train: Loss 0.008903 Accuracy 0.6735 | validation: Loss 0.139513 Accuracy -7.7646\n",
      "Epoch 798 | train: Loss 0.008180 Accuracy 0.7000 | validation: Loss 0.120041 Accuracy -6.5413\n",
      "Epoch 799 | train: Loss 0.008721 Accuracy 0.6802 | validation: Loss 0.122979 Accuracy -6.7259\n",
      "Epoch 800 | train: Loss 0.008757 Accuracy 0.6789 | validation: Loss 0.144747 Accuracy -8.0934\n",
      "Epoch 801 | train: Loss 0.008095 Accuracy 0.7031 | validation: Loss 0.164879 Accuracy -9.3582\n",
      "Epoch 802 | train: Loss 0.007836 Accuracy 0.7126 | validation: Loss 0.164444 Accuracy -9.3309\n",
      "Epoch 803 | train: Loss 0.007536 Accuracy 0.7236 | validation: Loss 0.167506 Accuracy -9.5232\n",
      "Epoch 804 | train: Loss 0.007599 Accuracy 0.7213 | validation: Loss 0.197496 Accuracy -11.4073\n",
      "Epoch 805 | train: Loss 0.007171 Accuracy 0.7370 | validation: Loss 0.220610 Accuracy -12.8593\n",
      "Epoch 806 | train: Loss 0.007112 Accuracy 0.7392 | validation: Loss 0.217239 Accuracy -12.6476\n",
      "Epoch 807 | train: Loss 0.006646 Accuracy 0.7563 | validation: Loss 0.231946 Accuracy -13.5715\n",
      "Epoch 808 | train: Loss 0.006284 Accuracy 0.7696 | validation: Loss 0.266152 Accuracy -15.7205\n",
      "Epoch 809 | train: Loss 0.005950 Accuracy 0.7818 | validation: Loss 0.275894 Accuracy -16.3325\n",
      "Epoch 810 | train: Loss 0.005384 Accuracy 0.8025 | validation: Loss 0.276193 Accuracy -16.3512\n",
      "Epoch 811 | train: Loss 0.005173 Accuracy 0.8103 | validation: Loss 0.306534 Accuracy -18.2574\n",
      "Epoch 812 | train: Loss 0.004700 Accuracy 0.8276 | validation: Loss 0.310497 Accuracy -18.5063\n",
      "Epoch 813 | train: Loss 0.004520 Accuracy 0.8342 | validation: Loss 0.312967 Accuracy -18.6615\n",
      "Epoch 814 | train: Loss 0.004802 Accuracy 0.8239 | validation: Loss 0.345114 Accuracy -20.6811\n",
      "Epoch 815 | train: Loss 0.005759 Accuracy 0.7888 | validation: Loss 0.329557 Accuracy -19.7038\n",
      "Epoch 816 | train: Loss 0.005430 Accuracy 0.8009 | validation: Loss 0.285564 Accuracy -16.9400\n",
      "Epoch 817 | train: Loss 0.005847 Accuracy 0.7856 | validation: Loss 0.331648 Accuracy -19.8351\n",
      "Epoch 818 | train: Loss 0.005112 Accuracy 0.8125 | validation: Loss 0.346074 Accuracy -20.7414\n",
      "Epoch 819 | train: Loss 0.005054 Accuracy 0.8147 | validation: Loss 0.316633 Accuracy -18.8918\n",
      "Epoch 820 | train: Loss 0.005580 Accuracy 0.7954 | validation: Loss 0.345089 Accuracy -20.6795\n",
      "Epoch 821 | train: Loss 0.004404 Accuracy 0.8385 | validation: Loss 0.363084 Accuracy -21.8100\n",
      "Epoch 822 | train: Loss 0.004746 Accuracy 0.8259 | validation: Loss 0.342866 Accuracy -20.5398\n",
      "Epoch 823 | train: Loss 0.004803 Accuracy 0.8239 | validation: Loss 0.339160 Accuracy -20.3070\n",
      "Epoch 824 | train: Loss 0.004666 Accuracy 0.8289 | validation: Loss 0.338164 Accuracy -20.2445\n",
      "Epoch 825 | train: Loss 0.004576 Accuracy 0.8322 | validation: Loss 0.315648 Accuracy -18.8300\n",
      "Epoch 826 | train: Loss 0.004254 Accuracy 0.8440 | validation: Loss 0.303432 Accuracy -18.0625\n",
      "Epoch 827 | train: Loss 0.004599 Accuracy 0.8313 | validation: Loss 0.317426 Accuracy -18.9416\n",
      "Epoch 828 | train: Loss 0.004376 Accuracy 0.8395 | validation: Loss 0.321348 Accuracy -19.1880\n",
      "Epoch 829 | train: Loss 0.004287 Accuracy 0.8428 | validation: Loss 0.306604 Accuracy -18.2617\n",
      "Epoch 830 | train: Loss 0.004199 Accuracy 0.8460 | validation: Loss 0.309774 Accuracy -18.4609\n",
      "Epoch 831 | train: Loss 0.004338 Accuracy 0.8409 | validation: Loss 0.337501 Accuracy -20.2028\n",
      "Epoch 832 | train: Loss 0.004317 Accuracy 0.8417 | validation: Loss 0.331414 Accuracy -19.8204\n",
      "Epoch 833 | train: Loss 0.004176 Accuracy 0.8468 | validation: Loss 0.303581 Accuracy -18.0719\n",
      "Epoch 834 | train: Loss 0.004255 Accuracy 0.8440 | validation: Loss 0.304151 Accuracy -18.1076\n",
      "Epoch 835 | train: Loss 0.004038 Accuracy 0.8519 | validation: Loss 0.302330 Accuracy -17.9933\n",
      "Epoch 836 | train: Loss 0.004224 Accuracy 0.8451 | validation: Loss 0.282244 Accuracy -16.7314\n",
      "Epoch 837 | train: Loss 0.004100 Accuracy 0.8496 | validation: Loss 0.271487 Accuracy -16.0556\n",
      "Epoch 838 | train: Loss 0.004161 Accuracy 0.8474 | validation: Loss 0.286451 Accuracy -16.9957\n",
      "Epoch 839 | train: Loss 0.003958 Accuracy 0.8549 | validation: Loss 0.300340 Accuracy -17.8682\n",
      "Epoch 840 | train: Loss 0.004001 Accuracy 0.8533 | validation: Loss 0.293994 Accuracy -17.4696\n",
      "Epoch 841 | train: Loss 0.003996 Accuracy 0.8535 | validation: Loss 0.295581 Accuracy -17.5693\n",
      "Epoch 842 | train: Loss 0.003957 Accuracy 0.8549 | validation: Loss 0.299878 Accuracy -17.8392\n",
      "Epoch 843 | train: Loss 0.003949 Accuracy 0.8552 | validation: Loss 0.281217 Accuracy -16.6669\n",
      "Epoch 844 | train: Loss 0.003834 Accuracy 0.8594 | validation: Loss 0.268674 Accuracy -15.8789\n",
      "Epoch 845 | train: Loss 0.003870 Accuracy 0.8581 | validation: Loss 0.273442 Accuracy -16.1784\n",
      "Epoch 846 | train: Loss 0.003843 Accuracy 0.8591 | validation: Loss 0.272196 Accuracy -16.1002\n",
      "Epoch 847 | train: Loss 0.003832 Accuracy 0.8595 | validation: Loss 0.264340 Accuracy -15.6066\n",
      "Epoch 848 | train: Loss 0.003828 Accuracy 0.8596 | validation: Loss 0.273459 Accuracy -16.1795\n",
      "Epoch 849 | train: Loss 0.003751 Accuracy 0.8625 | validation: Loss 0.284678 Accuracy -16.8843\n",
      "Epoch 850 | train: Loss 0.003752 Accuracy 0.8624 | validation: Loss 0.280998 Accuracy -16.6531\n",
      "Epoch 851 | train: Loss 0.003762 Accuracy 0.8621 | validation: Loss 0.289491 Accuracy -17.1867\n",
      "Epoch 852 | train: Loss 0.003711 Accuracy 0.8639 | validation: Loss 0.290765 Accuracy -17.2667\n",
      "Epoch 853 | train: Loss 0.003686 Accuracy 0.8648 | validation: Loss 0.279916 Accuracy -16.5852\n",
      "Epoch 854 | train: Loss 0.003690 Accuracy 0.8647 | validation: Loss 0.283460 Accuracy -16.8078\n",
      "Epoch 855 | train: Loss 0.003671 Accuracy 0.8654 | validation: Loss 0.268027 Accuracy -15.8382\n",
      "Epoch 856 | train: Loss 0.003605 Accuracy 0.8678 | validation: Loss 0.261124 Accuracy -15.4046\n",
      "Epoch 857 | train: Loss 0.003590 Accuracy 0.8683 | validation: Loss 0.264363 Accuracy -15.6081\n",
      "Epoch 858 | train: Loss 0.003592 Accuracy 0.8683 | validation: Loss 0.255826 Accuracy -15.0717\n",
      "Epoch 859 | train: Loss 0.003531 Accuracy 0.8705 | validation: Loss 0.261050 Accuracy -15.3999\n",
      "Epoch 860 | train: Loss 0.003472 Accuracy 0.8727 | validation: Loss 0.264869 Accuracy -15.6399\n",
      "Epoch 861 | train: Loss 0.003442 Accuracy 0.8738 | validation: Loss 0.259690 Accuracy -15.3145\n",
      "Epoch 862 | train: Loss 0.003458 Accuracy 0.8732 | validation: Loss 0.278395 Accuracy -16.4896\n",
      "Epoch 863 | train: Loss 0.003662 Accuracy 0.8657 | validation: Loss 0.246050 Accuracy -14.4576\n",
      "Epoch 864 | train: Loss 0.003906 Accuracy 0.8568 | validation: Loss 0.284742 Accuracy -16.8883\n",
      "Epoch 865 | train: Loss 0.004419 Accuracy 0.8379 | validation: Loss 0.231136 Accuracy -13.5206\n",
      "Epoch 866 | train: Loss 0.003758 Accuracy 0.8622 | validation: Loss 0.243740 Accuracy -14.3124\n",
      "Epoch 867 | train: Loss 0.003203 Accuracy 0.8825 | validation: Loss 0.258238 Accuracy -15.2233\n",
      "Epoch 868 | train: Loss 0.003532 Accuracy 0.8705 | validation: Loss 0.225673 Accuracy -13.1774\n",
      "Epoch 869 | train: Loss 0.003619 Accuracy 0.8673 | validation: Loss 0.257453 Accuracy -15.1739\n",
      "Epoch 870 | train: Loss 0.003288 Accuracy 0.8794 | validation: Loss 0.243906 Accuracy -14.3229\n",
      "Epoch 871 | train: Loss 0.002942 Accuracy 0.8921 | validation: Loss 0.233546 Accuracy -13.6720\n",
      "Epoch 872 | train: Loss 0.003099 Accuracy 0.8863 | validation: Loss 0.267547 Accuracy -15.8081\n",
      "Epoch 873 | train: Loss 0.003796 Accuracy 0.8608 | validation: Loss 0.208684 Accuracy -12.1102\n",
      "Epoch 874 | train: Loss 0.004566 Accuracy 0.8325 | validation: Loss 0.270834 Accuracy -16.0146\n",
      "Epoch 875 | train: Loss 0.005953 Accuracy 0.7817 | validation: Loss 0.187661 Accuracy -10.7894\n",
      "Epoch 876 | train: Loss 0.003463 Accuracy 0.8730 | validation: Loss 0.171316 Accuracy -9.7626\n",
      "Epoch 877 | train: Loss 0.004057 Accuracy 0.8512 | validation: Loss 0.223975 Accuracy -13.0708\n",
      "Epoch 878 | train: Loss 0.004191 Accuracy 0.8463 | validation: Loss 0.201818 Accuracy -11.6788\n",
      "Epoch 879 | train: Loss 0.002921 Accuracy 0.8929 | validation: Loss 0.166801 Accuracy -9.4789\n",
      "Epoch 880 | train: Loss 0.004469 Accuracy 0.8361 | validation: Loss 0.213749 Accuracy -12.4284\n",
      "Epoch 881 | train: Loss 0.003130 Accuracy 0.8852 | validation: Loss 0.215690 Accuracy -12.5503\n",
      "Epoch 882 | train: Loss 0.003273 Accuracy 0.8800 | validation: Loss 0.169909 Accuracy -9.6742\n",
      "Epoch 883 | train: Loss 0.003597 Accuracy 0.8681 | validation: Loss 0.185922 Accuracy -10.6802\n",
      "Epoch 884 | train: Loss 0.002586 Accuracy 0.9052 | validation: Loss 0.213816 Accuracy -12.4325\n",
      "Epoch 885 | train: Loss 0.003353 Accuracy 0.8770 | validation: Loss 0.176677 Accuracy -10.0994\n",
      "Epoch 886 | train: Loss 0.002529 Accuracy 0.9073 | validation: Loss 0.167583 Accuracy -9.5280\n",
      "Epoch 887 | train: Loss 0.002749 Accuracy 0.8992 | validation: Loss 0.197428 Accuracy -11.4030\n",
      "Epoch 888 | train: Loss 0.002880 Accuracy 0.8944 | validation: Loss 0.171352 Accuracy -9.7649\n",
      "Epoch 889 | train: Loss 0.002135 Accuracy 0.9217 | validation: Loss 0.156619 Accuracy -8.8393\n",
      "Epoch 890 | train: Loss 0.002522 Accuracy 0.9075 | validation: Loss 0.185932 Accuracy -10.6808\n",
      "Epoch 891 | train: Loss 0.002758 Accuracy 0.8989 | validation: Loss 0.157083 Accuracy -8.8684\n",
      "Epoch 892 | train: Loss 0.002111 Accuracy 0.9226 | validation: Loss 0.158718 Accuracy -8.9711\n",
      "Epoch 893 | train: Loss 0.001927 Accuracy 0.9293 | validation: Loss 0.176615 Accuracy -10.0955\n",
      "Epoch 894 | train: Loss 0.002439 Accuracy 0.9105 | validation: Loss 0.141475 Accuracy -7.8879\n",
      "Epoch 895 | train: Loss 0.003010 Accuracy 0.8896 | validation: Loss 0.181105 Accuracy -10.3775\n",
      "Epoch 896 | train: Loss 0.003334 Accuracy 0.8777 | validation: Loss 0.137215 Accuracy -7.6202\n",
      "Epoch 897 | train: Loss 0.002209 Accuracy 0.9190 | validation: Loss 0.133005 Accuracy -7.3557\n",
      "Epoch 898 | train: Loss 0.002303 Accuracy 0.9155 | validation: Loss 0.162682 Accuracy -9.2201\n",
      "Epoch 899 | train: Loss 0.002688 Accuracy 0.9014 | validation: Loss 0.142091 Accuracy -7.9266\n",
      "Epoch 900 | train: Loss 0.001868 Accuracy 0.9315 | validation: Loss 0.133791 Accuracy -7.4052\n",
      "Epoch 901 | train: Loss 0.002279 Accuracy 0.9164 | validation: Loss 0.166844 Accuracy -9.4816\n",
      "Epoch 902 | train: Loss 0.002235 Accuracy 0.9180 | validation: Loss 0.155663 Accuracy -8.7792\n",
      "Epoch 903 | train: Loss 0.001745 Accuracy 0.9360 | validation: Loss 0.146559 Accuracy -8.2073\n",
      "Epoch 904 | train: Loss 0.002113 Accuracy 0.9225 | validation: Loss 0.172360 Accuracy -9.8282\n",
      "Epoch 905 | train: Loss 0.002203 Accuracy 0.9192 | validation: Loss 0.151503 Accuracy -8.5179\n",
      "Epoch 906 | train: Loss 0.001691 Accuracy 0.9380 | validation: Loss 0.137685 Accuracy -7.6498\n",
      "Epoch 907 | train: Loss 0.002035 Accuracy 0.9254 | validation: Loss 0.156846 Accuracy -8.8535\n",
      "Epoch 908 | train: Loss 0.001847 Accuracy 0.9322 | validation: Loss 0.154754 Accuracy -8.7221\n",
      "Epoch 909 | train: Loss 0.001785 Accuracy 0.9345 | validation: Loss 0.137434 Accuracy -7.6340\n",
      "Epoch 910 | train: Loss 0.001895 Accuracy 0.9305 | validation: Loss 0.150159 Accuracy -8.4334\n",
      "Epoch 911 | train: Loss 0.001586 Accuracy 0.9418 | validation: Loss 0.164714 Accuracy -9.3478\n",
      "Epoch 912 | train: Loss 0.001763 Accuracy 0.9353 | validation: Loss 0.148958 Accuracy -8.3580\n",
      "Epoch 913 | train: Loss 0.001721 Accuracy 0.9369 | validation: Loss 0.157053 Accuracy -8.8665\n",
      "Epoch 914 | train: Loss 0.001525 Accuracy 0.9441 | validation: Loss 0.163878 Accuracy -9.2953\n",
      "Epoch 915 | train: Loss 0.001634 Accuracy 0.9401 | validation: Loss 0.144342 Accuracy -8.0680\n",
      "Epoch 916 | train: Loss 0.001583 Accuracy 0.9419 | validation: Loss 0.149041 Accuracy -8.3632\n",
      "Epoch 917 | train: Loss 0.001438 Accuracy 0.9473 | validation: Loss 0.153847 Accuracy -8.6651\n",
      "Epoch 918 | train: Loss 0.001523 Accuracy 0.9442 | validation: Loss 0.141827 Accuracy -7.9100\n",
      "Epoch 919 | train: Loss 0.001495 Accuracy 0.9452 | validation: Loss 0.151220 Accuracy -8.5001\n",
      "Epoch 920 | train: Loss 0.001366 Accuracy 0.9499 | validation: Loss 0.158537 Accuracy -8.9597\n",
      "Epoch 921 | train: Loss 0.001384 Accuracy 0.9493 | validation: Loss 0.150347 Accuracy -8.4452\n",
      "Epoch 922 | train: Loss 0.001439 Accuracy 0.9472 | validation: Loss 0.165423 Accuracy -9.3924\n",
      "Epoch 923 | train: Loss 0.001412 Accuracy 0.9482 | validation: Loss 0.153565 Accuracy -8.6474\n",
      "Epoch 924 | train: Loss 0.001316 Accuracy 0.9517 | validation: Loss 0.153623 Accuracy -8.6510\n",
      "Epoch 925 | train: Loss 0.001260 Accuracy 0.9538 | validation: Loss 0.152786 Accuracy -8.5985\n",
      "Epoch 926 | train: Loss 0.001260 Accuracy 0.9538 | validation: Loss 0.147444 Accuracy -8.2629\n",
      "Epoch 927 | train: Loss 0.001257 Accuracy 0.9539 | validation: Loss 0.153703 Accuracy -8.6561\n",
      "Epoch 928 | train: Loss 0.001273 Accuracy 0.9533 | validation: Loss 0.144895 Accuracy -8.1028\n",
      "Epoch 929 | train: Loss 0.001275 Accuracy 0.9532 | validation: Loss 0.156506 Accuracy -8.8322\n",
      "Epoch 930 | train: Loss 0.001305 Accuracy 0.9521 | validation: Loss 0.142837 Accuracy -7.9734\n",
      "Epoch 931 | train: Loss 0.001409 Accuracy 0.9483 | validation: Loss 0.160551 Accuracy -9.0863\n",
      "Epoch 932 | train: Loss 0.001365 Accuracy 0.9499 | validation: Loss 0.143445 Accuracy -8.0116\n",
      "Epoch 933 | train: Loss 0.001376 Accuracy 0.9495 | validation: Loss 0.158399 Accuracy -8.9511\n",
      "Epoch 934 | train: Loss 0.001268 Accuracy 0.9535 | validation: Loss 0.146425 Accuracy -8.1988\n",
      "Epoch 935 | train: Loss 0.001182 Accuracy 0.9567 | validation: Loss 0.153343 Accuracy -8.6334\n",
      "Epoch 936 | train: Loss 0.001115 Accuracy 0.9591 | validation: Loss 0.152342 Accuracy -8.5706\n",
      "Epoch 937 | train: Loss 0.001091 Accuracy 0.9600 | validation: Loss 0.151545 Accuracy -8.5205\n",
      "Epoch 938 | train: Loss 0.001072 Accuracy 0.9607 | validation: Loss 0.154984 Accuracy -8.7365\n",
      "Epoch 939 | train: Loss 0.001099 Accuracy 0.9597 | validation: Loss 0.145816 Accuracy -8.1606\n",
      "Epoch 940 | train: Loss 0.001126 Accuracy 0.9587 | validation: Loss 0.156761 Accuracy -8.8482\n",
      "Epoch 941 | train: Loss 0.001226 Accuracy 0.9550 | validation: Loss 0.139408 Accuracy -7.7580\n",
      "Epoch 942 | train: Loss 0.001424 Accuracy 0.9478 | validation: Loss 0.165438 Accuracy -9.3933\n",
      "Epoch 943 | train: Loss 0.001818 Accuracy 0.9333 | validation: Loss 0.128866 Accuracy -7.0957\n",
      "Epoch 944 | train: Loss 0.002129 Accuracy 0.9219 | validation: Loss 0.158148 Accuracy -8.9353\n",
      "Epoch 945 | train: Loss 0.001447 Accuracy 0.9469 | validation: Loss 0.147821 Accuracy -8.2865\n",
      "Epoch 946 | train: Loss 0.001096 Accuracy 0.9598 | validation: Loss 0.134468 Accuracy -7.4477\n",
      "Epoch 947 | train: Loss 0.001448 Accuracy 0.9469 | validation: Loss 0.157094 Accuracy -8.8691\n",
      "Epoch 948 | train: Loss 0.001391 Accuracy 0.9490 | validation: Loss 0.140583 Accuracy -7.8319\n",
      "Epoch 949 | train: Loss 0.001126 Accuracy 0.9587 | validation: Loss 0.142396 Accuracy -7.9457\n",
      "Epoch 950 | train: Loss 0.001027 Accuracy 0.9624 | validation: Loss 0.150599 Accuracy -8.4611\n",
      "Epoch 951 | train: Loss 0.001176 Accuracy 0.9569 | validation: Loss 0.133383 Accuracy -7.3795\n",
      "Epoch 952 | train: Loss 0.001173 Accuracy 0.9570 | validation: Loss 0.144445 Accuracy -8.0745\n",
      "Epoch 953 | train: Loss 0.001013 Accuracy 0.9628 | validation: Loss 0.146181 Accuracy -8.1835\n",
      "Epoch 954 | train: Loss 0.000998 Accuracy 0.9634 | validation: Loss 0.139354 Accuracy -7.7546\n",
      "Epoch 955 | train: Loss 0.001111 Accuracy 0.9593 | validation: Loss 0.156546 Accuracy -8.8347\n",
      "Epoch 956 | train: Loss 0.001159 Accuracy 0.9575 | validation: Loss 0.141182 Accuracy -7.8695\n",
      "Epoch 957 | train: Loss 0.001023 Accuracy 0.9625 | validation: Loss 0.145807 Accuracy -8.1600\n",
      "Epoch 958 | train: Loss 0.000892 Accuracy 0.9673 | validation: Loss 0.151954 Accuracy -8.5462\n",
      "Epoch 959 | train: Loss 0.001019 Accuracy 0.9626 | validation: Loss 0.136932 Accuracy -7.6024\n",
      "Epoch 960 | train: Loss 0.001165 Accuracy 0.9573 | validation: Loss 0.152957 Accuracy -8.6092\n",
      "Epoch 961 | train: Loss 0.001044 Accuracy 0.9617 | validation: Loss 0.141450 Accuracy -7.8863\n",
      "Epoch 962 | train: Loss 0.000916 Accuracy 0.9664 | validation: Loss 0.142301 Accuracy -7.9397\n",
      "Epoch 963 | train: Loss 0.000880 Accuracy 0.9677 | validation: Loss 0.150982 Accuracy -8.4851\n",
      "Epoch 964 | train: Loss 0.001001 Accuracy 0.9633 | validation: Loss 0.134451 Accuracy -7.4466\n",
      "Epoch 965 | train: Loss 0.001142 Accuracy 0.9581 | validation: Loss 0.159371 Accuracy -9.0122\n",
      "Epoch 966 | train: Loss 0.001238 Accuracy 0.9546 | validation: Loss 0.128044 Accuracy -7.0441\n",
      "Epoch 967 | train: Loss 0.001383 Accuracy 0.9493 | validation: Loss 0.149746 Accuracy -8.4075\n",
      "Epoch 968 | train: Loss 0.001126 Accuracy 0.9587 | validation: Loss 0.138678 Accuracy -7.7122\n",
      "Epoch 969 | train: Loss 0.000825 Accuracy 0.9697 | validation: Loss 0.133185 Accuracy -7.3671\n",
      "Epoch 970 | train: Loss 0.000988 Accuracy 0.9638 | validation: Loss 0.154126 Accuracy -8.6827\n",
      "Epoch 971 | train: Loss 0.001356 Accuracy 0.9503 | validation: Loss 0.129002 Accuracy -7.1043\n",
      "Epoch 972 | train: Loss 0.001223 Accuracy 0.9552 | validation: Loss 0.143435 Accuracy -8.0110\n",
      "Epoch 973 | train: Loss 0.000862 Accuracy 0.9684 | validation: Loss 0.144643 Accuracy -8.0869\n",
      "Epoch 974 | train: Loss 0.000920 Accuracy 0.9662 | validation: Loss 0.128580 Accuracy -7.0778\n",
      "Epoch 975 | train: Loss 0.001096 Accuracy 0.9598 | validation: Loss 0.145227 Accuracy -8.1236\n",
      "Epoch 976 | train: Loss 0.000884 Accuracy 0.9676 | validation: Loss 0.142630 Accuracy -7.9605\n",
      "Epoch 977 | train: Loss 0.000780 Accuracy 0.9714 | validation: Loss 0.135857 Accuracy -7.5349\n",
      "Epoch 978 | train: Loss 0.000948 Accuracy 0.9652 | validation: Loss 0.155546 Accuracy -8.7719\n",
      "Epoch 979 | train: Loss 0.001145 Accuracy 0.9580 | validation: Loss 0.133820 Accuracy -7.4070\n",
      "Epoch 980 | train: Loss 0.000968 Accuracy 0.9645 | validation: Loss 0.141007 Accuracy -7.8585\n",
      "Epoch 981 | train: Loss 0.000764 Accuracy 0.9720 | validation: Loss 0.151717 Accuracy -8.5313\n",
      "Epoch 982 | train: Loss 0.000939 Accuracy 0.9656 | validation: Loss 0.134827 Accuracy -7.4702\n",
      "Epoch 983 | train: Loss 0.001016 Accuracy 0.9628 | validation: Loss 0.152069 Accuracy -8.5534\n",
      "Epoch 984 | train: Loss 0.000875 Accuracy 0.9679 | validation: Loss 0.143035 Accuracy -7.9859\n",
      "Epoch 985 | train: Loss 0.000702 Accuracy 0.9742 | validation: Loss 0.138204 Accuracy -7.6824\n",
      "Epoch 986 | train: Loss 0.000785 Accuracy 0.9712 | validation: Loss 0.132210 Accuracy -7.3058\n",
      "Epoch 987 | train: Loss 0.000884 Accuracy 0.9676 | validation: Loss 0.118779 Accuracy -6.4620\n",
      "Epoch 988 | train: Loss 0.000775 Accuracy 0.9716 | validation: Loss 0.147955 Accuracy -8.2949\n",
      "Epoch 989 | train: Loss 0.000663 Accuracy 0.9757 | validation: Loss 0.151556 Accuracy -8.5212\n",
      "Epoch 990 | train: Loss 0.000681 Accuracy 0.9750 | validation: Loss 0.144058 Accuracy -8.0502\n",
      "Epoch 991 | train: Loss 0.000741 Accuracy 0.9728 | validation: Loss 0.155722 Accuracy -8.7829\n",
      "Epoch 992 | train: Loss 0.000795 Accuracy 0.9709 | validation: Loss 0.139900 Accuracy -7.7889\n",
      "Epoch 993 | train: Loss 0.000743 Accuracy 0.9727 | validation: Loss 0.144840 Accuracy -8.0993\n",
      "Epoch 994 | train: Loss 0.000634 Accuracy 0.9767 | validation: Loss 0.146149 Accuracy -8.1815\n",
      "Epoch 995 | train: Loss 0.000651 Accuracy 0.9761 | validation: Loss 0.134745 Accuracy -7.4651\n",
      "Epoch 996 | train: Loss 0.000774 Accuracy 0.9716 | validation: Loss 0.148814 Accuracy -8.3489\n",
      "Epoch 997 | train: Loss 0.000892 Accuracy 0.9673 | validation: Loss 0.128483 Accuracy -7.0717\n",
      "Epoch 998 | train: Loss 0.000816 Accuracy 0.9701 | validation: Loss 0.137684 Accuracy -7.6497\n",
      "Epoch 999 | train: Loss 0.000613 Accuracy 0.9775 | validation: Loss 0.141392 Accuracy -7.8827\n",
      "Epoch 1000 | train: Loss 0.000653 Accuracy 0.9761 | validation: Loss 0.128798 Accuracy -7.0915\n",
      "Epoch 1001 | train: Loss 0.000874 Accuracy 0.9679 | validation: Loss 0.150468 Accuracy -8.4529\n",
      "Epoch 1002 | train: Loss 0.001164 Accuracy 0.9573 | validation: Loss 0.122101 Accuracy -6.6708\n",
      "Epoch 1003 | train: Loss 0.000947 Accuracy 0.9653 | validation: Loss 0.129910 Accuracy -7.1613\n",
      "Epoch 1004 | train: Loss 0.000594 Accuracy 0.9782 | validation: Loss 0.136978 Accuracy -7.6054\n",
      "Epoch 1005 | train: Loss 0.000802 Accuracy 0.9706 | validation: Loss 0.118801 Accuracy -6.4634\n",
      "Epoch 1006 | train: Loss 0.001014 Accuracy 0.9628 | validation: Loss 0.140592 Accuracy -7.8324\n",
      "Epoch 1007 | train: Loss 0.000931 Accuracy 0.9658 | validation: Loss 0.125273 Accuracy -6.8700\n",
      "Epoch 1008 | train: Loss 0.000633 Accuracy 0.9768 | validation: Loss 0.121964 Accuracy -6.6622\n",
      "Epoch 1009 | train: Loss 0.000684 Accuracy 0.9749 | validation: Loss 0.138865 Accuracy -7.7239\n",
      "Epoch 1010 | train: Loss 0.000872 Accuracy 0.9680 | validation: Loss 0.122426 Accuracy -6.6911\n",
      "Epoch 1011 | train: Loss 0.000756 Accuracy 0.9723 | validation: Loss 0.139734 Accuracy -7.7785\n",
      "Epoch 1012 | train: Loss 0.000543 Accuracy 0.9801 | validation: Loss 0.141124 Accuracy -7.8658\n",
      "Epoch 1013 | train: Loss 0.000574 Accuracy 0.9790 | validation: Loss 0.121139 Accuracy -6.6103\n",
      "Epoch 1014 | train: Loss 0.000768 Accuracy 0.9718 | validation: Loss 0.135862 Accuracy -7.5352\n",
      "Epoch 1015 | train: Loss 0.000728 Accuracy 0.9733 | validation: Loss 0.120402 Accuracy -6.5640\n",
      "Epoch 1016 | train: Loss 0.000535 Accuracy 0.9804 | validation: Loss 0.120005 Accuracy -6.5391\n",
      "Epoch 1017 | train: Loss 0.000543 Accuracy 0.9801 | validation: Loss 0.136230 Accuracy -7.5584\n",
      "Epoch 1018 | train: Loss 0.000668 Accuracy 0.9755 | validation: Loss 0.124091 Accuracy -6.7957\n",
      "Epoch 1019 | train: Loss 0.000602 Accuracy 0.9779 | validation: Loss 0.131338 Accuracy -7.2510\n",
      "Epoch 1020 | train: Loss 0.000471 Accuracy 0.9827 | validation: Loss 0.133783 Accuracy -7.4046\n",
      "Epoch 1021 | train: Loss 0.000523 Accuracy 0.9808 | validation: Loss 0.122486 Accuracy -6.6949\n",
      "Epoch 1022 | train: Loss 0.000589 Accuracy 0.9784 | validation: Loss 0.132252 Accuracy -7.3085\n",
      "Epoch 1023 | train: Loss 0.000524 Accuracy 0.9808 | validation: Loss 0.126860 Accuracy -6.9697\n",
      "Epoch 1024 | train: Loss 0.000457 Accuracy 0.9832 | validation: Loss 0.120907 Accuracy -6.5957\n",
      "Epoch 1025 | train: Loss 0.000523 Accuracy 0.9808 | validation: Loss 0.131805 Accuracy -7.2804\n",
      "Epoch 1026 | train: Loss 0.000595 Accuracy 0.9782 | validation: Loss 0.120271 Accuracy -6.5558\n",
      "Epoch 1027 | train: Loss 0.000511 Accuracy 0.9813 | validation: Loss 0.124340 Accuracy -6.8114\n",
      "Epoch 1028 | train: Loss 0.000430 Accuracy 0.9842 | validation: Loss 0.130077 Accuracy -7.1718\n",
      "Epoch 1029 | train: Loss 0.000506 Accuracy 0.9814 | validation: Loss 0.119739 Accuracy -6.5224\n",
      "Epoch 1030 | train: Loss 0.000562 Accuracy 0.9794 | validation: Loss 0.131218 Accuracy -7.2435\n",
      "Epoch 1031 | train: Loss 0.000531 Accuracy 0.9805 | validation: Loss 0.123792 Accuracy -6.7770\n",
      "Epoch 1032 | train: Loss 0.000435 Accuracy 0.9841 | validation: Loss 0.119921 Accuracy -6.5338\n",
      "Epoch 1033 | train: Loss 0.000475 Accuracy 0.9826 | validation: Loss 0.129638 Accuracy -7.1442\n",
      "Epoch 1034 | train: Loss 0.000530 Accuracy 0.9806 | validation: Loss 0.120578 Accuracy -6.5751\n",
      "Epoch 1035 | train: Loss 0.000492 Accuracy 0.9820 | validation: Loss 0.126249 Accuracy -6.9313\n",
      "Epoch 1036 | train: Loss 0.000415 Accuracy 0.9848 | validation: Loss 0.126432 Accuracy -6.9429\n",
      "Epoch 1037 | train: Loss 0.000435 Accuracy 0.9841 | validation: Loss 0.116911 Accuracy -6.3447\n",
      "Epoch 1038 | train: Loss 0.000490 Accuracy 0.9820 | validation: Loss 0.126489 Accuracy -6.9464\n",
      "Epoch 1039 | train: Loss 0.000499 Accuracy 0.9817 | validation: Loss 0.119656 Accuracy -6.5171\n",
      "Epoch 1040 | train: Loss 0.000432 Accuracy 0.9841 | validation: Loss 0.118223 Accuracy -6.4271\n",
      "Epoch 1041 | train: Loss 0.000424 Accuracy 0.9844 | validation: Loss 0.125912 Accuracy -6.9102\n",
      "Epoch 1042 | train: Loss 0.000468 Accuracy 0.9829 | validation: Loss 0.118349 Accuracy -6.4350\n",
      "Epoch 1043 | train: Loss 0.000498 Accuracy 0.9818 | validation: Loss 0.130767 Accuracy -7.2151\n",
      "Epoch 1044 | train: Loss 0.000569 Accuracy 0.9791 | validation: Loss 0.114798 Accuracy -6.2119\n",
      "Epoch 1045 | train: Loss 0.000474 Accuracy 0.9826 | validation: Loss 0.118258 Accuracy -6.4293\n",
      "Epoch 1046 | train: Loss 0.000410 Accuracy 0.9850 | validation: Loss 0.118157 Accuracy -6.4230\n",
      "Epoch 1047 | train: Loss 0.000387 Accuracy 0.9858 | validation: Loss 0.114188 Accuracy -6.1737\n",
      "Epoch 1048 | train: Loss 0.000433 Accuracy 0.9841 | validation: Loss 0.120684 Accuracy -6.5817\n",
      "Epoch 1049 | train: Loss 0.000400 Accuracy 0.9853 | validation: Loss 0.115994 Accuracy -6.2871\n",
      "Epoch 1050 | train: Loss 0.000391 Accuracy 0.9857 | validation: Loss 0.118575 Accuracy -6.4492\n",
      "Epoch 1051 | train: Loss 0.000357 Accuracy 0.9869 | validation: Loss 0.116554 Accuracy -6.3222\n",
      "Epoch 1052 | train: Loss 0.000357 Accuracy 0.9869 | validation: Loss 0.113924 Accuracy -6.1570\n",
      "Epoch 1053 | train: Loss 0.000357 Accuracy 0.9869 | validation: Loss 0.115779 Accuracy -6.2736\n",
      "Epoch 1054 | train: Loss 0.000346 Accuracy 0.9873 | validation: Loss 0.115791 Accuracy -6.2743\n",
      "Epoch 1055 | train: Loss 0.000350 Accuracy 0.9872 | validation: Loss 0.115941 Accuracy -6.2838\n",
      "Epoch 1056 | train: Loss 0.000336 Accuracy 0.9877 | validation: Loss 0.113327 Accuracy -6.1196\n",
      "Epoch 1057 | train: Loss 0.000344 Accuracy 0.9874 | validation: Loss 0.112532 Accuracy -6.0696\n",
      "Epoch 1058 | train: Loss 0.000333 Accuracy 0.9878 | validation: Loss 0.113545 Accuracy -6.1332\n",
      "Epoch 1059 | train: Loss 0.000335 Accuracy 0.9877 | validation: Loss 0.112093 Accuracy -6.0420\n",
      "Epoch 1060 | train: Loss 0.000323 Accuracy 0.9881 | validation: Loss 0.112677 Accuracy -6.0787\n",
      "Epoch 1061 | train: Loss 0.000323 Accuracy 0.9882 | validation: Loss 0.113244 Accuracy -6.1143\n",
      "Epoch 1062 | train: Loss 0.000320 Accuracy 0.9883 | validation: Loss 0.109217 Accuracy -5.8614\n",
      "Epoch 1063 | train: Loss 0.000320 Accuracy 0.9883 | validation: Loss 0.110940 Accuracy -5.9696\n",
      "Epoch 1064 | train: Loss 0.000332 Accuracy 0.9878 | validation: Loss 0.106518 Accuracy -5.6918\n",
      "Epoch 1065 | train: Loss 0.000338 Accuracy 0.9876 | validation: Loss 0.109481 Accuracy -5.8779\n",
      "Epoch 1066 | train: Loss 0.000314 Accuracy 0.9885 | validation: Loss 0.106186 Accuracy -5.6709\n",
      "Epoch 1067 | train: Loss 0.000319 Accuracy 0.9883 | validation: Loss 0.110254 Accuracy -5.9265\n",
      "Epoch 1068 | train: Loss 0.000300 Accuracy 0.9890 | validation: Loss 0.108102 Accuracy -5.7913\n",
      "Epoch 1069 | train: Loss 0.000302 Accuracy 0.9889 | validation: Loss 0.104978 Accuracy -5.5950\n",
      "Epoch 1070 | train: Loss 0.000298 Accuracy 0.9891 | validation: Loss 0.105372 Accuracy -5.6198\n",
      "Epoch 1071 | train: Loss 0.000285 Accuracy 0.9896 | validation: Loss 0.104245 Accuracy -5.5490\n",
      "Epoch 1072 | train: Loss 0.000298 Accuracy 0.9891 | validation: Loss 0.104257 Accuracy -5.5497\n",
      "Epoch 1073 | train: Loss 0.000295 Accuracy 0.9892 | validation: Loss 0.101304 Accuracy -5.3642\n",
      "Epoch 1074 | train: Loss 0.000285 Accuracy 0.9896 | validation: Loss 0.103409 Accuracy -5.4965\n",
      "Epoch 1075 | train: Loss 0.000289 Accuracy 0.9894 | validation: Loss 0.100437 Accuracy -5.3097\n",
      "Epoch 1076 | train: Loss 0.000275 Accuracy 0.9899 | validation: Loss 0.101155 Accuracy -5.3548\n",
      "Epoch 1077 | train: Loss 0.000269 Accuracy 0.9901 | validation: Loss 0.100073 Accuracy -5.2869\n",
      "Epoch 1078 | train: Loss 0.000270 Accuracy 0.9901 | validation: Loss 0.099654 Accuracy -5.2605\n",
      "Epoch 1079 | train: Loss 0.000261 Accuracy 0.9904 | validation: Loss 0.097478 Accuracy -5.1239\n",
      "Epoch 1080 | train: Loss 0.000261 Accuracy 0.9904 | validation: Loss 0.097686 Accuracy -5.1369\n",
      "Epoch 1081 | train: Loss 0.000257 Accuracy 0.9906 | validation: Loss 0.097246 Accuracy -5.1093\n",
      "Epoch 1082 | train: Loss 0.000252 Accuracy 0.9907 | validation: Loss 0.096850 Accuracy -5.0844\n",
      "Epoch 1083 | train: Loss 0.000249 Accuracy 0.9909 | validation: Loss 0.096125 Accuracy -5.0388\n",
      "Epoch 1084 | train: Loss 0.000247 Accuracy 0.9909 | validation: Loss 0.095347 Accuracy -4.9900\n",
      "Epoch 1085 | train: Loss 0.000246 Accuracy 0.9910 | validation: Loss 0.095593 Accuracy -5.0054\n",
      "Epoch 1086 | train: Loss 0.000245 Accuracy 0.9910 | validation: Loss 0.094120 Accuracy -4.9129\n",
      "Epoch 1087 | train: Loss 0.000242 Accuracy 0.9911 | validation: Loss 0.093927 Accuracy -4.9007\n",
      "Epoch 1088 | train: Loss 0.000238 Accuracy 0.9913 | validation: Loss 0.092389 Accuracy -4.8041\n",
      "Epoch 1089 | train: Loss 0.000236 Accuracy 0.9913 | validation: Loss 0.093059 Accuracy -4.8463\n",
      "Epoch 1090 | train: Loss 0.000232 Accuracy 0.9915 | validation: Loss 0.091838 Accuracy -4.7695\n",
      "Epoch 1091 | train: Loss 0.000234 Accuracy 0.9914 | validation: Loss 0.092336 Accuracy -4.8008\n",
      "Epoch 1092 | train: Loss 0.000236 Accuracy 0.9914 | validation: Loss 0.089400 Accuracy -4.6164\n",
      "Epoch 1093 | train: Loss 0.000247 Accuracy 0.9909 | validation: Loss 0.092866 Accuracy -4.8341\n",
      "Epoch 1094 | train: Loss 0.000267 Accuracy 0.9902 | validation: Loss 0.086705 Accuracy -4.4471\n",
      "Epoch 1095 | train: Loss 0.000331 Accuracy 0.9878 | validation: Loss 0.094842 Accuracy -4.9582\n",
      "Epoch 1096 | train: Loss 0.000493 Accuracy 0.9819 | validation: Loss 0.081846 Accuracy -4.1418\n",
      "Epoch 1097 | train: Loss 0.000863 Accuracy 0.9684 | validation: Loss 0.098223 Accuracy -5.1707\n",
      "Epoch 1098 | train: Loss 0.001058 Accuracy 0.9612 | validation: Loss 0.076547 Accuracy -3.8089\n",
      "Epoch 1099 | train: Loss 0.001155 Accuracy 0.9576 | validation: Loss 0.093942 Accuracy -4.9017\n",
      "Epoch 1100 | train: Loss 0.000930 Accuracy 0.9659 | validation: Loss 0.076420 Accuracy -3.8009\n",
      "Epoch 1101 | train: Loss 0.000535 Accuracy 0.9804 | validation: Loss 0.079355 Accuracy -3.9853\n",
      "Epoch 1102 | train: Loss 0.000290 Accuracy 0.9894 | validation: Loss 0.090914 Accuracy -4.7115\n",
      "Epoch 1103 | train: Loss 0.000657 Accuracy 0.9759 | validation: Loss 0.077240 Accuracy -3.8525\n",
      "Epoch 1104 | train: Loss 0.000631 Accuracy 0.9768 | validation: Loss 0.088841 Accuracy -4.5813\n",
      "Epoch 1105 | train: Loss 0.000617 Accuracy 0.9774 | validation: Loss 0.083862 Accuracy -4.2684\n",
      "Epoch 1106 | train: Loss 0.000603 Accuracy 0.9779 | validation: Loss 0.091573 Accuracy -4.7529\n",
      "Epoch 1107 | train: Loss 0.000324 Accuracy 0.9881 | validation: Loss 0.092508 Accuracy -4.8117\n",
      "Epoch 1108 | train: Loss 0.000449 Accuracy 0.9835 | validation: Loss 0.086142 Accuracy -4.4117\n",
      "Epoch 1109 | train: Loss 0.000498 Accuracy 0.9817 | validation: Loss 0.099332 Accuracy -5.2403\n",
      "Epoch 1110 | train: Loss 0.000598 Accuracy 0.9781 | validation: Loss 0.084056 Accuracy -4.2807\n",
      "Epoch 1111 | train: Loss 0.000378 Accuracy 0.9861 | validation: Loss 0.082790 Accuracy -4.2011\n",
      "Epoch 1112 | train: Loss 0.000289 Accuracy 0.9894 | validation: Loss 0.086891 Accuracy -4.4587\n",
      "Epoch 1113 | train: Loss 0.000426 Accuracy 0.9844 | validation: Loss 0.081545 Accuracy -4.1229\n",
      "Epoch 1114 | train: Loss 0.000452 Accuracy 0.9834 | validation: Loss 0.094373 Accuracy -4.9288\n",
      "Epoch 1115 | train: Loss 0.000375 Accuracy 0.9863 | validation: Loss 0.092593 Accuracy -4.8169\n",
      "Epoch 1116 | train: Loss 0.000234 Accuracy 0.9914 | validation: Loss 0.090764 Accuracy -4.7021\n",
      "Epoch 1117 | train: Loss 0.000347 Accuracy 0.9873 | validation: Loss 0.099619 Accuracy -5.2584\n",
      "Epoch 1118 | train: Loss 0.000411 Accuracy 0.9849 | validation: Loss 0.087051 Accuracy -4.4688\n",
      "Epoch 1119 | train: Loss 0.000355 Accuracy 0.9870 | validation: Loss 0.090721 Accuracy -4.6994\n",
      "Epoch 1120 | train: Loss 0.000285 Accuracy 0.9896 | validation: Loss 0.089736 Accuracy -4.6375\n",
      "Epoch 1121 | train: Loss 0.000236 Accuracy 0.9913 | validation: Loss 0.082341 Accuracy -4.1729\n",
      "Epoch 1122 | train: Loss 0.000356 Accuracy 0.9869 | validation: Loss 0.090368 Accuracy -4.6772\n",
      "Epoch 1123 | train: Loss 0.000337 Accuracy 0.9876 | validation: Loss 0.085194 Accuracy -4.3521\n",
      "Epoch 1124 | train: Loss 0.000266 Accuracy 0.9902 | validation: Loss 0.090093 Accuracy -4.6599\n",
      "Epoch 1125 | train: Loss 0.000252 Accuracy 0.9908 | validation: Loss 0.089993 Accuracy -4.6536\n",
      "Epoch 1126 | train: Loss 0.000202 Accuracy 0.9926 | validation: Loss 0.086541 Accuracy -4.4367\n",
      "Epoch 1127 | train: Loss 0.000246 Accuracy 0.9910 | validation: Loss 0.091353 Accuracy -4.7391\n",
      "Epoch 1128 | train: Loss 0.000262 Accuracy 0.9904 | validation: Loss 0.087027 Accuracy -4.4673\n",
      "Epoch 1129 | train: Loss 0.000199 Accuracy 0.9927 | validation: Loss 0.088844 Accuracy -4.5815\n",
      "Epoch 1130 | train: Loss 0.000210 Accuracy 0.9923 | validation: Loss 0.087136 Accuracy -4.4741\n",
      "Epoch 1131 | train: Loss 0.000196 Accuracy 0.9928 | validation: Loss 0.082928 Accuracy -4.2098\n",
      "Epoch 1132 | train: Loss 0.000199 Accuracy 0.9927 | validation: Loss 0.086496 Accuracy -4.4339\n",
      "Epoch 1133 | train: Loss 0.000221 Accuracy 0.9919 | validation: Loss 0.083933 Accuracy -4.2729\n",
      "Epoch 1134 | train: Loss 0.000192 Accuracy 0.9930 | validation: Loss 0.088020 Accuracy -4.5297\n",
      "Epoch 1135 | train: Loss 0.000172 Accuracy 0.9937 | validation: Loss 0.087901 Accuracy -4.5222\n",
      "Epoch 1136 | train: Loss 0.000175 Accuracy 0.9936 | validation: Loss 0.086912 Accuracy -4.4601\n",
      "Epoch 1137 | train: Loss 0.000168 Accuracy 0.9938 | validation: Loss 0.089442 Accuracy -4.6190\n",
      "Epoch 1138 | train: Loss 0.000182 Accuracy 0.9933 | validation: Loss 0.084835 Accuracy -4.3296\n",
      "Epoch 1139 | train: Loss 0.000183 Accuracy 0.9933 | validation: Loss 0.087459 Accuracy -4.4944\n",
      "Epoch 1140 | train: Loss 0.000160 Accuracy 0.9941 | validation: Loss 0.085298 Accuracy -4.3587\n",
      "Epoch 1141 | train: Loss 0.000153 Accuracy 0.9944 | validation: Loss 0.084650 Accuracy -4.3180\n",
      "Epoch 1142 | train: Loss 0.000149 Accuracy 0.9945 | validation: Loss 0.085364 Accuracy -4.3628\n",
      "Epoch 1143 | train: Loss 0.000149 Accuracy 0.9945 | validation: Loss 0.082369 Accuracy -4.1747\n",
      "Epoch 1144 | train: Loss 0.000158 Accuracy 0.9942 | validation: Loss 0.085296 Accuracy -4.3585\n",
      "Epoch 1145 | train: Loss 0.000154 Accuracy 0.9944 | validation: Loss 0.082819 Accuracy -4.2029\n",
      "Epoch 1146 | train: Loss 0.000141 Accuracy 0.9948 | validation: Loss 0.084467 Accuracy -4.3065\n",
      "Epoch 1147 | train: Loss 0.000137 Accuracy 0.9950 | validation: Loss 0.082430 Accuracy -4.1785\n",
      "Epoch 1148 | train: Loss 0.000130 Accuracy 0.9952 | validation: Loss 0.081859 Accuracy -4.1426\n",
      "Epoch 1149 | train: Loss 0.000125 Accuracy 0.9954 | validation: Loss 0.080904 Accuracy -4.0826\n",
      "Epoch 1150 | train: Loss 0.000126 Accuracy 0.9954 | validation: Loss 0.079735 Accuracy -4.0092\n",
      "Epoch 1151 | train: Loss 0.000122 Accuracy 0.9955 | validation: Loss 0.080291 Accuracy -4.0441\n",
      "Epoch 1152 | train: Loss 0.000120 Accuracy 0.9956 | validation: Loss 0.078398 Accuracy -3.9252\n",
      "Epoch 1153 | train: Loss 0.000122 Accuracy 0.9955 | validation: Loss 0.078982 Accuracy -3.9619\n",
      "Epoch 1154 | train: Loss 0.000118 Accuracy 0.9957 | validation: Loss 0.076648 Accuracy -3.8152\n",
      "Epoch 1155 | train: Loss 0.000116 Accuracy 0.9957 | validation: Loss 0.077341 Accuracy -3.8588\n",
      "Epoch 1156 | train: Loss 0.000111 Accuracy 0.9959 | validation: Loss 0.076167 Accuracy -3.7850\n",
      "Epoch 1157 | train: Loss 0.000108 Accuracy 0.9960 | validation: Loss 0.076441 Accuracy -3.8022\n",
      "Epoch 1158 | train: Loss 0.000108 Accuracy 0.9960 | validation: Loss 0.075406 Accuracy -3.7372\n",
      "Epoch 1159 | train: Loss 0.000102 Accuracy 0.9963 | validation: Loss 0.074679 Accuracy -3.6916\n",
      "Epoch 1160 | train: Loss 0.000100 Accuracy 0.9963 | validation: Loss 0.073860 Accuracy -3.6401\n",
      "Epoch 1161 | train: Loss 0.000100 Accuracy 0.9963 | validation: Loss 0.072900 Accuracy -3.5798\n",
      "Epoch 1162 | train: Loss 0.000097 Accuracy 0.9964 | validation: Loss 0.073083 Accuracy -3.5913\n",
      "Epoch 1163 | train: Loss 0.000096 Accuracy 0.9965 | validation: Loss 0.071604 Accuracy -3.4984\n",
      "Epoch 1164 | train: Loss 0.000100 Accuracy 0.9963 | validation: Loss 0.072180 Accuracy -3.5345\n",
      "Epoch 1165 | train: Loss 0.000111 Accuracy 0.9959 | validation: Loss 0.069580 Accuracy -3.3712\n",
      "Epoch 1166 | train: Loss 0.000112 Accuracy 0.9959 | validation: Loss 0.071100 Accuracy -3.4667\n",
      "Epoch 1167 | train: Loss 0.000110 Accuracy 0.9960 | validation: Loss 0.067715 Accuracy -3.2540\n",
      "Epoch 1168 | train: Loss 0.000113 Accuracy 0.9958 | validation: Loss 0.070174 Accuracy -3.4086\n",
      "Epoch 1169 | train: Loss 0.000124 Accuracy 0.9955 | validation: Loss 0.066213 Accuracy -3.1597\n",
      "Epoch 1170 | train: Loss 0.000133 Accuracy 0.9951 | validation: Loss 0.069209 Accuracy -3.3479\n",
      "Epoch 1171 | train: Loss 0.000133 Accuracy 0.9951 | validation: Loss 0.064572 Accuracy -3.0566\n",
      "Epoch 1172 | train: Loss 0.000138 Accuracy 0.9949 | validation: Loss 0.068494 Accuracy -3.3030\n",
      "Epoch 1173 | train: Loss 0.000155 Accuracy 0.9943 | validation: Loss 0.063388 Accuracy -2.9822\n",
      "Epoch 1174 | train: Loss 0.000167 Accuracy 0.9939 | validation: Loss 0.068112 Accuracy -3.2790\n",
      "Epoch 1175 | train: Loss 0.000199 Accuracy 0.9927 | validation: Loss 0.061978 Accuracy -2.8937\n",
      "Epoch 1176 | train: Loss 0.000202 Accuracy 0.9926 | validation: Loss 0.067908 Accuracy -3.2662\n",
      "Epoch 1177 | train: Loss 0.000207 Accuracy 0.9924 | validation: Loss 0.061200 Accuracy -2.8448\n",
      "Epoch 1178 | train: Loss 0.000232 Accuracy 0.9915 | validation: Loss 0.067059 Accuracy -3.2128\n",
      "Epoch 1179 | train: Loss 0.000258 Accuracy 0.9905 | validation: Loss 0.060283 Accuracy -2.7872\n",
      "Epoch 1180 | train: Loss 0.000258 Accuracy 0.9905 | validation: Loss 0.066100 Accuracy -3.1526\n",
      "Epoch 1181 | train: Loss 0.000219 Accuracy 0.9920 | validation: Loss 0.059262 Accuracy -2.7230\n",
      "Epoch 1182 | train: Loss 0.000230 Accuracy 0.9916 | validation: Loss 0.064627 Accuracy -3.0600\n",
      "Epoch 1183 | train: Loss 0.000267 Accuracy 0.9902 | validation: Loss 0.058130 Accuracy -2.6519\n",
      "Epoch 1184 | train: Loss 0.000210 Accuracy 0.9923 | validation: Loss 0.062762 Accuracy -2.9429\n",
      "Epoch 1185 | train: Loss 0.000148 Accuracy 0.9946 | validation: Loss 0.058845 Accuracy -2.6968\n",
      "Epoch 1186 | train: Loss 0.000143 Accuracy 0.9948 | validation: Loss 0.061349 Accuracy -2.8541\n",
      "Epoch 1187 | train: Loss 0.000095 Accuracy 0.9965 | validation: Loss 0.060505 Accuracy -2.8011\n",
      "Epoch 1188 | train: Loss 0.000064 Accuracy 0.9976 | validation: Loss 0.059920 Accuracy -2.7644\n",
      "Epoch 1189 | train: Loss 0.000095 Accuracy 0.9965 | validation: Loss 0.061233 Accuracy -2.8468\n",
      "Epoch 1190 | train: Loss 0.000103 Accuracy 0.9962 | validation: Loss 0.057820 Accuracy -2.6324\n",
      "Epoch 1191 | train: Loss 0.000109 Accuracy 0.9960 | validation: Loss 0.061492 Accuracy -2.8631\n",
      "Epoch 1192 | train: Loss 0.000158 Accuracy 0.9942 | validation: Loss 0.056422 Accuracy -2.5446\n",
      "Epoch 1193 | train: Loss 0.000159 Accuracy 0.9942 | validation: Loss 0.060680 Accuracy -2.8121\n",
      "Epoch 1194 | train: Loss 0.000172 Accuracy 0.9937 | validation: Loss 0.054903 Accuracy -2.4492\n",
      "Epoch 1195 | train: Loss 0.000203 Accuracy 0.9925 | validation: Loss 0.060070 Accuracy -2.7738\n",
      "Epoch 1196 | train: Loss 0.000175 Accuracy 0.9936 | validation: Loss 0.055037 Accuracy -2.4576\n",
      "Epoch 1197 | train: Loss 0.000157 Accuracy 0.9943 | validation: Loss 0.059609 Accuracy -2.7448\n",
      "Epoch 1198 | train: Loss 0.000152 Accuracy 0.9944 | validation: Loss 0.055113 Accuracy -2.4624\n",
      "Epoch 1199 | train: Loss 0.000107 Accuracy 0.9961 | validation: Loss 0.057444 Accuracy -2.6088\n",
      "Epoch 1200 | train: Loss 0.000075 Accuracy 0.9972 | validation: Loss 0.055230 Accuracy -2.4697\n",
      "Epoch 1201 | train: Loss 0.000066 Accuracy 0.9976 | validation: Loss 0.055910 Accuracy -2.5125\n",
      "Epoch 1202 | train: Loss 0.000046 Accuracy 0.9983 | validation: Loss 0.056252 Accuracy -2.5339\n",
      "Epoch 1203 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.054790 Accuracy -2.4421\n",
      "Epoch 1204 | train: Loss 0.000070 Accuracy 0.9974 | validation: Loss 0.056474 Accuracy -2.5479\n",
      "Epoch 1205 | train: Loss 0.000075 Accuracy 0.9972 | validation: Loss 0.052805 Accuracy -2.3174\n",
      "Epoch 1206 | train: Loss 0.000104 Accuracy 0.9962 | validation: Loss 0.056056 Accuracy -2.5216\n",
      "Epoch 1207 | train: Loss 0.000134 Accuracy 0.9951 | validation: Loss 0.051407 Accuracy -2.2295\n",
      "Epoch 1208 | train: Loss 0.000139 Accuracy 0.9949 | validation: Loss 0.055344 Accuracy -2.4768\n",
      "Epoch 1209 | train: Loss 0.000145 Accuracy 0.9947 | validation: Loss 0.050764 Accuracy -2.1892\n",
      "Epoch 1210 | train: Loss 0.000152 Accuracy 0.9944 | validation: Loss 0.054776 Accuracy -2.4412\n",
      "Epoch 1211 | train: Loss 0.000143 Accuracy 0.9948 | validation: Loss 0.050272 Accuracy -2.1582\n",
      "Epoch 1212 | train: Loss 0.000145 Accuracy 0.9947 | validation: Loss 0.054250 Accuracy -2.4082\n",
      "Epoch 1213 | train: Loss 0.000149 Accuracy 0.9945 | validation: Loss 0.049877 Accuracy -2.1335\n",
      "Epoch 1214 | train: Loss 0.000126 Accuracy 0.9954 | validation: Loss 0.053186 Accuracy -2.3413\n",
      "Epoch 1215 | train: Loss 0.000105 Accuracy 0.9962 | validation: Loss 0.049987 Accuracy -2.1403\n",
      "Epoch 1216 | train: Loss 0.000089 Accuracy 0.9967 | validation: Loss 0.052538 Accuracy -2.3006\n",
      "Epoch 1217 | train: Loss 0.000067 Accuracy 0.9976 | validation: Loss 0.050312 Accuracy -2.1607\n",
      "Epoch 1218 | train: Loss 0.000054 Accuracy 0.9980 | validation: Loss 0.051456 Accuracy -2.2326\n",
      "Epoch 1219 | train: Loss 0.000042 Accuracy 0.9985 | validation: Loss 0.050092 Accuracy -2.1469\n",
      "Epoch 1220 | train: Loss 0.000034 Accuracy 0.9988 | validation: Loss 0.050149 Accuracy -2.1505\n",
      "Epoch 1221 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.049926 Accuracy -2.1365\n",
      "Epoch 1222 | train: Loss 0.000030 Accuracy 0.9989 | validation: Loss 0.049302 Accuracy -2.0973\n",
      "Epoch 1223 | train: Loss 0.000031 Accuracy 0.9989 | validation: Loss 0.049793 Accuracy -2.1281\n",
      "Epoch 1224 | train: Loss 0.000034 Accuracy 0.9987 | validation: Loss 0.048430 Accuracy -2.0425\n",
      "Epoch 1225 | train: Loss 0.000042 Accuracy 0.9985 | validation: Loss 0.049678 Accuracy -2.1209\n",
      "Epoch 1226 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.047475 Accuracy -1.9825\n",
      "Epoch 1227 | train: Loss 0.000068 Accuracy 0.9975 | validation: Loss 0.049947 Accuracy -2.1378\n",
      "Epoch 1228 | train: Loss 0.000102 Accuracy 0.9963 | validation: Loss 0.046162 Accuracy -1.9000\n",
      "Epoch 1229 | train: Loss 0.000161 Accuracy 0.9941 | validation: Loss 0.050475 Accuracy -2.1710\n",
      "Epoch 1230 | train: Loss 0.000269 Accuracy 0.9902 | validation: Loss 0.044216 Accuracy -1.7778\n",
      "Epoch 1231 | train: Loss 0.000485 Accuracy 0.9822 | validation: Loss 0.051772 Accuracy -2.2525\n",
      "Epoch 1232 | train: Loss 0.000893 Accuracy 0.9673 | validation: Loss 0.041713 Accuracy -1.6205\n",
      "Epoch 1233 | train: Loss 0.001456 Accuracy 0.9466 | validation: Loss 0.051571 Accuracy -2.2398\n",
      "Epoch 1234 | train: Loss 0.001644 Accuracy 0.9397 | validation: Loss 0.040450 Accuracy -1.5412\n",
      "Epoch 1235 | train: Loss 0.001067 Accuracy 0.9609 | validation: Loss 0.045100 Accuracy -1.8333\n",
      "Epoch 1236 | train: Loss 0.000735 Accuracy 0.9730 | validation: Loss 0.045034 Accuracy -1.8292\n",
      "Epoch 1237 | train: Loss 0.000595 Accuracy 0.9782 | validation: Loss 0.043025 Accuracy -1.7030\n",
      "Epoch 1238 | train: Loss 0.000545 Accuracy 0.9800 | validation: Loss 0.045631 Accuracy -1.8667\n",
      "Epoch 1239 | train: Loss 0.000438 Accuracy 0.9839 | validation: Loss 0.045832 Accuracy -1.8793\n",
      "Epoch 1240 | train: Loss 0.000434 Accuracy 0.9841 | validation: Loss 0.045006 Accuracy -1.8274\n",
      "Epoch 1241 | train: Loss 0.000314 Accuracy 0.9885 | validation: Loss 0.049378 Accuracy -2.1020\n",
      "Epoch 1242 | train: Loss 0.000374 Accuracy 0.9863 | validation: Loss 0.048038 Accuracy -2.0179\n",
      "Epoch 1243 | train: Loss 0.000386 Accuracy 0.9858 | validation: Loss 0.052935 Accuracy -2.3255\n",
      "Epoch 1244 | train: Loss 0.000165 Accuracy 0.9940 | validation: Loss 0.055408 Accuracy -2.4809\n",
      "Epoch 1245 | train: Loss 0.000224 Accuracy 0.9918 | validation: Loss 0.052989 Accuracy -2.3289\n",
      "Epoch 1246 | train: Loss 0.000285 Accuracy 0.9896 | validation: Loss 0.058862 Accuracy -2.6979\n",
      "Epoch 1247 | train: Loss 0.000285 Accuracy 0.9895 | validation: Loss 0.051934 Accuracy -2.2627\n",
      "Epoch 1248 | train: Loss 0.000198 Accuracy 0.9927 | validation: Loss 0.053906 Accuracy -2.3866\n",
      "Epoch 1249 | train: Loss 0.000147 Accuracy 0.9946 | validation: Loss 0.052721 Accuracy -2.3121\n",
      "Epoch 1250 | train: Loss 0.000154 Accuracy 0.9943 | validation: Loss 0.051462 Accuracy -2.2330\n",
      "Epoch 1251 | train: Loss 0.000171 Accuracy 0.9937 | validation: Loss 0.055996 Accuracy -2.5178\n",
      "Epoch 1252 | train: Loss 0.000177 Accuracy 0.9935 | validation: Loss 0.052239 Accuracy -2.2818\n",
      "Epoch 1253 | train: Loss 0.000160 Accuracy 0.9941 | validation: Loss 0.054921 Accuracy -2.4503\n",
      "Epoch 1254 | train: Loss 0.000113 Accuracy 0.9958 | validation: Loss 0.052340 Accuracy -2.2882\n",
      "Epoch 1255 | train: Loss 0.000084 Accuracy 0.9969 | validation: Loss 0.051642 Accuracy -2.2443\n",
      "Epoch 1256 | train: Loss 0.000096 Accuracy 0.9965 | validation: Loss 0.054031 Accuracy -2.3944\n",
      "Epoch 1257 | train: Loss 0.000111 Accuracy 0.9959 | validation: Loss 0.051531 Accuracy -2.2373\n",
      "Epoch 1258 | train: Loss 0.000112 Accuracy 0.9959 | validation: Loss 0.054703 Accuracy -2.4366\n",
      "Epoch 1259 | train: Loss 0.000111 Accuracy 0.9959 | validation: Loss 0.051552 Accuracy -2.2387\n",
      "Epoch 1260 | train: Loss 0.000092 Accuracy 0.9966 | validation: Loss 0.052078 Accuracy -2.2717\n",
      "Epoch 1261 | train: Loss 0.000070 Accuracy 0.9974 | validation: Loss 0.050143 Accuracy -2.1502\n",
      "Epoch 1262 | train: Loss 0.000054 Accuracy 0.9980 | validation: Loss 0.049106 Accuracy -2.0850\n",
      "Epoch 1263 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.049804 Accuracy -2.1288\n",
      "Epoch 1264 | train: Loss 0.000056 Accuracy 0.9980 | validation: Loss 0.048362 Accuracy -2.0383\n",
      "Epoch 1265 | train: Loss 0.000062 Accuracy 0.9977 | validation: Loss 0.049808 Accuracy -2.1291\n",
      "Epoch 1266 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.048671 Accuracy -2.0576\n",
      "Epoch 1267 | train: Loss 0.000046 Accuracy 0.9983 | validation: Loss 0.049031 Accuracy -2.0803\n",
      "Epoch 1268 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.049303 Accuracy -2.0974\n",
      "Epoch 1269 | train: Loss 0.000042 Accuracy 0.9984 | validation: Loss 0.048563 Accuracy -2.0509\n",
      "Epoch 1270 | train: Loss 0.000044 Accuracy 0.9984 | validation: Loss 0.049783 Accuracy -2.1275\n",
      "Epoch 1271 | train: Loss 0.000048 Accuracy 0.9983 | validation: Loss 0.047935 Accuracy -2.0114\n",
      "Epoch 1272 | train: Loss 0.000052 Accuracy 0.9981 | validation: Loss 0.049194 Accuracy -2.0905\n",
      "Epoch 1273 | train: Loss 0.000056 Accuracy 0.9979 | validation: Loss 0.046729 Accuracy -1.9357\n",
      "Epoch 1274 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.048193 Accuracy -2.0276\n",
      "Epoch 1275 | train: Loss 0.000053 Accuracy 0.9981 | validation: Loss 0.046496 Accuracy -1.9210\n",
      "Epoch 1276 | train: Loss 0.000041 Accuracy 0.9985 | validation: Loss 0.047129 Accuracy -1.9608\n",
      "Epoch 1277 | train: Loss 0.000031 Accuracy 0.9988 | validation: Loss 0.047050 Accuracy -1.9559\n",
      "Epoch 1278 | train: Loss 0.000027 Accuracy 0.9990 | validation: Loss 0.046464 Accuracy -1.9190\n",
      "Epoch 1279 | train: Loss 0.000030 Accuracy 0.9989 | validation: Loss 0.047440 Accuracy -1.9803\n",
      "Epoch 1280 | train: Loss 0.000034 Accuracy 0.9987 | validation: Loss 0.045769 Accuracy -1.8753\n",
      "Epoch 1281 | train: Loss 0.000044 Accuracy 0.9984 | validation: Loss 0.047426 Accuracy -1.9794\n",
      "Epoch 1282 | train: Loss 0.000047 Accuracy 0.9983 | validation: Loss 0.045480 Accuracy -1.8572\n",
      "Epoch 1283 | train: Loss 0.000049 Accuracy 0.9982 | validation: Loss 0.047099 Accuracy -1.9589\n",
      "Epoch 1284 | train: Loss 0.000048 Accuracy 0.9982 | validation: Loss 0.045097 Accuracy -1.8332\n",
      "Epoch 1285 | train: Loss 0.000042 Accuracy 0.9984 | validation: Loss 0.046336 Accuracy -1.9110\n",
      "Epoch 1286 | train: Loss 0.000034 Accuracy 0.9988 | validation: Loss 0.045095 Accuracy -1.8330\n",
      "Epoch 1287 | train: Loss 0.000023 Accuracy 0.9992 | validation: Loss 0.045339 Accuracy -1.8483\n",
      "Epoch 1288 | train: Loss 0.000020 Accuracy 0.9993 | validation: Loss 0.045270 Accuracy -1.8440\n",
      "Epoch 1289 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.044687 Accuracy -1.8074\n",
      "Epoch 1290 | train: Loss 0.000023 Accuracy 0.9991 | validation: Loss 0.045427 Accuracy -1.8539\n",
      "Epoch 1291 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.044049 Accuracy -1.7673\n",
      "Epoch 1292 | train: Loss 0.000034 Accuracy 0.9987 | validation: Loss 0.045542 Accuracy -1.8611\n",
      "Epoch 1293 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.043692 Accuracy -1.7448\n",
      "Epoch 1294 | train: Loss 0.000044 Accuracy 0.9984 | validation: Loss 0.045281 Accuracy -1.8447\n",
      "Epoch 1295 | train: Loss 0.000046 Accuracy 0.9983 | validation: Loss 0.043433 Accuracy -1.7286\n",
      "Epoch 1296 | train: Loss 0.000044 Accuracy 0.9984 | validation: Loss 0.044980 Accuracy -1.8258\n",
      "Epoch 1297 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.043102 Accuracy -1.7078\n",
      "Epoch 1298 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.044660 Accuracy -1.8057\n",
      "Epoch 1299 | train: Loss 0.000045 Accuracy 0.9984 | validation: Loss 0.042800 Accuracy -1.6888\n",
      "Epoch 1300 | train: Loss 0.000049 Accuracy 0.9982 | validation: Loss 0.044401 Accuracy -1.7894\n",
      "Epoch 1301 | train: Loss 0.000060 Accuracy 0.9978 | validation: Loss 0.042147 Accuracy -1.6478\n",
      "Epoch 1302 | train: Loss 0.000066 Accuracy 0.9976 | validation: Loss 0.044380 Accuracy -1.7881\n",
      "Epoch 1303 | train: Loss 0.000077 Accuracy 0.9972 | validation: Loss 0.041914 Accuracy -1.6331\n",
      "Epoch 1304 | train: Loss 0.000078 Accuracy 0.9971 | validation: Loss 0.044215 Accuracy -1.7777\n",
      "Epoch 1305 | train: Loss 0.000089 Accuracy 0.9967 | validation: Loss 0.041594 Accuracy -1.6131\n",
      "Epoch 1306 | train: Loss 0.000105 Accuracy 0.9961 | validation: Loss 0.044226 Accuracy -1.7784\n",
      "Epoch 1307 | train: Loss 0.000142 Accuracy 0.9948 | validation: Loss 0.040468 Accuracy -1.5423\n",
      "Epoch 1308 | train: Loss 0.000186 Accuracy 0.9932 | validation: Loss 0.044432 Accuracy -1.7914\n",
      "Epoch 1309 | train: Loss 0.000235 Accuracy 0.9914 | validation: Loss 0.040196 Accuracy -1.5252\n",
      "Epoch 1310 | train: Loss 0.000216 Accuracy 0.9921 | validation: Loss 0.043451 Accuracy -1.7297\n",
      "Epoch 1311 | train: Loss 0.000216 Accuracy 0.9921 | validation: Loss 0.039865 Accuracy -1.5044\n",
      "Epoch 1312 | train: Loss 0.000155 Accuracy 0.9943 | validation: Loss 0.042877 Accuracy -1.6937\n",
      "Epoch 1313 | train: Loss 0.000168 Accuracy 0.9938 | validation: Loss 0.040291 Accuracy -1.5312\n",
      "Epoch 1314 | train: Loss 0.000072 Accuracy 0.9974 | validation: Loss 0.041765 Accuracy -1.6238\n",
      "Epoch 1315 | train: Loss 0.000071 Accuracy 0.9974 | validation: Loss 0.041460 Accuracy -1.6047\n",
      "Epoch 1316 | train: Loss 0.000028 Accuracy 0.9990 | validation: Loss 0.041261 Accuracy -1.5922\n",
      "Epoch 1317 | train: Loss 0.000031 Accuracy 0.9989 | validation: Loss 0.041939 Accuracy -1.6347\n",
      "Epoch 1318 | train: Loss 0.000050 Accuracy 0.9982 | validation: Loss 0.040391 Accuracy -1.5375\n",
      "Epoch 1319 | train: Loss 0.000047 Accuracy 0.9983 | validation: Loss 0.041680 Accuracy -1.6184\n",
      "Epoch 1320 | train: Loss 0.000064 Accuracy 0.9977 | validation: Loss 0.040037 Accuracy -1.5153\n",
      "Epoch 1321 | train: Loss 0.000046 Accuracy 0.9983 | validation: Loss 0.040749 Accuracy -1.5600\n",
      "Epoch 1322 | train: Loss 0.000050 Accuracy 0.9982 | validation: Loss 0.039961 Accuracy -1.5105\n",
      "Epoch 1323 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.040376 Accuracy -1.5365\n",
      "Epoch 1324 | train: Loss 0.000047 Accuracy 0.9983 | validation: Loss 0.039966 Accuracy -1.5108\n",
      "Epoch 1325 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.039611 Accuracy -1.4885\n",
      "Epoch 1326 | train: Loss 0.000036 Accuracy 0.9987 | validation: Loss 0.040232 Accuracy -1.5275\n",
      "Epoch 1327 | train: Loss 0.000023 Accuracy 0.9991 | validation: Loss 0.039491 Accuracy -1.4810\n",
      "Epoch 1328 | train: Loss 0.000031 Accuracy 0.9989 | validation: Loss 0.039830 Accuracy -1.5023\n",
      "Epoch 1329 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.039096 Accuracy -1.4561\n",
      "Epoch 1330 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.039645 Accuracy -1.4906\n",
      "Epoch 1331 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.039127 Accuracy -1.4581\n",
      "Epoch 1332 | train: Loss 0.000016 Accuracy 0.9994 | validation: Loss 0.038856 Accuracy -1.4411\n",
      "Epoch 1333 | train: Loss 0.000026 Accuracy 0.9990 | validation: Loss 0.039076 Accuracy -1.4548\n",
      "Epoch 1334 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.038599 Accuracy -1.4249\n",
      "Epoch 1335 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.039124 Accuracy -1.4579\n",
      "Epoch 1336 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.038021 Accuracy -1.3886\n",
      "Epoch 1337 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.039408 Accuracy -1.4758\n",
      "Epoch 1338 | train: Loss 0.000052 Accuracy 0.9981 | validation: Loss 0.037743 Accuracy -1.3711\n",
      "Epoch 1339 | train: Loss 0.000061 Accuracy 0.9978 | validation: Loss 0.039231 Accuracy -1.4646\n",
      "Epoch 1340 | train: Loss 0.000089 Accuracy 0.9967 | validation: Loss 0.037142 Accuracy -1.3334\n",
      "Epoch 1341 | train: Loss 0.000105 Accuracy 0.9961 | validation: Loss 0.039729 Accuracy -1.4959\n",
      "Epoch 1342 | train: Loss 0.000164 Accuracy 0.9940 | validation: Loss 0.036542 Accuracy -1.2957\n",
      "Epoch 1343 | train: Loss 0.000173 Accuracy 0.9937 | validation: Loss 0.039161 Accuracy -1.4602\n",
      "Epoch 1344 | train: Loss 0.000241 Accuracy 0.9912 | validation: Loss 0.036104 Accuracy -1.2682\n",
      "Epoch 1345 | train: Loss 0.000234 Accuracy 0.9914 | validation: Loss 0.039991 Accuracy -1.5123\n",
      "Epoch 1346 | train: Loss 0.000289 Accuracy 0.9894 | validation: Loss 0.036253 Accuracy -1.2775\n",
      "Epoch 1347 | train: Loss 0.000199 Accuracy 0.9927 | validation: Loss 0.039049 Accuracy -1.4532\n",
      "Epoch 1348 | train: Loss 0.000189 Accuracy 0.9931 | validation: Loss 0.037069 Accuracy -1.3288\n",
      "Epoch 1349 | train: Loss 0.000092 Accuracy 0.9966 | validation: Loss 0.038909 Accuracy -1.4444\n",
      "Epoch 1350 | train: Loss 0.000086 Accuracy 0.9969 | validation: Loss 0.037451 Accuracy -1.3528\n",
      "Epoch 1351 | train: Loss 0.000026 Accuracy 0.9991 | validation: Loss 0.037299 Accuracy -1.3433\n",
      "Epoch 1352 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.038158 Accuracy -1.3972\n",
      "Epoch 1353 | train: Loss 0.000036 Accuracy 0.9987 | validation: Loss 0.037100 Accuracy -1.3307\n",
      "Epoch 1354 | train: Loss 0.000091 Accuracy 0.9966 | validation: Loss 0.038856 Accuracy -1.4411\n",
      "Epoch 1355 | train: Loss 0.000088 Accuracy 0.9968 | validation: Loss 0.036263 Accuracy -1.2782\n",
      "Epoch 1356 | train: Loss 0.000125 Accuracy 0.9954 | validation: Loss 0.038629 Accuracy -1.4268\n",
      "Epoch 1357 | train: Loss 0.000114 Accuracy 0.9958 | validation: Loss 0.036172 Accuracy -1.2724\n",
      "Epoch 1358 | train: Loss 0.000122 Accuracy 0.9955 | validation: Loss 0.038025 Accuracy -1.3888\n",
      "Epoch 1359 | train: Loss 0.000091 Accuracy 0.9966 | validation: Loss 0.036017 Accuracy -1.2627\n",
      "Epoch 1360 | train: Loss 0.000062 Accuracy 0.9977 | validation: Loss 0.037049 Accuracy -1.3275\n",
      "Epoch 1361 | train: Loss 0.000036 Accuracy 0.9987 | validation: Loss 0.036657 Accuracy -1.3029\n",
      "Epoch 1362 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.036363 Accuracy -1.2845\n",
      "Epoch 1363 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.037105 Accuracy -1.3310\n",
      "Epoch 1364 | train: Loss 0.000042 Accuracy 0.9985 | validation: Loss 0.035751 Accuracy -1.2460\n",
      "Epoch 1365 | train: Loss 0.000050 Accuracy 0.9982 | validation: Loss 0.037355 Accuracy -1.3468\n",
      "Epoch 1366 | train: Loss 0.000061 Accuracy 0.9978 | validation: Loss 0.035352 Accuracy -1.2209\n",
      "Epoch 1367 | train: Loss 0.000066 Accuracy 0.9976 | validation: Loss 0.037065 Accuracy -1.3286\n",
      "Epoch 1368 | train: Loss 0.000063 Accuracy 0.9977 | validation: Loss 0.035136 Accuracy -1.2073\n",
      "Epoch 1369 | train: Loss 0.000058 Accuracy 0.9979 | validation: Loss 0.036547 Accuracy -1.2960\n",
      "Epoch 1370 | train: Loss 0.000036 Accuracy 0.9987 | validation: Loss 0.035413 Accuracy -1.2248\n",
      "Epoch 1371 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.036266 Accuracy -1.2783\n",
      "Epoch 1372 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.035806 Accuracy -1.2494\n",
      "Epoch 1373 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.035534 Accuracy -1.2324\n",
      "Epoch 1374 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.035682 Accuracy -1.2416\n",
      "Epoch 1375 | train: Loss 0.000012 Accuracy 0.9995 | validation: Loss 0.035151 Accuracy -1.2083\n",
      "Epoch 1376 | train: Loss 0.000015 Accuracy 0.9994 | validation: Loss 0.035850 Accuracy -1.2522\n",
      "Epoch 1377 | train: Loss 0.000023 Accuracy 0.9992 | validation: Loss 0.034680 Accuracy -1.1787\n",
      "Epoch 1378 | train: Loss 0.000027 Accuracy 0.9990 | validation: Loss 0.035963 Accuracy -1.2593\n",
      "Epoch 1379 | train: Loss 0.000039 Accuracy 0.9986 | validation: Loss 0.034547 Accuracy -1.1704\n",
      "Epoch 1380 | train: Loss 0.000055 Accuracy 0.9980 | validation: Loss 0.036505 Accuracy -1.2933\n",
      "Epoch 1381 | train: Loss 0.000083 Accuracy 0.9970 | validation: Loss 0.033794 Accuracy -1.1230\n",
      "Epoch 1382 | train: Loss 0.000134 Accuracy 0.9951 | validation: Loss 0.037115 Accuracy -1.3316\n",
      "Epoch 1383 | train: Loss 0.000219 Accuracy 0.9920 | validation: Loss 0.033180 Accuracy -1.0844\n",
      "Epoch 1384 | train: Loss 0.000352 Accuracy 0.9871 | validation: Loss 0.038481 Accuracy -1.4175\n",
      "Epoch 1385 | train: Loss 0.000508 Accuracy 0.9814 | validation: Loss 0.032511 Accuracy -1.0424\n",
      "Epoch 1386 | train: Loss 0.000669 Accuracy 0.9755 | validation: Loss 0.039176 Accuracy -1.4612\n",
      "Epoch 1387 | train: Loss 0.000752 Accuracy 0.9724 | validation: Loss 0.032742 Accuracy -1.0569\n",
      "Epoch 1388 | train: Loss 0.000652 Accuracy 0.9761 | validation: Loss 0.037320 Accuracy -1.3445\n",
      "Epoch 1389 | train: Loss 0.000384 Accuracy 0.9859 | validation: Loss 0.033771 Accuracy -1.1216\n",
      "Epoch 1390 | train: Loss 0.000114 Accuracy 0.9958 | validation: Loss 0.034593 Accuracy -1.1732\n",
      "Epoch 1391 | train: Loss 0.000071 Accuracy 0.9974 | validation: Loss 0.037349 Accuracy -1.3464\n",
      "Epoch 1392 | train: Loss 0.000167 Accuracy 0.9939 | validation: Loss 0.034073 Accuracy -1.1406\n",
      "Epoch 1393 | train: Loss 0.000277 Accuracy 0.9898 | validation: Loss 0.038599 Accuracy -1.4249\n",
      "Epoch 1394 | train: Loss 0.000263 Accuracy 0.9903 | validation: Loss 0.034298 Accuracy -1.1547\n",
      "Epoch 1395 | train: Loss 0.000165 Accuracy 0.9939 | validation: Loss 0.036232 Accuracy -1.2762\n",
      "Epoch 1396 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.036349 Accuracy -1.2835\n",
      "Epoch 1397 | train: Loss 0.000052 Accuracy 0.9981 | validation: Loss 0.033931 Accuracy -1.1317\n",
      "Epoch 1398 | train: Loss 0.000147 Accuracy 0.9946 | validation: Loss 0.036999 Accuracy -1.3244\n",
      "Epoch 1399 | train: Loss 0.000150 Accuracy 0.9945 | validation: Loss 0.034450 Accuracy -1.1642\n",
      "Epoch 1400 | train: Loss 0.000101 Accuracy 0.9963 | validation: Loss 0.035847 Accuracy -1.2520\n",
      "Epoch 1401 | train: Loss 0.000042 Accuracy 0.9985 | validation: Loss 0.035213 Accuracy -1.2122\n",
      "Epoch 1402 | train: Loss 0.000027 Accuracy 0.9990 | validation: Loss 0.033772 Accuracy -1.1217\n",
      "Epoch 1403 | train: Loss 0.000072 Accuracy 0.9974 | validation: Loss 0.035908 Accuracy -1.2558\n",
      "Epoch 1404 | train: Loss 0.000077 Accuracy 0.9972 | validation: Loss 0.034265 Accuracy -1.1526\n",
      "Epoch 1405 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.034978 Accuracy -1.1974\n",
      "Epoch 1406 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.034755 Accuracy -1.1834\n",
      "Epoch 1407 | train: Loss 0.000028 Accuracy 0.9990 | validation: Loss 0.033572 Accuracy -1.1091\n",
      "Epoch 1408 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.034480 Accuracy -1.1662\n",
      "Epoch 1409 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.032420 Accuracy -1.0367\n",
      "Epoch 1410 | train: Loss 0.000045 Accuracy 0.9983 | validation: Loss 0.033768 Accuracy -1.1214\n",
      "Epoch 1411 | train: Loss 0.000024 Accuracy 0.9991 | validation: Loss 0.033538 Accuracy -1.1069\n",
      "Epoch 1412 | train: Loss 0.000015 Accuracy 0.9995 | validation: Loss 0.033407 Accuracy -1.0987\n",
      "Epoch 1413 | train: Loss 0.000018 Accuracy 0.9993 | validation: Loss 0.034300 Accuracy -1.1548\n",
      "Epoch 1414 | train: Loss 0.000033 Accuracy 0.9988 | validation: Loss 0.032833 Accuracy -1.0626\n",
      "Epoch 1415 | train: Loss 0.000042 Accuracy 0.9985 | validation: Loss 0.033936 Accuracy -1.1320\n",
      "Epoch 1416 | train: Loss 0.000034 Accuracy 0.9987 | validation: Loss 0.033004 Accuracy -1.0734\n",
      "Epoch 1417 | train: Loss 0.000023 Accuracy 0.9991 | validation: Loss 0.033637 Accuracy -1.1131\n",
      "Epoch 1418 | train: Loss 0.000016 Accuracy 0.9994 | validation: Loss 0.033182 Accuracy -1.0846\n",
      "Epoch 1419 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.032770 Accuracy -1.0587\n",
      "Epoch 1420 | train: Loss 0.000015 Accuracy 0.9994 | validation: Loss 0.033379 Accuracy -1.0969\n",
      "Epoch 1421 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.032834 Accuracy -1.0627\n",
      "Epoch 1422 | train: Loss 0.000021 Accuracy 0.9992 | validation: Loss 0.033582 Accuracy -1.1097\n",
      "Epoch 1423 | train: Loss 0.000019 Accuracy 0.9993 | validation: Loss 0.032569 Accuracy -1.0461\n",
      "Epoch 1424 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.033116 Accuracy -1.0804\n",
      "Epoch 1425 | train: Loss 0.000016 Accuracy 0.9994 | validation: Loss 0.032740 Accuracy -1.0568\n",
      "Epoch 1426 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.033053 Accuracy -1.0765\n",
      "Epoch 1427 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.032929 Accuracy -1.0687\n",
      "Epoch 1428 | train: Loss 0.000006 Accuracy 0.9998 | validation: Loss 0.032677 Accuracy -1.0529\n",
      "Epoch 1429 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.033148 Accuracy -1.0825\n",
      "Epoch 1430 | train: Loss 0.000012 Accuracy 0.9996 | validation: Loss 0.032600 Accuracy -1.0480\n",
      "Epoch 1431 | train: Loss 0.000018 Accuracy 0.9994 | validation: Loss 0.033334 Accuracy -1.0941\n",
      "Epoch 1432 | train: Loss 0.000025 Accuracy 0.9991 | validation: Loss 0.032092 Accuracy -1.0161\n",
      "Epoch 1433 | train: Loss 0.000031 Accuracy 0.9989 | validation: Loss 0.033245 Accuracy -1.0886\n",
      "Epoch 1434 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.032049 Accuracy -1.0134\n",
      "Epoch 1435 | train: Loss 0.000044 Accuracy 0.9984 | validation: Loss 0.033462 Accuracy -1.1022\n",
      "Epoch 1436 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.031759 Accuracy -0.9952\n",
      "Epoch 1437 | train: Loss 0.000059 Accuracy 0.9978 | validation: Loss 0.033433 Accuracy -1.1004\n",
      "Epoch 1438 | train: Loss 0.000072 Accuracy 0.9974 | validation: Loss 0.031561 Accuracy -0.9827\n",
      "Epoch 1439 | train: Loss 0.000090 Accuracy 0.9967 | validation: Loss 0.033516 Accuracy -1.1056\n",
      "Epoch 1440 | train: Loss 0.000109 Accuracy 0.9960 | validation: Loss 0.031138 Accuracy -0.9562\n",
      "Epoch 1441 | train: Loss 0.000119 Accuracy 0.9956 | validation: Loss 0.033337 Accuracy -1.0943\n",
      "Epoch 1442 | train: Loss 0.000123 Accuracy 0.9955 | validation: Loss 0.031209 Accuracy -0.9606\n",
      "Epoch 1443 | train: Loss 0.000109 Accuracy 0.9960 | validation: Loss 0.033067 Accuracy -1.0774\n",
      "Epoch 1444 | train: Loss 0.000089 Accuracy 0.9967 | validation: Loss 0.031226 Accuracy -0.9617\n",
      "Epoch 1445 | train: Loss 0.000058 Accuracy 0.9979 | validation: Loss 0.032355 Accuracy -1.0326\n",
      "Epoch 1446 | train: Loss 0.000034 Accuracy 0.9988 | validation: Loss 0.031567 Accuracy -0.9831\n",
      "Epoch 1447 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.031838 Accuracy -1.0001\n",
      "Epoch 1448 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.032043 Accuracy -1.0130\n",
      "Epoch 1449 | train: Loss 0.000014 Accuracy 0.9995 | validation: Loss 0.031253 Accuracy -0.9634\n",
      "Epoch 1450 | train: Loss 0.000026 Accuracy 0.9991 | validation: Loss 0.032188 Accuracy -1.0221\n",
      "Epoch 1451 | train: Loss 0.000045 Accuracy 0.9983 | validation: Loss 0.030911 Accuracy -0.9419\n",
      "Epoch 1452 | train: Loss 0.000066 Accuracy 0.9976 | validation: Loss 0.032489 Accuracy -1.0411\n",
      "Epoch 1453 | train: Loss 0.000090 Accuracy 0.9967 | validation: Loss 0.030663 Accuracy -0.9263\n",
      "Epoch 1454 | train: Loss 0.000105 Accuracy 0.9962 | validation: Loss 0.032579 Accuracy -1.0467\n",
      "Epoch 1455 | train: Loss 0.000115 Accuracy 0.9958 | validation: Loss 0.030578 Accuracy -0.9210\n",
      "Epoch 1456 | train: Loss 0.000116 Accuracy 0.9957 | validation: Loss 0.032463 Accuracy -1.0395\n",
      "Epoch 1457 | train: Loss 0.000106 Accuracy 0.9961 | validation: Loss 0.030526 Accuracy -0.9178\n",
      "Epoch 1458 | train: Loss 0.000091 Accuracy 0.9967 | validation: Loss 0.032036 Accuracy -1.0126\n",
      "Epoch 1459 | train: Loss 0.000069 Accuracy 0.9975 | validation: Loss 0.030720 Accuracy -0.9299\n",
      "Epoch 1460 | train: Loss 0.000043 Accuracy 0.9984 | validation: Loss 0.031594 Accuracy -0.9848\n",
      "Epoch 1461 | train: Loss 0.000021 Accuracy 0.9992 | validation: Loss 0.031127 Accuracy -0.9555\n",
      "Epoch 1462 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.031112 Accuracy -0.9546\n",
      "Epoch 1463 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.031547 Accuracy -0.9819\n",
      "Epoch 1464 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.030844 Accuracy -0.9377\n",
      "Epoch 1465 | train: Loss 0.000032 Accuracy 0.9988 | validation: Loss 0.031845 Accuracy -1.0006\n",
      "Epoch 1466 | train: Loss 0.000049 Accuracy 0.9982 | validation: Loss 0.030591 Accuracy -0.9218\n",
      "Epoch 1467 | train: Loss 0.000065 Accuracy 0.9976 | validation: Loss 0.031949 Accuracy -1.0071\n",
      "Epoch 1468 | train: Loss 0.000082 Accuracy 0.9970 | validation: Loss 0.030406 Accuracy -0.9102\n",
      "Epoch 1469 | train: Loss 0.000091 Accuracy 0.9967 | validation: Loss 0.031984 Accuracy -1.0093\n",
      "Epoch 1470 | train: Loss 0.000095 Accuracy 0.9965 | validation: Loss 0.030355 Accuracy -0.9070\n",
      "Epoch 1471 | train: Loss 0.000095 Accuracy 0.9965 | validation: Loss 0.031952 Accuracy -1.0073\n",
      "Epoch 1472 | train: Loss 0.000092 Accuracy 0.9966 | validation: Loss 0.030376 Accuracy -0.9083\n",
      "Epoch 1473 | train: Loss 0.000079 Accuracy 0.9971 | validation: Loss 0.031508 Accuracy -0.9794\n",
      "Epoch 1474 | train: Loss 0.000057 Accuracy 0.9979 | validation: Loss 0.030341 Accuracy -0.9061\n",
      "Epoch 1475 | train: Loss 0.000031 Accuracy 0.9989 | validation: Loss 0.030795 Accuracy -0.9347\n",
      "Epoch 1476 | train: Loss 0.000011 Accuracy 0.9996 | validation: Loss 0.030752 Accuracy -0.9319\n",
      "Epoch 1477 | train: Loss 0.000007 Accuracy 0.9997 | validation: Loss 0.030484 Accuracy -0.9151\n",
      "Epoch 1478 | train: Loss 0.000017 Accuracy 0.9994 | validation: Loss 0.031198 Accuracy -0.9600\n",
      "Epoch 1479 | train: Loss 0.000034 Accuracy 0.9988 | validation: Loss 0.030235 Accuracy -0.8995\n",
      "Epoch 1480 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.031401 Accuracy -0.9727\n",
      "Epoch 1481 | train: Loss 0.000064 Accuracy 0.9977 | validation: Loss 0.030137 Accuracy -0.8933\n",
      "Epoch 1482 | train: Loss 0.000068 Accuracy 0.9975 | validation: Loss 0.031264 Accuracy -0.9641\n",
      "Epoch 1483 | train: Loss 0.000061 Accuracy 0.9977 | validation: Loss 0.030178 Accuracy -0.8959\n",
      "Epoch 1484 | train: Loss 0.000051 Accuracy 0.9981 | validation: Loss 0.031257 Accuracy -0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-19 23:55:08,239] Trial 3 failed with parameters: {'hidden_units': 23, 'depth': 2, 'learning_rate': 0.02589127268308155, 'weight_decay': 0.0004423629019296653} because of the following error: The number of the values 1500 did not match the number of the objectives 1.\n",
      "[W 2024-05-19 23:55:08,239] Trial 3 failed with value [0.20931893587112427, 0.3947395086288452, 0.3330007791519165, 0.1725020408630371, 0.11326546967029572, 0.09478644281625748, 0.08260568976402283, 0.08860520273447037, 0.12247966974973679, 0.14168180525302887, 0.12095966935157776, 0.08120030909776688, 0.05171327292919159, 0.04289146885275841, 0.041781555861234665, 0.04600977525115013, 0.05607667565345764, 0.06461877375841141, 0.06479117274284363, 0.058575358241796494, 0.052534330636262894, 0.04902547225356102, 0.0490320548415184, 0.053101859986782074, 0.058430422097444534, 0.06543346494436264, 0.07242130488157272, 0.07485879957675934, 0.07184319943189621, 0.06708967685699463, 0.06449340283870697, 0.06559836864471436, 0.07038603723049164, 0.07769300788640976, 0.0821816623210907, 0.08078216761350632, 0.07503420859575272, 0.06950999796390533, 0.06798981875181198, 0.07010561227798462, 0.07365240156650543, 0.07540114223957062, 0.0734616219997406, 0.07003886997699738, 0.06780543923377991, 0.06754537671804428, 0.06881465017795563, 0.07017053663730621, 0.07061140984296799, 0.07092680782079697, 0.07270701974630356, 0.07616832107305527, 0.08063378930091858, 0.08382163941860199, 0.0854138731956482, 0.09024755656719208, 0.09909620136022568, 0.10220572352409363, 0.10612422227859497, 0.12511742115020752, 0.11021212488412857, 0.12681463360786438, 0.13302993774414062, 0.12084125727415085, 0.12831509113311768, 0.14086084067821503, 0.12874840199947357, 0.12624458968639374, 0.13641943037509918, 0.13243357837200165, 0.12380331009626389, 0.13389401137828827, 0.14228108525276184, 0.1356114000082016, 0.13764198124408722, 0.14793862402439117, 0.14259079098701477, 0.13691776990890503, 0.14317767322063446, 0.14234083890914917, 0.13187456130981445, 0.14088299870491028, 0.14310668408870697, 0.13652929663658142, 0.14323943853378296, 0.14153294265270233, 0.1321346014738083, 0.13832396268844604, 0.12954117357730865, 0.12963217496871948, 0.13309256732463837, 0.12446656078100204, 0.14271286129951477, 0.10792304575443268, 0.14259718358516693, 0.11415013670921326, 0.10910937935113907, 0.13327786326408386, 0.11533066630363464, 0.1089819073677063, 0.13002422451972961, 0.12122035026550293, 0.11120247095823288, 0.12872253358364105, 0.12653902173042297, 0.11428962647914886, 0.12891052663326263, 0.12952911853790283, 0.11707832664251328, 0.13096509873867035, 0.12803052365779877, 0.11777627468109131, 0.1410994529724121, 0.12868045270442963, 0.1339418888092041, 0.1426844596862793, 0.1148487776517868, 0.17412549257278442, 0.08929726481437683, 0.13129356503486633, 0.12988731265068054, 0.09534280002117157, 0.14068523049354553, 0.15158028900623322, 0.10766617953777313, 0.11452373117208481, 0.14954380691051483, 0.14159011840820312, 0.11323375254869461, 0.11730934679508209, 0.14611823856830597, 0.1485980600118637, 0.12575417757034302, 0.12518510222434998, 0.14863404631614685, 0.15322941541671753, 0.13212604820728302, 0.13187721371650696, 0.15219691395759583, 0.1392059177160263, 0.12957614660263062, 0.15969285368919373, 0.11889223754405975, 0.1689710021018982, 0.132450670003891, 0.12084109336137772, 0.1534043550491333, 0.16155430674552917, 0.13220873475074768, 0.1322527676820755, 0.16323530673980713, 0.15683330595493317, 0.13631819188594818, 0.16325534880161285, 0.18278412520885468, 0.15224502980709076, 0.18902011215686798, 0.17736929655075073, 0.1609739512205124, 0.18651586771011353, 0.1683046668767929, 0.15855585038661957, 0.17911414802074432, 0.17645561695098877, 0.16422918438911438, 0.1824776828289032, 0.1926468461751938, 0.17891323566436768, 0.19372253119945526, 0.20087414979934692, 0.18359540402889252, 0.19307231903076172, 0.19395406544208527, 0.17776323854923248, 0.18466614186763763, 0.1867118626832962, 0.1708456426858902, 0.18454615771770477, 0.1660924255847931, 0.17782320082187653, 0.1680944263935089, 0.16906633973121643, 0.1769055873155594, 0.1668601632118225, 0.18437084555625916, 0.15738213062286377, 0.18669258058071136, 0.16608665883541107, 0.17292913794517517, 0.18692010641098022, 0.1604214608669281, 0.1905207484960556, 0.1708737164735794, 0.1679431051015854, 0.17553040385246277, 0.16788969933986664, 0.16676077246665955, 0.15294289588928223, 0.16360412538051605, 0.1603388786315918, 0.16739358007907867, 0.1352730691432953, 0.15880855917930603, 0.1642625629901886, 0.14717227220535278, 0.17578111588954926, 0.16794852912425995, 0.1626344919204712, 0.19076824188232422, 0.16466674208641052, 0.17417937517166138, 0.17295081913471222, 0.1634901612997055, 0.17465496063232422, 0.17211219668388367, 0.16171583533287048, 0.17487654089927673, 0.1798562854528427, 0.17181870341300964, 0.1849566400051117, 0.17873762547969818, 0.1684136837720871, 0.18140363693237305, 0.18043164908885956, 0.17163565754890442, 0.18449372053146362, 0.177321657538414, 0.1709391474723816, 0.17314733564853668, 0.1573513150215149, 0.1656661182641983, 0.15967398881912231, 0.15562020242214203, 0.16632217168807983, 0.16069291532039642, 0.1663915514945984, 0.17096231877803802, 0.1636749655008316, 0.17180022597312927, 0.16165801882743835, 0.16353477537631989, 0.1633434146642685, 0.15837973356246948, 0.16781291365623474, 0.15501245856285095, 0.1722836047410965, 0.1441165804862976, 0.1761009395122528, 0.1398002952337265, 0.1575731337070465, 0.164678156375885, 0.13989683985710144, 0.16543219983577728, 0.15218253433704376, 0.14232538640499115, 0.15806511044502258, 0.13981294631958008, 0.13621598482131958, 0.14854858815670013, 0.13407982885837555, 0.13763847947120667, 0.14678853750228882, 0.13276389241218567, 0.14043772220611572, 0.13846315443515778, 0.1302926391363144, 0.1399589329957962, 0.1355798989534378, 0.13142794370651245, 0.13877828419208527, 0.13176210224628448, 0.13078469038009644, 0.13669289648532867, 0.12944354116916656, 0.1317628175020218, 0.13155142962932587, 0.1252865046262741, 0.1305026113986969, 0.1252380609512329, 0.12607786059379578, 0.1295296549797058, 0.1230853796005249, 0.12843573093414307, 0.12492892891168594, 0.12551970779895782, 0.12745919823646545, 0.12419476360082626, 0.12842915952205658, 0.12122676521539688, 0.12913353741168976, 0.12097208201885223, 0.12975327670574188, 0.12152734398841858, 0.1286463588476181, 0.12225714325904846, 0.12642621994018555, 0.12196838855743408, 0.12286565452814102, 0.12230348587036133, 0.12038642168045044, 0.12370319664478302, 0.1204819455742836, 0.12460474669933319, 0.12069712579250336, 0.12588344514369965, 0.11919094622135162, 0.12627314031124115, 0.1184765100479126, 0.1248713955283165, 0.12077099829912186, 0.12337472289800644, 0.12355507165193558, 0.12247785925865173, 0.12101667374372482, 0.12367573380470276, 0.11429790407419205, 0.12956921756267548, 0.11089061945676804, 0.1348932981491089, 0.11587157845497131, 0.12123450636863708, 0.1255253702402115, 0.11249072104692459, 0.12564094364643097, 0.11828459054231644, 0.11946948617696762, 0.12008651345968246, 0.11719469726085663, 0.1193707212805748, 0.12041977792978287, 0.12572081387043, 0.11909978836774826, 0.12719257175922394, 0.1245058998465538, 0.11745652556419373, 0.12849023938179016, 0.11730962991714478, 0.12532883882522583, 0.12692572176456451, 0.12198495119810104, 0.12747548520565033, 0.12359356135129929, 0.12000932544469833, 0.13036078214645386, 0.12062256038188934, 0.13087905943393707, 0.12346406280994415, 0.12336215376853943, 0.12277279794216156, 0.12060817331075668, 0.1223939061164856, 0.12661321461200714, 0.1263868510723114, 0.12378119677305222, 0.12295634299516678, 0.1227760910987854, 0.11776865273714066, 0.12563619017601013, 0.12615616619586945, 0.11752215027809143, 0.131924569606781, 0.11404440551996231, 0.12360609322786331, 0.1150883287191391, 0.11914721131324768, 0.12732960283756256, 0.11681191623210907, 0.12392451614141464, 0.12284982949495316, 0.11957469582557678, 0.12082931399345398, 0.11084824800491333, 0.12149765342473984, 0.11727670580148697, 0.1186443418264389, 0.11619703471660614, 0.11917988955974579, 0.12136834114789963, 0.11542696505784988, 0.11769673973321915, 0.1182934045791626, 0.11946126818656921, 0.11847357451915741, 0.1145634651184082, 0.11997418105602264, 0.11416551470756531, 0.1180226281285286, 0.1156507357954979, 0.12131207436323166, 0.11416375637054443, 0.12040053308010101, 0.11598916351795197, 0.12056595832109451, 0.11179962754249573, 0.12440197914838791, 0.10742337256669998, 0.12690557539463043, 0.10130565613508224, 0.1379869282245636, 0.09386779367923737, 0.12934517860412598, 0.10719043761491776, 0.10574907809495926, 0.1246393546462059, 0.0970788449048996, 0.11515721678733826, 0.11469241976737976, 0.10043296217918396, 0.12053874880075455, 0.10573967546224594, 0.10457979142665863, 0.11958355456590652, 0.10310732573270798, 0.10978741198778152, 0.11429592967033386, 0.1020611822605133, 0.11477596312761307, 0.1121797114610672, 0.10557883232831955, 0.11637134104967117, 0.1080629825592041, 0.1083056628704071, 0.11586157977581024, 0.10635387897491455, 0.11019862443208694, 0.11099245399236679, 0.10378938913345337, 0.11059090495109558, 0.10776447504758835, 0.105341836810112, 0.11139149963855743, 0.10648185014724731, 0.10780271142721176, 0.10992375016212463, 0.10509616881608963, 0.10870158672332764, 0.10739326477050781, 0.10514618456363678, 0.10857720673084259, 0.1044832393527031, 0.10548102855682373, 0.10732412338256836, 0.10394280403852463, 0.106630340218544, 0.10515496134757996, 0.10381928086280823, 0.10593883693218231, 0.10295142978429794, 0.10440503805875778, 0.10434095561504364, 0.10242641717195511, 0.10452763736248016, 0.1024048924446106, 0.10262510180473328, 0.10316874086856842, 0.1019919291138649, 0.10250658541917801, 0.10188364237546921, 0.10192400217056274, 0.10143668204545975, 0.10168812423944473, 0.10108418762683868, 0.10088616609573364, 0.1011568233370781, 0.1001008152961731, 0.10125573724508286, 0.09928563982248306, 0.10113593935966492, 0.09891696274280548, 0.10044681280851364, 0.0989973247051239, 0.09996533393859863, 0.09893805533647537, 0.10002172738313675, 0.09879600256681442, 0.1000857800245285, 0.09924532473087311, 0.0997975692152977, 0.09980498999357224, 0.09937797486782074, 0.10052040964365005, 0.09867896884679794, 0.1013479232788086, 0.09710539132356644, 0.10263514518737793, 0.09310528635978699, 0.1044355109333992, 0.08932092040777206, 0.10146136581897736, 0.09815412759780884, 0.09961285442113876, 0.10127435624599457, 0.09674780815839767, 0.1009511724114418, 0.10074474662542343, 0.10153798013925552, 0.10574919730424881, 0.10319746285676956, 0.106998011469841, 0.10338816046714783, 0.10424596071243286, 0.10742989182472229, 0.10578682273626328, 0.10769956558942795, 0.1048017218708992, 0.10637857764959335, 0.10460885614156723, 0.10645885020494461, 0.10446485877037048, 0.1044715940952301, 0.10434503108263016, 0.10368847846984863, 0.10472119599580765, 0.1039777472615242, 0.10484666377305984, 0.1044861301779747, 0.10576822608709335, 0.1045975387096405, 0.10565705597400665, 0.10400658845901489, 0.10548671334981918, 0.10411807894706726, 0.10565335303544998, 0.10427093505859375, 0.10606048256158829, 0.10430268198251724, 0.10596418380737305, 0.1038159653544426, 0.10627627372741699, 0.10363918542861938, 0.10694815218448639, 0.10287349671125412, 0.10754340142011642, 0.1019153818488121, 0.10981009155511856, 0.1003376692533493, 0.11264625191688538, 0.09621588885784149, 0.11842469871044159, 0.09008853137493134, 0.1282048225402832, 0.08167783915996552, 0.13172881305217743, 0.082369863986969, 0.10914919525384903, 0.10473814606666565, 0.08401855826377869, 0.11336098611354828, 0.08580207079648972, 0.0905599296092987, 0.10567673295736313, 0.0815991684794426, 0.09556238353252411, 0.09597164392471313, 0.08110005408525467, 0.09701988846063614, 0.0910264253616333, 0.08348406106233597, 0.09693726897239685, 0.08899963647127151, 0.08513884246349335, 0.09438052773475647, 0.08717337995767593, 0.08810346573591232, 0.09547174721956253, 0.08756586909294128, 0.0889102965593338, 0.09369265288114548, 0.08817540854215622, 0.09282012283802032, 0.09442193061113358, 0.08822235465049744, 0.09359545260667801, 0.09368261694908142, 0.08976519107818604, 0.09484215825796127, 0.092227041721344, 0.09028145670890808, 0.09508364647626877, 0.09116978943347931, 0.09094458818435669, 0.0946674793958664, 0.09106102585792542, 0.09252581000328064, 0.09385362267494202, 0.09073380380868912, 0.09410608559846878, 0.0938592329621315, 0.09179944545030594, 0.09483631700277328, 0.09333440661430359, 0.09359145909547806, 0.0955834612250328, 0.09307067096233368, 0.09506594389677048, 0.095264732837677, 0.0936681404709816, 0.09594908356666565, 0.09431692957878113, 0.09516558051109314, 0.09565519541501999, 0.09433013945817947, 0.09623891115188599, 0.09492342919111252, 0.09528884291648865, 0.09587971866130829, 0.09512914717197418, 0.0960526168346405, 0.09523124247789383, 0.09587573260068893, 0.09529590606689453, 0.0959557518362999, 0.09494733065366745, 0.0961809828877449, 0.09430429339408875, 0.09629936516284943, 0.09412931650876999, 0.0960100069642067, 0.0937170684337616, 0.09612039476633072, 0.09266671538352966, 0.09707489609718323, 0.08988600224256516, 0.09875642508268356, 0.08690065890550613, 0.09704125672578812, 0.09091611206531525, 0.09121588617563248, 0.09539514780044556, 0.0917101725935936, 0.09544987976551056, 0.09005378186702728, 0.09613323211669922, 0.09072364121675491, 0.09173007309436798, 0.09426204860210419, 0.09017622470855713, 0.09554038941860199, 0.08976483345031738, 0.09604497253894806, 0.09144606441259384, 0.09344451874494553, 0.09303954988718033, 0.09353607147932053, 0.09450699388980865, 0.08861721307039261, 0.09684126824140549, 0.09159175306558609, 0.09460123628377914, 0.09275642037391663, 0.0951644629240036, 0.09363071620464325, 0.09175542742013931, 0.09602227807044983, 0.0913476049900055, 0.09520316123962402, 0.09078718721866608, 0.09548761695623398, 0.09177122265100479, 0.093377485871315, 0.0922337993979454, 0.09379013627767563, 0.09220042079687119, 0.09136280417442322, 0.09371969103813171, 0.09083325415849686, 0.09355440735816956, 0.09106393158435822, 0.0937323123216629, 0.0897253006696701, 0.09454040974378586, 0.08890309184789658, 0.0949137806892395, 0.08677662163972855, 0.09763404726982117, 0.08378954976797104, 0.10016486793756485, 0.08064448833465576, 0.10333916544914246, 0.07537476718425751, 0.10445070266723633, 0.07478358596563339, 0.09483188390731812, 0.08408970385789871, 0.08198940008878708, 0.09116558730602264, 0.07937366515398026, 0.0923224687576294, 0.07924454659223557, 0.08548580855131149, 0.08767415583133698, 0.07763183861970901, 0.08893327414989471, 0.08150200545787811, 0.08410567790269852, 0.08446616679430008, 0.08027932047843933, 0.0861251950263977, 0.07990791648626328, 0.083302803337574, 0.0828612744808197, 0.08118180185556412, 0.08423959463834763, 0.0800425186753273, 0.0846247673034668, 0.08207591623067856, 0.08071013540029526, 0.08449006080627441, 0.08044467866420746, 0.08254636824131012, 0.08283894509077072, 0.0827329158782959, 0.0827796682715416, 0.0815298929810524, 0.08347980678081512, 0.08090389519929886, 0.08180017024278641, 0.08196763694286346, 0.0804901272058487, 0.08251264691352844, 0.0803275778889656, 0.08162589371204376, 0.08057839423418045, 0.08038902282714844, 0.08096953481435776, 0.08027228713035583, 0.08077757805585861, 0.07971493899822235, 0.08128701895475388, 0.07945898175239563, 0.07995713502168655, 0.08023048937320709, 0.07916317880153656, 0.07985726743936539, 0.07896546274423599, 0.07973785698413849, 0.07855150103569031, 0.07908426225185394, 0.07854717969894409, 0.07826351374387741, 0.07836512476205826, 0.0781213715672493, 0.07815839350223541, 0.07729620486497879, 0.07832667231559753, 0.07710408419370651, 0.07753931730985641, 0.07695076614618301, 0.07726771384477615, 0.07662767916917801, 0.07698351889848709, 0.0766879990696907, 0.07638207077980042, 0.07644107192754745, 0.07618416845798492, 0.07615058869123459, 0.07579194754362106, 0.0760154277086258, 0.07535535842180252, 0.07573866099119186, 0.075202077627182, 0.07551674544811249, 0.07490908354520798, 0.07543668150901794, 0.07454580068588257, 0.07543759047985077, 0.07400660961866379, 0.07559172809123993, 0.07328497618436813, 0.07613498717546463, 0.07152918726205826, 0.07802566885948181, 0.06827839463949203, 0.0824286937713623, 0.07215694338083267, 0.09170705080032349, 0.050090063363313675, 0.1009543389081955, 0.10962507128715515, 0.037576332688331604, 0.041863713413476944, 0.09042448550462723, 0.115196093916893, 0.22818057239055634, 0.1268128752708435, 0.07056222856044769, 0.08106610178947449, 0.15408141911029816, 0.2240743637084961, 0.1943579465150833, 0.17974171042442322, 0.22099222242832184, 0.21707910299301147, 0.16794680058956146, 0.16733741760253906, 0.16465918719768524, 0.13951271772384644, 0.12004116922616959, 0.1229793131351471, 0.14474701881408691, 0.16487857699394226, 0.16444435715675354, 0.16750621795654297, 0.19749601185321808, 0.2206096202135086, 0.21723949909210205, 0.23194585740566254, 0.266152024269104, 0.27589401602745056, 0.27619263529777527, 0.3065337836742401, 0.3104969561100006, 0.31296730041503906, 0.34511446952819824, 0.3295573890209198, 0.28556448221206665, 0.33164840936660767, 0.3460735082626343, 0.31663328409194946, 0.3450893759727478, 0.3630840480327606, 0.3428659439086914, 0.3391600251197815, 0.3381640315055847, 0.31564849615097046, 0.3034324645996094, 0.31742575764656067, 0.32134759426116943, 0.30660369992256165, 0.30977386236190796, 0.3375014364719391, 0.3314141035079956, 0.3035813271999359, 0.3041505813598633, 0.30233001708984375, 0.2822435796260834, 0.27148717641830444, 0.28645065426826477, 0.3003399670124054, 0.29399392008781433, 0.2955811321735382, 0.2998778223991394, 0.281217485666275, 0.268673837184906, 0.2734416425228119, 0.2721961438655853, 0.2643403708934784, 0.2734588384628296, 0.28467777371406555, 0.2809980809688568, 0.2894907295703888, 0.2907649576663971, 0.27991607785224915, 0.2834603488445282, 0.26802703738212585, 0.26112401485443115, 0.2643631100654602, 0.255825936794281, 0.2610498070716858, 0.2648691236972809, 0.2596898078918457, 0.2783946096897125, 0.24605001509189606, 0.28474169969558716, 0.23113574087619781, 0.2437395453453064, 0.2582378089427948, 0.22567270696163177, 0.25745251774787903, 0.2439061850309372, 0.2335459589958191, 0.2675468921661377, 0.20868425071239471, 0.270834356546402, 0.18766112625598907, 0.17131619155406952, 0.22397491335868835, 0.20181803405284882, 0.1668005883693695, 0.2137494832277298, 0.215690478682518, 0.16990914940834045, 0.18592189252376556, 0.21381598711013794, 0.17667712271213531, 0.1675826758146286, 0.19742804765701294, 0.17135243117809296, 0.1566191017627716, 0.18593211472034454, 0.15708298981189728, 0.15871752798557281, 0.17661501467227936, 0.1414746642112732, 0.18110452592372894, 0.1372147500514984, 0.13300450146198273, 0.1626817286014557, 0.14209064841270447, 0.13379134237766266, 0.16684354841709137, 0.15566334128379822, 0.14655937254428864, 0.1723601520061493, 0.15150301158428192, 0.13768517971038818, 0.1568460464477539, 0.1547544002532959, 0.13743406534194946, 0.15015879273414612, 0.16471385955810547, 0.14895766973495483, 0.15705297887325287, 0.16387847065925598, 0.14434193074703217, 0.14904062449932098, 0.15384724736213684, 0.14182697236537933, 0.15121996402740479, 0.15853653848171234, 0.15034650266170502, 0.16542305052280426, 0.1535654067993164, 0.1536225825548172, 0.15278582274913788, 0.1474442332983017, 0.15370328724384308, 0.14489541947841644, 0.15650618076324463, 0.14283651113510132, 0.16055123507976532, 0.14344480633735657, 0.15839911997318268, 0.14642475545406342, 0.15334279835224152, 0.15234240889549255, 0.15154486894607544, 0.15498366951942444, 0.1458161175251007, 0.15676088631153107, 0.13940776884555817, 0.16543835401535034, 0.12886565923690796, 0.158147931098938, 0.14782088994979858, 0.13446830213069916, 0.1570938527584076, 0.140583336353302, 0.14239554107189178, 0.15059947967529297, 0.13338281214237213, 0.14444531500339508, 0.1461808830499649, 0.13935358822345734, 0.1565457582473755, 0.1411823183298111, 0.14580689370632172, 0.15195444226264954, 0.13693150877952576, 0.15295737981796265, 0.14145027101039886, 0.14230069518089294, 0.15098212659358978, 0.13445065915584564, 0.15937116742134094, 0.12804411351680756, 0.14974640309810638, 0.1386781930923462, 0.13318544626235962, 0.15412643551826477, 0.12900225818157196, 0.14343543350696564, 0.1446428894996643, 0.12858009338378906, 0.1452273726463318, 0.14263038337230682, 0.13585709035396576, 0.15554627776145935, 0.13382048904895782, 0.14100703597068787, 0.15171651542186737, 0.13482698798179626, 0.15206894278526306, 0.14303463697433472, 0.13820403814315796, 0.1322103589773178, 0.11877891421318054, 0.14795465767383575, 0.15155598521232605, 0.1440581977367401, 0.15572179853916168, 0.1399002969264984, 0.1448398381471634, 0.14614912867546082, 0.13474534451961517, 0.1488138735294342, 0.12848298251628876, 0.1376843899488449, 0.1413918286561966, 0.12879826128482819, 0.1504681259393692, 0.12210147082805634, 0.12990954518318176, 0.13697823882102966, 0.11880113929510117, 0.14059236645698547, 0.12527306377887726, 0.121964231133461, 0.13886502385139465, 0.12242576479911804, 0.1397339254617691, 0.1411239057779312, 0.12113887071609497, 0.1358618140220642, 0.1204017847776413, 0.12000536173582077, 0.1362302601337433, 0.12409079074859619, 0.13133786618709564, 0.13378311693668365, 0.12248628586530685, 0.13225246965885162, 0.12686040997505188, 0.12090718746185303, 0.13180489838123322, 0.12027060985565186, 0.12434019893407822, 0.13007669150829315, 0.11973930895328522, 0.13121843338012695, 0.12379190325737, 0.1199210062623024, 0.12963762879371643, 0.12057837843894958, 0.12624916434288025, 0.1264324188232422, 0.11691129952669144, 0.1264892965555191, 0.11965610086917877, 0.11822280287742615, 0.1259121298789978, 0.11834871768951416, 0.13076664507389069, 0.11479754745960236, 0.11825799196958542, 0.1181572750210762, 0.11418844759464264, 0.12068435549736023, 0.11599445343017578, 0.11857492476701736, 0.11655375361442566, 0.11392375826835632, 0.11577945947647095, 0.11579110473394394, 0.11594121903181076, 0.11332748085260391, 0.1125316321849823, 0.11354461312294006, 0.11209281533956528, 0.1126769632101059, 0.11324398964643478, 0.10921728610992432, 0.11094021052122116, 0.10651829093694687, 0.1094805970788002, 0.10618634521961212, 0.11025435477495193, 0.10810184478759766, 0.10497808456420898, 0.10537198930978775, 0.1042453721165657, 0.10425707697868347, 0.10130352526903152, 0.10340914130210876, 0.10043664276599884, 0.10115453600883484, 0.10007302463054657, 0.09965374320745468, 0.09747819602489471, 0.09768630564212799, 0.09724634140729904, 0.09685005992650986, 0.09612450003623962, 0.0953465923666954, 0.09559277445077896, 0.09412042796611786, 0.09392668306827545, 0.09238877147436142, 0.09305925667285919, 0.09183761477470398, 0.09233551472425461, 0.08940018713474274, 0.09286615997552872, 0.08670517802238464, 0.09484159201383591, 0.08184625953435898, 0.09822320938110352, 0.07654721289873123, 0.09394194185733795, 0.07642018049955368, 0.07935523986816406, 0.0909138098359108, 0.07724028080701828, 0.08884148299694061, 0.08386163413524628, 0.09157269448041916, 0.09250842779874802, 0.08614227920770645, 0.09933167695999146, 0.08405628800392151, 0.08278994262218475, 0.08689052611589432, 0.08154512941837311, 0.09437298774719238, 0.09259270131587982, 0.09076382964849472, 0.09961943328380585, 0.08705060184001923, 0.09072112292051315, 0.08973579108715057, 0.08234142512083054, 0.09036783874034882, 0.08519405871629715, 0.09009305387735367, 0.08999304473400116, 0.08654076606035233, 0.09135324507951736, 0.08702676743268967, 0.0888441875576973, 0.08713597059249878, 0.08292834460735321, 0.08649588376283646, 0.0839327797293663, 0.08801984786987305, 0.0879010483622551, 0.08691203594207764, 0.08944174647331238, 0.08483545482158661, 0.08745899796485901, 0.08529840409755707, 0.08465005457401276, 0.08536405861377716, 0.08236946165561676, 0.08529573678970337, 0.08281906694173813, 0.08446693420410156, 0.08243001997470856, 0.08185870200395584, 0.08090409636497498, 0.07973484694957733, 0.08029133081436157, 0.07839833945035934, 0.07898172736167908, 0.07664763182401657, 0.07734072953462601, 0.0761670395731926, 0.07644099742174149, 0.07540610432624817, 0.07467948645353317, 0.07385999709367752, 0.072899729013443, 0.0730833038687706, 0.0716039389371872, 0.07217954099178314, 0.06958045065402985, 0.0711003839969635, 0.06771466881036758, 0.07017437368631363, 0.06621347367763519, 0.0692092627286911, 0.0645715519785881, 0.06849396228790283, 0.06338845938444138, 0.0681123211979866, 0.061978425830602646, 0.06790830194950104, 0.0612000972032547, 0.06705902516841888, 0.06028303876519203, 0.0660998672246933, 0.05926179885864258, 0.06462682038545609, 0.05812970921397209, 0.06276165693998337, 0.058844588696956635, 0.061348509043455124, 0.06050503998994827, 0.05992037057876587, 0.06123272702097893, 0.057819854468107224, 0.06149238348007202, 0.05642187222838402, 0.06068044155836105, 0.05490301921963692, 0.06007004901766777, 0.05503670498728752, 0.05960869416594505, 0.05511319637298584, 0.057444289326667786, 0.05522951856255531, 0.055910468101501465, 0.05625196918845177, 0.05479002743959427, 0.05647383630275726, 0.0528053380548954, 0.056056320667266846, 0.051406510174274445, 0.05534355342388153, 0.050764165818691254, 0.05477605760097504, 0.050271715968847275, 0.054250236600637436, 0.04987748712301254, 0.05318626016378403, 0.049987148493528366, 0.05253821611404419, 0.05031195282936096, 0.05145644024014473, 0.05009175464510918, 0.0501488633453846, 0.04992571473121643, 0.04930249974131584, 0.04979274421930313, 0.04842956364154816, 0.04967788979411125, 0.04747478663921356, 0.04994656890630722, 0.04616203159093857, 0.050474848598241806, 0.04421620070934296, 0.05177217349410057, 0.04171322286128998, 0.051570869982242584, 0.04044997692108154, 0.04510005563497543, 0.04503421112895012, 0.04302508011460304, 0.04563113674521446, 0.04583200439810753, 0.04500589147210121, 0.04937765374779701, 0.048037536442279816, 0.05293481796979904, 0.055408164858818054, 0.052988938987255096, 0.058861538767814636, 0.05193411186337471, 0.053906459361314774, 0.05272068455815315, 0.05146235600113869, 0.05599550902843475, 0.05223902687430382, 0.05492108315229416, 0.05234042927622795, 0.05164163559675217, 0.05403101444244385, 0.051530782133340836, 0.054702769964933395, 0.05155206844210625, 0.05207829177379608, 0.05014337971806526, 0.049106430262327194, 0.049803927540779114, 0.04836248233914375, 0.04980803653597832, 0.04867079108953476, 0.04903075471520424, 0.049303364008665085, 0.04856327548623085, 0.04978325217962265, 0.04793502017855644, 0.04919447377324104, 0.04672929272055626, 0.04819297417998314, 0.046496450901031494, 0.04712925851345062, 0.04705049470067024, 0.04646379500627518, 0.047440171241760254, 0.04576898366212845, 0.047425590455532074, 0.04548005759716034, 0.04709919914603233, 0.04509739205241203, 0.04633612558245659, 0.04509521275758743, 0.04533889889717102, 0.04526950418949127, 0.04468727856874466, 0.045427028089761734, 0.044049009680747986, 0.04554161801934242, 0.04369184747338295, 0.04528127610683441, 0.043433208018541336, 0.04497993737459183, 0.04310237988829613, 0.04466034844517708, 0.04280012473464012, 0.04440128803253174, 0.042147062718868256, 0.044380445033311844, 0.04191364347934723, 0.04421502351760864, 0.041594408452510834, 0.04422594606876373, 0.0404679998755455, 0.04443227872252464, 0.04019584879279137, 0.04345056787133217, 0.039864540100097656, 0.04287722706794739, 0.04029085487127304, 0.04176466166973114, 0.04146035015583038, 0.041261326521635056, 0.04193858802318573, 0.04039076715707779, 0.041679635643959045, 0.04003731906414032, 0.040749117732048035, 0.03996143490076065, 0.04037558659911156, 0.039965663105249405, 0.039610881358385086, 0.040231864899396896, 0.039491213858127594, 0.03983030095696449, 0.0390961654484272, 0.039644572883844376, 0.03912675753235817, 0.03885616734623909, 0.03907556086778641, 0.038599081337451935, 0.03912421688437462, 0.03802122175693512, 0.03940847888588905, 0.03774329647421837, 0.03923121094703674, 0.03714191913604736, 0.03972916305065155, 0.03654170408844948, 0.03916073590517044, 0.03610438480973244, 0.039990849792957306, 0.03625344857573509, 0.03904867172241211, 0.03706906735897064, 0.03890896588563919, 0.03745089843869209, 0.03729939088225365, 0.038158200681209564, 0.037100013345479965, 0.03885621204972267, 0.03626323491334915, 0.03862931951880455, 0.03617165982723236, 0.038025032728910446, 0.03601674735546112, 0.03704899176955223, 0.036657433956861496, 0.036363374441862106, 0.0371050164103508, 0.035750776529312134, 0.037355467677116394, 0.035351794213056564, 0.037065472453832626, 0.035135719925165176, 0.03654679283499718, 0.03541344031691551, 0.03626582399010658, 0.03580569848418236, 0.03553425520658493, 0.035681601613759995, 0.035151153802871704, 0.03584999218583107, 0.03468012437224388, 0.035963088274002075, 0.03454742208123207, 0.036504775285720825, 0.033793844282627106, 0.037114545702934265, 0.03317973390221596, 0.03848094865679741, 0.032510627061128616, 0.03917642682790756, 0.03274153918027878, 0.037319738417863846, 0.03377104550600052, 0.03459285944700241, 0.03734898194670677, 0.03407277539372444, 0.03859866410493851, 0.034297727048397064, 0.03623226284980774, 0.03634852170944214, 0.03393139690160751, 0.03699922561645508, 0.03444993123412132, 0.03584655746817589, 0.03521270304918289, 0.0337722972035408, 0.03590767830610275, 0.03426515683531761, 0.03497792035341263, 0.03475510701537132, 0.0335717648267746, 0.03448041155934334, 0.03242035210132599, 0.033767662942409515, 0.03353763744235039, 0.033407025039196014, 0.034300051629543304, 0.03283272311091423, 0.033936358988285065, 0.033003825694322586, 0.03363658860325813, 0.033181849867105484, 0.032770462334156036, 0.0333787277340889, 0.032834041863679886, 0.03358206897974014, 0.03256943076848984, 0.033116038888692856, 0.03274025395512581, 0.0330532006919384, 0.03292946144938469, 0.03267725184559822, 0.033148184418678284, 0.03260026127099991, 0.03333397954702377, 0.03209157660603523, 0.033245332539081573, 0.03204901143908501, 0.033462364226579666, 0.0317593552172184, 0.033433206379413605, 0.03156084194779396, 0.033515721559524536, 0.03113844059407711, 0.03333667293190956, 0.031209051609039307, 0.03306727111339569, 0.031226055696606636, 0.032355133444070816, 0.031567126512527466, 0.03183763101696968, 0.03204289451241493, 0.03125312179327011, 0.03218753635883331, 0.03091117925941944, 0.03248948976397514, 0.030662931501865387, 0.032579224556684494, 0.030577819794416428, 0.03246348351240158, 0.03052632324397564, 0.032036181539297104, 0.03071962110698223, 0.03159372881054878, 0.031127214431762695, 0.031112415716052055, 0.03154730796813965, 0.030843907967209816, 0.031844522804021835, 0.03059106320142746, 0.03194860741496086, 0.030406491830945015, 0.03198433294892311, 0.030354661867022514, 0.031951531767845154, 0.03037622570991516, 0.031508319079875946, 0.030340522527694702, 0.0307953879237175, 0.030751898884773254, 0.03048374503850937, 0.031198041513562202, 0.03023535944521427, 0.03140127658843994, 0.03013695403933525, 0.031263548880815506, 0.030177973210811615, 0.031257010996341705, 0.030497830361127853, 0.031106144189834595, 0.0305741298943758, 0.03078044019639492, 0.030863575637340546, 0.030600130558013916, 0.030915163457393646, 0.03039792738854885, 0.031090736389160156, 0.03025546669960022, 0.031136900186538696, 0.030104761943221092, 0.031503211706876755, 0.03014436922967434, 0.03172294795513153, 0.029670091345906258].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1485 | train: Loss 0.000039 Accuracy 0.9986 | validation: Loss 0.030498 Accuracy -0.9160\n",
      "Epoch 1486 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.031106 Accuracy -0.9542\n",
      "Epoch 1487 | train: Loss 0.000019 Accuracy 0.9993 | validation: Loss 0.030574 Accuracy -0.9208\n",
      "Epoch 1488 | train: Loss 0.000010 Accuracy 0.9996 | validation: Loss 0.030780 Accuracy -0.9337\n",
      "Epoch 1489 | train: Loss 0.000007 Accuracy 0.9998 | validation: Loss 0.030864 Accuracy -0.9389\n",
      "Epoch 1490 | train: Loss 0.000008 Accuracy 0.9997 | validation: Loss 0.030600 Accuracy -0.9224\n",
      "Epoch 1491 | train: Loss 0.000009 Accuracy 0.9997 | validation: Loss 0.030915 Accuracy -0.9422\n",
      "Epoch 1492 | train: Loss 0.000015 Accuracy 0.9995 | validation: Loss 0.030398 Accuracy -0.9097\n",
      "Epoch 1493 | train: Loss 0.000021 Accuracy 0.9992 | validation: Loss 0.031091 Accuracy -0.9532\n",
      "Epoch 1494 | train: Loss 0.000029 Accuracy 0.9989 | validation: Loss 0.030255 Accuracy -0.9007\n",
      "Epoch 1495 | train: Loss 0.000040 Accuracy 0.9985 | validation: Loss 0.031137 Accuracy -0.9561\n",
      "Epoch 1496 | train: Loss 0.000052 Accuracy 0.9981 | validation: Loss 0.030105 Accuracy -0.8913\n",
      "Epoch 1497 | train: Loss 0.000069 Accuracy 0.9975 | validation: Loss 0.031503 Accuracy -0.9791\n",
      "Epoch 1498 | train: Loss 0.000092 Accuracy 0.9966 | validation: Loss 0.030144 Accuracy -0.8938\n",
      "Epoch 1499 | train: Loss 0.000125 Accuracy 0.9954 | validation: Loss 0.031723 Accuracy -0.9929\n",
      "Epoch 1500 | train: Loss 0.000164 Accuracy 0.9940 | validation: Loss 0.029670 Accuracy -0.8640\n",
      "Epoch 1500 | train: Loss 0.000164 Accuracy 0.9940 | validation: Loss 0.029670 Accuracy -0.8640\n",
      "1 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174dbd87667342e0900161f1f2034b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train: Loss 0.108212 Accuracy -2.9685 | validation: Loss 0.149280 Accuracy -8.3782\n",
      "Epoch 2 | train: Loss 0.091772 Accuracy -2.3656 | validation: Loss 0.113598 Accuracy -6.1365\n",
      "Epoch 3 | train: Loss 0.077669 Accuracy -1.8484 | validation: Loss 0.084734 Accuracy -4.3232\n",
      "Epoch 4 | train: Loss 0.065718 Accuracy -1.4101 | validation: Loss 0.061977 Accuracy -2.8936\n",
      "Epoch 5 | train: Loss 0.055802 Accuracy -1.0465 | validation: Loss 0.045291 Accuracy -1.8453\n",
      "Epoch 6 | train: Loss 0.047781 Accuracy -0.7523 | validation: Loss 0.033116 Accuracy -1.0804\n",
      "Epoch 7 | train: Loss 0.041467 Accuracy -0.5207 | validation: Loss 0.024873 Accuracy -0.5626\n",
      "Epoch 8 | train: Loss 0.036741 Accuracy -0.3474 | validation: Loss 0.020084 Accuracy -0.2617\n",
      "Epoch 9 | train: Loss 0.033397 Accuracy -0.2248 | validation: Loss 0.018273 Accuracy -0.1480\n",
      "Epoch 10 | train: Loss 0.031221 Accuracy -0.1450 | validation: Loss 0.019262 Accuracy -0.2101\n",
      "Epoch 11 | train: Loss 0.029950 Accuracy -0.0984 | validation: Loss 0.022174 Accuracy -0.3930\n",
      "Epoch 12 | train: Loss 0.029352 Accuracy -0.0764 | validation: Loss 0.026198 Accuracy -0.6459\n",
      "Epoch 13 | train: Loss 0.029218 Accuracy -0.0715 | validation: Loss 0.030574 Accuracy -0.9208\n",
      "Epoch 14 | train: Loss 0.029334 Accuracy -0.0758 | validation: Loss 0.034651 Accuracy -1.1769\n",
      "Epoch 15 | train: Loss 0.029536 Accuracy -0.0832 | validation: Loss 0.037939 Accuracy -1.3834\n",
      "Epoch 16 | train: Loss 0.029675 Accuracy -0.0883 | validation: Loss 0.040136 Accuracy -1.5215\n",
      "Epoch 17 | train: Loss 0.029658 Accuracy -0.0877 | validation: Loss 0.041129 Accuracy -1.5838\n",
      "Epoch 18 | train: Loss 0.029436 Accuracy -0.0795 | validation: Loss 0.040968 Accuracy -1.5737\n",
      "Epoch 19 | train: Loss 0.029010 Accuracy -0.0639 | validation: Loss 0.039827 Accuracy -1.5020\n",
      "Epoch 20 | train: Loss 0.028411 Accuracy -0.0419 | validation: Loss 0.037950 Accuracy -1.3842\n",
      "Epoch 21 | train: Loss 0.027696 Accuracy -0.0157 | validation: Loss 0.035480 Accuracy -1.2289\n",
      "Epoch 22 | train: Loss 0.026914 Accuracy 0.0130 | validation: Loss 0.032650 Accuracy -1.0512\n",
      "Epoch 23 | train: Loss 0.026123 Accuracy 0.0420 | validation: Loss 0.029767 Accuracy -0.8700\n",
      "Epoch 24 | train: Loss 0.025390 Accuracy 0.0688 | validation: Loss 0.027135 Accuracy -0.7047\n",
      "Epoch 25 | train: Loss 0.024759 Accuracy 0.0920 | validation: Loss 0.024956 Accuracy -0.5678\n",
      "Epoch 26 | train: Loss 0.024252 Accuracy 0.1106 | validation: Loss 0.023289 Accuracy -0.4631\n",
      "Epoch 27 | train: Loss 0.023875 Accuracy 0.1244 | validation: Loss 0.022099 Accuracy -0.3883\n",
      "Epoch 28 | train: Loss 0.023628 Accuracy 0.1335 | validation: Loss 0.021284 Accuracy -0.3371\n",
      "Epoch 29 | train: Loss 0.023475 Accuracy 0.1391 | validation: Loss 0.020863 Accuracy -0.3106\n",
      "Epoch 30 | train: Loss 0.023409 Accuracy 0.1415 | validation: Loss 0.020726 Accuracy -0.3021\n",
      "Epoch 31 | train: Loss 0.023391 Accuracy 0.1422 | validation: Loss 0.020784 Accuracy -0.3057\n",
      "Epoch 32 | train: Loss 0.023386 Accuracy 0.1423 | validation: Loss 0.020888 Accuracy -0.3122\n",
      "Epoch 33 | train: Loss 0.023365 Accuracy 0.1431 | validation: Loss 0.020977 Accuracy -0.3178\n",
      "Epoch 34 | train: Loss 0.023305 Accuracy 0.1453 | validation: Loss 0.021002 Accuracy -0.3194\n",
      "Epoch 35 | train: Loss 0.023194 Accuracy 0.1494 | validation: Loss 0.020950 Accuracy -0.3161\n",
      "Epoch 36 | train: Loss 0.023026 Accuracy 0.1556 | validation: Loss 0.020829 Accuracy -0.3086\n",
      "Epoch 37 | train: Loss 0.022802 Accuracy 0.1638 | validation: Loss 0.020672 Accuracy -0.2987\n",
      "Epoch 38 | train: Loss 0.022533 Accuracy 0.1736 | validation: Loss 0.020520 Accuracy -0.2892\n",
      "Epoch 39 | train: Loss 0.022231 Accuracy 0.1847 | validation: Loss 0.020409 Accuracy -0.2822\n",
      "Epoch 40 | train: Loss 0.021907 Accuracy 0.1966 | validation: Loss 0.020420 Accuracy -0.2829\n",
      "Epoch 41 | train: Loss 0.021588 Accuracy 0.2083 | validation: Loss 0.020511 Accuracy -0.2886\n",
      "Epoch 42 | train: Loss 0.021295 Accuracy 0.2191 | validation: Loss 0.020697 Accuracy -0.3002\n",
      "Epoch 43 | train: Loss 0.021040 Accuracy 0.2284 | validation: Loss 0.021016 Accuracy -0.3203\n",
      "Epoch 44 | train: Loss 0.020816 Accuracy 0.2366 | validation: Loss 0.021458 Accuracy -0.3481\n",
      "Epoch 45 | train: Loss 0.020617 Accuracy 0.2439 | validation: Loss 0.021989 Accuracy -0.3814\n",
      "Epoch 46 | train: Loss 0.020439 Accuracy 0.2504 | validation: Loss 0.022577 Accuracy -0.4183\n",
      "Epoch 47 | train: Loss 0.020285 Accuracy 0.2561 | validation: Loss 0.023186 Accuracy -0.4566\n",
      "Epoch 48 | train: Loss 0.020150 Accuracy 0.2610 | validation: Loss 0.023779 Accuracy -0.4938\n",
      "Epoch 49 | train: Loss 0.020028 Accuracy 0.2655 | validation: Loss 0.024326 Accuracy -0.5282\n",
      "Epoch 50 | train: Loss 0.019920 Accuracy 0.2695 | validation: Loss 0.024802 Accuracy -0.5582\n",
      "Epoch 51 | train: Loss 0.019818 Accuracy 0.2732 | validation: Loss 0.025192 Accuracy -0.5826\n",
      "Epoch 52 | train: Loss 0.019722 Accuracy 0.2767 | validation: Loss 0.025482 Accuracy -0.6009\n",
      "Epoch 53 | train: Loss 0.019628 Accuracy 0.2802 | validation: Loss 0.025678 Accuracy -0.6132\n",
      "Epoch 54 | train: Loss 0.019538 Accuracy 0.2835 | validation: Loss 0.025789 Accuracy -0.6202\n",
      "Epoch 55 | train: Loss 0.019454 Accuracy 0.2866 | validation: Loss 0.025827 Accuracy -0.6225\n",
      "Epoch 56 | train: Loss 0.019374 Accuracy 0.2895 | validation: Loss 0.025807 Accuracy -0.6213\n",
      "Epoch 57 | train: Loss 0.019301 Accuracy 0.2922 | validation: Loss 0.025751 Accuracy -0.6178\n",
      "Epoch 58 | train: Loss 0.019234 Accuracy 0.2946 | validation: Loss 0.025677 Accuracy -0.6131\n",
      "Epoch 59 | train: Loss 0.019172 Accuracy 0.2969 | validation: Loss 0.025591 Accuracy -0.6077\n",
      "Epoch 60 | train: Loss 0.019118 Accuracy 0.2989 | validation: Loss 0.025511 Accuracy -0.6027\n",
      "Epoch 61 | train: Loss 0.019063 Accuracy 0.3009 | validation: Loss 0.025452 Accuracy -0.5989\n",
      "Epoch 62 | train: Loss 0.019012 Accuracy 0.3028 | validation: Loss 0.025380 Accuracy -0.5944\n",
      "Epoch 63 | train: Loss 0.018963 Accuracy 0.3046 | validation: Loss 0.025334 Accuracy -0.5916\n",
      "Epoch 64 | train: Loss 0.018914 Accuracy 0.3063 | validation: Loss 0.025328 Accuracy -0.5912\n",
      "Epoch 65 | train: Loss 0.018866 Accuracy 0.3081 | validation: Loss 0.025375 Accuracy -0.5942\n",
      "Epoch 66 | train: Loss 0.018814 Accuracy 0.3100 | validation: Loss 0.025474 Accuracy -0.6004\n",
      "Epoch 67 | train: Loss 0.018759 Accuracy 0.3120 | validation: Loss 0.025627 Accuracy -0.6099\n",
      "Epoch 68 | train: Loss 0.018699 Accuracy 0.3142 | validation: Loss 0.025830 Accuracy -0.6227\n",
      "Epoch 69 | train: Loss 0.018635 Accuracy 0.3166 | validation: Loss 0.026069 Accuracy -0.6378\n",
      "Epoch 70 | train: Loss 0.018569 Accuracy 0.3190 | validation: Loss 0.026333 Accuracy -0.6543\n",
      "Epoch 71 | train: Loss 0.018504 Accuracy 0.3214 | validation: Loss 0.026603 Accuracy -0.6713\n",
      "Epoch 72 | train: Loss 0.018442 Accuracy 0.3237 | validation: Loss 0.026861 Accuracy -0.6875\n",
      "Epoch 73 | train: Loss 0.018384 Accuracy 0.3258 | validation: Loss 0.027089 Accuracy -0.7018\n",
      "Epoch 74 | train: Loss 0.018332 Accuracy 0.3277 | validation: Loss 0.027270 Accuracy -0.7132\n",
      "Epoch 75 | train: Loss 0.018286 Accuracy 0.3294 | validation: Loss 0.027384 Accuracy -0.7203\n",
      "Epoch 76 | train: Loss 0.018245 Accuracy 0.3309 | validation: Loss 0.027425 Accuracy -0.7229\n",
      "Epoch 77 | train: Loss 0.018206 Accuracy 0.3323 | validation: Loss 0.027393 Accuracy -0.7209\n",
      "Epoch 78 | train: Loss 0.018169 Accuracy 0.3337 | validation: Loss 0.027295 Accuracy -0.7147\n",
      "Epoch 79 | train: Loss 0.018133 Accuracy 0.3350 | validation: Loss 0.027137 Accuracy -0.7048\n",
      "Epoch 80 | train: Loss 0.018099 Accuracy 0.3362 | validation: Loss 0.026926 Accuracy -0.6916\n",
      "Epoch 81 | train: Loss 0.018068 Accuracy 0.3374 | validation: Loss 0.026689 Accuracy -0.6767\n",
      "Epoch 82 | train: Loss 0.018039 Accuracy 0.3385 | validation: Loss 0.026448 Accuracy -0.6615\n",
      "Epoch 83 | train: Loss 0.018011 Accuracy 0.3395 | validation: Loss 0.026216 Accuracy -0.6470\n",
      "Epoch 84 | train: Loss 0.017986 Accuracy 0.3404 | validation: Loss 0.026007 Accuracy -0.6339\n",
      "Epoch 85 | train: Loss 0.017960 Accuracy 0.3413 | validation: Loss 0.025830 Accuracy -0.6227\n",
      "Epoch 86 | train: Loss 0.017934 Accuracy 0.3423 | validation: Loss 0.025686 Accuracy -0.6137\n",
      "Epoch 87 | train: Loss 0.017907 Accuracy 0.3433 | validation: Loss 0.025574 Accuracy -0.6067\n",
      "Epoch 88 | train: Loss 0.017877 Accuracy 0.3444 | validation: Loss 0.025489 Accuracy -0.6013\n",
      "Epoch 89 | train: Loss 0.017845 Accuracy 0.3456 | validation: Loss 0.025420 Accuracy -0.5970\n",
      "Epoch 90 | train: Loss 0.017812 Accuracy 0.3468 | validation: Loss 0.025365 Accuracy -0.5935\n",
      "Epoch 91 | train: Loss 0.017779 Accuracy 0.3480 | validation: Loss 0.025314 Accuracy -0.5903\n",
      "Epoch 92 | train: Loss 0.017746 Accuracy 0.3492 | validation: Loss 0.025261 Accuracy -0.5869\n",
      "Epoch 93 | train: Loss 0.017714 Accuracy 0.3504 | validation: Loss 0.025198 Accuracy -0.5830\n",
      "Epoch 94 | train: Loss 0.017684 Accuracy 0.3515 | validation: Loss 0.025127 Accuracy -0.5786\n",
      "Epoch 95 | train: Loss 0.017655 Accuracy 0.3525 | validation: Loss 0.025028 Accuracy -0.5723\n",
      "Epoch 96 | train: Loss 0.017629 Accuracy 0.3535 | validation: Loss 0.024904 Accuracy -0.5645\n",
      "Epoch 97 | train: Loss 0.017603 Accuracy 0.3544 | validation: Loss 0.024753 Accuracy -0.5550\n",
      "Epoch 98 | train: Loss 0.017579 Accuracy 0.3553 | validation: Loss 0.024582 Accuracy -0.5443\n",
      "Epoch 99 | train: Loss 0.017556 Accuracy 0.3562 | validation: Loss 0.024400 Accuracy -0.5329\n",
      "Epoch 100 | train: Loss 0.017535 Accuracy 0.3569 | validation: Loss 0.024222 Accuracy -0.5217\n",
      "Epoch 101 | train: Loss 0.017514 Accuracy 0.3577 | validation: Loss 0.024051 Accuracy -0.5109\n",
      "Epoch 102 | train: Loss 0.017494 Accuracy 0.3584 | validation: Loss 0.023894 Accuracy -0.5011\n",
      "Epoch 103 | train: Loss 0.017474 Accuracy 0.3592 | validation: Loss 0.023752 Accuracy -0.4921\n",
      "Epoch 104 | train: Loss 0.017453 Accuracy 0.3599 | validation: Loss 0.023626 Accuracy -0.4843\n",
      "Epoch 105 | train: Loss 0.017431 Accuracy 0.3607 | validation: Loss 0.023517 Accuracy -0.4774\n",
      "Epoch 106 | train: Loss 0.017409 Accuracy 0.3615 | validation: Loss 0.023419 Accuracy -0.4713\n",
      "Epoch 107 | train: Loss 0.017387 Accuracy 0.3624 | validation: Loss 0.023328 Accuracy -0.4655\n",
      "Epoch 108 | train: Loss 0.017364 Accuracy 0.3632 | validation: Loss 0.023240 Accuracy -0.4600\n",
      "Epoch 109 | train: Loss 0.017342 Accuracy 0.3640 | validation: Loss 0.023153 Accuracy -0.4545\n",
      "Epoch 110 | train: Loss 0.017321 Accuracy 0.3648 | validation: Loss 0.023066 Accuracy -0.4490\n",
      "Epoch 111 | train: Loss 0.017300 Accuracy 0.3655 | validation: Loss 0.022978 Accuracy -0.4435\n",
      "Epoch 112 | train: Loss 0.017281 Accuracy 0.3663 | validation: Loss 0.022886 Accuracy -0.4378\n",
      "Epoch 113 | train: Loss 0.017262 Accuracy 0.3669 | validation: Loss 0.022792 Accuracy -0.4319\n",
      "Epoch 114 | train: Loss 0.017244 Accuracy 0.3676 | validation: Loss 0.022697 Accuracy -0.4259\n",
      "Epoch 115 | train: Loss 0.017227 Accuracy 0.3682 | validation: Loss 0.022604 Accuracy -0.4200\n",
      "Epoch 116 | train: Loss 0.017210 Accuracy 0.3688 | validation: Loss 0.022514 Accuracy -0.4144\n",
      "Epoch 117 | train: Loss 0.017194 Accuracy 0.3694 | validation: Loss 0.022436 Accuracy -0.4095\n",
      "Epoch 118 | train: Loss 0.017178 Accuracy 0.3700 | validation: Loss 0.022364 Accuracy -0.4050\n",
      "Epoch 119 | train: Loss 0.017161 Accuracy 0.3707 | validation: Loss 0.022298 Accuracy -0.4009\n",
      "Epoch 120 | train: Loss 0.017143 Accuracy 0.3713 | validation: Loss 0.022238 Accuracy -0.3970\n",
      "Epoch 121 | train: Loss 0.017125 Accuracy 0.3720 | validation: Loss 0.022180 Accuracy -0.3934\n",
      "Epoch 122 | train: Loss 0.017108 Accuracy 0.3726 | validation: Loss 0.022130 Accuracy -0.3903\n",
      "Epoch 123 | train: Loss 0.017091 Accuracy 0.3732 | validation: Loss 0.022082 Accuracy -0.3872\n",
      "Epoch 124 | train: Loss 0.017074 Accuracy 0.3738 | validation: Loss 0.022036 Accuracy -0.3844\n",
      "Epoch 125 | train: Loss 0.017058 Accuracy 0.3744 | validation: Loss 0.021992 Accuracy -0.3816\n",
      "Epoch 126 | train: Loss 0.017043 Accuracy 0.3750 | validation: Loss 0.021952 Accuracy -0.3791\n",
      "Epoch 127 | train: Loss 0.017027 Accuracy 0.3756 | validation: Loss 0.021917 Accuracy -0.3769\n",
      "Epoch 128 | train: Loss 0.017012 Accuracy 0.3761 | validation: Loss 0.021884 Accuracy -0.3748\n",
      "Epoch 129 | train: Loss 0.016997 Accuracy 0.3767 | validation: Loss 0.021861 Accuracy -0.3733\n",
      "Epoch 130 | train: Loss 0.016983 Accuracy 0.3772 | validation: Loss 0.021845 Accuracy -0.3724\n",
      "Epoch 131 | train: Loss 0.016966 Accuracy 0.3778 | validation: Loss 0.021836 Accuracy -0.3718\n",
      "Epoch 132 | train: Loss 0.016952 Accuracy 0.3783 | validation: Loss 0.021831 Accuracy -0.3715\n",
      "Epoch 133 | train: Loss 0.016938 Accuracy 0.3788 | validation: Loss 0.021826 Accuracy -0.3712\n",
      "Epoch 134 | train: Loss 0.016924 Accuracy 0.3793 | validation: Loss 0.021823 Accuracy -0.3710\n",
      "Epoch 135 | train: Loss 0.016910 Accuracy 0.3799 | validation: Loss 0.021822 Accuracy -0.3709\n",
      "Epoch 136 | train: Loss 0.016896 Accuracy 0.3804 | validation: Loss 0.021825 Accuracy -0.3711\n",
      "Epoch 137 | train: Loss 0.016882 Accuracy 0.3809 | validation: Loss 0.021833 Accuracy -0.3716\n",
      "Epoch 138 | train: Loss 0.016869 Accuracy 0.3814 | validation: Loss 0.021845 Accuracy -0.3724\n",
      "Epoch 139 | train: Loss 0.016856 Accuracy 0.3818 | validation: Loss 0.021863 Accuracy -0.3735\n",
      "Epoch 140 | train: Loss 0.016844 Accuracy 0.3823 | validation: Loss 0.021885 Accuracy -0.3749\n",
      "Epoch 141 | train: Loss 0.016832 Accuracy 0.3827 | validation: Loss 0.021908 Accuracy -0.3763\n",
      "Epoch 142 | train: Loss 0.016820 Accuracy 0.3831 | validation: Loss 0.021933 Accuracy -0.3779\n",
      "Epoch 143 | train: Loss 0.016809 Accuracy 0.3836 | validation: Loss 0.021958 Accuracy -0.3794\n",
      "Epoch 144 | train: Loss 0.016797 Accuracy 0.3840 | validation: Loss 0.021983 Accuracy -0.3810\n",
      "Epoch 145 | train: Loss 0.016785 Accuracy 0.3844 | validation: Loss 0.022008 Accuracy -0.3826\n",
      "Epoch 146 | train: Loss 0.016772 Accuracy 0.3849 | validation: Loss 0.022032 Accuracy -0.3841\n",
      "Epoch 147 | train: Loss 0.016759 Accuracy 0.3854 | validation: Loss 0.022059 Accuracy -0.3858\n",
      "Epoch 148 | train: Loss 0.016747 Accuracy 0.3858 | validation: Loss 0.022089 Accuracy -0.3877\n",
      "Epoch 149 | train: Loss 0.016735 Accuracy 0.3863 | validation: Loss 0.022128 Accuracy -0.3901\n",
      "Epoch 150 | train: Loss 0.016724 Accuracy 0.3867 | validation: Loss 0.022171 Accuracy -0.3928\n",
      "Epoch 151 | train: Loss 0.016714 Accuracy 0.3871 | validation: Loss 0.022217 Accuracy -0.3957\n",
      "Epoch 152 | train: Loss 0.016703 Accuracy 0.3875 | validation: Loss 0.022268 Accuracy -0.3990\n",
      "Epoch 153 | train: Loss 0.016692 Accuracy 0.3878 | validation: Loss 0.022323 Accuracy -0.4024\n",
      "Epoch 154 | train: Loss 0.016682 Accuracy 0.3882 | validation: Loss 0.022378 Accuracy -0.4058\n",
      "Epoch 155 | train: Loss 0.016672 Accuracy 0.3886 | validation: Loss 0.022431 Accuracy -0.4092\n",
      "Epoch 156 | train: Loss 0.016662 Accuracy 0.3890 | validation: Loss 0.022483 Accuracy -0.4125\n",
      "Epoch 157 | train: Loss 0.016652 Accuracy 0.3893 | validation: Loss 0.022533 Accuracy -0.4156\n",
      "Epoch 158 | train: Loss 0.016641 Accuracy 0.3897 | validation: Loss 0.022580 Accuracy -0.4186\n",
      "Epoch 159 | train: Loss 0.016631 Accuracy 0.3901 | validation: Loss 0.022627 Accuracy -0.4215\n",
      "Epoch 160 | train: Loss 0.016620 Accuracy 0.3905 | validation: Loss 0.022681 Accuracy -0.4249\n",
      "Epoch 161 | train: Loss 0.016610 Accuracy 0.3908 | validation: Loss 0.022744 Accuracy -0.4288\n",
      "Epoch 162 | train: Loss 0.016601 Accuracy 0.3912 | validation: Loss 0.022814 Accuracy -0.4332\n",
      "Epoch 163 | train: Loss 0.016592 Accuracy 0.3915 | validation: Loss 0.022884 Accuracy -0.4377\n",
      "Epoch 164 | train: Loss 0.016582 Accuracy 0.3919 | validation: Loss 0.022953 Accuracy -0.4420\n",
      "Epoch 165 | train: Loss 0.016573 Accuracy 0.3922 | validation: Loss 0.023013 Accuracy -0.4457\n",
      "Epoch 166 | train: Loss 0.016564 Accuracy 0.3926 | validation: Loss 0.023073 Accuracy -0.4495\n",
      "Epoch 167 | train: Loss 0.016554 Accuracy 0.3929 | validation: Loss 0.023131 Accuracy -0.4532\n",
      "Epoch 168 | train: Loss 0.016545 Accuracy 0.3933 | validation: Loss 0.023194 Accuracy -0.4571\n",
      "Epoch 169 | train: Loss 0.016535 Accuracy 0.3936 | validation: Loss 0.023266 Accuracy -0.4617\n",
      "Epoch 170 | train: Loss 0.016526 Accuracy 0.3939 | validation: Loss 0.023346 Accuracy -0.4667\n",
      "Epoch 171 | train: Loss 0.016517 Accuracy 0.3942 | validation: Loss 0.023426 Accuracy -0.4717\n",
      "Epoch 172 | train: Loss 0.016509 Accuracy 0.3946 | validation: Loss 0.023502 Accuracy -0.4765\n",
      "Epoch 173 | train: Loss 0.016500 Accuracy 0.3949 | validation: Loss 0.023570 Accuracy -0.4807\n",
      "Epoch 174 | train: Loss 0.016490 Accuracy 0.3952 | validation: Loss 0.023630 Accuracy -0.4845\n",
      "Epoch 175 | train: Loss 0.016481 Accuracy 0.3956 | validation: Loss 0.023690 Accuracy -0.4883\n",
      "Epoch 176 | train: Loss 0.016471 Accuracy 0.3959 | validation: Loss 0.023751 Accuracy -0.4921\n",
      "Epoch 177 | train: Loss 0.016462 Accuracy 0.3963 | validation: Loss 0.023816 Accuracy -0.4962\n",
      "Epoch 178 | train: Loss 0.016452 Accuracy 0.3966 | validation: Loss 0.023891 Accuracy -0.5009\n",
      "Epoch 179 | train: Loss 0.016443 Accuracy 0.3970 | validation: Loss 0.023976 Accuracy -0.5062\n",
      "Epoch 180 | train: Loss 0.016434 Accuracy 0.3973 | validation: Loss 0.024058 Accuracy -0.5114\n",
      "Epoch 181 | train: Loss 0.016425 Accuracy 0.3976 | validation: Loss 0.024134 Accuracy -0.5162\n",
      "Epoch 182 | train: Loss 0.016416 Accuracy 0.3980 | validation: Loss 0.024204 Accuracy -0.5206\n",
      "Epoch 183 | train: Loss 0.016406 Accuracy 0.3983 | validation: Loss 0.024274 Accuracy -0.5250\n",
      "Epoch 184 | train: Loss 0.016396 Accuracy 0.3987 | validation: Loss 0.024333 Accuracy -0.5287\n",
      "Epoch 185 | train: Loss 0.016386 Accuracy 0.3991 | validation: Loss 0.024401 Accuracy -0.5329\n",
      "Epoch 186 | train: Loss 0.016375 Accuracy 0.3995 | validation: Loss 0.024468 Accuracy -0.5371\n",
      "Epoch 187 | train: Loss 0.016364 Accuracy 0.3999 | validation: Loss 0.024534 Accuracy -0.5413\n",
      "Epoch 188 | train: Loss 0.016353 Accuracy 0.4003 | validation: Loss 0.024603 Accuracy -0.5456\n",
      "Epoch 189 | train: Loss 0.016342 Accuracy 0.4007 | validation: Loss 0.024674 Accuracy -0.5501\n",
      "Epoch 190 | train: Loss 0.016330 Accuracy 0.4011 | validation: Loss 0.024740 Accuracy -0.5543\n",
      "Epoch 191 | train: Loss 0.016317 Accuracy 0.4016 | validation: Loss 0.024813 Accuracy -0.5588\n",
      "Epoch 192 | train: Loss 0.016304 Accuracy 0.4021 | validation: Loss 0.024884 Accuracy -0.5633\n",
      "Epoch 193 | train: Loss 0.016289 Accuracy 0.4026 | validation: Loss 0.024932 Accuracy -0.5663\n",
      "Epoch 194 | train: Loss 0.016274 Accuracy 0.4032 | validation: Loss 0.024970 Accuracy -0.5687\n",
      "Epoch 195 | train: Loss 0.016257 Accuracy 0.4038 | validation: Loss 0.025005 Accuracy -0.5709\n",
      "Epoch 196 | train: Loss 0.016240 Accuracy 0.4044 | validation: Loss 0.025044 Accuracy -0.5733\n",
      "Epoch 197 | train: Loss 0.016222 Accuracy 0.4051 | validation: Loss 0.025090 Accuracy -0.5762\n",
      "Epoch 198 | train: Loss 0.016203 Accuracy 0.4058 | validation: Loss 0.025140 Accuracy -0.5794\n",
      "Epoch 199 | train: Loss 0.016184 Accuracy 0.4065 | validation: Loss 0.025196 Accuracy -0.5829\n",
      "Epoch 200 | train: Loss 0.016163 Accuracy 0.4072 | validation: Loss 0.025231 Accuracy -0.5851\n",
      "Epoch 201 | train: Loss 0.016142 Accuracy 0.4080 | validation: Loss 0.025259 Accuracy -0.5868\n",
      "Epoch 202 | train: Loss 0.016119 Accuracy 0.4089 | validation: Loss 0.025281 Accuracy -0.5882\n",
      "Epoch 203 | train: Loss 0.016094 Accuracy 0.4098 | validation: Loss 0.025297 Accuracy -0.5892\n",
      "Epoch 204 | train: Loss 0.016069 Accuracy 0.4107 | validation: Loss 0.025297 Accuracy -0.5892\n",
      "Epoch 205 | train: Loss 0.016043 Accuracy 0.4116 | validation: Loss 0.025286 Accuracy -0.5885\n",
      "Epoch 206 | train: Loss 0.016017 Accuracy 0.4126 | validation: Loss 0.025270 Accuracy -0.5875\n",
      "Epoch 207 | train: Loss 0.015990 Accuracy 0.4136 | validation: Loss 0.025259 Accuracy -0.5868\n",
      "Epoch 208 | train: Loss 0.015963 Accuracy 0.4146 | validation: Loss 0.025253 Accuracy -0.5865\n",
      "Epoch 209 | train: Loss 0.015937 Accuracy 0.4155 | validation: Loss 0.025255 Accuracy -0.5866\n",
      "Epoch 210 | train: Loss 0.015912 Accuracy 0.4164 | validation: Loss 0.025255 Accuracy -0.5866\n",
      "Epoch 211 | train: Loss 0.015887 Accuracy 0.4174 | validation: Loss 0.025238 Accuracy -0.5855\n",
      "Epoch 212 | train: Loss 0.015862 Accuracy 0.4183 | validation: Loss 0.025186 Accuracy -0.5823\n",
      "Epoch 213 | train: Loss 0.015837 Accuracy 0.4192 | validation: Loss 0.025121 Accuracy -0.5782\n",
      "Epoch 214 | train: Loss 0.015812 Accuracy 0.4201 | validation: Loss 0.025068 Accuracy -0.5749\n",
      "Epoch 215 | train: Loss 0.015789 Accuracy 0.4210 | validation: Loss 0.025044 Accuracy -0.5733\n",
      "Epoch 216 | train: Loss 0.015767 Accuracy 0.4218 | validation: Loss 0.025046 Accuracy -0.5735\n",
      "Epoch 217 | train: Loss 0.015746 Accuracy 0.4225 | validation: Loss 0.025069 Accuracy -0.5749\n",
      "Epoch 218 | train: Loss 0.015727 Accuracy 0.4233 | validation: Loss 0.025068 Accuracy -0.5749\n",
      "Epoch 219 | train: Loss 0.015706 Accuracy 0.4240 | validation: Loss 0.025024 Accuracy -0.5721\n",
      "Epoch 220 | train: Loss 0.015686 Accuracy 0.4247 | validation: Loss 0.024978 Accuracy -0.5692\n",
      "Epoch 221 | train: Loss 0.015667 Accuracy 0.4254 | validation: Loss 0.024967 Accuracy -0.5685\n",
      "Epoch 222 | train: Loss 0.015648 Accuracy 0.4261 | validation: Loss 0.025015 Accuracy -0.5715\n",
      "Epoch 223 | train: Loss 0.015631 Accuracy 0.4268 | validation: Loss 0.025086 Accuracy -0.5760\n",
      "Epoch 224 | train: Loss 0.015614 Accuracy 0.4274 | validation: Loss 0.025117 Accuracy -0.5779\n",
      "Epoch 225 | train: Loss 0.015596 Accuracy 0.4280 | validation: Loss 0.025096 Accuracy -0.5766\n",
      "Epoch 226 | train: Loss 0.015578 Accuracy 0.4287 | validation: Loss 0.025051 Accuracy -0.5738\n",
      "Epoch 227 | train: Loss 0.015558 Accuracy 0.4294 | validation: Loss 0.025037 Accuracy -0.5729\n",
      "Epoch 228 | train: Loss 0.015538 Accuracy 0.4302 | validation: Loss 0.025086 Accuracy -0.5760\n",
      "Epoch 229 | train: Loss 0.015518 Accuracy 0.4309 | validation: Loss 0.025139 Accuracy -0.5793\n",
      "Epoch 230 | train: Loss 0.015497 Accuracy 0.4317 | validation: Loss 0.025141 Accuracy -0.5794\n",
      "Epoch 231 | train: Loss 0.015474 Accuracy 0.4325 | validation: Loss 0.025085 Accuracy -0.5759\n",
      "Epoch 232 | train: Loss 0.015448 Accuracy 0.4335 | validation: Loss 0.025012 Accuracy -0.5713\n",
      "Epoch 233 | train: Loss 0.015420 Accuracy 0.4345 | validation: Loss 0.024973 Accuracy -0.5689\n",
      "Epoch 234 | train: Loss 0.015392 Accuracy 0.4355 | validation: Loss 0.024976 Accuracy -0.5691\n",
      "Epoch 235 | train: Loss 0.015363 Accuracy 0.4366 | validation: Loss 0.024982 Accuracy -0.5694\n",
      "Epoch 236 | train: Loss 0.015334 Accuracy 0.4376 | validation: Loss 0.024924 Accuracy -0.5658\n",
      "Epoch 237 | train: Loss 0.015304 Accuracy 0.4388 | validation: Loss 0.024828 Accuracy -0.5598\n",
      "Epoch 238 | train: Loss 0.015273 Accuracy 0.4399 | validation: Loss 0.024780 Accuracy -0.5567\n",
      "Epoch 239 | train: Loss 0.015243 Accuracy 0.4410 | validation: Loss 0.024795 Accuracy -0.5577\n",
      "Epoch 240 | train: Loss 0.015213 Accuracy 0.4421 | validation: Loss 0.024811 Accuracy -0.5587\n",
      "Epoch 241 | train: Loss 0.015182 Accuracy 0.4432 | validation: Loss 0.024769 Accuracy -0.5561\n",
      "Epoch 242 | train: Loss 0.015150 Accuracy 0.4444 | validation: Loss 0.024684 Accuracy -0.5507\n",
      "Epoch 243 | train: Loss 0.015118 Accuracy 0.4456 | validation: Loss 0.024633 Accuracy -0.5475\n",
      "Epoch 244 | train: Loss 0.015085 Accuracy 0.4468 | validation: Loss 0.024640 Accuracy -0.5480\n",
      "Epoch 245 | train: Loss 0.015053 Accuracy 0.4480 | validation: Loss 0.024658 Accuracy -0.5491\n",
      "Epoch 246 | train: Loss 0.015020 Accuracy 0.4492 | validation: Loss 0.024631 Accuracy -0.5474\n",
      "Epoch 247 | train: Loss 0.014987 Accuracy 0.4504 | validation: Loss 0.024558 Accuracy -0.5428\n",
      "Epoch 248 | train: Loss 0.014951 Accuracy 0.4517 | validation: Loss 0.024500 Accuracy -0.5392\n",
      "Epoch 249 | train: Loss 0.014915 Accuracy 0.4530 | validation: Loss 0.024497 Accuracy -0.5390\n",
      "Epoch 250 | train: Loss 0.014879 Accuracy 0.4543 | validation: Loss 0.024489 Accuracy -0.5385\n",
      "Epoch 251 | train: Loss 0.014841 Accuracy 0.4557 | validation: Loss 0.024404 Accuracy -0.5331\n",
      "Epoch 252 | train: Loss 0.014802 Accuracy 0.4572 | validation: Loss 0.024312 Accuracy -0.5274\n",
      "Epoch 253 | train: Loss 0.014761 Accuracy 0.4587 | validation: Loss 0.024276 Accuracy -0.5251\n",
      "Epoch 254 | train: Loss 0.014719 Accuracy 0.4602 | validation: Loss 0.024231 Accuracy -0.5223\n",
      "Epoch 255 | train: Loss 0.014676 Accuracy 0.4618 | validation: Loss 0.024150 Accuracy -0.5171\n",
      "Epoch 256 | train: Loss 0.014631 Accuracy 0.4634 | validation: Loss 0.024053 Accuracy -0.5111\n",
      "Epoch 257 | train: Loss 0.014584 Accuracy 0.4652 | validation: Loss 0.023978 Accuracy -0.5063\n",
      "Epoch 258 | train: Loss 0.014536 Accuracy 0.4669 | validation: Loss 0.023912 Accuracy -0.5022\n",
      "Epoch 259 | train: Loss 0.014486 Accuracy 0.4687 | validation: Loss 0.023825 Accuracy -0.4968\n",
      "Epoch 260 | train: Loss 0.014435 Accuracy 0.4706 | validation: Loss 0.023705 Accuracy -0.4892\n",
      "Epoch 261 | train: Loss 0.014383 Accuracy 0.4725 | validation: Loss 0.023632 Accuracy -0.4847\n",
      "Epoch 262 | train: Loss 0.014329 Accuracy 0.4745 | validation: Loss 0.023582 Accuracy -0.4815\n",
      "Epoch 263 | train: Loss 0.014274 Accuracy 0.4765 | validation: Loss 0.023456 Accuracy -0.4736\n",
      "Epoch 264 | train: Loss 0.014217 Accuracy 0.4786 | validation: Loss 0.023345 Accuracy -0.4666\n",
      "Epoch 265 | train: Loss 0.014159 Accuracy 0.4807 | validation: Loss 0.023309 Accuracy -0.4644\n",
      "Epoch 266 | train: Loss 0.014100 Accuracy 0.4829 | validation: Loss 0.023204 Accuracy -0.4578\n",
      "Epoch 267 | train: Loss 0.014039 Accuracy 0.4851 | validation: Loss 0.023084 Accuracy -0.4502\n",
      "Epoch 268 | train: Loss 0.013977 Accuracy 0.4874 | validation: Loss 0.023040 Accuracy -0.4475\n",
      "Epoch 269 | train: Loss 0.013914 Accuracy 0.4897 | validation: Loss 0.022951 Accuracy -0.4418\n",
      "Epoch 270 | train: Loss 0.013850 Accuracy 0.4921 | validation: Loss 0.022814 Accuracy -0.4332\n",
      "Epoch 271 | train: Loss 0.013785 Accuracy 0.4945 | validation: Loss 0.022791 Accuracy -0.4318\n",
      "Epoch 272 | train: Loss 0.013719 Accuracy 0.4969 | validation: Loss 0.022673 Accuracy -0.4244\n",
      "Epoch 273 | train: Loss 0.013651 Accuracy 0.4994 | validation: Loss 0.022586 Accuracy -0.4189\n",
      "Epoch 274 | train: Loss 0.013583 Accuracy 0.5018 | validation: Loss 0.022543 Accuracy -0.4162\n",
      "Epoch 275 | train: Loss 0.013515 Accuracy 0.5043 | validation: Loss 0.022440 Accuracy -0.4098\n",
      "Epoch 276 | train: Loss 0.013446 Accuracy 0.5069 | validation: Loss 0.022393 Accuracy -0.4068\n",
      "Epoch 277 | train: Loss 0.013375 Accuracy 0.5095 | validation: Loss 0.022322 Accuracy -0.4023\n",
      "Epoch 278 | train: Loss 0.013306 Accuracy 0.5120 | validation: Loss 0.022251 Accuracy -0.3978\n",
      "Epoch 279 | train: Loss 0.013236 Accuracy 0.5146 | validation: Loss 0.022232 Accuracy -0.3967\n",
      "Epoch 280 | train: Loss 0.013166 Accuracy 0.5172 | validation: Loss 0.022100 Accuracy -0.3884\n",
      "Epoch 281 | train: Loss 0.013096 Accuracy 0.5197 | validation: Loss 0.022217 Accuracy -0.3957\n",
      "Epoch 282 | train: Loss 0.013027 Accuracy 0.5223 | validation: Loss 0.021845 Accuracy -0.3724\n",
      "Epoch 283 | train: Loss 0.012960 Accuracy 0.5247 | validation: Loss 0.022396 Accuracy -0.4070\n",
      "Epoch 284 | train: Loss 0.012900 Accuracy 0.5269 | validation: Loss 0.021673 Accuracy -0.3615\n",
      "Epoch 285 | train: Loss 0.012828 Accuracy 0.5296 | validation: Loss 0.022014 Accuracy -0.3830\n",
      "Epoch 286 | train: Loss 0.012763 Accuracy 0.5319 | validation: Loss 0.022177 Accuracy -0.3932\n",
      "Epoch 287 | train: Loss 0.012705 Accuracy 0.5341 | validation: Loss 0.021623 Accuracy -0.3584\n",
      "Epoch 288 | train: Loss 0.012644 Accuracy 0.5363 | validation: Loss 0.022051 Accuracy -0.3853\n",
      "Epoch 289 | train: Loss 0.012587 Accuracy 0.5384 | validation: Loss 0.022048 Accuracy -0.3851\n",
      "Epoch 290 | train: Loss 0.012532 Accuracy 0.5404 | validation: Loss 0.021621 Accuracy -0.3583\n",
      "Epoch 291 | train: Loss 0.012480 Accuracy 0.5423 | validation: Loss 0.022124 Accuracy -0.3899\n",
      "Epoch 292 | train: Loss 0.012432 Accuracy 0.5441 | validation: Loss 0.021920 Accuracy -0.3771\n",
      "Epoch 293 | train: Loss 0.012381 Accuracy 0.5459 | validation: Loss 0.021651 Accuracy -0.3602\n",
      "Epoch 294 | train: Loss 0.012336 Accuracy 0.5476 | validation: Loss 0.022172 Accuracy -0.3929\n",
      "Epoch 295 | train: Loss 0.012298 Accuracy 0.5490 | validation: Loss 0.021796 Accuracy -0.3693\n",
      "Epoch 296 | train: Loss 0.012252 Accuracy 0.5507 | validation: Loss 0.021773 Accuracy -0.3678\n",
      "Epoch 297 | train: Loss 0.012213 Accuracy 0.5521 | validation: Loss 0.022166 Accuracy -0.3925\n",
      "Epoch 298 | train: Loss 0.012180 Accuracy 0.5533 | validation: Loss 0.021717 Accuracy -0.3643\n",
      "Epoch 299 | train: Loss 0.012140 Accuracy 0.5548 | validation: Loss 0.021960 Accuracy -0.3796\n",
      "Epoch 300 | train: Loss 0.012105 Accuracy 0.5561 | validation: Loss 0.022022 Accuracy -0.3835\n",
      "Epoch 301 | train: Loss 0.012073 Accuracy 0.5573 | validation: Loss 0.021696 Accuracy -0.3630\n",
      "Epoch 302 | train: Loss 0.012039 Accuracy 0.5585 | validation: Loss 0.022048 Accuracy -0.3851\n",
      "Epoch 303 | train: Loss 0.012009 Accuracy 0.5596 | validation: Loss 0.021821 Accuracy -0.3709\n",
      "Epoch 304 | train: Loss 0.011975 Accuracy 0.5608 | validation: Loss 0.021743 Accuracy -0.3659\n",
      "Epoch 305 | train: Loss 0.011943 Accuracy 0.5620 | validation: Loss 0.021982 Accuracy -0.3809\n",
      "Epoch 306 | train: Loss 0.011915 Accuracy 0.5630 | validation: Loss 0.021627 Accuracy -0.3587\n",
      "Epoch 307 | train: Loss 0.011881 Accuracy 0.5643 | validation: Loss 0.021798 Accuracy -0.3694\n",
      "Epoch 308 | train: Loss 0.011851 Accuracy 0.5654 | validation: Loss 0.021714 Accuracy -0.3641\n",
      "Epoch 309 | train: Loss 0.011819 Accuracy 0.5665 | validation: Loss 0.021507 Accuracy -0.3512\n",
      "Epoch 310 | train: Loss 0.011787 Accuracy 0.5677 | validation: Loss 0.021704 Accuracy -0.3635\n",
      "Epoch 311 | train: Loss 0.011757 Accuracy 0.5688 | validation: Loss 0.021361 Accuracy -0.3420\n",
      "Epoch 312 | train: Loss 0.011723 Accuracy 0.5701 | validation: Loss 0.021471 Accuracy -0.3489\n",
      "Epoch 313 | train: Loss 0.011692 Accuracy 0.5712 | validation: Loss 0.021348 Accuracy -0.3411\n",
      "Epoch 314 | train: Loss 0.011658 Accuracy 0.5725 | validation: Loss 0.021153 Accuracy -0.3289\n",
      "Epoch 315 | train: Loss 0.011625 Accuracy 0.5737 | validation: Loss 0.021270 Accuracy -0.3362\n",
      "Epoch 316 | train: Loss 0.011593 Accuracy 0.5749 | validation: Loss 0.020928 Accuracy -0.3147\n",
      "Epoch 317 | train: Loss 0.011558 Accuracy 0.5761 | validation: Loss 0.021078 Accuracy -0.3242\n",
      "Epoch 318 | train: Loss 0.011526 Accuracy 0.5773 | validation: Loss 0.020784 Accuracy -0.3057\n",
      "Epoch 319 | train: Loss 0.011491 Accuracy 0.5786 | validation: Loss 0.020837 Accuracy -0.3090\n",
      "Epoch 320 | train: Loss 0.011458 Accuracy 0.5798 | validation: Loss 0.020635 Accuracy -0.2964\n",
      "Epoch 321 | train: Loss 0.011424 Accuracy 0.5810 | validation: Loss 0.020604 Accuracy -0.2944\n",
      "Epoch 322 | train: Loss 0.011392 Accuracy 0.5822 | validation: Loss 0.020457 Accuracy -0.2852\n",
      "Epoch 323 | train: Loss 0.011359 Accuracy 0.5834 | validation: Loss 0.020413 Accuracy -0.2824\n",
      "Epoch 324 | train: Loss 0.011327 Accuracy 0.5846 | validation: Loss 0.020274 Accuracy -0.2736\n",
      "Epoch 325 | train: Loss 0.011295 Accuracy 0.5858 | validation: Loss 0.020245 Accuracy -0.2719\n",
      "Epoch 326 | train: Loss 0.011264 Accuracy 0.5869 | validation: Loss 0.020093 Accuracy -0.2623\n",
      "Epoch 327 | train: Loss 0.011232 Accuracy 0.5881 | validation: Loss 0.020094 Accuracy -0.2624\n",
      "Epoch 328 | train: Loss 0.011202 Accuracy 0.5892 | validation: Loss 0.019898 Accuracy -0.2500\n",
      "Epoch 329 | train: Loss 0.011170 Accuracy 0.5904 | validation: Loss 0.020011 Accuracy -0.2571\n",
      "Epoch 330 | train: Loss 0.011141 Accuracy 0.5914 | validation: Loss 0.019619 Accuracy -0.2325\n",
      "Epoch 331 | train: Loss 0.011111 Accuracy 0.5925 | validation: Loss 0.020158 Accuracy -0.2664\n",
      "Epoch 332 | train: Loss 0.011089 Accuracy 0.5933 | validation: Loss 0.019089 Accuracy -0.1992\n",
      "Epoch 333 | train: Loss 0.011065 Accuracy 0.5942 | validation: Loss 0.020546 Accuracy -0.2907\n",
      "Epoch 334 | train: Loss 0.011060 Accuracy 0.5944 | validation: Loss 0.019020 Accuracy -0.1949\n",
      "Epoch 335 | train: Loss 0.011010 Accuracy 0.5962 | validation: Loss 0.019639 Accuracy -0.2338\n",
      "Epoch 336 | train: Loss 0.010973 Accuracy 0.5976 | validation: Loss 0.020061 Accuracy -0.2603\n",
      "Epoch 337 | train: Loss 0.010963 Accuracy 0.5980 | validation: Loss 0.018920 Accuracy -0.1886\n",
      "Epoch 338 | train: Loss 0.010934 Accuracy 0.5990 | validation: Loss 0.019660 Accuracy -0.2351\n",
      "Epoch 339 | train: Loss 0.010904 Accuracy 0.6001 | validation: Loss 0.019741 Accuracy -0.2402\n",
      "Epoch 340 | train: Loss 0.010885 Accuracy 0.6008 | validation: Loss 0.018871 Accuracy -0.1855\n",
      "Epoch 341 | train: Loss 0.010863 Accuracy 0.6016 | validation: Loss 0.019600 Accuracy -0.2313\n",
      "Epoch 342 | train: Loss 0.010839 Accuracy 0.6025 | validation: Loss 0.019493 Accuracy -0.2246\n",
      "Epoch 343 | train: Loss 0.010816 Accuracy 0.6033 | validation: Loss 0.018822 Accuracy -0.1825\n",
      "Epoch 344 | train: Loss 0.010799 Accuracy 0.6040 | validation: Loss 0.019526 Accuracy -0.2267\n",
      "Epoch 345 | train: Loss 0.010780 Accuracy 0.6047 | validation: Loss 0.019257 Accuracy -0.2098\n",
      "Epoch 346 | train: Loss 0.010755 Accuracy 0.6056 | validation: Loss 0.018774 Accuracy -0.1794\n",
      "Epoch 347 | train: Loss 0.010738 Accuracy 0.6062 | validation: Loss 0.019419 Accuracy -0.2200\n",
      "Epoch 348 | train: Loss 0.010724 Accuracy 0.6067 | validation: Loss 0.019016 Accuracy -0.1946\n",
      "Epoch 349 | train: Loss 0.010697 Accuracy 0.6077 | validation: Loss 0.018736 Accuracy -0.1771\n",
      "Epoch 350 | train: Loss 0.010680 Accuracy 0.6083 | validation: Loss 0.019303 Accuracy -0.2126\n",
      "Epoch 351 | train: Loss 0.010669 Accuracy 0.6087 | validation: Loss 0.018753 Accuracy -0.1781\n",
      "Epoch 352 | train: Loss 0.010641 Accuracy 0.6098 | validation: Loss 0.018781 Accuracy -0.1799\n",
      "Epoch 353 | train: Loss 0.010621 Accuracy 0.6105 | validation: Loss 0.019134 Accuracy -0.2021\n",
      "Epoch 354 | train: Loss 0.010609 Accuracy 0.6109 | validation: Loss 0.018528 Accuracy -0.1640\n",
      "Epoch 355 | train: Loss 0.010584 Accuracy 0.6119 | validation: Loss 0.018920 Accuracy -0.1886\n",
      "Epoch 356 | train: Loss 0.010564 Accuracy 0.6126 | validation: Loss 0.018765 Accuracy -0.1789\n",
      "Epoch 357 | train: Loss 0.010540 Accuracy 0.6135 | validation: Loss 0.018523 Accuracy -0.1637\n",
      "Epoch 358 | train: Loss 0.010518 Accuracy 0.6143 | validation: Loss 0.018955 Accuracy -0.1908\n",
      "Epoch 359 | train: Loss 0.010500 Accuracy 0.6149 | validation: Loss 0.018417 Accuracy -0.1570\n",
      "Epoch 360 | train: Loss 0.010473 Accuracy 0.6159 | validation: Loss 0.018856 Accuracy -0.1846\n",
      "Epoch 361 | train: Loss 0.010450 Accuracy 0.6168 | validation: Loss 0.018521 Accuracy -0.1636\n",
      "Epoch 362 | train: Loss 0.010421 Accuracy 0.6178 | validation: Loss 0.018642 Accuracy -0.1712\n",
      "Epoch 363 | train: Loss 0.010395 Accuracy 0.6188 | validation: Loss 0.018700 Accuracy -0.1748\n",
      "Epoch 364 | train: Loss 0.010370 Accuracy 0.6197 | validation: Loss 0.018480 Accuracy -0.1610\n",
      "Epoch 365 | train: Loss 0.010342 Accuracy 0.6207 | validation: Loss 0.018877 Accuracy -0.1859\n",
      "Epoch 366 | train: Loss 0.010319 Accuracy 0.6216 | validation: Loss 0.018317 Accuracy -0.1508\n",
      "Epoch 367 | train: Loss 0.010291 Accuracy 0.6226 | validation: Loss 0.019208 Accuracy -0.2067\n",
      "Epoch 368 | train: Loss 0.010279 Accuracy 0.6230 | validation: Loss 0.018034 Accuracy -0.1329\n",
      "Epoch 369 | train: Loss 0.010257 Accuracy 0.6239 | validation: Loss 0.019584 Accuracy -0.2304\n",
      "Epoch 370 | train: Loss 0.010256 Accuracy 0.6239 | validation: Loss 0.018171 Accuracy -0.1416\n",
      "Epoch 371 | train: Loss 0.010193 Accuracy 0.6262 | validation: Loss 0.018910 Accuracy -0.1880\n",
      "Epoch 372 | train: Loss 0.010147 Accuracy 0.6279 | validation: Loss 0.019106 Accuracy -0.2003\n",
      "Epoch 373 | train: Loss 0.010126 Accuracy 0.6287 | validation: Loss 0.018259 Accuracy -0.1471\n",
      "Epoch 374 | train: Loss 0.010109 Accuracy 0.6293 | validation: Loss 0.019490 Accuracy -0.2244\n",
      "Epoch 375 | train: Loss 0.010094 Accuracy 0.6298 | validation: Loss 0.018623 Accuracy -0.1700\n",
      "Epoch 376 | train: Loss 0.010031 Accuracy 0.6321 | validation: Loss 0.018786 Accuracy -0.1802\n",
      "Epoch 377 | train: Loss 0.009998 Accuracy 0.6333 | validation: Loss 0.019487 Accuracy -0.2242\n",
      "Epoch 378 | train: Loss 0.009991 Accuracy 0.6336 | validation: Loss 0.018564 Accuracy -0.1663\n",
      "Epoch 379 | train: Loss 0.009954 Accuracy 0.6349 | validation: Loss 0.019491 Accuracy -0.2245\n",
      "Epoch 380 | train: Loss 0.009918 Accuracy 0.6363 | validation: Loss 0.019143 Accuracy -0.2026\n",
      "Epoch 381 | train: Loss 0.009871 Accuracy 0.6380 | validation: Loss 0.019045 Accuracy -0.1964\n",
      "Epoch 382 | train: Loss 0.009840 Accuracy 0.6391 | validation: Loss 0.019827 Accuracy -0.2456\n",
      "Epoch 383 | train: Loss 0.009826 Accuracy 0.6396 | validation: Loss 0.019024 Accuracy -0.1951\n",
      "Epoch 384 | train: Loss 0.009787 Accuracy 0.6411 | validation: Loss 0.020014 Accuracy -0.2573\n",
      "Epoch 385 | train: Loss 0.009756 Accuracy 0.6422 | validation: Loss 0.019491 Accuracy -0.2245\n",
      "Epoch 386 | train: Loss 0.009699 Accuracy 0.6443 | validation: Loss 0.019834 Accuracy -0.2461\n",
      "Epoch 387 | train: Loss 0.009660 Accuracy 0.6457 | validation: Loss 0.020160 Accuracy -0.2665\n",
      "Epoch 388 | train: Loss 0.009630 Accuracy 0.6468 | validation: Loss 0.019792 Accuracy -0.2434\n",
      "Epoch 389 | train: Loss 0.009598 Accuracy 0.6480 | validation: Loss 0.020821 Accuracy -0.3080\n",
      "Epoch 390 | train: Loss 0.009592 Accuracy 0.6482 | validation: Loss 0.019884 Accuracy -0.2492\n",
      "Epoch 391 | train: Loss 0.009562 Accuracy 0.6493 | validation: Loss 0.021329 Accuracy -0.3400\n",
      "Epoch 392 | train: Loss 0.009558 Accuracy 0.6495 | validation: Loss 0.020308 Accuracy -0.2758\n",
      "Epoch 393 | train: Loss 0.009476 Accuracy 0.6525 | validation: Loss 0.021103 Accuracy -0.3258\n",
      "Epoch 394 | train: Loss 0.009417 Accuracy 0.6546 | validation: Loss 0.021222 Accuracy -0.3332\n",
      "Epoch 395 | train: Loss 0.009379 Accuracy 0.6560 | validation: Loss 0.020922 Accuracy -0.3144\n",
      "Epoch 396 | train: Loss 0.009363 Accuracy 0.6566 | validation: Loss 0.022047 Accuracy -0.3850\n",
      "Epoch 397 | train: Loss 0.009370 Accuracy 0.6564 | validation: Loss 0.021233 Accuracy -0.3339\n",
      "Epoch 398 | train: Loss 0.009312 Accuracy 0.6585 | validation: Loss 0.022157 Accuracy -0.3920\n",
      "Epoch 399 | train: Loss 0.009260 Accuracy 0.6604 | validation: Loss 0.021971 Accuracy -0.3803\n",
      "Epoch 400 | train: Loss 0.009197 Accuracy 0.6627 | validation: Loss 0.021998 Accuracy -0.3820\n",
      "Epoch 401 | train: Loss 0.009169 Accuracy 0.6637 | validation: Loss 0.022758 Accuracy -0.4297\n",
      "Epoch 402 | train: Loss 0.009171 Accuracy 0.6637 | validation: Loss 0.022177 Accuracy -0.3932\n",
      "Epoch 403 | train: Loss 0.009138 Accuracy 0.6649 | validation: Loss 0.023169 Accuracy -0.4555\n",
      "Epoch 404 | train: Loss 0.009111 Accuracy 0.6659 | validation: Loss 0.022700 Accuracy -0.4261\n",
      "Epoch 405 | train: Loss 0.009030 Accuracy 0.6688 | validation: Loss 0.023122 Accuracy -0.4526\n",
      "Epoch 406 | train: Loss 0.008978 Accuracy 0.6707 | validation: Loss 0.023391 Accuracy -0.4695\n",
      "Epoch 407 | train: Loss 0.008950 Accuracy 0.6718 | validation: Loss 0.023160 Accuracy -0.4550\n",
      "Epoch 408 | train: Loss 0.008929 Accuracy 0.6726 | validation: Loss 0.024003 Accuracy -0.5079\n",
      "Epoch 409 | train: Loss 0.008933 Accuracy 0.6724 | validation: Loss 0.023416 Accuracy -0.4711\n",
      "Epoch 410 | train: Loss 0.008887 Accuracy 0.6741 | validation: Loss 0.024286 Accuracy -0.5257\n",
      "Epoch 411 | train: Loss 0.008852 Accuracy 0.6754 | validation: Loss 0.023877 Accuracy -0.5001\n",
      "Epoch 412 | train: Loss 0.008763 Accuracy 0.6786 | validation: Loss 0.024201 Accuracy -0.5204\n",
      "Epoch 413 | train: Loss 0.008712 Accuracy 0.6805 | validation: Loss 0.024515 Accuracy -0.5401\n",
      "Epoch 414 | train: Loss 0.008691 Accuracy 0.6813 | validation: Loss 0.024266 Accuracy -0.5245\n",
      "Epoch 415 | train: Loss 0.008674 Accuracy 0.6819 | validation: Loss 0.025148 Accuracy -0.5799\n",
      "Epoch 416 | train: Loss 0.008686 Accuracy 0.6814 | validation: Loss 0.024547 Accuracy -0.5421\n",
      "Epoch 417 | train: Loss 0.008636 Accuracy 0.6833 | validation: Loss 0.025363 Accuracy -0.5934\n",
      "Epoch 418 | train: Loss 0.008591 Accuracy 0.6849 | validation: Loss 0.024985 Accuracy -0.5696\n",
      "Epoch 419 | train: Loss 0.008503 Accuracy 0.6882 | validation: Loss 0.025224 Accuracy -0.5846\n",
      "Epoch 420 | train: Loss 0.008462 Accuracy 0.6897 | validation: Loss 0.025697 Accuracy -0.6143\n",
      "Epoch 421 | train: Loss 0.008460 Accuracy 0.6898 | validation: Loss 0.025338 Accuracy -0.5918\n",
      "Epoch 422 | train: Loss 0.008451 Accuracy 0.6901 | validation: Loss 0.026331 Accuracy -0.6542\n",
      "Epoch 423 | train: Loss 0.008464 Accuracy 0.6896 | validation: Loss 0.025634 Accuracy -0.6104\n",
      "Epoch 424 | train: Loss 0.008396 Accuracy 0.6921 | validation: Loss 0.026260 Accuracy -0.6497\n",
      "Epoch 425 | train: Loss 0.008341 Accuracy 0.6941 | validation: Loss 0.026101 Accuracy -0.6397\n",
      "Epoch 426 | train: Loss 0.008293 Accuracy 0.6959 | validation: Loss 0.026061 Accuracy -0.6372\n",
      "Epoch 427 | train: Loss 0.008283 Accuracy 0.6962 | validation: Loss 0.026885 Accuracy -0.6890\n",
      "Epoch 428 | train: Loss 0.008305 Accuracy 0.6954 | validation: Loss 0.026246 Accuracy -0.6489\n",
      "Epoch 429 | train: Loss 0.008287 Accuracy 0.6961 | validation: Loss 0.027162 Accuracy -0.7064\n",
      "Epoch 430 | train: Loss 0.008271 Accuracy 0.6967 | validation: Loss 0.026478 Accuracy -0.6634\n",
      "Epoch 431 | train: Loss 0.008209 Accuracy 0.6990 | validation: Loss 0.026720 Accuracy -0.6786\n",
      "Epoch 432 | train: Loss 0.008182 Accuracy 0.6999 | validation: Loss 0.027064 Accuracy -0.7002\n",
      "Epoch 433 | train: Loss 0.008185 Accuracy 0.6998 | validation: Loss 0.026668 Accuracy -0.6754\n",
      "Epoch 434 | train: Loss 0.008187 Accuracy 0.6998 | validation: Loss 0.027731 Accuracy -0.7422\n",
      "Epoch 435 | train: Loss 0.008201 Accuracy 0.6992 | validation: Loss 0.026900 Accuracy -0.6899\n",
      "Epoch 436 | train: Loss 0.008164 Accuracy 0.7006 | validation: Loss 0.027475 Accuracy -0.7260\n",
      "Epoch 437 | train: Loss 0.008140 Accuracy 0.7015 | validation: Loss 0.027227 Accuracy -0.7105\n",
      "Epoch 438 | train: Loss 0.008119 Accuracy 0.7022 | validation: Loss 0.027127 Accuracy -0.7042\n",
      "Epoch 439 | train: Loss 0.008118 Accuracy 0.7023 | validation: Loss 0.027980 Accuracy -0.7578\n",
      "Epoch 440 | train: Loss 0.008135 Accuracy 0.7017 | validation: Loss 0.027319 Accuracy -0.7162\n",
      "Epoch 441 | train: Loss 0.008125 Accuracy 0.7020 | validation: Loss 0.028225 Accuracy -0.7732\n",
      "Epoch 442 | train: Loss 0.008120 Accuracy 0.7022 | validation: Loss 0.027555 Accuracy -0.7311\n",
      "Epoch 443 | train: Loss 0.008093 Accuracy 0.7032 | validation: Loss 0.027849 Accuracy -0.7495\n",
      "Epoch 444 | train: Loss 0.008084 Accuracy 0.7035 | validation: Loss 0.028139 Accuracy -0.7678\n",
      "Epoch 445 | train: Loss 0.008085 Accuracy 0.7035 | validation: Loss 0.027791 Accuracy -0.7459\n",
      "Epoch 446 | train: Loss 0.008085 Accuracy 0.7035 | validation: Loss 0.028677 Accuracy -0.8016\n",
      "Epoch 447 | train: Loss 0.008095 Accuracy 0.7031 | validation: Loss 0.027874 Accuracy -0.7511\n",
      "Epoch 448 | train: Loss 0.008081 Accuracy 0.7037 | validation: Loss 0.028586 Accuracy -0.7958\n",
      "Epoch 449 | train: Loss 0.008076 Accuracy 0.7038 | validation: Loss 0.028223 Accuracy -0.7730\n",
      "Epoch 450 | train: Loss 0.008062 Accuracy 0.7043 | validation: Loss 0.028433 Accuracy -0.7862\n",
      "Epoch 451 | train: Loss 0.008058 Accuracy 0.7045 | validation: Loss 0.028806 Accuracy -0.8097\n",
      "Epoch 452 | train: Loss 0.008061 Accuracy 0.7044 | validation: Loss 0.028296 Accuracy -0.7776\n",
      "Epoch 453 | train: Loss 0.008059 Accuracy 0.7045 | validation: Loss 0.029039 Accuracy -0.8243\n",
      "Epoch 454 | train: Loss 0.008065 Accuracy 0.7042 | validation: Loss 0.028332 Accuracy -0.7799\n",
      "Epoch 455 | train: Loss 0.008055 Accuracy 0.7046 | validation: Loss 0.029119 Accuracy -0.8293\n",
      "Epoch 456 | train: Loss 0.008053 Accuracy 0.7047 | validation: Loss 0.028746 Accuracy -0.8059\n",
      "Epoch 457 | train: Loss 0.008042 Accuracy 0.7051 | validation: Loss 0.029016 Accuracy -0.8229\n",
      "Epoch 458 | train: Loss 0.008039 Accuracy 0.7052 | validation: Loss 0.029110 Accuracy -0.8288\n",
      "Epoch 459 | train: Loss 0.008037 Accuracy 0.7053 | validation: Loss 0.028846 Accuracy -0.8122\n",
      "Epoch 460 | train: Loss 0.008033 Accuracy 0.7054 | validation: Loss 0.029486 Accuracy -0.8524\n",
      "Epoch 461 | train: Loss 0.008036 Accuracy 0.7053 | validation: Loss 0.028909 Accuracy -0.8161\n",
      "Epoch 462 | train: Loss 0.008029 Accuracy 0.7055 | validation: Loss 0.029744 Accuracy -0.8686\n",
      "Epoch 463 | train: Loss 0.008031 Accuracy 0.7055 | validation: Loss 0.028992 Accuracy -0.8214\n",
      "Epoch 464 | train: Loss 0.008021 Accuracy 0.7058 | validation: Loss 0.029790 Accuracy -0.8715\n",
      "Epoch 465 | train: Loss 0.008020 Accuracy 0.7059 | validation: Loss 0.029173 Accuracy -0.8327\n",
      "Epoch 466 | train: Loss 0.008009 Accuracy 0.7063 | validation: Loss 0.029877 Accuracy -0.8769\n",
      "Epoch 467 | train: Loss 0.008006 Accuracy 0.7064 | validation: Loss 0.029379 Accuracy -0.8457\n",
      "Epoch 468 | train: Loss 0.007996 Accuracy 0.7068 | validation: Loss 0.029897 Accuracy -0.8782\n",
      "Epoch 469 | train: Loss 0.007992 Accuracy 0.7069 | validation: Loss 0.029463 Accuracy -0.8509\n",
      "Epoch 470 | train: Loss 0.007982 Accuracy 0.7073 | validation: Loss 0.029983 Accuracy -0.8836\n",
      "Epoch 471 | train: Loss 0.007977 Accuracy 0.7074 | validation: Loss 0.029515 Accuracy -0.8542\n",
      "Epoch 472 | train: Loss 0.007967 Accuracy 0.7078 | validation: Loss 0.030222 Accuracy -0.8986\n",
      "Epoch 473 | train: Loss 0.007964 Accuracy 0.7079 | validation: Loss 0.029351 Accuracy -0.8439\n",
      "Epoch 474 | train: Loss 0.007954 Accuracy 0.7083 | validation: Loss 0.030653 Accuracy -0.9257\n",
      "Epoch 475 | train: Loss 0.007962 Accuracy 0.7080 | validation: Loss 0.028829 Accuracy -0.8111\n",
      "Epoch 476 | train: Loss 0.007969 Accuracy 0.7078 | validation: Loss 0.031781 Accuracy -0.9966\n",
      "Epoch 477 | train: Loss 0.008036 Accuracy 0.7053 | validation: Loss 0.027916 Accuracy -0.7538\n",
      "Epoch 478 | train: Loss 0.008091 Accuracy 0.7033 | validation: Loss 0.033033 Accuracy -1.0752\n",
      "Epoch 479 | train: Loss 0.008204 Accuracy 0.6991 | validation: Loss 0.028323 Accuracy -0.7794\n",
      "Epoch 480 | train: Loss 0.008027 Accuracy 0.7056 | validation: Loss 0.030515 Accuracy -0.9170\n",
      "Epoch 481 | train: Loss 0.007889 Accuracy 0.7107 | validation: Loss 0.031068 Accuracy -0.9518\n",
      "Epoch 482 | train: Loss 0.007927 Accuracy 0.7093 | validation: Loss 0.028043 Accuracy -0.7618\n",
      "Epoch 483 | train: Loss 0.008016 Accuracy 0.7060 | validation: Loss 0.032351 Accuracy -1.0324\n",
      "Epoch 484 | train: Loss 0.008019 Accuracy 0.7059 | validation: Loss 0.029839 Accuracy -0.8746\n",
      "Epoch 485 | train: Loss 0.007857 Accuracy 0.7119 | validation: Loss 0.029086 Accuracy -0.8273\n",
      "Epoch 486 | train: Loss 0.007879 Accuracy 0.7110 | validation: Loss 0.032077 Accuracy -1.0152\n",
      "Epoch 487 | train: Loss 0.007987 Accuracy 0.7071 | validation: Loss 0.028887 Accuracy -0.8148\n",
      "Epoch 488 | train: Loss 0.007875 Accuracy 0.7112 | validation: Loss 0.030280 Accuracy -0.9023\n",
      "Epoch 489 | train: Loss 0.007803 Accuracy 0.7138 | validation: Loss 0.031876 Accuracy -1.0026\n",
      "Epoch 490 | train: Loss 0.007864 Accuracy 0.7116 | validation: Loss 0.029238 Accuracy -0.8368\n",
      "Epoch 491 | train: Loss 0.007836 Accuracy 0.7126 | validation: Loss 0.030592 Accuracy -0.9219\n",
      "Epoch 492 | train: Loss 0.007780 Accuracy 0.7147 | validation: Loss 0.031022 Accuracy -0.9489\n",
      "Epoch 493 | train: Loss 0.007793 Accuracy 0.7142 | validation: Loss 0.029353 Accuracy -0.8440\n",
      "Epoch 494 | train: Loss 0.007802 Accuracy 0.7139 | validation: Loss 0.031592 Accuracy -0.9847\n",
      "Epoch 495 | train: Loss 0.007775 Accuracy 0.7149 | validation: Loss 0.030821 Accuracy -0.9363\n",
      "Epoch 496 | train: Loss 0.007736 Accuracy 0.7163 | validation: Loss 0.029605 Accuracy -0.8599\n",
      "Epoch 497 | train: Loss 0.007752 Accuracy 0.7157 | validation: Loss 0.031510 Accuracy -0.9795\n",
      "Epoch 498 | train: Loss 0.007767 Accuracy 0.7152 | validation: Loss 0.030166 Accuracy -0.8951\n",
      "Epoch 499 | train: Loss 0.007713 Accuracy 0.7171 | validation: Loss 0.030422 Accuracy -0.9112\n",
      "Epoch 500 | train: Loss 0.007701 Accuracy 0.7176 | validation: Loss 0.031744 Accuracy -0.9942\n",
      "Epoch 501 | train: Loss 0.007722 Accuracy 0.7168 | validation: Loss 0.030056 Accuracy -0.8882\n",
      "Epoch 502 | train: Loss 0.007694 Accuracy 0.7178 | validation: Loss 0.030698 Accuracy -0.9285\n",
      "Epoch 503 | train: Loss 0.007676 Accuracy 0.7185 | validation: Loss 0.031235 Accuracy -0.9622\n",
      "Epoch 504 | train: Loss 0.007678 Accuracy 0.7184 | validation: Loss 0.030159 Accuracy -0.8947\n",
      "Epoch 505 | train: Loss 0.007671 Accuracy 0.7187 | validation: Loss 0.031475 Accuracy -0.9774\n",
      "Epoch 506 | train: Loss 0.007658 Accuracy 0.7191 | validation: Loss 0.031002 Accuracy -0.9476\n",
      "Epoch 507 | train: Loss 0.007640 Accuracy 0.7198 | validation: Loss 0.030362 Accuracy -0.9074\n",
      "Epoch 508 | train: Loss 0.007641 Accuracy 0.7198 | validation: Loss 0.031554 Accuracy -0.9823\n",
      "Epoch 509 | train: Loss 0.007643 Accuracy 0.7197 | validation: Loss 0.030606 Accuracy -0.9228\n",
      "Epoch 510 | train: Loss 0.007618 Accuracy 0.7206 | validation: Loss 0.031092 Accuracy -0.9533\n",
      "Epoch 511 | train: Loss 0.007606 Accuracy 0.7211 | validation: Loss 0.031598 Accuracy -0.9851\n",
      "Epoch 512 | train: Loss 0.007607 Accuracy 0.7210 | validation: Loss 0.030577 Accuracy -0.9210\n",
      "Epoch 513 | train: Loss 0.007600 Accuracy 0.7213 | validation: Loss 0.031498 Accuracy -0.9788\n",
      "Epoch 514 | train: Loss 0.007589 Accuracy 0.7217 | validation: Loss 0.031109 Accuracy -0.9543\n",
      "Epoch 515 | train: Loss 0.007574 Accuracy 0.7223 | validation: Loss 0.031003 Accuracy -0.9477\n",
      "Epoch 516 | train: Loss 0.007567 Accuracy 0.7225 | validation: Loss 0.031803 Accuracy -0.9979\n",
      "Epoch 517 | train: Loss 0.007568 Accuracy 0.7224 | validation: Loss 0.030927 Accuracy -0.9429\n",
      "Epoch 518 | train: Loss 0.007555 Accuracy 0.7229 | validation: Loss 0.031479 Accuracy -0.9776\n",
      "Epoch 519 | train: Loss 0.007544 Accuracy 0.7233 | validation: Loss 0.031414 Accuracy -0.9735\n",
      "Epoch 520 | train: Loss 0.007535 Accuracy 0.7237 | validation: Loss 0.031123 Accuracy -0.9552\n",
      "Epoch 521 | train: Loss 0.007530 Accuracy 0.7239 | validation: Loss 0.031881 Accuracy -1.0029\n",
      "Epoch 522 | train: Loss 0.007527 Accuracy 0.7239 | validation: Loss 0.031199 Accuracy -0.9600\n",
      "Epoch 523 | train: Loss 0.007514 Accuracy 0.7244 | validation: Loss 0.031602 Accuracy -0.9854\n",
      "Epoch 524 | train: Loss 0.007506 Accuracy 0.7247 | validation: Loss 0.031587 Accuracy -0.9844\n",
      "Epoch 525 | train: Loss 0.007499 Accuracy 0.7250 | validation: Loss 0.031346 Accuracy -0.9693\n",
      "Epoch 526 | train: Loss 0.007493 Accuracy 0.7252 | validation: Loss 0.031961 Accuracy -1.0079\n",
      "Epoch 527 | train: Loss 0.007489 Accuracy 0.7254 | validation: Loss 0.031398 Accuracy -0.9725\n",
      "Epoch 528 | train: Loss 0.007478 Accuracy 0.7257 | validation: Loss 0.031833 Accuracy -0.9999\n",
      "Epoch 529 | train: Loss 0.007472 Accuracy 0.7260 | validation: Loss 0.031681 Accuracy -0.9903\n",
      "Epoch 530 | train: Loss 0.007464 Accuracy 0.7263 | validation: Loss 0.031628 Accuracy -0.9870\n",
      "Epoch 531 | train: Loss 0.007457 Accuracy 0.7265 | validation: Loss 0.032012 Accuracy -1.0111\n",
      "Epoch 532 | train: Loss 0.007452 Accuracy 0.7267 | validation: Loss 0.031588 Accuracy -0.9845\n",
      "Epoch 533 | train: Loss 0.007444 Accuracy 0.7270 | validation: Loss 0.032076 Accuracy -1.0151\n",
      "Epoch 534 | train: Loss 0.007439 Accuracy 0.7272 | validation: Loss 0.031710 Accuracy -0.9921\n",
      "Epoch 535 | train: Loss 0.007431 Accuracy 0.7275 | validation: Loss 0.031956 Accuracy -1.0076\n",
      "Epoch 536 | train: Loss 0.007424 Accuracy 0.7278 | validation: Loss 0.031966 Accuracy -1.0082\n",
      "Epoch 537 | train: Loss 0.007416 Accuracy 0.7280 | validation: Loss 0.031882 Accuracy -1.0029\n",
      "Epoch 538 | train: Loss 0.007410 Accuracy 0.7283 | validation: Loss 0.032165 Accuracy -1.0207\n",
      "Epoch 539 | train: Loss 0.007405 Accuracy 0.7284 | validation: Loss 0.031815 Accuracy -0.9987\n",
      "Epoch 540 | train: Loss 0.007397 Accuracy 0.7287 | validation: Loss 0.032229 Accuracy -1.0247\n",
      "Epoch 541 | train: Loss 0.007392 Accuracy 0.7289 | validation: Loss 0.031908 Accuracy -1.0046\n",
      "Epoch 542 | train: Loss 0.007384 Accuracy 0.7292 | validation: Loss 0.032293 Accuracy -1.0287\n",
      "Epoch 543 | train: Loss 0.007379 Accuracy 0.7294 | validation: Loss 0.032012 Accuracy -1.0111\n",
      "Epoch 544 | train: Loss 0.007371 Accuracy 0.7297 | validation: Loss 0.032234 Accuracy -1.0250\n",
      "Epoch 545 | train: Loss 0.007364 Accuracy 0.7299 | validation: Loss 0.032130 Accuracy -1.0185\n",
      "Epoch 546 | train: Loss 0.007357 Accuracy 0.7302 | validation: Loss 0.032263 Accuracy -1.0269\n",
      "Epoch 547 | train: Loss 0.007351 Accuracy 0.7304 | validation: Loss 0.032265 Accuracy -1.0270\n",
      "Epoch 548 | train: Loss 0.007344 Accuracy 0.7307 | validation: Loss 0.032236 Accuracy -1.0252\n",
      "Epoch 549 | train: Loss 0.007338 Accuracy 0.7309 | validation: Loss 0.032344 Accuracy -1.0320\n",
      "Epoch 550 | train: Loss 0.007332 Accuracy 0.7311 | validation: Loss 0.032250 Accuracy -1.0261\n",
      "Epoch 551 | train: Loss 0.007325 Accuracy 0.7314 | validation: Loss 0.032473 Accuracy -1.0401\n",
      "Epoch 552 | train: Loss 0.007320 Accuracy 0.7316 | validation: Loss 0.032230 Accuracy -1.0248\n",
      "Epoch 553 | train: Loss 0.007313 Accuracy 0.7318 | validation: Loss 0.032602 Accuracy -1.0482\n",
      "Epoch 554 | train: Loss 0.007308 Accuracy 0.7320 | validation: Loss 0.032151 Accuracy -1.0198\n",
      "Epoch 555 | train: Loss 0.007302 Accuracy 0.7322 | validation: Loss 0.032844 Accuracy -1.0634\n",
      "Epoch 556 | train: Loss 0.007300 Accuracy 0.7323 | validation: Loss 0.031932 Accuracy -1.0060\n",
      "Epoch 557 | train: Loss 0.007296 Accuracy 0.7324 | validation: Loss 0.033318 Accuracy -1.0932\n",
      "Epoch 558 | train: Loss 0.007307 Accuracy 0.7320 | validation: Loss 0.031408 Accuracy -0.9731\n",
      "Epoch 559 | train: Loss 0.007320 Accuracy 0.7315 | validation: Loss 0.034221 Accuracy -1.1498\n",
      "Epoch 560 | train: Loss 0.007372 Accuracy 0.7296 | validation: Loss 0.030593 Accuracy -0.9220\n",
      "Epoch 561 | train: Loss 0.007414 Accuracy 0.7281 | validation: Loss 0.035131 Accuracy -1.2070\n",
      "Epoch 562 | train: Loss 0.007469 Accuracy 0.7261 | validation: Loss 0.030883 Accuracy -0.9402\n",
      "Epoch 563 | train: Loss 0.007381 Accuracy 0.7293 | validation: Loss 0.033700 Accuracy -1.1171\n",
      "Epoch 564 | train: Loss 0.007287 Accuracy 0.7328 | validation: Loss 0.032889 Accuracy -1.0662\n",
      "Epoch 565 | train: Loss 0.007251 Accuracy 0.7341 | validation: Loss 0.031303 Accuracy -0.9666\n",
      "Epoch 566 | train: Loss 0.007298 Accuracy 0.7323 | validation: Loss 0.034510 Accuracy -1.1680\n",
      "Epoch 567 | train: Loss 0.007356 Accuracy 0.7302 | validation: Loss 0.031581 Accuracy -0.9840\n",
      "Epoch 568 | train: Loss 0.007294 Accuracy 0.7325 | validation: Loss 0.033348 Accuracy -1.0950\n",
      "Epoch 569 | train: Loss 0.007233 Accuracy 0.7347 | validation: Loss 0.033358 Accuracy -1.0957\n",
      "Epoch 570 | train: Loss 0.007237 Accuracy 0.7346 | validation: Loss 0.031314 Accuracy -0.9673\n",
      "Epoch 571 | train: Loss 0.007271 Accuracy 0.7333 | validation: Loss 0.034118 Accuracy -1.1434\n",
      "Epoch 572 | train: Loss 0.007277 Accuracy 0.7331 | validation: Loss 0.032341 Accuracy -1.0318\n",
      "Epoch 573 | train: Loss 0.007214 Accuracy 0.7355 | validation: Loss 0.032772 Accuracy -1.0588\n",
      "Epoch 574 | train: Loss 0.007193 Accuracy 0.7362 | validation: Loss 0.033764 Accuracy -1.1212\n",
      "Epoch 575 | train: Loss 0.007224 Accuracy 0.7351 | validation: Loss 0.031616 Accuracy -0.9862\n",
      "Epoch 576 | train: Loss 0.007224 Accuracy 0.7351 | validation: Loss 0.033622 Accuracy -1.1122\n",
      "Epoch 577 | train: Loss 0.007199 Accuracy 0.7360 | validation: Loss 0.032980 Accuracy -1.0719\n",
      "Epoch 578 | train: Loss 0.007168 Accuracy 0.7371 | validation: Loss 0.032454 Accuracy -1.0388\n",
      "Epoch 579 | train: Loss 0.007174 Accuracy 0.7369 | validation: Loss 0.033876 Accuracy -1.1282\n",
      "Epoch 580 | train: Loss 0.007193 Accuracy 0.7362 | validation: Loss 0.032164 Accuracy -1.0207\n",
      "Epoch 581 | train: Loss 0.007172 Accuracy 0.7370 | validation: Loss 0.033192 Accuracy -1.0852\n",
      "Epoch 582 | train: Loss 0.007152 Accuracy 0.7377 | validation: Loss 0.033367 Accuracy -1.0962\n",
      "Epoch 583 | train: Loss 0.007148 Accuracy 0.7379 | validation: Loss 0.032447 Accuracy -1.0384\n",
      "Epoch 584 | train: Loss 0.007153 Accuracy 0.7377 | validation: Loss 0.033810 Accuracy -1.1240\n",
      "Epoch 585 | train: Loss 0.007155 Accuracy 0.7376 | validation: Loss 0.032673 Accuracy -1.0526\n",
      "Epoch 586 | train: Loss 0.007134 Accuracy 0.7384 | validation: Loss 0.032965 Accuracy -1.0710\n",
      "Epoch 587 | train: Loss 0.007124 Accuracy 0.7387 | validation: Loss 0.033542 Accuracy -1.1072\n",
      "Epoch 588 | train: Loss 0.007128 Accuracy 0.7386 | validation: Loss 0.032615 Accuracy -1.0490\n",
      "Epoch 589 | train: Loss 0.007125 Accuracy 0.7387 | validation: Loss 0.033713 Accuracy -1.1179\n",
      "Epoch 590 | train: Loss 0.007120 Accuracy 0.7389 | validation: Loss 0.032993 Accuracy -1.0727\n",
      "Epoch 591 | train: Loss 0.007104 Accuracy 0.7395 | validation: Loss 0.032970 Accuracy -1.0713\n",
      "Epoch 592 | train: Loss 0.007098 Accuracy 0.7397 | validation: Loss 0.033606 Accuracy -1.1112\n",
      "Epoch 593 | train: Loss 0.007101 Accuracy 0.7396 | validation: Loss 0.032807 Accuracy -1.0610\n",
      "Epoch 594 | train: Loss 0.007095 Accuracy 0.7398 | validation: Loss 0.033691 Accuracy -1.1165\n",
      "Epoch 595 | train: Loss 0.007088 Accuracy 0.7400 | validation: Loss 0.033147 Accuracy -1.0824\n",
      "Epoch 596 | train: Loss 0.007076 Accuracy 0.7405 | validation: Loss 0.033153 Accuracy -1.0828\n",
      "Epoch 597 | train: Loss 0.007071 Accuracy 0.7407 | validation: Loss 0.033610 Accuracy -1.1115\n",
      "Epoch 598 | train: Loss 0.007071 Accuracy 0.7407 | validation: Loss 0.032975 Accuracy -1.0716\n",
      "Epoch 599 | train: Loss 0.007066 Accuracy 0.7409 | validation: Loss 0.033758 Accuracy -1.1208\n",
      "Epoch 600 | train: Loss 0.007062 Accuracy 0.7410 | validation: Loss 0.033206 Accuracy -1.0861\n",
      "Epoch 601 | train: Loss 0.007052 Accuracy 0.7414 | validation: Loss 0.033439 Accuracy -1.1007\n",
      "Epoch 602 | train: Loss 0.007047 Accuracy 0.7416 | validation: Loss 0.033540 Accuracy -1.1071\n",
      "Epoch 603 | train: Loss 0.007043 Accuracy 0.7417 | validation: Loss 0.033191 Accuracy -1.0852\n",
      "Epoch 604 | train: Loss 0.007038 Accuracy 0.7419 | validation: Loss 0.033827 Accuracy -1.1251\n",
      "Epoch 605 | train: Loss 0.007037 Accuracy 0.7419 | validation: Loss 0.033254 Accuracy -1.0891\n",
      "Epoch 606 | train: Loss 0.007029 Accuracy 0.7422 | validation: Loss 0.033742 Accuracy -1.1198\n",
      "Epoch 607 | train: Loss 0.007025 Accuracy 0.7424 | validation: Loss 0.033406 Accuracy -1.0987\n",
      "Epoch 608 | train: Loss 0.007017 Accuracy 0.7427 | validation: Loss 0.033552 Accuracy -1.1079\n",
      "Epoch 609 | train: Loss 0.007012 Accuracy 0.7428 | validation: Loss 0.033735 Accuracy -1.1193\n",
      "Epoch 610 | train: Loss 0.007008 Accuracy 0.7430 | validation: Loss 0.033462 Accuracy -1.1022\n",
      "Epoch 611 | train: Loss 0.007003 Accuracy 0.7432 | validation: Loss 0.033869 Accuracy -1.1278\n",
      "Epoch 612 | train: Loss 0.007000 Accuracy 0.7433 | validation: Loss 0.033391 Accuracy -1.0977\n",
      "Epoch 613 | train: Loss 0.006993 Accuracy 0.7435 | validation: Loss 0.033932 Accuracy -1.1317\n",
      "Epoch 614 | train: Loss 0.006990 Accuracy 0.7437 | validation: Loss 0.033507 Accuracy -1.1050\n",
      "Epoch 615 | train: Loss 0.006982 Accuracy 0.7439 | validation: Loss 0.033939 Accuracy -1.1322\n",
      "Epoch 616 | train: Loss 0.006978 Accuracy 0.7441 | validation: Loss 0.033606 Accuracy -1.1112\n",
      "Epoch 617 | train: Loss 0.006971 Accuracy 0.7444 | validation: Loss 0.033880 Accuracy -1.1285\n",
      "Epoch 618 | train: Loss 0.006966 Accuracy 0.7445 | validation: Loss 0.033723 Accuracy -1.1186\n",
      "Epoch 619 | train: Loss 0.006960 Accuracy 0.7447 | validation: Loss 0.033877 Accuracy -1.1282\n",
      "Epoch 620 | train: Loss 0.006955 Accuracy 0.7449 | validation: Loss 0.033850 Accuracy -1.1266\n",
      "Epoch 621 | train: Loss 0.006950 Accuracy 0.7451 | validation: Loss 0.033869 Accuracy -1.1278\n",
      "Epoch 622 | train: Loss 0.006945 Accuracy 0.7453 | validation: Loss 0.033920 Accuracy -1.1309\n",
      "Epoch 623 | train: Loss 0.006940 Accuracy 0.7455 | validation: Loss 0.033857 Accuracy -1.1270\n",
      "Epoch 624 | train: Loss 0.006934 Accuracy 0.7457 | validation: Loss 0.034036 Accuracy -1.1382\n",
      "Epoch 625 | train: Loss 0.006930 Accuracy 0.7459 | validation: Loss 0.033871 Accuracy -1.1279\n",
      "Epoch 626 | train: Loss 0.006924 Accuracy 0.7461 | validation: Loss 0.034155 Accuracy -1.1457\n",
      "Epoch 627 | train: Loss 0.006921 Accuracy 0.7462 | validation: Loss 0.033784 Accuracy -1.1224\n",
      "Epoch 628 | train: Loss 0.006915 Accuracy 0.7464 | validation: Loss 0.034357 Accuracy -1.1584\n",
      "Epoch 629 | train: Loss 0.006914 Accuracy 0.7464 | validation: Loss 0.033621 Accuracy -1.1121\n",
      "Epoch 630 | train: Loss 0.006911 Accuracy 0.7466 | validation: Loss 0.034782 Accuracy -1.1851\n",
      "Epoch 631 | train: Loss 0.006920 Accuracy 0.7462 | validation: Loss 0.033129 Accuracy -1.0812\n",
      "Epoch 632 | train: Loss 0.006932 Accuracy 0.7458 | validation: Loss 0.035620 Accuracy -1.2377\n",
      "Epoch 633 | train: Loss 0.006984 Accuracy 0.7439 | validation: Loss 0.032192 Accuracy -1.0224\n",
      "Epoch 634 | train: Loss 0.007049 Accuracy 0.7415 | validation: Loss 0.036880 Accuracy -1.3169\n",
      "Epoch 635 | train: Loss 0.007154 Accuracy 0.7376 | validation: Loss 0.031946 Accuracy -1.0070\n",
      "Epoch 636 | train: Loss 0.007121 Accuracy 0.7388 | validation: Loss 0.036184 Accuracy -1.2732\n",
      "Epoch 637 | train: Loss 0.007000 Accuracy 0.7433 | validation: Loss 0.033942 Accuracy -1.1324\n",
      "Epoch 638 | train: Loss 0.006874 Accuracy 0.7479 | validation: Loss 0.033087 Accuracy -1.0786\n",
      "Epoch 639 | train: Loss 0.006908 Accuracy 0.7467 | validation: Loss 0.036127 Accuracy -1.2696\n",
      "Epoch 640 | train: Loss 0.007020 Accuracy 0.7425 | validation: Loss 0.032711 Accuracy -1.0550\n",
      "Epoch 641 | train: Loss 0.006995 Accuracy 0.7435 | validation: Loss 0.035650 Accuracy -1.2396\n",
      "Epoch 642 | train: Loss 0.006893 Accuracy 0.7472 | validation: Loss 0.034750 Accuracy -1.1831\n",
      "Epoch 643 | train: Loss 0.006854 Accuracy 0.7486 | validation: Loss 0.032728 Accuracy -1.0560\n",
      "Epoch 644 | train: Loss 0.006920 Accuracy 0.7462 | validation: Loss 0.035951 Accuracy -1.2586\n",
      "Epoch 645 | train: Loss 0.006963 Accuracy 0.7447 | validation: Loss 0.033602 Accuracy -1.1110\n",
      "Epoch 646 | train: Loss 0.006869 Accuracy 0.7481 | validation: Loss 0.034573 Accuracy -1.1720\n",
      "Epoch 647 | train: Loss 0.006821 Accuracy 0.7499 | validation: Loss 0.035482 Accuracy -1.2291\n",
      "Epoch 648 | train: Loss 0.006868 Accuracy 0.7481 | validation: Loss 0.032959 Accuracy -1.0706\n",
      "Epoch 649 | train: Loss 0.006885 Accuracy 0.7475 | validation: Loss 0.035321 Accuracy -1.2190\n",
      "Epoch 650 | train: Loss 0.006848 Accuracy 0.7488 | validation: Loss 0.034525 Accuracy -1.1690\n",
      "Epoch 651 | train: Loss 0.006798 Accuracy 0.7507 | validation: Loss 0.033779 Accuracy -1.1221\n",
      "Epoch 652 | train: Loss 0.006820 Accuracy 0.7499 | validation: Loss 0.035738 Accuracy -1.2452\n",
      "Epoch 653 | train: Loss 0.006849 Accuracy 0.7488 | validation: Loss 0.033838 Accuracy -1.1258\n",
      "Epoch 654 | train: Loss 0.006810 Accuracy 0.7502 | validation: Loss 0.034495 Accuracy -1.1671\n",
      "Epoch 655 | train: Loss 0.006783 Accuracy 0.7512 | validation: Loss 0.035119 Accuracy -1.2063\n",
      "Epoch 656 | train: Loss 0.006801 Accuracy 0.7506 | validation: Loss 0.033726 Accuracy -1.1188\n",
      "Epoch 657 | train: Loss 0.006806 Accuracy 0.7504 | validation: Loss 0.035497 Accuracy -1.2301\n",
      "Epoch 658 | train: Loss 0.006793 Accuracy 0.7509 | validation: Loss 0.034664 Accuracy -1.1777\n",
      "Epoch 659 | train: Loss 0.006765 Accuracy 0.7519 | validation: Loss 0.033980 Accuracy -1.1347\n",
      "Epoch 660 | train: Loss 0.006770 Accuracy 0.7517 | validation: Loss 0.035328 Accuracy -1.2194\n",
      "Epoch 661 | train: Loss 0.006786 Accuracy 0.7511 | validation: Loss 0.034170 Accuracy -1.1466\n",
      "Epoch 662 | train: Loss 0.006765 Accuracy 0.7519 | validation: Loss 0.035031 Accuracy -1.2008\n",
      "Epoch 663 | train: Loss 0.006747 Accuracy 0.7526 | validation: Loss 0.035086 Accuracy -1.2042\n",
      "Epoch 664 | train: Loss 0.006746 Accuracy 0.7526 | validation: Loss 0.033987 Accuracy -1.1352\n",
      "Epoch 665 | train: Loss 0.006749 Accuracy 0.7525 | validation: Loss 0.035309 Accuracy -1.2182\n",
      "Epoch 666 | train: Loss 0.006748 Accuracy 0.7525 | validation: Loss 0.034582 Accuracy -1.1725\n",
      "Epoch 667 | train: Loss 0.006726 Accuracy 0.7533 | validation: Loss 0.034703 Accuracy -1.1802\n",
      "Epoch 668 | train: Loss 0.006717 Accuracy 0.7537 | validation: Loss 0.035268 Accuracy -1.2156\n",
      "Epoch 669 | train: Loss 0.006723 Accuracy 0.7534 | validation: Loss 0.034286 Accuracy -1.1540\n",
      "Epoch 670 | train: Loss 0.006718 Accuracy 0.7536 | validation: Loss 0.035236 Accuracy -1.2136\n",
      "Epoch 671 | train: Loss 0.006712 Accuracy 0.7538 | validation: Loss 0.034790 Accuracy -1.1856\n",
      "Epoch 672 | train: Loss 0.006697 Accuracy 0.7544 | validation: Loss 0.034649 Accuracy -1.1768\n",
      "Epoch 673 | train: Loss 0.006694 Accuracy 0.7545 | validation: Loss 0.035385 Accuracy -1.2230\n",
      "Epoch 674 | train: Loss 0.006698 Accuracy 0.7544 | validation: Loss 0.034567 Accuracy -1.1716\n",
      "Epoch 675 | train: Loss 0.006691 Accuracy 0.7546 | validation: Loss 0.035187 Accuracy -1.2105\n",
      "Epoch 676 | train: Loss 0.006683 Accuracy 0.7549 | validation: Loss 0.034901 Accuracy -1.1926\n",
      "Epoch 677 | train: Loss 0.006673 Accuracy 0.7553 | validation: Loss 0.034798 Accuracy -1.1861\n",
      "Epoch 678 | train: Loss 0.006669 Accuracy 0.7554 | validation: Loss 0.035435 Accuracy -1.2261\n",
      "Epoch 679 | train: Loss 0.006671 Accuracy 0.7554 | validation: Loss 0.034717 Accuracy -1.1810\n",
      "Epoch 680 | train: Loss 0.006662 Accuracy 0.7557 | validation: Loss 0.035235 Accuracy -1.2136\n",
      "Epoch 681 | train: Loss 0.006656 Accuracy 0.7559 | validation: Loss 0.034975 Accuracy -1.1972\n",
      "Epoch 682 | train: Loss 0.006647 Accuracy 0.7562 | validation: Loss 0.035033 Accuracy -1.2009\n",
      "Epoch 683 | train: Loss 0.006642 Accuracy 0.7564 | validation: Loss 0.035375 Accuracy -1.2224\n",
      "Epoch 684 | train: Loss 0.006640 Accuracy 0.7565 | validation: Loss 0.034837 Accuracy -1.1886\n",
      "Epoch 685 | train: Loss 0.006633 Accuracy 0.7568 | validation: Loss 0.035383 Accuracy -1.2228\n",
      "Epoch 686 | train: Loss 0.006629 Accuracy 0.7569 | validation: Loss 0.035000 Accuracy -1.1988\n",
      "Epoch 687 | train: Loss 0.006621 Accuracy 0.7572 | validation: Loss 0.035284 Accuracy -1.2166\n",
      "Epoch 688 | train: Loss 0.006615 Accuracy 0.7574 | validation: Loss 0.035241 Accuracy -1.2139\n",
      "Epoch 689 | train: Loss 0.006608 Accuracy 0.7576 | validation: Loss 0.035084 Accuracy -1.2041\n",
      "Epoch 690 | train: Loss 0.006603 Accuracy 0.7578 | validation: Loss 0.035458 Accuracy -1.2276\n",
      "Epoch 691 | train: Loss 0.006600 Accuracy 0.7579 | validation: Loss 0.035050 Accuracy -1.2019\n",
      "Epoch 692 | train: Loss 0.006594 Accuracy 0.7582 | validation: Loss 0.035487 Accuracy -1.2294\n",
      "Epoch 693 | train: Loss 0.006589 Accuracy 0.7584 | validation: Loss 0.035128 Accuracy -1.2069\n",
      "Epoch 694 | train: Loss 0.006581 Accuracy 0.7586 | validation: Loss 0.035470 Accuracy -1.2283\n",
      "Epoch 695 | train: Loss 0.006577 Accuracy 0.7588 | validation: Loss 0.035285 Accuracy -1.2167\n",
      "Epoch 696 | train: Loss 0.006570 Accuracy 0.7591 | validation: Loss 0.035345 Accuracy -1.2205\n",
      "Epoch 697 | train: Loss 0.006564 Accuracy 0.7593 | validation: Loss 0.035424 Accuracy -1.2254\n",
      "Epoch 698 | train: Loss 0.006558 Accuracy 0.7595 | validation: Loss 0.035324 Accuracy -1.2192\n",
      "Epoch 699 | train: Loss 0.006552 Accuracy 0.7597 | validation: Loss 0.035570 Accuracy -1.2346\n",
      "Epoch 700 | train: Loss 0.006547 Accuracy 0.7599 | validation: Loss 0.035254 Accuracy -1.2148\n",
      "Epoch 701 | train: Loss 0.006540 Accuracy 0.7601 | validation: Loss 0.035637 Accuracy -1.2388\n",
      "Epoch 702 | train: Loss 0.006536 Accuracy 0.7603 | validation: Loss 0.035265 Accuracy -1.2155\n",
      "Epoch 703 | train: Loss 0.006529 Accuracy 0.7605 | validation: Loss 0.035763 Accuracy -1.2468\n",
      "Epoch 704 | train: Loss 0.006526 Accuracy 0.7607 | validation: Loss 0.035216 Accuracy -1.2124\n",
      "Epoch 705 | train: Loss 0.006518 Accuracy 0.7610 | validation: Loss 0.035859 Accuracy -1.2528\n",
      "Epoch 706 | train: Loss 0.006516 Accuracy 0.7610 | validation: Loss 0.035150 Accuracy -1.2082\n",
      "Epoch 707 | train: Loss 0.006509 Accuracy 0.7613 | validation: Loss 0.036082 Accuracy -1.2668\n",
      "Epoch 708 | train: Loss 0.006510 Accuracy 0.7612 | validation: Loss 0.034962 Accuracy -1.1964\n",
      "Epoch 709 | train: Loss 0.006506 Accuracy 0.7614 | validation: Loss 0.036412 Accuracy -1.2875\n",
      "Epoch 710 | train: Loss 0.006517 Accuracy 0.7610 | validation: Loss 0.034616 Accuracy -1.1747\n",
      "Epoch 711 | train: Loss 0.006523 Accuracy 0.7608 | validation: Loss 0.036986 Accuracy -1.3236\n",
      "Epoch 712 | train: Loss 0.006555 Accuracy 0.7596 | validation: Loss 0.034097 Accuracy -1.1421\n",
      "Epoch 713 | train: Loss 0.006576 Accuracy 0.7588 | validation: Loss 0.037600 Accuracy -1.3622\n",
      "Epoch 714 | train: Loss 0.006622 Accuracy 0.7571 | validation: Loss 0.033917 Accuracy -1.1308\n",
      "Epoch 715 | train: Loss 0.006607 Accuracy 0.7577 | validation: Loss 0.037457 Accuracy -1.3532\n",
      "Epoch 716 | train: Loss 0.006578 Accuracy 0.7588 | validation: Loss 0.034718 Accuracy -1.1811\n",
      "Epoch 717 | train: Loss 0.006493 Accuracy 0.7619 | validation: Loss 0.036090 Accuracy -1.2673\n",
      "Epoch 718 | train: Loss 0.006448 Accuracy 0.7635 | validation: Loss 0.036088 Accuracy -1.2672\n",
      "Epoch 719 | train: Loss 0.006441 Accuracy 0.7638 | validation: Loss 0.034903 Accuracy -1.1927\n",
      "Epoch 720 | train: Loss 0.006462 Accuracy 0.7630 | validation: Loss 0.037107 Accuracy -1.3312\n",
      "Epoch 721 | train: Loss 0.006497 Accuracy 0.7617 | validation: Loss 0.034579 Accuracy -1.1724\n",
      "Epoch 722 | train: Loss 0.006485 Accuracy 0.7622 | validation: Loss 0.036937 Accuracy -1.3205\n",
      "Epoch 723 | train: Loss 0.006462 Accuracy 0.7630 | validation: Loss 0.035218 Accuracy -1.2125\n",
      "Epoch 724 | train: Loss 0.006413 Accuracy 0.7648 | validation: Loss 0.035836 Accuracy -1.2513\n",
      "Epoch 725 | train: Loss 0.006392 Accuracy 0.7656 | validation: Loss 0.036263 Accuracy -1.2781\n",
      "Epoch 726 | train: Loss 0.006395 Accuracy 0.7655 | validation: Loss 0.035092 Accuracy -1.2046\n",
      "Epoch 727 | train: Loss 0.006404 Accuracy 0.7651 | validation: Loss 0.036907 Accuracy -1.3186\n",
      "Epoch 728 | train: Loss 0.006420 Accuracy 0.7646 | validation: Loss 0.034963 Accuracy -1.1965\n",
      "Epoch 729 | train: Loss 0.006401 Accuracy 0.7653 | validation: Loss 0.036572 Accuracy -1.2976\n",
      "Epoch 730 | train: Loss 0.006384 Accuracy 0.7659 | validation: Loss 0.035566 Accuracy -1.2343\n",
      "Epoch 731 | train: Loss 0.006357 Accuracy 0.7669 | validation: Loss 0.035909 Accuracy -1.2559\n",
      "Epoch 732 | train: Loss 0.006347 Accuracy 0.7672 | validation: Loss 0.036325 Accuracy -1.2821\n",
      "Epoch 733 | train: Loss 0.006349 Accuracy 0.7672 | validation: Loss 0.035352 Accuracy -1.2209\n",
      "Epoch 734 | train: Loss 0.006350 Accuracy 0.7671 | validation: Loss 0.036721 Accuracy -1.3069\n",
      "Epoch 735 | train: Loss 0.006356 Accuracy 0.7669 | validation: Loss 0.035308 Accuracy -1.2182\n",
      "Epoch 736 | train: Loss 0.006342 Accuracy 0.7674 | validation: Loss 0.036585 Accuracy -1.2984\n",
      "Epoch 737 | train: Loss 0.006332 Accuracy 0.7678 | validation: Loss 0.035683 Accuracy -1.2417\n",
      "Epoch 738 | train: Loss 0.006313 Accuracy 0.7685 | validation: Loss 0.036137 Accuracy -1.2703\n",
      "Epoch 739 | train: Loss 0.006302 Accuracy 0.7689 | validation: Loss 0.036179 Accuracy -1.2729\n",
      "Epoch 740 | train: Loss 0.006296 Accuracy 0.7691 | validation: Loss 0.035722 Accuracy -1.2441\n",
      "Epoch 741 | train: Loss 0.006291 Accuracy 0.7693 | validation: Loss 0.036554 Accuracy -1.2964\n",
      "Epoch 742 | train: Loss 0.006293 Accuracy 0.7692 | validation: Loss 0.035547 Accuracy -1.2332\n",
      "Epoch 743 | train: Loss 0.006286 Accuracy 0.7695 | validation: Loss 0.036682 Accuracy -1.3045\n",
      "Epoch 744 | train: Loss 0.006284 Accuracy 0.7695 | validation: Loss 0.035561 Accuracy -1.2340\n",
      "Epoch 745 | train: Loss 0.006271 Accuracy 0.7700 | validation: Loss 0.036593 Accuracy -1.2989\n",
      "Epoch 746 | train: Loss 0.006265 Accuracy 0.7702 | validation: Loss 0.035779 Accuracy -1.2477\n",
      "Epoch 747 | train: Loss 0.006251 Accuracy 0.7707 | validation: Loss 0.036436 Accuracy -1.2890\n",
      "Epoch 748 | train: Loss 0.006244 Accuracy 0.7710 | validation: Loss 0.036014 Accuracy -1.2625\n",
      "Epoch 749 | train: Loss 0.006234 Accuracy 0.7714 | validation: Loss 0.036230 Accuracy -1.2761\n",
      "Epoch 750 | train: Loss 0.006227 Accuracy 0.7716 | validation: Loss 0.036259 Accuracy -1.2779\n",
      "Epoch 751 | train: Loss 0.006220 Accuracy 0.7719 | validation: Loss 0.036100 Accuracy -1.2679\n",
      "Epoch 752 | train: Loss 0.006213 Accuracy 0.7721 | validation: Loss 0.036456 Accuracy -1.2903\n",
      "Epoch 753 | train: Loss 0.006209 Accuracy 0.7723 | validation: Loss 0.035964 Accuracy -1.2593\n",
      "Epoch 754 | train: Loss 0.006202 Accuracy 0.7726 | validation: Loss 0.036637 Accuracy -1.3016\n",
      "Epoch 755 | train: Loss 0.006200 Accuracy 0.7726 | validation: Loss 0.035842 Accuracy -1.2517\n",
      "Epoch 756 | train: Loss 0.006193 Accuracy 0.7729 | validation: Loss 0.036850 Accuracy -1.3150\n",
      "Epoch 757 | train: Loss 0.006194 Accuracy 0.7729 | validation: Loss 0.035662 Accuracy -1.2404\n",
      "Epoch 758 | train: Loss 0.006189 Accuracy 0.7730 | validation: Loss 0.037140 Accuracy -1.3332\n",
      "Epoch 759 | train: Loss 0.006197 Accuracy 0.7727 | validation: Loss 0.035387 Accuracy -1.2231\n",
      "Epoch 760 | train: Loss 0.006197 Accuracy 0.7727 | validation: Loss 0.037527 Accuracy -1.3576\n",
      "Epoch 761 | train: Loss 0.006217 Accuracy 0.7720 | validation: Loss 0.035075 Accuracy -1.2035\n",
      "Epoch 762 | train: Loss 0.006221 Accuracy 0.7718 | validation: Loss 0.037899 Accuracy -1.3809\n",
      "Epoch 763 | train: Loss 0.006243 Accuracy 0.7710 | validation: Loss 0.034970 Accuracy -1.1969\n",
      "Epoch 764 | train: Loss 0.006228 Accuracy 0.7716 | validation: Loss 0.037820 Accuracy -1.3760\n",
      "Epoch 765 | train: Loss 0.006216 Accuracy 0.7720 | validation: Loss 0.035419 Accuracy -1.2251\n",
      "Epoch 766 | train: Loss 0.006168 Accuracy 0.7738 | validation: Loss 0.037162 Accuracy -1.3346\n",
      "Epoch 767 | train: Loss 0.006136 Accuracy 0.7750 | validation: Loss 0.036242 Accuracy -1.2768\n",
      "Epoch 768 | train: Loss 0.006107 Accuracy 0.7760 | validation: Loss 0.036326 Accuracy -1.2821\n",
      "Epoch 769 | train: Loss 0.006098 Accuracy 0.7764 | validation: Loss 0.037033 Accuracy -1.3266\n",
      "Epoch 770 | train: Loss 0.006104 Accuracy 0.7761 | validation: Loss 0.035733 Accuracy -1.2448\n",
      "Epoch 771 | train: Loss 0.006109 Accuracy 0.7760 | validation: Loss 0.037504 Accuracy -1.3561\n",
      "Epoch 772 | train: Loss 0.006122 Accuracy 0.7755 | validation: Loss 0.035518 Accuracy -1.2313\n",
      "Epoch 773 | train: Loss 0.006112 Accuracy 0.7759 | validation: Loss 0.037491 Accuracy -1.3553\n",
      "Epoch 774 | train: Loss 0.006106 Accuracy 0.7761 | validation: Loss 0.035778 Accuracy -1.2477\n",
      "Epoch 775 | train: Loss 0.006077 Accuracy 0.7771 | validation: Loss 0.037092 Accuracy -1.3302\n",
      "Epoch 776 | train: Loss 0.006060 Accuracy 0.7778 | validation: Loss 0.036314 Accuracy -1.2813\n",
      "Epoch 777 | train: Loss 0.006039 Accuracy 0.7785 | validation: Loss 0.036568 Accuracy -1.2973\n",
      "Epoch 778 | train: Loss 0.006031 Accuracy 0.7788 | validation: Loss 0.036859 Accuracy -1.3156\n",
      "Epoch 779 | train: Loss 0.006028 Accuracy 0.7789 | validation: Loss 0.036170 Accuracy -1.2723\n",
      "Epoch 780 | train: Loss 0.006026 Accuracy 0.7790 | validation: Loss 0.037254 Accuracy -1.3404\n",
      "Epoch 781 | train: Loss 0.006031 Accuracy 0.7788 | validation: Loss 0.035972 Accuracy -1.2599\n",
      "Epoch 782 | train: Loss 0.006025 Accuracy 0.7790 | validation: Loss 0.037408 Accuracy -1.3501\n",
      "Epoch 783 | train: Loss 0.006026 Accuracy 0.7790 | validation: Loss 0.036004 Accuracy -1.2619\n",
      "Epoch 784 | train: Loss 0.006012 Accuracy 0.7795 | validation: Loss 0.037345 Accuracy -1.3461\n",
      "Epoch 785 | train: Loss 0.006005 Accuracy 0.7798 | validation: Loss 0.036226 Accuracy -1.2758\n",
      "Epoch 786 | train: Loss 0.005988 Accuracy 0.7804 | validation: Loss 0.037138 Accuracy -1.3331\n",
      "Epoch 787 | train: Loss 0.005979 Accuracy 0.7807 | validation: Loss 0.036510 Accuracy -1.2937\n",
      "Epoch 788 | train: Loss 0.005965 Accuracy 0.7812 | validation: Loss 0.036901 Accuracy -1.3182\n",
      "Epoch 789 | train: Loss 0.005957 Accuracy 0.7815 | validation: Loss 0.036790 Accuracy -1.3112\n",
      "Epoch 790 | train: Loss 0.005948 Accuracy 0.7819 | validation: Loss 0.036707 Accuracy -1.3060\n",
      "Epoch 791 | train: Loss 0.005941 Accuracy 0.7821 | validation: Loss 0.037017 Accuracy -1.3255\n",
      "Epoch 792 | train: Loss 0.005937 Accuracy 0.7823 | validation: Loss 0.036544 Accuracy -1.2958\n",
      "Epoch 793 | train: Loss 0.005931 Accuracy 0.7825 | validation: Loss 0.037231 Accuracy -1.3390\n",
      "Epoch 794 | train: Loss 0.005929 Accuracy 0.7826 | validation: Loss 0.036398 Accuracy -1.2866\n",
      "Epoch 795 | train: Loss 0.005924 Accuracy 0.7828 | validation: Loss 0.037474 Accuracy -1.3542\n",
      "Epoch 796 | train: Loss 0.005927 Accuracy 0.7826 | validation: Loss 0.036210 Accuracy -1.2748\n",
      "Epoch 797 | train: Loss 0.005924 Accuracy 0.7828 | validation: Loss 0.037781 Accuracy -1.3735\n",
      "Epoch 798 | train: Loss 0.005934 Accuracy 0.7824 | validation: Loss 0.035974 Accuracy -1.2600\n",
      "Epoch 799 | train: Loss 0.005936 Accuracy 0.7823 | validation: Loss 0.038138 Accuracy -1.3959\n",
      "Epoch 800 | train: Loss 0.005954 Accuracy 0.7816 | validation: Loss 0.035771 Accuracy -1.2472\n",
      "Epoch 801 | train: Loss 0.005953 Accuracy 0.7817 | validation: Loss 0.038356 Accuracy -1.4096\n",
      "Epoch 802 | train: Loss 0.005963 Accuracy 0.7813 | validation: Loss 0.035846 Accuracy -1.2519\n",
      "Epoch 803 | train: Loss 0.005940 Accuracy 0.7822 | validation: Loss 0.038171 Accuracy -1.3980\n",
      "Epoch 804 | train: Loss 0.005922 Accuracy 0.7828 | validation: Loss 0.036322 Accuracy -1.2818\n",
      "Epoch 805 | train: Loss 0.005883 Accuracy 0.7842 | validation: Loss 0.037619 Accuracy -1.3633\n",
      "Epoch 806 | train: Loss 0.005860 Accuracy 0.7851 | validation: Loss 0.036969 Accuracy -1.3225\n",
      "Epoch 807 | train: Loss 0.005839 Accuracy 0.7859 | validation: Loss 0.037031 Accuracy -1.3264\n",
      "Epoch 808 | train: Loss 0.005831 Accuracy 0.7862 | validation: Loss 0.037538 Accuracy -1.3582\n",
      "Epoch 809 | train: Loss 0.005832 Accuracy 0.7861 | validation: Loss 0.036568 Accuracy -1.2973\n",
      "Epoch 810 | train: Loss 0.005833 Accuracy 0.7861 | validation: Loss 0.037937 Accuracy -1.3833\n",
      "Epoch 811 | train: Loss 0.005844 Accuracy 0.7857 | validation: Loss 0.036291 Accuracy -1.2799\n",
      "Epoch 812 | train: Loss 0.005842 Accuracy 0.7858 | validation: Loss 0.038143 Accuracy -1.3962\n",
      "Epoch 813 | train: Loss 0.005849 Accuracy 0.7855 | validation: Loss 0.036276 Accuracy -1.2789\n",
      "Epoch 814 | train: Loss 0.005834 Accuracy 0.7860 | validation: Loss 0.038098 Accuracy -1.3934\n",
      "Epoch 815 | train: Loss 0.005828 Accuracy 0.7863 | validation: Loss 0.036528 Accuracy -1.2948\n",
      "Epoch 816 | train: Loss 0.005804 Accuracy 0.7871 | validation: Loss 0.037830 Accuracy -1.3766\n",
      "Epoch 817 | train: Loss 0.005791 Accuracy 0.7876 | validation: Loss 0.036947 Accuracy -1.3211\n",
      "Epoch 818 | train: Loss 0.005772 Accuracy 0.7883 | validation: Loss 0.037502 Accuracy -1.3560\n",
      "Epoch 819 | train: Loss 0.005762 Accuracy 0.7887 | validation: Loss 0.037364 Accuracy -1.3473\n",
      "Epoch 820 | train: Loss 0.005754 Accuracy 0.7890 | validation: Loss 0.037200 Accuracy -1.3370\n",
      "Epoch 821 | train: Loss 0.005748 Accuracy 0.7892 | validation: Loss 0.037701 Accuracy -1.3685\n",
      "Epoch 822 | train: Loss 0.005747 Accuracy 0.7892 | validation: Loss 0.036968 Accuracy -1.3224\n",
      "Epoch 823 | train: Loss 0.005743 Accuracy 0.7894 | validation: Loss 0.037965 Accuracy -1.3851\n",
      "Epoch 824 | train: Loss 0.005746 Accuracy 0.7893 | validation: Loss 0.036779 Accuracy -1.3106\n",
      "Epoch 825 | train: Loss 0.005742 Accuracy 0.7894 | validation: Loss 0.038180 Accuracy -1.3985\n",
      "Epoch 826 | train: Loss 0.005749 Accuracy 0.7892 | validation: Loss 0.036637 Accuracy -1.3016\n",
      "Epoch 827 | train: Loss 0.005744 Accuracy 0.7893 | validation: Loss 0.038356 Accuracy -1.4096\n",
      "Epoch 828 | train: Loss 0.005751 Accuracy 0.7891 | validation: Loss 0.036573 Accuracy -1.2976\n",
      "Epoch 829 | train: Loss 0.005743 Accuracy 0.7894 | validation: Loss 0.038439 Accuracy -1.4149\n",
      "Epoch 830 | train: Loss 0.005746 Accuracy 0.7893 | validation: Loss 0.036647 Accuracy -1.3023\n",
      "Epoch 831 | train: Loss 0.005729 Accuracy 0.7899 | validation: Loss 0.038372 Accuracy -1.4106\n",
      "Epoch 832 | train: Loss 0.005723 Accuracy 0.7901 | validation: Loss 0.036877 Accuracy -1.3167\n",
      "Epoch 833 | train: Loss 0.005701 Accuracy 0.7909 | validation: Loss 0.038176 Accuracy -1.3983\n",
      "Epoch 834 | train: Loss 0.005690 Accuracy 0.7913 | validation: Loss 0.037181 Accuracy -1.3358\n",
      "Epoch 835 | train: Loss 0.005671 Accuracy 0.7920 | validation: Loss 0.037934 Accuracy -1.3831\n",
      "Epoch 836 | train: Loss 0.005661 Accuracy 0.7924 | validation: Loss 0.037464 Accuracy -1.3536\n",
      "Epoch 837 | train: Loss 0.005649 Accuracy 0.7928 | validation: Loss 0.037726 Accuracy -1.3700\n",
      "Epoch 838 | train: Loss 0.005641 Accuracy 0.7931 | validation: Loss 0.037684 Accuracy -1.3674\n",
      "Epoch 839 | train: Loss 0.005634 Accuracy 0.7934 | validation: Loss 0.037554 Accuracy -1.3593\n",
      "Epoch 840 | train: Loss 0.005627 Accuracy 0.7936 | validation: Loss 0.037867 Accuracy -1.3789\n",
      "Epoch 841 | train: Loss 0.005623 Accuracy 0.7938 | validation: Loss 0.037400 Accuracy -1.3496\n",
      "Epoch 842 | train: Loss 0.005618 Accuracy 0.7940 | validation: Loss 0.038073 Accuracy -1.3919\n",
      "Epoch 843 | train: Loss 0.005619 Accuracy 0.7939 | validation: Loss 0.037204 Accuracy -1.3373\n",
      "Epoch 844 | train: Loss 0.005617 Accuracy 0.7940 | validation: Loss 0.038374 Accuracy -1.4108\n",
      "Epoch 845 | train: Loss 0.005628 Accuracy 0.7936 | validation: Loss 0.036909 Accuracy -1.3187\n",
      "Epoch 846 | train: Loss 0.005635 Accuracy 0.7933 | validation: Loss 0.038822 Accuracy -1.4389\n",
      "Epoch 847 | train: Loss 0.005666 Accuracy 0.7922 | validation: Loss 0.036543 Accuracy -1.2957\n",
      "Epoch 848 | train: Loss 0.005684 Accuracy 0.7916 | validation: Loss 0.039258 Accuracy -1.4663\n",
      "Epoch 849 | train: Loss 0.005719 Accuracy 0.7903 | validation: Loss 0.036467 Accuracy -1.2910\n",
      "Epoch 850 | train: Loss 0.005704 Accuracy 0.7908 | validation: Loss 0.039173 Accuracy -1.4610\n",
      "Epoch 851 | train: Loss 0.005680 Accuracy 0.7917 | validation: Loss 0.037015 Accuracy -1.3254\n",
      "Epoch 852 | train: Loss 0.005614 Accuracy 0.7941 | validation: Loss 0.038409 Accuracy -1.4130\n",
      "Epoch 853 | train: Loss 0.005574 Accuracy 0.7956 | validation: Loss 0.037851 Accuracy -1.3779\n",
      "Epoch 854 | train: Loss 0.005548 Accuracy 0.7965 | validation: Loss 0.037655 Accuracy -1.3656\n",
      "Epoch 855 | train: Loss 0.005546 Accuracy 0.7966 | validation: Loss 0.038547 Accuracy -1.4217\n",
      "Epoch 856 | train: Loss 0.005563 Accuracy 0.7960 | validation: Loss 0.037085 Accuracy -1.3298\n",
      "Epoch 857 | train: Loss 0.005575 Accuracy 0.7955 | validation: Loss 0.038902 Accuracy -1.4440\n",
      "Epoch 858 | train: Loss 0.005597 Accuracy 0.7948 | validation: Loss 0.036942 Accuracy -1.3208\n",
      "Epoch 859 | train: Loss 0.005581 Accuracy 0.7953 | validation: Loss 0.038783 Accuracy -1.4364\n",
      "Epoch 860 | train: Loss 0.005569 Accuracy 0.7958 | validation: Loss 0.037268 Accuracy -1.3413\n",
      "Epoch 861 | train: Loss 0.005530 Accuracy 0.7972 | validation: Loss 0.038303 Accuracy -1.4063\n",
      "Epoch 862 | train: Loss 0.005509 Accuracy 0.7980 | validation: Loss 0.037794 Accuracy -1.3743\n",
      "Epoch 863 | train: Loss 0.005491 Accuracy 0.7986 | validation: Loss 0.037813 Accuracy -1.3756\n",
      "Epoch 864 | train: Loss 0.005485 Accuracy 0.7988 | validation: Loss 0.038301 Accuracy -1.4062\n",
      "Epoch 865 | train: Loss 0.005489 Accuracy 0.7987 | validation: Loss 0.037487 Accuracy -1.3551\n",
      "Epoch 866 | train: Loss 0.005492 Accuracy 0.7986 | validation: Loss 0.038634 Accuracy -1.4271\n",
      "Epoch 867 | train: Loss 0.005504 Accuracy 0.7982 | validation: Loss 0.037318 Accuracy -1.3444\n",
      "Epoch 868 | train: Loss 0.005499 Accuracy 0.7983 | validation: Loss 0.038752 Accuracy -1.4345\n",
      "Epoch 869 | train: Loss 0.005502 Accuracy 0.7982 | validation: Loss 0.037406 Accuracy -1.3500\n",
      "Epoch 870 | train: Loss 0.005484 Accuracy 0.7989 | validation: Loss 0.038657 Accuracy -1.4286\n",
      "Epoch 871 | train: Loss 0.005476 Accuracy 0.7992 | validation: Loss 0.037609 Accuracy -1.3627\n",
      "Epoch 872 | train: Loss 0.005455 Accuracy 0.8000 | validation: Loss 0.038413 Accuracy -1.4132\n",
      "Epoch 873 | train: Loss 0.005444 Accuracy 0.8004 | validation: Loss 0.037878 Accuracy -1.3796\n",
      "Epoch 874 | train: Loss 0.005430 Accuracy 0.8009 | validation: Loss 0.038206 Accuracy -1.4002\n",
      "Epoch 875 | train: Loss 0.005422 Accuracy 0.8012 | validation: Loss 0.038085 Accuracy -1.3926\n",
      "Epoch 876 | train: Loss 0.005414 Accuracy 0.8015 | validation: Loss 0.038004 Accuracy -1.3875\n",
      "Epoch 877 | train: Loss 0.005408 Accuracy 0.8017 | validation: Loss 0.038259 Accuracy -1.4035\n",
      "Epoch 878 | train: Loss 0.005405 Accuracy 0.8018 | validation: Loss 0.037858 Accuracy -1.3783\n",
      "Epoch 879 | train: Loss 0.005401 Accuracy 0.8019 | validation: Loss 0.038444 Accuracy -1.4152\n",
      "Epoch 880 | train: Loss 0.005402 Accuracy 0.8019 | validation: Loss 0.037686 Accuracy -1.3675\n",
      "Epoch 881 | train: Loss 0.005400 Accuracy 0.8019 | validation: Loss 0.038688 Accuracy -1.4305\n",
      "Epoch 882 | train: Loss 0.005412 Accuracy 0.8015 | validation: Loss 0.037453 Accuracy -1.3529\n",
      "Epoch 883 | train: Loss 0.005417 Accuracy 0.8013 | validation: Loss 0.039044 Accuracy -1.4528\n",
      "Epoch 884 | train: Loss 0.005447 Accuracy 0.8002 | validation: Loss 0.037190 Accuracy -1.3364\n",
      "Epoch 885 | train: Loss 0.005460 Accuracy 0.7998 | validation: Loss 0.039402 Accuracy -1.4754\n",
      "Epoch 886 | train: Loss 0.005498 Accuracy 0.7984 | validation: Loss 0.037115 Accuracy -1.3317\n",
      "Epoch 887 | train: Loss 0.005483 Accuracy 0.7989 | validation: Loss 0.039382 Accuracy -1.4741\n",
      "Epoch 888 | train: Loss 0.005472 Accuracy 0.7993 | validation: Loss 0.037477 Accuracy -1.3544\n",
      "Epoch 889 | train: Loss 0.005411 Accuracy 0.8016 | validation: Loss 0.038851 Accuracy -1.4407\n",
      "Epoch 890 | train: Loss 0.005373 Accuracy 0.8030 | validation: Loss 0.038044 Accuracy -1.3900\n",
      "Epoch 891 | train: Loss 0.005338 Accuracy 0.8042 | validation: Loss 0.038269 Accuracy -1.4042\n",
      "Epoch 892 | train: Loss 0.005326 Accuracy 0.8047 | validation: Loss 0.038573 Accuracy -1.4232\n",
      "Epoch 893 | train: Loss 0.005328 Accuracy 0.8046 | validation: Loss 0.037800 Accuracy -1.3747\n",
      "Epoch 894 | train: Loss 0.005336 Accuracy 0.8043 | validation: Loss 0.038934 Accuracy -1.4459\n",
      "Epoch 895 | train: Loss 0.005359 Accuracy 0.8035 | validation: Loss 0.037472 Accuracy -1.3541\n",
      "Epoch 896 | train: Loss 0.005364 Accuracy 0.8033 | validation: Loss 0.039116 Accuracy -1.4574\n",
      "Epoch 897 | train: Loss 0.005383 Accuracy 0.8026 | validation: Loss 0.037456 Accuracy -1.3531\n",
      "Epoch 898 | train: Loss 0.005361 Accuracy 0.8034 | validation: Loss 0.039026 Accuracy -1.4517\n",
      "Epoch 899 | train: Loss 0.005352 Accuracy 0.8037 | validation: Loss 0.037691 Accuracy -1.3679\n",
      "Epoch 900 | train: Loss 0.005314 Accuracy 0.8051 | validation: Loss 0.038666 Accuracy -1.4291\n",
      "Epoch 901 | train: Loss 0.005294 Accuracy 0.8058 | validation: Loss 0.038084 Accuracy -1.3926\n",
      "Epoch 902 | train: Loss 0.005272 Accuracy 0.8067 | validation: Loss 0.038382 Accuracy -1.4113\n",
      "Epoch 903 | train: Loss 0.005262 Accuracy 0.8070 | validation: Loss 0.038440 Accuracy -1.4149\n",
      "Epoch 904 | train: Loss 0.005258 Accuracy 0.8072 | validation: Loss 0.038095 Accuracy -1.3932\n",
      "Epoch 905 | train: Loss 0.005256 Accuracy 0.8072 | validation: Loss 0.038702 Accuracy -1.4314\n",
      "Epoch 906 | train: Loss 0.005263 Accuracy 0.8070 | validation: Loss 0.037889 Accuracy -1.3803\n",
      "Epoch 907 | train: Loss 0.005264 Accuracy 0.8069 | validation: Loss 0.038949 Accuracy -1.4469\n",
      "Epoch 908 | train: Loss 0.005280 Accuracy 0.8064 | validation: Loss 0.037712 Accuracy -1.3691\n",
      "Epoch 909 | train: Loss 0.005280 Accuracy 0.8064 | validation: Loss 0.039112 Accuracy -1.4571\n",
      "Epoch 910 | train: Loss 0.005299 Accuracy 0.8057 | validation: Loss 0.037624 Accuracy -1.3637\n",
      "Epoch 911 | train: Loss 0.005289 Accuracy 0.8060 | validation: Loss 0.039173 Accuracy -1.4610\n",
      "Epoch 912 | train: Loss 0.005298 Accuracy 0.8057 | validation: Loss 0.037707 Accuracy -1.3689\n",
      "Epoch 913 | train: Loss 0.005271 Accuracy 0.8067 | validation: Loss 0.039054 Accuracy -1.4535\n",
      "Epoch 914 | train: Loss 0.005261 Accuracy 0.8070 | validation: Loss 0.037902 Accuracy -1.3811\n",
      "Epoch 915 | train: Loss 0.005230 Accuracy 0.8082 | validation: Loss 0.038810 Accuracy -1.4381\n",
      "Epoch 916 | train: Loss 0.005215 Accuracy 0.8087 | validation: Loss 0.038152 Accuracy -1.3968\n",
      "Epoch 917 | train: Loss 0.005194 Accuracy 0.8095 | validation: Loss 0.038625 Accuracy -1.4266\n",
      "Epoch 918 | train: Loss 0.005184 Accuracy 0.8099 | validation: Loss 0.038343 Accuracy -1.4088\n",
      "Epoch 919 | train: Loss 0.005173 Accuracy 0.8103 | validation: Loss 0.038440 Accuracy -1.4149\n",
      "Epoch 920 | train: Loss 0.005166 Accuracy 0.8106 | validation: Loss 0.038474 Accuracy -1.4170\n",
      "Epoch 921 | train: Loss 0.005160 Accuracy 0.8108 | validation: Loss 0.038329 Accuracy -1.4079\n",
      "Epoch 922 | train: Loss 0.005155 Accuracy 0.8110 | validation: Loss 0.038614 Accuracy -1.4258\n",
      "Epoch 923 | train: Loss 0.005153 Accuracy 0.8110 | validation: Loss 0.038185 Accuracy -1.3989\n",
      "Epoch 924 | train: Loss 0.005150 Accuracy 0.8111 | validation: Loss 0.038780 Accuracy -1.4363\n",
      "Epoch 925 | train: Loss 0.005158 Accuracy 0.8108 | validation: Loss 0.037991 Accuracy -1.3867\n",
      "Epoch 926 | train: Loss 0.005164 Accuracy 0.8106 | validation: Loss 0.039105 Accuracy -1.4567\n",
      "Epoch 927 | train: Loss 0.005198 Accuracy 0.8094 | validation: Loss 0.037695 Accuracy -1.3681\n",
      "Epoch 928 | train: Loss 0.005226 Accuracy 0.8083 | validation: Loss 0.039554 Accuracy -1.4849\n",
      "Epoch 929 | train: Loss 0.005305 Accuracy 0.8055 | validation: Loss 0.037504 Accuracy -1.3561\n",
      "Epoch 930 | train: Loss 0.005320 Accuracy 0.8049 | validation: Loss 0.039815 Accuracy -1.5013\n",
      "Epoch 931 | train: Loss 0.005337 Accuracy 0.8043 | validation: Loss 0.037793 Accuracy -1.3743\n",
      "Epoch 932 | train: Loss 0.005234 Accuracy 0.8080 | validation: Loss 0.039188 Accuracy -1.4619\n",
      "Epoch 933 | train: Loss 0.005166 Accuracy 0.8106 | validation: Loss 0.038320 Accuracy -1.4074\n",
      "Epoch 934 | train: Loss 0.005103 Accuracy 0.8128 | validation: Loss 0.038487 Accuracy -1.4178\n",
      "Epoch 935 | train: Loss 0.005089 Accuracy 0.8134 | validation: Loss 0.038983 Accuracy -1.4491\n",
      "Epoch 936 | train: Loss 0.005109 Accuracy 0.8126 | validation: Loss 0.038057 Accuracy -1.3909\n",
      "Epoch 937 | train: Loss 0.005136 Accuracy 0.8117 | validation: Loss 0.039367 Accuracy -1.4731\n",
      "Epoch 938 | train: Loss 0.005187 Accuracy 0.8098 | validation: Loss 0.037830 Accuracy -1.3766\n",
      "Epoch 939 | train: Loss 0.005173 Accuracy 0.8103 | validation: Loss 0.039285 Accuracy -1.4680\n",
      "Epoch 940 | train: Loss 0.005168 Accuracy 0.8105 | validation: Loss 0.038050 Accuracy -1.3904\n",
      "Epoch 941 | train: Loss 0.005104 Accuracy 0.8128 | validation: Loss 0.038880 Accuracy -1.4426\n",
      "Epoch 942 | train: Loss 0.005069 Accuracy 0.8141 | validation: Loss 0.038424 Accuracy -1.4139\n",
      "Epoch 943 | train: Loss 0.005042 Accuracy 0.8151 | validation: Loss 0.038406 Accuracy -1.4128\n",
      "Epoch 944 | train: Loss 0.005038 Accuracy 0.8153 | validation: Loss 0.038823 Accuracy -1.4390\n",
      "Epoch 945 | train: Loss 0.005050 Accuracy 0.8148 | validation: Loss 0.038187 Accuracy -1.3990\n",
      "Epoch 946 | train: Loss 0.005061 Accuracy 0.8144 | validation: Loss 0.039153 Accuracy -1.4597\n",
      "Epoch 947 | train: Loss 0.005089 Accuracy 0.8134 | validation: Loss 0.038048 Accuracy -1.3903\n",
      "Epoch 948 | train: Loss 0.005082 Accuracy 0.8136 | validation: Loss 0.039155 Accuracy -1.4598\n",
      "Epoch 949 | train: Loss 0.005091 Accuracy 0.8133 | validation: Loss 0.038147 Accuracy -1.3965\n",
      "Epoch 950 | train: Loss 0.005059 Accuracy 0.8145 | validation: Loss 0.039087 Accuracy -1.4556\n",
      "Epoch 951 | train: Loss 0.005045 Accuracy 0.8150 | validation: Loss 0.038332 Accuracy -1.4081\n",
      "Epoch 952 | train: Loss 0.005013 Accuracy 0.8162 | validation: Loss 0.038799 Accuracy -1.4374\n",
      "Epoch 953 | train: Loss 0.004999 Accuracy 0.8167 | validation: Loss 0.038521 Accuracy -1.4200\n",
      "Epoch 954 | train: Loss 0.004984 Accuracy 0.8172 | validation: Loss 0.038656 Accuracy -1.4285\n",
      "Epoch 955 | train: Loss 0.004976 Accuracy 0.8175 | validation: Loss 0.038719 Accuracy -1.4324\n",
      "Epoch 956 | train: Loss 0.004972 Accuracy 0.8177 | validation: Loss 0.038496 Accuracy -1.4184\n",
      "Epoch 957 | train: Loss 0.004969 Accuracy 0.8178 | validation: Loss 0.038832 Accuracy -1.4395\n",
      "Epoch 958 | train: Loss 0.004973 Accuracy 0.8176 | validation: Loss 0.038343 Accuracy -1.4088\n",
      "Epoch 959 | train: Loss 0.004974 Accuracy 0.8176 | validation: Loss 0.039030 Accuracy -1.4520\n",
      "Epoch 960 | train: Loss 0.004992 Accuracy 0.8169 | validation: Loss 0.038209 Accuracy -1.4004\n",
      "Epoch 961 | train: Loss 0.005002 Accuracy 0.8166 | validation: Loss 0.039296 Accuracy -1.4687\n",
      "Epoch 962 | train: Loss 0.005048 Accuracy 0.8149 | validation: Loss 0.038046 Accuracy -1.3902\n",
      "Epoch 963 | train: Loss 0.005061 Accuracy 0.8144 | validation: Loss 0.039528 Accuracy -1.4833\n",
      "Epoch 964 | train: Loss 0.005117 Accuracy 0.8123 | validation: Loss 0.038100 Accuracy -1.3936\n",
      "Epoch 965 | train: Loss 0.005087 Accuracy 0.8135 | validation: Loss 0.039591 Accuracy -1.4873\n",
      "Epoch 966 | train: Loss 0.005081 Accuracy 0.8137 | validation: Loss 0.038274 Accuracy -1.4045\n",
      "Epoch 967 | train: Loss 0.004995 Accuracy 0.8168 | validation: Loss 0.039063 Accuracy -1.4541\n",
      "Epoch 968 | train: Loss 0.004953 Accuracy 0.8184 | validation: Loss 0.038599 Accuracy -1.4249\n",
      "Epoch 969 | train: Loss 0.004914 Accuracy 0.8198 | validation: Loss 0.038800 Accuracy -1.4375\n",
      "Epoch 970 | train: Loss 0.004901 Accuracy 0.8203 | validation: Loss 0.038962 Accuracy -1.4477\n",
      "Epoch 971 | train: Loss 0.004904 Accuracy 0.8202 | validation: Loss 0.038477 Accuracy -1.4172\n",
      "Epoch 972 | train: Loss 0.004913 Accuracy 0.8198 | validation: Loss 0.039143 Accuracy -1.4591\n",
      "Epoch 973 | train: Loss 0.004945 Accuracy 0.8187 | validation: Loss 0.038280 Accuracy -1.4049\n",
      "Epoch 974 | train: Loss 0.004954 Accuracy 0.8183 | validation: Loss 0.039343 Accuracy -1.4716\n",
      "Epoch 975 | train: Loss 0.004994 Accuracy 0.8169 | validation: Loss 0.038278 Accuracy -1.4047\n",
      "Epoch 976 | train: Loss 0.004972 Accuracy 0.8177 | validation: Loss 0.039375 Accuracy -1.4736\n",
      "Epoch 977 | train: Loss 0.004979 Accuracy 0.8174 | validation: Loss 0.038344 Accuracy -1.4089\n",
      "Epoch 978 | train: Loss 0.004925 Accuracy 0.8194 | validation: Loss 0.039070 Accuracy -1.4545\n",
      "Epoch 979 | train: Loss 0.004902 Accuracy 0.8202 | validation: Loss 0.038548 Accuracy -1.4217\n",
      "Epoch 980 | train: Loss 0.004866 Accuracy 0.8216 | validation: Loss 0.038956 Accuracy -1.4473\n",
      "Epoch 981 | train: Loss 0.004850 Accuracy 0.8221 | validation: Loss 0.038799 Accuracy -1.4375\n",
      "Epoch 982 | train: Loss 0.004837 Accuracy 0.8226 | validation: Loss 0.038727 Accuracy -1.4330\n",
      "Epoch 983 | train: Loss 0.004832 Accuracy 0.8228 | validation: Loss 0.038905 Accuracy -1.4441\n",
      "Epoch 984 | train: Loss 0.004835 Accuracy 0.8227 | validation: Loss 0.038609 Accuracy -1.4255\n",
      "Epoch 985 | train: Loss 0.004836 Accuracy 0.8227 | validation: Loss 0.039127 Accuracy -1.4581\n",
      "Epoch 986 | train: Loss 0.004853 Accuracy 0.8220 | validation: Loss 0.038471 Accuracy -1.4169\n",
      "Epoch 987 | train: Loss 0.004861 Accuracy 0.8217 | validation: Loss 0.039267 Accuracy -1.4669\n",
      "Epoch 988 | train: Loss 0.004904 Accuracy 0.8201 | validation: Loss 0.038342 Accuracy -1.4087\n",
      "Epoch 989 | train: Loss 0.004916 Accuracy 0.8197 | validation: Loss 0.039512 Accuracy -1.4823\n",
      "Epoch 990 | train: Loss 0.004981 Accuracy 0.8173 | validation: Loss 0.038365 Accuracy -1.4102\n",
      "Epoch 991 | train: Loss 0.004958 Accuracy 0.8182 | validation: Loss 0.039602 Accuracy -1.4879\n",
      "Epoch 992 | train: Loss 0.004975 Accuracy 0.8176 | validation: Loss 0.038471 Accuracy -1.4169\n",
      "Epoch 993 | train: Loss 0.004888 Accuracy 0.8207 | validation: Loss 0.039242 Accuracy -1.4653\n",
      "Epoch 994 | train: Loss 0.004848 Accuracy 0.8222 | validation: Loss 0.038649 Accuracy -1.4280\n",
      "Epoch 995 | train: Loss 0.004792 Accuracy 0.8243 | validation: Loss 0.038955 Accuracy -1.4473\n",
      "Epoch 996 | train: Loss 0.004770 Accuracy 0.8251 | validation: Loss 0.038980 Accuracy -1.4488\n",
      "Epoch 997 | train: Loss 0.004763 Accuracy 0.8253 | validation: Loss 0.038762 Accuracy -1.4352\n",
      "Epoch 998 | train: Loss 0.004768 Accuracy 0.8251 | validation: Loss 0.039165 Accuracy -1.4604\n",
      "Epoch 999 | train: Loss 0.004792 Accuracy 0.8243 | validation: Loss 0.038556 Accuracy -1.4222\n",
      "Epoch 1000 | train: Loss 0.004803 Accuracy 0.8238 | validation: Loss 0.039319 Accuracy -1.4701\n",
      "Epoch 1001 | train: Loss 0.004846 Accuracy 0.8223 | validation: Loss 0.038545 Accuracy -1.4215\n",
      "Epoch 1002 | train: Loss 0.004839 Accuracy 0.8225 | validation: Loss 0.039466 Accuracy -1.4794\n",
      "Epoch 1003 | train: Loss 0.004868 Accuracy 0.8215 | validation: Loss 0.038567 Accuracy -1.4229\n",
      "Epoch 1004 | train: Loss 0.004822 Accuracy 0.8232 | validation: Loss 0.039292 Accuracy -1.4685\n",
      "Epoch 1005 | train: Loss 0.004811 Accuracy 0.8236 | validation: Loss 0.038647 Accuracy -1.4279\n",
      "Epoch 1006 | train: Loss 0.004760 Accuracy 0.8254 | validation: Loss 0.039131 Accuracy -1.4583\n",
      "Epoch 1007 | train: Loss 0.004739 Accuracy 0.8262 | validation: Loss 0.038826 Accuracy -1.4391\n",
      "Epoch 1008 | train: Loss 0.004712 Accuracy 0.8272 | validation: Loss 0.038995 Accuracy -1.4498\n",
      "Epoch 1009 | train: Loss 0.004702 Accuracy 0.8276 | validation: Loss 0.038961 Accuracy -1.4476\n",
      "Epoch 1010 | train: Loss 0.004695 Accuracy 0.8278 | validation: Loss 0.038857 Accuracy -1.4411\n",
      "Epoch 1011 | train: Loss 0.004692 Accuracy 0.8279 | validation: Loss 0.039067 Accuracy -1.4543\n",
      "Epoch 1012 | train: Loss 0.004697 Accuracy 0.8278 | validation: Loss 0.038777 Accuracy -1.4361\n",
      "Epoch 1013 | train: Loss 0.004699 Accuracy 0.8277 | validation: Loss 0.039222 Accuracy -1.4641\n",
      "Epoch 1014 | train: Loss 0.004722 Accuracy 0.8268 | validation: Loss 0.038665 Accuracy -1.4291\n",
      "Epoch 1015 | train: Loss 0.004735 Accuracy 0.8264 | validation: Loss 0.039374 Accuracy -1.4736\n",
      "Epoch 1016 | train: Loss 0.004797 Accuracy 0.8241 | validation: Loss 0.038605 Accuracy -1.4253\n",
      "Epoch 1017 | train: Loss 0.004818 Accuracy 0.8233 | validation: Loss 0.039676 Accuracy -1.4926\n",
      "Epoch 1018 | train: Loss 0.004913 Accuracy 0.8198 | validation: Loss 0.038690 Accuracy -1.4306\n",
      "Epoch 1019 | train: Loss 0.004885 Accuracy 0.8208 | validation: Loss 0.039778 Accuracy -1.4990\n",
      "Epoch 1020 | train: Loss 0.004894 Accuracy 0.8205 | validation: Loss 0.038756 Accuracy -1.4348\n",
      "Epoch 1021 | train: Loss 0.004761 Accuracy 0.8254 | validation: Loss 0.039260 Accuracy -1.4664\n",
      "Epoch 1022 | train: Loss 0.004697 Accuracy 0.8278 | validation: Loss 0.038870 Accuracy -1.4419\n",
      "Epoch 1023 | train: Loss 0.004639 Accuracy 0.8299 | validation: Loss 0.038960 Accuracy -1.4476\n",
      "Epoch 1024 | train: Loss 0.004630 Accuracy 0.8302 | validation: Loss 0.039276 Accuracy -1.4674\n",
      "Epoch 1025 | train: Loss 0.004656 Accuracy 0.8293 | validation: Loss 0.038850 Accuracy -1.4407\n",
      "Epoch 1026 | train: Loss 0.004681 Accuracy 0.8283 | validation: Loss 0.039467 Accuracy -1.4794\n",
      "Epoch 1027 | train: Loss 0.004738 Accuracy 0.8262 | validation: Loss 0.038798 Accuracy -1.4374\n",
      "Epoch 1028 | train: Loss 0.004721 Accuracy 0.8269 | validation: Loss 0.039460 Accuracy -1.4790\n",
      "Epoch 1029 | train: Loss 0.004732 Accuracy 0.8265 | validation: Loss 0.038845 Accuracy -1.4404\n",
      "Epoch 1030 | train: Loss 0.004666 Accuracy 0.8289 | validation: Loss 0.039282 Accuracy -1.4678\n",
      "Epoch 1031 | train: Loss 0.004635 Accuracy 0.8300 | validation: Loss 0.038935 Accuracy -1.4460\n",
      "Epoch 1032 | train: Loss 0.004595 Accuracy 0.8315 | validation: Loss 0.039024 Accuracy -1.4516\n",
      "Epoch 1033 | train: Loss 0.004581 Accuracy 0.8320 | validation: Loss 0.039084 Accuracy -1.4554\n",
      "Epoch 1034 | train: Loss 0.004581 Accuracy 0.8320 | validation: Loss 0.038950 Accuracy -1.4470\n",
      "Epoch 1035 | train: Loss 0.004587 Accuracy 0.8318 | validation: Loss 0.039308 Accuracy -1.4694\n",
      "Epoch 1036 | train: Loss 0.004611 Accuracy 0.8309 | validation: Loss 0.038884 Accuracy -1.4428\n",
      "Epoch 1037 | train: Loss 0.004619 Accuracy 0.8306 | validation: Loss 0.039348 Accuracy -1.4719\n",
      "Epoch 1038 | train: Loss 0.004657 Accuracy 0.8292 | validation: Loss 0.038839 Accuracy -1.4400\n",
      "Epoch 1039 | train: Loss 0.004652 Accuracy 0.8294 | validation: Loss 0.039486 Accuracy -1.4806\n",
      "Epoch 1040 | train: Loss 0.004691 Accuracy 0.8280 | validation: Loss 0.038907 Accuracy -1.4443\n",
      "Epoch 1041 | train: Loss 0.004659 Accuracy 0.8291 | validation: Loss 0.039489 Accuracy -1.4808\n",
      "Epoch 1042 | train: Loss 0.004674 Accuracy 0.8286 | validation: Loss 0.038882 Accuracy -1.4427\n",
      "Epoch 1043 | train: Loss 0.004620 Accuracy 0.8306 | validation: Loss 0.039324 Accuracy -1.4704\n",
      "Epoch 1044 | train: Loss 0.004608 Accuracy 0.8310 | validation: Loss 0.038945 Accuracy -1.4467\n",
      "Epoch 1045 | train: Loss 0.004562 Accuracy 0.8327 | validation: Loss 0.039274 Accuracy -1.4673\n",
      "Epoch 1046 | train: Loss 0.004545 Accuracy 0.8333 | validation: Loss 0.039051 Accuracy -1.4533\n",
      "Epoch 1047 | train: Loss 0.004521 Accuracy 0.8342 | validation: Loss 0.039170 Accuracy -1.4607\n",
      "Epoch 1048 | train: Loss 0.004511 Accuracy 0.8346 | validation: Loss 0.039114 Accuracy -1.4573\n",
      "Epoch 1049 | train: Loss 0.004502 Accuracy 0.8349 | validation: Loss 0.039131 Accuracy -1.4583\n",
      "Epoch 1050 | train: Loss 0.004498 Accuracy 0.8351 | validation: Loss 0.039214 Accuracy -1.4636\n",
      "Epoch 1051 | train: Loss 0.004496 Accuracy 0.8351 | validation: Loss 0.039112 Accuracy -1.4572\n",
      "Epoch 1052 | train: Loss 0.004494 Accuracy 0.8352 | validation: Loss 0.039283 Accuracy -1.4679\n",
      "Epoch 1053 | train: Loss 0.004502 Accuracy 0.8349 | validation: Loss 0.039044 Accuracy -1.4529\n",
      "Epoch 1054 | train: Loss 0.004507 Accuracy 0.8347 | validation: Loss 0.039367 Accuracy -1.4732\n",
      "Epoch 1055 | train: Loss 0.004543 Accuracy 0.8334 | validation: Loss 0.038990 Accuracy -1.4494\n",
      "Epoch 1056 | train: Loss 0.004573 Accuracy 0.8323 | validation: Loss 0.039602 Accuracy -1.4879\n",
      "Epoch 1057 | train: Loss 0.004695 Accuracy 0.8278 | validation: Loss 0.039039 Accuracy -1.4525\n",
      "Epoch 1058 | train: Loss 0.004777 Accuracy 0.8248 | validation: Loss 0.040069 Accuracy -1.5172\n",
      "Epoch 1059 | train: Loss 0.004961 Accuracy 0.8181 | validation: Loss 0.039315 Accuracy -1.4699\n",
      "Epoch 1060 | train: Loss 0.004951 Accuracy 0.8184 | validation: Loss 0.040361 Accuracy -1.5356\n",
      "Epoch 1061 | train: Loss 0.004810 Accuracy 0.8236 | validation: Loss 0.039085 Accuracy -1.4555\n",
      "Epoch 1062 | train: Loss 0.004500 Accuracy 0.8350 | validation: Loss 0.038711 Accuracy -1.4320\n",
      "Epoch 1063 | train: Loss 0.004456 Accuracy 0.8366 | validation: Loss 0.039222 Accuracy -1.4640\n",
      "Epoch 1064 | train: Loss 0.004608 Accuracy 0.8310 | validation: Loss 0.039294 Accuracy -1.4686\n",
      "Epoch 1065 | train: Loss 0.004729 Accuracy 0.8266 | validation: Loss 0.040505 Accuracy -1.5447\n",
      "Epoch 1066 | train: Loss 0.004800 Accuracy 0.8240 | validation: Loss 0.039633 Accuracy -1.4898\n",
      "Epoch 1067 | train: Loss 0.004530 Accuracy 0.8339 | validation: Loss 0.039388 Accuracy -1.4745\n",
      "Epoch 1068 | train: Loss 0.004423 Accuracy 0.8378 | validation: Loss 0.039088 Accuracy -1.4557\n",
      "Epoch 1069 | train: Loss 0.004512 Accuracy 0.8345 | validation: Loss 0.038695 Accuracy -1.4309\n",
      "Epoch 1070 | train: Loss 0.004607 Accuracy 0.8310 | validation: Loss 0.039639 Accuracy -1.4902\n",
      "Epoch 1071 | train: Loss 0.004682 Accuracy 0.8283 | validation: Loss 0.039976 Accuracy -1.5114\n",
      "Epoch 1072 | train: Loss 0.004528 Accuracy 0.8340 | validation: Loss 0.040457 Accuracy -1.5416\n",
      "Epoch 1073 | train: Loss 0.004430 Accuracy 0.8375 | validation: Loss 0.039378 Accuracy -1.4739\n",
      "Epoch 1074 | train: Loss 0.004416 Accuracy 0.8380 | validation: Loss 0.038094 Accuracy -1.3932\n",
      "Epoch 1075 | train: Loss 0.004490 Accuracy 0.8354 | validation: Loss 0.038556 Accuracy -1.4222\n",
      "Epoch 1076 | train: Loss 0.004569 Accuracy 0.8324 | validation: Loss 0.039846 Accuracy -1.5032\n",
      "Epoch 1077 | train: Loss 0.004538 Accuracy 0.8336 | validation: Loss 0.041230 Accuracy -1.5902\n",
      "Epoch 1078 | train: Loss 0.004524 Accuracy 0.8341 | validation: Loss 0.039702 Accuracy -1.4942\n",
      "Epoch 1079 | train: Loss 0.004381 Accuracy 0.8393 | validation: Loss 0.038215 Accuracy -1.4008\n",
      "Epoch 1080 | train: Loss 0.004403 Accuracy 0.8385 | validation: Loss 0.038271 Accuracy -1.4043\n",
      "Epoch 1081 | train: Loss 0.004539 Accuracy 0.8335 | validation: Loss 0.039272 Accuracy -1.4672\n",
      "Epoch 1082 | train: Loss 0.004601 Accuracy 0.8313 | validation: Loss 0.040896 Accuracy -1.5692\n",
      "Epoch 1083 | train: Loss 0.004637 Accuracy 0.8299 | validation: Loss 0.039953 Accuracy -1.5100\n",
      "Epoch 1084 | train: Loss 0.004405 Accuracy 0.8385 | validation: Loss 0.039158 Accuracy -1.4600\n",
      "Epoch 1085 | train: Loss 0.004370 Accuracy 0.8397 | validation: Loss 0.038964 Accuracy -1.4478\n",
      "Epoch 1086 | train: Loss 0.004594 Accuracy 0.8315 | validation: Loss 0.038720 Accuracy -1.4325\n",
      "Epoch 1087 | train: Loss 0.004680 Accuracy 0.8284 | validation: Loss 0.039754 Accuracy -1.4974\n",
      "Epoch 1088 | train: Loss 0.004615 Accuracy 0.8307 | validation: Loss 0.040045 Accuracy -1.5158\n",
      "Epoch 1089 | train: Loss 0.004382 Accuracy 0.8393 | validation: Loss 0.040254 Accuracy -1.5289\n",
      "Epoch 1090 | train: Loss 0.004371 Accuracy 0.8397 | validation: Loss 0.040025 Accuracy -1.5145\n",
      "Epoch 1091 | train: Loss 0.004561 Accuracy 0.8327 | validation: Loss 0.038504 Accuracy -1.4189\n",
      "Epoch 1092 | train: Loss 0.004521 Accuracy 0.8342 | validation: Loss 0.038471 Accuracy -1.4169\n",
      "Epoch 1093 | train: Loss 0.004441 Accuracy 0.8371 | validation: Loss 0.039550 Accuracy -1.4846\n",
      "Epoch 1094 | train: Loss 0.004319 Accuracy 0.8416 | validation: Loss 0.040563 Accuracy -1.5483\n",
      "Epoch 1095 | train: Loss 0.004346 Accuracy 0.8406 | validation: Loss 0.040572 Accuracy -1.5488\n",
      "Epoch 1096 | train: Loss 0.004408 Accuracy 0.8383 | validation: Loss 0.038911 Accuracy -1.4445\n",
      "Epoch 1097 | train: Loss 0.004361 Accuracy 0.8401 | validation: Loss 0.038338 Accuracy -1.4085\n",
      "Epoch 1098 | train: Loss 0.004337 Accuracy 0.8409 | validation: Loss 0.039010 Accuracy -1.4507\n",
      "Epoch 1099 | train: Loss 0.004300 Accuracy 0.8423 | validation: Loss 0.039911 Accuracy -1.5074\n",
      "Epoch 1100 | train: Loss 0.004305 Accuracy 0.8421 | validation: Loss 0.040224 Accuracy -1.5270\n",
      "Epoch 1101 | train: Loss 0.004320 Accuracy 0.8416 | validation: Loss 0.039511 Accuracy -1.4822\n",
      "Epoch 1102 | train: Loss 0.004307 Accuracy 0.8421 | validation: Loss 0.039204 Accuracy -1.4629\n",
      "Epoch 1103 | train: Loss 0.004288 Accuracy 0.8428 | validation: Loss 0.039064 Accuracy -1.4541\n",
      "Epoch 1104 | train: Loss 0.004282 Accuracy 0.8430 | validation: Loss 0.039138 Accuracy -1.4587\n",
      "Epoch 1105 | train: Loss 0.004271 Accuracy 0.8434 | validation: Loss 0.039390 Accuracy -1.4746\n",
      "Epoch 1106 | train: Loss 0.004270 Accuracy 0.8434 | validation: Loss 0.039620 Accuracy -1.4891\n",
      "Epoch 1107 | train: Loss 0.004272 Accuracy 0.8433 | validation: Loss 0.039961 Accuracy -1.5105\n",
      "Epoch 1108 | train: Loss 0.004274 Accuracy 0.8433 | validation: Loss 0.039523 Accuracy -1.4830\n",
      "Epoch 1109 | train: Loss 0.004261 Accuracy 0.8437 | validation: Loss 0.039034 Accuracy -1.4522\n",
      "Epoch 1110 | train: Loss 0.004247 Accuracy 0.8442 | validation: Loss 0.038889 Accuracy -1.4431\n",
      "Epoch 1111 | train: Loss 0.004236 Accuracy 0.8447 | validation: Loss 0.039391 Accuracy -1.4747\n",
      "Epoch 1112 | train: Loss 0.004229 Accuracy 0.8449 | validation: Loss 0.039940 Accuracy -1.5091\n",
      "Epoch 1113 | train: Loss 0.004235 Accuracy 0.8447 | validation: Loss 0.039800 Accuracy -1.5004\n",
      "Epoch 1114 | train: Loss 0.004232 Accuracy 0.8448 | validation: Loss 0.039402 Accuracy -1.4754\n",
      "Epoch 1115 | train: Loss 0.004231 Accuracy 0.8448 | validation: Loss 0.039033 Accuracy -1.4522\n",
      "Epoch 1116 | train: Loss 0.004228 Accuracy 0.8449 | validation: Loss 0.039303 Accuracy -1.4691\n",
      "Epoch 1117 | train: Loss 0.004235 Accuracy 0.8447 | validation: Loss 0.039554 Accuracy -1.4849\n",
      "Epoch 1118 | train: Loss 0.004232 Accuracy 0.8448 | validation: Loss 0.039822 Accuracy -1.5017\n",
      "Epoch 1119 | train: Loss 0.004241 Accuracy 0.8445 | validation: Loss 0.039534 Accuracy -1.4837\n",
      "Epoch 1120 | train: Loss 0.004234 Accuracy 0.8447 | validation: Loss 0.039557 Accuracy -1.4851\n",
      "Epoch 1121 | train: Loss 0.004252 Accuracy 0.8441 | validation: Loss 0.039357 Accuracy -1.4725\n",
      "Epoch 1122 | train: Loss 0.004255 Accuracy 0.8440 | validation: Loss 0.039533 Accuracy -1.4836\n",
      "Epoch 1123 | train: Loss 0.004293 Accuracy 0.8425 | validation: Loss 0.039431 Accuracy -1.4771\n",
      "Epoch 1124 | train: Loss 0.004304 Accuracy 0.8422 | validation: Loss 0.039870 Accuracy -1.5048\n",
      "Epoch 1125 | train: Loss 0.004389 Accuracy 0.8390 | validation: Loss 0.039762 Accuracy -1.4979\n",
      "Epoch 1126 | train: Loss 0.004396 Accuracy 0.8388 | validation: Loss 0.040068 Accuracy -1.5172\n",
      "Epoch 1127 | train: Loss 0.004482 Accuracy 0.8356 | validation: Loss 0.039579 Accuracy -1.4864\n",
      "Epoch 1128 | train: Loss 0.004418 Accuracy 0.8380 | validation: Loss 0.039711 Accuracy -1.4948\n",
      "Epoch 1129 | train: Loss 0.004409 Accuracy 0.8383 | validation: Loss 0.039600 Accuracy -1.4878\n",
      "Epoch 1130 | train: Loss 0.004283 Accuracy 0.8429 | validation: Loss 0.039858 Accuracy -1.5040\n",
      "Epoch 1131 | train: Loss 0.004218 Accuracy 0.8453 | validation: Loss 0.039537 Accuracy -1.4838\n",
      "Epoch 1132 | train: Loss 0.004159 Accuracy 0.8475 | validation: Loss 0.039287 Accuracy -1.4681\n",
      "Epoch 1133 | train: Loss 0.004168 Accuracy 0.8472 | validation: Loss 0.039447 Accuracy -1.4782\n",
      "Epoch 1134 | train: Loss 0.004227 Accuracy 0.8450 | validation: Loss 0.039643 Accuracy -1.4905\n",
      "Epoch 1135 | train: Loss 0.004256 Accuracy 0.8439 | validation: Loss 0.040116 Accuracy -1.5202\n",
      "Epoch 1136 | train: Loss 0.004303 Accuracy 0.8422 | validation: Loss 0.039889 Accuracy -1.5060\n",
      "Epoch 1137 | train: Loss 0.004234 Accuracy 0.8447 | validation: Loss 0.039825 Accuracy -1.5019\n",
      "Epoch 1138 | train: Loss 0.004191 Accuracy 0.8463 | validation: Loss 0.039468 Accuracy -1.4795\n",
      "Epoch 1139 | train: Loss 0.004137 Accuracy 0.8483 | validation: Loss 0.039347 Accuracy -1.4719\n",
      "Epoch 1140 | train: Loss 0.004125 Accuracy 0.8487 | validation: Loss 0.039458 Accuracy -1.4789\n",
      "Epoch 1141 | train: Loss 0.004141 Accuracy 0.8482 | validation: Loss 0.039603 Accuracy -1.4880\n",
      "Epoch 1142 | train: Loss 0.004161 Accuracy 0.8474 | validation: Loss 0.039929 Accuracy -1.5084\n",
      "Epoch 1143 | train: Loss 0.004210 Accuracy 0.8456 | validation: Loss 0.039747 Accuracy -1.4970\n",
      "Epoch 1144 | train: Loss 0.004201 Accuracy 0.8459 | validation: Loss 0.039710 Accuracy -1.4947\n",
      "Epoch 1145 | train: Loss 0.004229 Accuracy 0.8449 | validation: Loss 0.039419 Accuracy -1.4764\n",
      "Epoch 1146 | train: Loss 0.004196 Accuracy 0.8461 | validation: Loss 0.039522 Accuracy -1.4829\n",
      "Epoch 1147 | train: Loss 0.004206 Accuracy 0.8458 | validation: Loss 0.039575 Accuracy -1.4862\n",
      "Epoch 1148 | train: Loss 0.004169 Accuracy 0.8471 | validation: Loss 0.039834 Accuracy -1.5025\n",
      "Epoch 1149 | train: Loss 0.004170 Accuracy 0.8471 | validation: Loss 0.039645 Accuracy -1.4906\n",
      "Epoch 1150 | train: Loss 0.004128 Accuracy 0.8486 | validation: Loss 0.039582 Accuracy -1.4867\n",
      "Epoch 1151 | train: Loss 0.004115 Accuracy 0.8491 | validation: Loss 0.039428 Accuracy -1.4770\n",
      "Epoch 1152 | train: Loss 0.004089 Accuracy 0.8500 | validation: Loss 0.039517 Accuracy -1.4826\n",
      "Epoch 1153 | train: Loss 0.004080 Accuracy 0.8504 | validation: Loss 0.039607 Accuracy -1.4882\n",
      "Epoch 1154 | train: Loss 0.004066 Accuracy 0.8509 | validation: Loss 0.039739 Accuracy -1.4965\n",
      "Epoch 1155 | train: Loss 0.004061 Accuracy 0.8511 | validation: Loss 0.039739 Accuracy -1.4965\n",
      "Epoch 1156 | train: Loss 0.004054 Accuracy 0.8513 | validation: Loss 0.039683 Accuracy -1.4930\n",
      "Epoch 1157 | train: Loss 0.004049 Accuracy 0.8515 | validation: Loss 0.039602 Accuracy -1.4879\n",
      "Epoch 1158 | train: Loss 0.004045 Accuracy 0.8517 | validation: Loss 0.039576 Accuracy -1.4863\n",
      "Epoch 1159 | train: Loss 0.004041 Accuracy 0.8518 | validation: Loss 0.039643 Accuracy -1.4905\n",
      "Epoch 1160 | train: Loss 0.004037 Accuracy 0.8519 | validation: Loss 0.039716 Accuracy -1.4950\n",
      "Epoch 1161 | train: Loss 0.004033 Accuracy 0.8521 | validation: Loss 0.039764 Accuracy -1.4981\n",
      "Epoch 1162 | train: Loss 0.004034 Accuracy 0.8521 | validation: Loss 0.039666 Accuracy -1.4920\n",
      "Epoch 1163 | train: Loss 0.004035 Accuracy 0.8520 | validation: Loss 0.039638 Accuracy -1.4901\n",
      "Epoch 1164 | train: Loss 0.004054 Accuracy 0.8513 | validation: Loss 0.039582 Accuracy -1.4867\n",
      "Epoch 1165 | train: Loss 0.004086 Accuracy 0.8501 | validation: Loss 0.039804 Accuracy -1.5006\n",
      "Epoch 1166 | train: Loss 0.004227 Accuracy 0.8450 | validation: Loss 0.039960 Accuracy -1.5104\n",
      "Epoch 1167 | train: Loss 0.004473 Accuracy 0.8360 | validation: Loss 0.040689 Accuracy -1.5562\n",
      "Epoch 1168 | train: Loss 0.005131 Accuracy 0.8118 | validation: Loss 0.040800 Accuracy -1.5632\n",
      "Epoch 1169 | train: Loss 0.006215 Accuracy 0.7721 | validation: Loss 0.043070 Accuracy -1.7058\n",
      "Epoch 1170 | train: Loss 0.004290 Accuracy 0.8427 | validation: Loss 0.040862 Accuracy -1.5671\n",
      "Epoch 1171 | train: Loss 0.004848 Accuracy 0.8222 | validation: Loss 0.038559 Accuracy -1.4224\n",
      "Epoch 1172 | train: Loss 0.006646 Accuracy 0.7563 | validation: Loss 0.040715 Accuracy -1.5578\n",
      "Epoch 1173 | train: Loss 0.004130 Accuracy 0.8485 | validation: Loss 0.042565 Accuracy -1.6740\n",
      "Epoch 1174 | train: Loss 0.005891 Accuracy 0.7839 | validation: Loss 0.040199 Accuracy -1.5254\n",
      "Epoch 1175 | train: Loss 0.007407 Accuracy 0.7284 | validation: Loss 0.043945 Accuracy -1.7607\n",
      "Epoch 1176 | train: Loss 0.005216 Accuracy 0.8087 | validation: Loss 0.047167 Accuracy -1.9632\n",
      "Epoch 1177 | train: Loss 0.009636 Accuracy 0.6466 | validation: Loss 0.045282 Accuracy -1.8448\n",
      "Epoch 1178 | train: Loss 0.004331 Accuracy 0.8412 | validation: Loss 0.040625 Accuracy -1.5522\n",
      "Epoch 1179 | train: Loss 0.008209 Accuracy 0.6989 | validation: Loss 0.044667 Accuracy -1.8061\n",
      "Epoch 1180 | train: Loss 0.004325 Accuracy 0.8414 | validation: Loss 0.044509 Accuracy -1.7962\n",
      "Epoch 1181 | train: Loss 0.008281 Accuracy 0.6963 | validation: Loss 0.041360 Accuracy -1.5983\n",
      "Epoch 1182 | train: Loss 0.005645 Accuracy 0.7930 | validation: Loss 0.040148 Accuracy -1.5222\n",
      "Epoch 1183 | train: Loss 0.007072 Accuracy 0.7407 | validation: Loss 0.044575 Accuracy -1.8004\n",
      "Epoch 1184 | train: Loss 0.004477 Accuracy 0.8358 | validation: Loss 0.044066 Accuracy -1.7683\n",
      "Epoch 1185 | train: Loss 0.006760 Accuracy 0.7521 | validation: Loss 0.042665 Accuracy -1.6803\n",
      "Epoch 1186 | train: Loss 0.005021 Accuracy 0.8158 | validation: Loss 0.042170 Accuracy -1.6492\n",
      "Epoch 1187 | train: Loss 0.005872 Accuracy 0.7847 | validation: Loss 0.044520 Accuracy -1.7969\n",
      "Epoch 1188 | train: Loss 0.004578 Accuracy 0.8321 | validation: Loss 0.042639 Accuracy -1.6787\n",
      "Epoch 1189 | train: Loss 0.005668 Accuracy 0.7921 | validation: Loss 0.040347 Accuracy -1.5347\n",
      "Epoch 1190 | train: Loss 0.004354 Accuracy 0.8403 | validation: Loss 0.039512 Accuracy -1.4823\n",
      "Epoch 1191 | train: Loss 0.005322 Accuracy 0.8048 | validation: Loss 0.040684 Accuracy -1.5559\n",
      "Epoch 1192 | train: Loss 0.004281 Accuracy 0.8430 | validation: Loss 0.041578 Accuracy -1.6120\n",
      "Epoch 1193 | train: Loss 0.005213 Accuracy 0.8088 | validation: Loss 0.041842 Accuracy -1.6287\n",
      "Epoch 1194 | train: Loss 0.004182 Accuracy 0.8466 | validation: Loss 0.041303 Accuracy -1.5948\n",
      "Epoch 1195 | train: Loss 0.005192 Accuracy 0.8096 | validation: Loss 0.042232 Accuracy -1.6531\n",
      "Epoch 1196 | train: Loss 0.004154 Accuracy 0.8476 | validation: Loss 0.041477 Accuracy -1.6057\n",
      "Epoch 1197 | train: Loss 0.004819 Accuracy 0.8233 | validation: Loss 0.040258 Accuracy -1.5291\n",
      "Epoch 1198 | train: Loss 0.004093 Accuracy 0.8499 | validation: Loss 0.039125 Accuracy -1.4579\n",
      "Epoch 1199 | train: Loss 0.004698 Accuracy 0.8277 | validation: Loss 0.039182 Accuracy -1.4615\n",
      "Epoch 1200 | train: Loss 0.004081 Accuracy 0.8503 | validation: Loss 0.039742 Accuracy -1.4967\n",
      "Epoch 1201 | train: Loss 0.004551 Accuracy 0.8331 | validation: Loss 0.040538 Accuracy -1.5467\n",
      "Epoch 1202 | train: Loss 0.004095 Accuracy 0.8498 | validation: Loss 0.040800 Accuracy -1.5632\n",
      "Epoch 1203 | train: Loss 0.004409 Accuracy 0.8383 | validation: Loss 0.041452 Accuracy -1.6041\n",
      "Epoch 1204 | train: Loss 0.004082 Accuracy 0.8503 | validation: Loss 0.041600 Accuracy -1.6134\n",
      "Epoch 1205 | train: Loss 0.004282 Accuracy 0.8430 | validation: Loss 0.040646 Accuracy -1.5535\n",
      "Epoch 1206 | train: Loss 0.004114 Accuracy 0.8491 | validation: Loss 0.039365 Accuracy -1.4730\n",
      "Epoch 1207 | train: Loss 0.004167 Accuracy 0.8472 | validation: Loss 0.038851 Accuracy -1.4407\n",
      "Epoch 1208 | train: Loss 0.004091 Accuracy 0.8500 | validation: Loss 0.039127 Accuracy -1.4581\n",
      "Epoch 1209 | train: Loss 0.004170 Accuracy 0.8471 | validation: Loss 0.039846 Accuracy -1.5032\n",
      "Epoch 1210 | train: Loss 0.004069 Accuracy 0.8508 | validation: Loss 0.040532 Accuracy -1.5463\n",
      "Epoch 1211 | train: Loss 0.004089 Accuracy 0.8500 | validation: Loss 0.041359 Accuracy -1.5983\n",
      "Epoch 1212 | train: Loss 0.004045 Accuracy 0.8517 | validation: Loss 0.041908 Accuracy -1.6328\n",
      "Epoch 1213 | train: Loss 0.004097 Accuracy 0.8497 | validation: Loss 0.041368 Accuracy -1.5988\n",
      "Epoch 1214 | train: Loss 0.004043 Accuracy 0.8517 | validation: Loss 0.040214 Accuracy -1.5264\n",
      "Epoch 1215 | train: Loss 0.004045 Accuracy 0.8516 | validation: Loss 0.039468 Accuracy -1.4795\n",
      "Epoch 1216 | train: Loss 0.004013 Accuracy 0.8528 | validation: Loss 0.039325 Accuracy -1.4705\n",
      "Epoch 1217 | train: Loss 0.004061 Accuracy 0.8511 | validation: Loss 0.039674 Accuracy -1.4924\n",
      "Epoch 1218 | train: Loss 0.004003 Accuracy 0.8532 | validation: Loss 0.040311 Accuracy -1.5324\n",
      "Epoch 1219 | train: Loss 0.004025 Accuracy 0.8524 | validation: Loss 0.041153 Accuracy -1.5853\n",
      "Epoch 1220 | train: Loss 0.003986 Accuracy 0.8538 | validation: Loss 0.041635 Accuracy -1.6156\n",
      "Epoch 1221 | train: Loss 0.004028 Accuracy 0.8523 | validation: Loss 0.041297 Accuracy -1.5944\n",
      "Epoch 1222 | train: Loss 0.003982 Accuracy 0.8540 | validation: Loss 0.040623 Accuracy -1.5521\n",
      "Epoch 1223 | train: Loss 0.004000 Accuracy 0.8533 | validation: Loss 0.040140 Accuracy -1.5217\n",
      "Epoch 1224 | train: Loss 0.003971 Accuracy 0.8544 | validation: Loss 0.039817 Accuracy -1.5015\n",
      "Epoch 1225 | train: Loss 0.003996 Accuracy 0.8534 | validation: Loss 0.039712 Accuracy -1.4948\n",
      "Epoch 1226 | train: Loss 0.003970 Accuracy 0.8544 | validation: Loss 0.040002 Accuracy -1.5130\n",
      "Epoch 1227 | train: Loss 0.003972 Accuracy 0.8543 | validation: Loss 0.040563 Accuracy -1.5483\n",
      "Epoch 1228 | train: Loss 0.003965 Accuracy 0.8546 | validation: Loss 0.040892 Accuracy -1.5690\n",
      "Epoch 1229 | train: Loss 0.003964 Accuracy 0.8546 | validation: Loss 0.040849 Accuracy -1.5663\n",
      "Epoch 1230 | train: Loss 0.003961 Accuracy 0.8547 | validation: Loss 0.040717 Accuracy -1.5580\n",
      "Epoch 1231 | train: Loss 0.003949 Accuracy 0.8552 | validation: Loss 0.040485 Accuracy -1.5434\n",
      "Epoch 1232 | train: Loss 0.003957 Accuracy 0.8549 | validation: Loss 0.040092 Accuracy -1.5187\n",
      "Epoch 1233 | train: Loss 0.003942 Accuracy 0.8555 | validation: Loss 0.039848 Accuracy -1.5034\n",
      "Epoch 1234 | train: Loss 0.003949 Accuracy 0.8552 | validation: Loss 0.039981 Accuracy -1.5117\n",
      "Epoch 1235 | train: Loss 0.003936 Accuracy 0.8556 | validation: Loss 0.040289 Accuracy -1.5311\n",
      "Epoch 1236 | train: Loss 0.003941 Accuracy 0.8555 | validation: Loss 0.040528 Accuracy -1.5461\n",
      "Epoch 1237 | train: Loss 0.003930 Accuracy 0.8559 | validation: Loss 0.040710 Accuracy -1.5575\n",
      "Epoch 1238 | train: Loss 0.003931 Accuracy 0.8558 | validation: Loss 0.040781 Accuracy -1.5620\n",
      "Epoch 1239 | train: Loss 0.003930 Accuracy 0.8559 | validation: Loss 0.040579 Accuracy -1.5493\n",
      "Epoch 1240 | train: Loss 0.003923 Accuracy 0.8561 | validation: Loss 0.040273 Accuracy -1.5301\n",
      "Epoch 1241 | train: Loss 0.003922 Accuracy 0.8562 | validation: Loss 0.040138 Accuracy -1.5216\n",
      "Epoch 1242 | train: Loss 0.003916 Accuracy 0.8564 | validation: Loss 0.040157 Accuracy -1.5228\n",
      "Epoch 1243 | train: Loss 0.003919 Accuracy 0.8563 | validation: Loss 0.040234 Accuracy -1.5276\n",
      "Epoch 1244 | train: Loss 0.003910 Accuracy 0.8566 | validation: Loss 0.040404 Accuracy -1.5383\n",
      "Epoch 1245 | train: Loss 0.003908 Accuracy 0.8567 | validation: Loss 0.040602 Accuracy -1.5508\n",
      "Epoch 1246 | train: Loss 0.003907 Accuracy 0.8567 | validation: Loss 0.040640 Accuracy -1.5531\n",
      "Epoch 1247 | train: Loss 0.003902 Accuracy 0.8569 | validation: Loss 0.040534 Accuracy -1.5465\n",
      "Epoch 1248 | train: Loss 0.003900 Accuracy 0.8570 | validation: Loss 0.040426 Accuracy -1.5397\n",
      "Epoch 1249 | train: Loss 0.003895 Accuracy 0.8571 | validation: Loss 0.040306 Accuracy -1.5322\n",
      "Epoch 1250 | train: Loss 0.003895 Accuracy 0.8571 | validation: Loss 0.040191 Accuracy -1.5249\n",
      "Epoch 1251 | train: Loss 0.003890 Accuracy 0.8573 | validation: Loss 0.040208 Accuracy -1.5260\n",
      "Epoch 1252 | train: Loss 0.003887 Accuracy 0.8575 | validation: Loss 0.040342 Accuracy -1.5344\n",
      "Epoch 1253 | train: Loss 0.003885 Accuracy 0.8575 | validation: Loss 0.040450 Accuracy -1.5412\n",
      "Epoch 1254 | train: Loss 0.003880 Accuracy 0.8577 | validation: Loss 0.040500 Accuracy -1.5443\n",
      "Epoch 1255 | train: Loss 0.003878 Accuracy 0.8578 | validation: Loss 0.040510 Accuracy -1.5450\n",
      "Epoch 1256 | train: Loss 0.003875 Accuracy 0.8579 | validation: Loss 0.040423 Accuracy -1.5395\n",
      "Epoch 1257 | train: Loss 0.003872 Accuracy 0.8580 | validation: Loss 0.040282 Accuracy -1.5306\n",
      "Epoch 1258 | train: Loss 0.003868 Accuracy 0.8581 | validation: Loss 0.040216 Accuracy -1.5265\n",
      "Epoch 1259 | train: Loss 0.003865 Accuracy 0.8583 | validation: Loss 0.040227 Accuracy -1.5272\n",
      "Epoch 1260 | train: Loss 0.003863 Accuracy 0.8583 | validation: Loss 0.040262 Accuracy -1.5294\n",
      "Epoch 1261 | train: Loss 0.003859 Accuracy 0.8585 | validation: Loss 0.040330 Accuracy -1.5336\n",
      "Epoch 1262 | train: Loss 0.003856 Accuracy 0.8586 | validation: Loss 0.040398 Accuracy -1.5379\n",
      "Epoch 1263 | train: Loss 0.003853 Accuracy 0.8587 | validation: Loss 0.040391 Accuracy -1.5375\n",
      "Epoch 1264 | train: Loss 0.003849 Accuracy 0.8588 | validation: Loss 0.040339 Accuracy -1.5342\n",
      "Epoch 1265 | train: Loss 0.003846 Accuracy 0.8589 | validation: Loss 0.040294 Accuracy -1.5314\n",
      "Epoch 1266 | train: Loss 0.003843 Accuracy 0.8590 | validation: Loss 0.040239 Accuracy -1.5280\n",
      "Epoch 1267 | train: Loss 0.003841 Accuracy 0.8592 | validation: Loss 0.040203 Accuracy -1.5256\n",
      "Epoch 1268 | train: Loss 0.003837 Accuracy 0.8593 | validation: Loss 0.040229 Accuracy -1.5273\n",
      "Epoch 1269 | train: Loss 0.003834 Accuracy 0.8594 | validation: Loss 0.040282 Accuracy -1.5306\n",
      "Epoch 1270 | train: Loss 0.003832 Accuracy 0.8595 | validation: Loss 0.040319 Accuracy -1.5330\n",
      "Epoch 1271 | train: Loss 0.003828 Accuracy 0.8596 | validation: Loss 0.040347 Accuracy -1.5347\n",
      "Epoch 1272 | train: Loss 0.003825 Accuracy 0.8597 | validation: Loss 0.040342 Accuracy -1.5344\n",
      "Epoch 1273 | train: Loss 0.003823 Accuracy 0.8598 | validation: Loss 0.040291 Accuracy -1.5312\n",
      "Epoch 1274 | train: Loss 0.003820 Accuracy 0.8599 | validation: Loss 0.040247 Accuracy -1.5284\n",
      "Epoch 1275 | train: Loss 0.003817 Accuracy 0.8600 | validation: Loss 0.040235 Accuracy -1.5277\n",
      "Epoch 1276 | train: Loss 0.003814 Accuracy 0.8601 | validation: Loss 0.040240 Accuracy -1.5280\n",
      "Epoch 1277 | train: Loss 0.003811 Accuracy 0.8602 | validation: Loss 0.040267 Accuracy -1.5297\n",
      "Epoch 1278 | train: Loss 0.003808 Accuracy 0.8604 | validation: Loss 0.040306 Accuracy -1.5321\n",
      "Epoch 1279 | train: Loss 0.003805 Accuracy 0.8605 | validation: Loss 0.040317 Accuracy -1.5328\n",
      "Epoch 1280 | train: Loss 0.003802 Accuracy 0.8606 | validation: Loss 0.040304 Accuracy -1.5320\n",
      "Epoch 1281 | train: Loss 0.003799 Accuracy 0.8607 | validation: Loss 0.040288 Accuracy -1.5310\n",
      "Epoch 1282 | train: Loss 0.003796 Accuracy 0.8608 | validation: Loss 0.040262 Accuracy -1.5294\n",
      "Epoch 1283 | train: Loss 0.003793 Accuracy 0.8609 | validation: Loss 0.040240 Accuracy -1.5280\n",
      "Epoch 1284 | train: Loss 0.003790 Accuracy 0.8610 | validation: Loss 0.040243 Accuracy -1.5282\n",
      "Epoch 1285 | train: Loss 0.003787 Accuracy 0.8611 | validation: Loss 0.040257 Accuracy -1.5291\n",
      "Epoch 1286 | train: Loss 0.003784 Accuracy 0.8612 | validation: Loss 0.040274 Accuracy -1.5301\n",
      "Epoch 1287 | train: Loss 0.003781 Accuracy 0.8613 | validation: Loss 0.040294 Accuracy -1.5314\n",
      "Epoch 1288 | train: Loss 0.003778 Accuracy 0.8614 | validation: Loss 0.040298 Accuracy -1.5316\n",
      "Epoch 1289 | train: Loss 0.003776 Accuracy 0.8615 | validation: Loss 0.040281 Accuracy -1.5306\n",
      "Epoch 1290 | train: Loss 0.003773 Accuracy 0.8616 | validation: Loss 0.040265 Accuracy -1.5296\n",
      "Epoch 1291 | train: Loss 0.003770 Accuracy 0.8618 | validation: Loss 0.040254 Accuracy -1.5289\n",
      "Epoch 1292 | train: Loss 0.003767 Accuracy 0.8619 | validation: Loss 0.040250 Accuracy -1.5286\n",
      "Epoch 1293 | train: Loss 0.003764 Accuracy 0.8620 | validation: Loss 0.040264 Accuracy -1.5295\n",
      "Epoch 1294 | train: Loss 0.003761 Accuracy 0.8621 | validation: Loss 0.040282 Accuracy -1.5306\n",
      "Epoch 1295 | train: Loss 0.003758 Accuracy 0.8622 | validation: Loss 0.040291 Accuracy -1.5312\n",
      "Epoch 1296 | train: Loss 0.003755 Accuracy 0.8623 | validation: Loss 0.040294 Accuracy -1.5314\n",
      "Epoch 1297 | train: Loss 0.003752 Accuracy 0.8624 | validation: Loss 0.040290 Accuracy -1.5311\n",
      "Epoch 1298 | train: Loss 0.003750 Accuracy 0.8625 | validation: Loss 0.040278 Accuracy -1.5304\n",
      "Epoch 1299 | train: Loss 0.003747 Accuracy 0.8626 | validation: Loss 0.040273 Accuracy -1.5300\n",
      "Epoch 1300 | train: Loss 0.003744 Accuracy 0.8627 | validation: Loss 0.040274 Accuracy -1.5301\n",
      "Epoch 1301 | train: Loss 0.003741 Accuracy 0.8628 | validation: Loss 0.040278 Accuracy -1.5304\n",
      "Epoch 1302 | train: Loss 0.003738 Accuracy 0.8629 | validation: Loss 0.040289 Accuracy -1.5311\n",
      "Epoch 1303 | train: Loss 0.003735 Accuracy 0.8630 | validation: Loss 0.040300 Accuracy -1.5318\n",
      "Epoch 1304 | train: Loss 0.003732 Accuracy 0.8631 | validation: Loss 0.040303 Accuracy -1.5320\n",
      "Epoch 1305 | train: Loss 0.003730 Accuracy 0.8632 | validation: Loss 0.040303 Accuracy -1.5320\n",
      "Epoch 1306 | train: Loss 0.003727 Accuracy 0.8633 | validation: Loss 0.040298 Accuracy -1.5317\n",
      "Epoch 1307 | train: Loss 0.003724 Accuracy 0.8634 | validation: Loss 0.040292 Accuracy -1.5313\n",
      "Epoch 1308 | train: Loss 0.003721 Accuracy 0.8635 | validation: Loss 0.040294 Accuracy -1.5314\n",
      "Epoch 1309 | train: Loss 0.003718 Accuracy 0.8636 | validation: Loss 0.040300 Accuracy -1.5318\n",
      "Epoch 1310 | train: Loss 0.003716 Accuracy 0.8637 | validation: Loss 0.040308 Accuracy -1.5322\n",
      "Epoch 1311 | train: Loss 0.003713 Accuracy 0.8638 | validation: Loss 0.040316 Accuracy -1.5328\n",
      "Epoch 1312 | train: Loss 0.003710 Accuracy 0.8639 | validation: Loss 0.040319 Accuracy -1.5330\n",
      "Epoch 1313 | train: Loss 0.003707 Accuracy 0.8640 | validation: Loss 0.040318 Accuracy -1.5329\n",
      "Epoch 1314 | train: Loss 0.003704 Accuracy 0.8641 | validation: Loss 0.040316 Accuracy -1.5328\n",
      "Epoch 1315 | train: Loss 0.003702 Accuracy 0.8643 | validation: Loss 0.040315 Accuracy -1.5327\n",
      "Epoch 1316 | train: Loss 0.003699 Accuracy 0.8644 | validation: Loss 0.040315 Accuracy -1.5327\n",
      "Epoch 1317 | train: Loss 0.003696 Accuracy 0.8645 | validation: Loss 0.040320 Accuracy -1.5330\n",
      "Epoch 1318 | train: Loss 0.003693 Accuracy 0.8646 | validation: Loss 0.040325 Accuracy -1.5334\n",
      "Epoch 1319 | train: Loss 0.003690 Accuracy 0.8647 | validation: Loss 0.040331 Accuracy -1.5337\n",
      "Epoch 1320 | train: Loss 0.003688 Accuracy 0.8648 | validation: Loss 0.040335 Accuracy -1.5340\n",
      "Epoch 1321 | train: Loss 0.003685 Accuracy 0.8649 | validation: Loss 0.040335 Accuracy -1.5340\n",
      "Epoch 1322 | train: Loss 0.003682 Accuracy 0.8650 | validation: Loss 0.040334 Accuracy -1.5339\n",
      "Epoch 1323 | train: Loss 0.003679 Accuracy 0.8651 | validation: Loss 0.040334 Accuracy -1.5339\n",
      "Epoch 1324 | train: Loss 0.003677 Accuracy 0.8652 | validation: Loss 0.040335 Accuracy -1.5340\n",
      "Epoch 1325 | train: Loss 0.003674 Accuracy 0.8653 | validation: Loss 0.040339 Accuracy -1.5342\n",
      "Epoch 1326 | train: Loss 0.003671 Accuracy 0.8654 | validation: Loss 0.040345 Accuracy -1.5346\n",
      "Epoch 1327 | train: Loss 0.003668 Accuracy 0.8655 | validation: Loss 0.040349 Accuracy -1.5348\n",
      "Epoch 1328 | train: Loss 0.003666 Accuracy 0.8656 | validation: Loss 0.040352 Accuracy -1.5350\n",
      "Epoch 1329 | train: Loss 0.003663 Accuracy 0.8657 | validation: Loss 0.040352 Accuracy -1.5351\n",
      "Epoch 1330 | train: Loss 0.003660 Accuracy 0.8658 | validation: Loss 0.040352 Accuracy -1.5350\n",
      "Epoch 1331 | train: Loss 0.003657 Accuracy 0.8659 | validation: Loss 0.040353 Accuracy -1.5351\n",
      "Epoch 1332 | train: Loss 0.003655 Accuracy 0.8660 | validation: Loss 0.040355 Accuracy -1.5352\n",
      "Epoch 1333 | train: Loss 0.003652 Accuracy 0.8661 | validation: Loss 0.040358 Accuracy -1.5354\n",
      "Epoch 1334 | train: Loss 0.003649 Accuracy 0.8662 | validation: Loss 0.040362 Accuracy -1.5356\n",
      "Epoch 1335 | train: Loss 0.003646 Accuracy 0.8663 | validation: Loss 0.040364 Accuracy -1.5358\n",
      "Epoch 1336 | train: Loss 0.003644 Accuracy 0.8664 | validation: Loss 0.040367 Accuracy -1.5359\n",
      "Epoch 1337 | train: Loss 0.003641 Accuracy 0.8665 | validation: Loss 0.040368 Accuracy -1.5360\n",
      "Epoch 1338 | train: Loss 0.003638 Accuracy 0.8666 | validation: Loss 0.040368 Accuracy -1.5360\n",
      "Epoch 1339 | train: Loss 0.003635 Accuracy 0.8667 | validation: Loss 0.040368 Accuracy -1.5361\n",
      "Epoch 1340 | train: Loss 0.003633 Accuracy 0.8668 | validation: Loss 0.040370 Accuracy -1.5361\n",
      "Epoch 1341 | train: Loss 0.003630 Accuracy 0.8669 | validation: Loss 0.040372 Accuracy -1.5363\n",
      "Epoch 1342 | train: Loss 0.003627 Accuracy 0.8670 | validation: Loss 0.040375 Accuracy -1.5365\n",
      "Epoch 1343 | train: Loss 0.003624 Accuracy 0.8671 | validation: Loss 0.040377 Accuracy -1.5366\n",
      "Epoch 1344 | train: Loss 0.003622 Accuracy 0.8672 | validation: Loss 0.040378 Accuracy -1.5367\n",
      "Epoch 1345 | train: Loss 0.003619 Accuracy 0.8673 | validation: Loss 0.040379 Accuracy -1.5367\n",
      "Epoch 1346 | train: Loss 0.003616 Accuracy 0.8674 | validation: Loss 0.040379 Accuracy -1.5367\n",
      "Epoch 1347 | train: Loss 0.003613 Accuracy 0.8675 | validation: Loss 0.040380 Accuracy -1.5368\n",
      "Epoch 1348 | train: Loss 0.003611 Accuracy 0.8676 | validation: Loss 0.040381 Accuracy -1.5368\n",
      "Epoch 1349 | train: Loss 0.003608 Accuracy 0.8677 | validation: Loss 0.040382 Accuracy -1.5369\n",
      "Epoch 1350 | train: Loss 0.003605 Accuracy 0.8678 | validation: Loss 0.040384 Accuracy -1.5370\n",
      "Epoch 1351 | train: Loss 0.003603 Accuracy 0.8679 | validation: Loss 0.040385 Accuracy -1.5371\n",
      "Epoch 1352 | train: Loss 0.003600 Accuracy 0.8680 | validation: Loss 0.040386 Accuracy -1.5371\n",
      "Epoch 1353 | train: Loss 0.003597 Accuracy 0.8681 | validation: Loss 0.040386 Accuracy -1.5372\n",
      "Epoch 1354 | train: Loss 0.003594 Accuracy 0.8682 | validation: Loss 0.040386 Accuracy -1.5371\n",
      "Epoch 1355 | train: Loss 0.003592 Accuracy 0.8683 | validation: Loss 0.040386 Accuracy -1.5372\n",
      "Epoch 1356 | train: Loss 0.003589 Accuracy 0.8684 | validation: Loss 0.040386 Accuracy -1.5372\n",
      "Epoch 1357 | train: Loss 0.003586 Accuracy 0.8685 | validation: Loss 0.040387 Accuracy -1.5372\n",
      "Epoch 1358 | train: Loss 0.003584 Accuracy 0.8686 | validation: Loss 0.040388 Accuracy -1.5373\n",
      "Epoch 1359 | train: Loss 0.003581 Accuracy 0.8687 | validation: Loss 0.040388 Accuracy -1.5373\n",
      "Epoch 1360 | train: Loss 0.003578 Accuracy 0.8688 | validation: Loss 0.040388 Accuracy -1.5373\n",
      "Epoch 1361 | train: Loss 0.003575 Accuracy 0.8689 | validation: Loss 0.040387 Accuracy -1.5372\n",
      "Epoch 1362 | train: Loss 0.003573 Accuracy 0.8690 | validation: Loss 0.040387 Accuracy -1.5372\n",
      "Epoch 1363 | train: Loss 0.003570 Accuracy 0.8691 | validation: Loss 0.040386 Accuracy -1.5372\n",
      "Epoch 1364 | train: Loss 0.003567 Accuracy 0.8692 | validation: Loss 0.040386 Accuracy -1.5371\n",
      "Epoch 1365 | train: Loss 0.003564 Accuracy 0.8693 | validation: Loss 0.040385 Accuracy -1.5371\n",
      "Epoch 1366 | train: Loss 0.003562 Accuracy 0.8694 | validation: Loss 0.040385 Accuracy -1.5371\n",
      "Epoch 1367 | train: Loss 0.003559 Accuracy 0.8695 | validation: Loss 0.040384 Accuracy -1.5370\n",
      "Epoch 1368 | train: Loss 0.003556 Accuracy 0.8696 | validation: Loss 0.040383 Accuracy -1.5370\n",
      "Epoch 1369 | train: Loss 0.003553 Accuracy 0.8697 | validation: Loss 0.040381 Accuracy -1.5369\n",
      "Epoch 1370 | train: Loss 0.003551 Accuracy 0.8698 | validation: Loss 0.040380 Accuracy -1.5368\n",
      "Epoch 1371 | train: Loss 0.003548 Accuracy 0.8699 | validation: Loss 0.040379 Accuracy -1.5367\n",
      "Epoch 1372 | train: Loss 0.003545 Accuracy 0.8700 | validation: Loss 0.040377 Accuracy -1.5366\n",
      "Epoch 1373 | train: Loss 0.003542 Accuracy 0.8701 | validation: Loss 0.040376 Accuracy -1.5365\n",
      "Epoch 1374 | train: Loss 0.003540 Accuracy 0.8702 | validation: Loss 0.040374 Accuracy -1.5364\n",
      "Epoch 1375 | train: Loss 0.003537 Accuracy 0.8703 | validation: Loss 0.040372 Accuracy -1.5363\n",
      "Epoch 1376 | train: Loss 0.003534 Accuracy 0.8704 | validation: Loss 0.040370 Accuracy -1.5361\n",
      "Epoch 1377 | train: Loss 0.003531 Accuracy 0.8705 | validation: Loss 0.040367 Accuracy -1.5360\n",
      "Epoch 1378 | train: Loss 0.003529 Accuracy 0.8706 | validation: Loss 0.040365 Accuracy -1.5358\n",
      "Epoch 1379 | train: Loss 0.003526 Accuracy 0.8707 | validation: Loss 0.040362 Accuracy -1.5357\n",
      "Epoch 1380 | train: Loss 0.003523 Accuracy 0.8708 | validation: Loss 0.040360 Accuracy -1.5355\n",
      "Epoch 1381 | train: Loss 0.003520 Accuracy 0.8709 | validation: Loss 0.040357 Accuracy -1.5353\n",
      "Epoch 1382 | train: Loss 0.003518 Accuracy 0.8710 | validation: Loss 0.040354 Accuracy -1.5351\n",
      "Epoch 1383 | train: Loss 0.003515 Accuracy 0.8711 | validation: Loss 0.040351 Accuracy -1.5349\n",
      "Epoch 1384 | train: Loss 0.003512 Accuracy 0.8712 | validation: Loss 0.040347 Accuracy -1.5347\n",
      "Epoch 1385 | train: Loss 0.003509 Accuracy 0.8713 | validation: Loss 0.040343 Accuracy -1.5345\n",
      "Epoch 1386 | train: Loss 0.003506 Accuracy 0.8714 | validation: Loss 0.040339 Accuracy -1.5342\n",
      "Epoch 1387 | train: Loss 0.003504 Accuracy 0.8715 | validation: Loss 0.040336 Accuracy -1.5340\n",
      "Epoch 1388 | train: Loss 0.003501 Accuracy 0.8716 | validation: Loss 0.040331 Accuracy -1.5337\n",
      "Epoch 1389 | train: Loss 0.003498 Accuracy 0.8717 | validation: Loss 0.040327 Accuracy -1.5335\n",
      "Epoch 1390 | train: Loss 0.003495 Accuracy 0.8718 | validation: Loss 0.040323 Accuracy -1.5332\n",
      "Epoch 1391 | train: Loss 0.003492 Accuracy 0.8719 | validation: Loss 0.040318 Accuracy -1.5329\n",
      "Epoch 1392 | train: Loss 0.003490 Accuracy 0.8720 | validation: Loss 0.040313 Accuracy -1.5326\n",
      "Epoch 1393 | train: Loss 0.003487 Accuracy 0.8721 | validation: Loss 0.040308 Accuracy -1.5322\n",
      "Epoch 1394 | train: Loss 0.003484 Accuracy 0.8722 | validation: Loss 0.040302 Accuracy -1.5319\n",
      "Epoch 1395 | train: Loss 0.003481 Accuracy 0.8723 | validation: Loss 0.040297 Accuracy -1.5316\n",
      "Epoch 1396 | train: Loss 0.003478 Accuracy 0.8724 | validation: Loss 0.040291 Accuracy -1.5312\n",
      "Epoch 1397 | train: Loss 0.003476 Accuracy 0.8725 | validation: Loss 0.040285 Accuracy -1.5308\n",
      "Epoch 1398 | train: Loss 0.003473 Accuracy 0.8726 | validation: Loss 0.040279 Accuracy -1.5304\n",
      "Epoch 1399 | train: Loss 0.003470 Accuracy 0.8728 | validation: Loss 0.040272 Accuracy -1.5300\n",
      "Epoch 1400 | train: Loss 0.003467 Accuracy 0.8729 | validation: Loss 0.040266 Accuracy -1.5296\n",
      "Epoch 1401 | train: Loss 0.003464 Accuracy 0.8730 | validation: Loss 0.040259 Accuracy -1.5292\n",
      "Epoch 1402 | train: Loss 0.003461 Accuracy 0.8731 | validation: Loss 0.040252 Accuracy -1.5287\n",
      "Epoch 1403 | train: Loss 0.003458 Accuracy 0.8732 | validation: Loss 0.040245 Accuracy -1.5283\n",
      "Epoch 1404 | train: Loss 0.003455 Accuracy 0.8733 | validation: Loss 0.040237 Accuracy -1.5278\n",
      "Epoch 1405 | train: Loss 0.003453 Accuracy 0.8734 | validation: Loss 0.040229 Accuracy -1.5273\n",
      "Epoch 1406 | train: Loss 0.003450 Accuracy 0.8735 | validation: Loss 0.040221 Accuracy -1.5268\n",
      "Epoch 1407 | train: Loss 0.003447 Accuracy 0.8736 | validation: Loss 0.040213 Accuracy -1.5263\n",
      "Epoch 1408 | train: Loss 0.003444 Accuracy 0.8737 | validation: Loss 0.040204 Accuracy -1.5258\n",
      "Epoch 1409 | train: Loss 0.003441 Accuracy 0.8738 | validation: Loss 0.040196 Accuracy -1.5252\n",
      "Epoch 1410 | train: Loss 0.003438 Accuracy 0.8739 | validation: Loss 0.040187 Accuracy -1.5247\n",
      "Epoch 1411 | train: Loss 0.003435 Accuracy 0.8740 | validation: Loss 0.040178 Accuracy -1.5241\n",
      "Epoch 1412 | train: Loss 0.003432 Accuracy 0.8741 | validation: Loss 0.040168 Accuracy -1.5235\n",
      "Epoch 1413 | train: Loss 0.003429 Accuracy 0.8742 | validation: Loss 0.040159 Accuracy -1.5229\n",
      "Epoch 1414 | train: Loss 0.003427 Accuracy 0.8743 | validation: Loss 0.040149 Accuracy -1.5223\n",
      "Epoch 1415 | train: Loss 0.003424 Accuracy 0.8744 | validation: Loss 0.040139 Accuracy -1.5217\n",
      "Epoch 1416 | train: Loss 0.003421 Accuracy 0.8746 | validation: Loss 0.040128 Accuracy -1.5210\n",
      "Epoch 1417 | train: Loss 0.003418 Accuracy 0.8747 | validation: Loss 0.040118 Accuracy -1.5203\n",
      "Epoch 1418 | train: Loss 0.003415 Accuracy 0.8748 | validation: Loss 0.040107 Accuracy -1.5196\n",
      "Epoch 1419 | train: Loss 0.003412 Accuracy 0.8749 | validation: Loss 0.040097 Accuracy -1.5190\n",
      "Epoch 1420 | train: Loss 0.003409 Accuracy 0.8750 | validation: Loss 0.040084 Accuracy -1.5182\n",
      "Epoch 1421 | train: Loss 0.003406 Accuracy 0.8751 | validation: Loss 0.040075 Accuracy -1.5176\n",
      "Epoch 1422 | train: Loss 0.003403 Accuracy 0.8752 | validation: Loss 0.040061 Accuracy -1.5167\n",
      "Epoch 1423 | train: Loss 0.003400 Accuracy 0.8753 | validation: Loss 0.040053 Accuracy -1.5162\n",
      "Epoch 1424 | train: Loss 0.003397 Accuracy 0.8754 | validation: Loss 0.040035 Accuracy -1.5151\n",
      "Epoch 1425 | train: Loss 0.003394 Accuracy 0.8755 | validation: Loss 0.040031 Accuracy -1.5149\n",
      "Epoch 1426 | train: Loss 0.003392 Accuracy 0.8756 | validation: Loss 0.040008 Accuracy -1.5134\n",
      "Epoch 1427 | train: Loss 0.003389 Accuracy 0.8757 | validation: Loss 0.040014 Accuracy -1.5138\n",
      "Epoch 1428 | train: Loss 0.003389 Accuracy 0.8757 | validation: Loss 0.039978 Accuracy -1.5115\n",
      "Epoch 1429 | train: Loss 0.003390 Accuracy 0.8757 | validation: Loss 0.040014 Accuracy -1.5138\n",
      "Epoch 1430 | train: Loss 0.003401 Accuracy 0.8753 | validation: Loss 0.039958 Accuracy -1.5103\n",
      "Epoch 1431 | train: Loss 0.003420 Accuracy 0.8746 | validation: Loss 0.040091 Accuracy -1.5187\n",
      "Epoch 1432 | train: Loss 0.003508 Accuracy 0.8714 | validation: Loss 0.040084 Accuracy -1.5182\n",
      "Epoch 1433 | train: Loss 0.003630 Accuracy 0.8669 | validation: Loss 0.040592 Accuracy -1.5501\n",
      "Epoch 1434 | train: Loss 0.004236 Accuracy 0.8446 | validation: Loss 0.041697 Accuracy -1.6195\n",
      "Epoch 1435 | train: Loss 0.005204 Accuracy 0.8092 | validation: Loss 0.042584 Accuracy -1.6753\n",
      "Epoch 1436 | train: Loss 0.004343 Accuracy 0.8407 | validation: Loss 0.041466 Accuracy -1.6050\n",
      "Epoch 1437 | train: Loss 0.003422 Accuracy 0.8745 | validation: Loss 0.040091 Accuracy -1.5187\n",
      "Epoch 1438 | train: Loss 0.004066 Accuracy 0.8509 | validation: Loss 0.040549 Accuracy -1.5474\n",
      "Epoch 1439 | train: Loss 0.006788 Accuracy 0.7511 | validation: Loss 0.043176 Accuracy -1.7125\n",
      "Epoch 1440 | train: Loss 0.007576 Accuracy 0.7222 | validation: Loss 0.047042 Accuracy -1.9553\n",
      "Epoch 1441 | train: Loss 0.006351 Accuracy 0.7671 | validation: Loss 0.048790 Accuracy -2.0652\n",
      "Epoch 1442 | train: Loss 0.007644 Accuracy 0.7197 | validation: Loss 0.043225 Accuracy -1.7155\n",
      "Epoch 1443 | train: Loss 0.003635 Accuracy 0.8667 | validation: Loss 0.040805 Accuracy -1.5635\n",
      "Epoch 1444 | train: Loss 0.007004 Accuracy 0.7431 | validation: Loss 0.040483 Accuracy -1.5433\n",
      "Epoch 1445 | train: Loss 0.005275 Accuracy 0.8066 | validation: Loss 0.043970 Accuracy -1.7623\n",
      "Epoch 1446 | train: Loss 0.003947 Accuracy 0.8552 | validation: Loss 0.045707 Accuracy -1.8714\n",
      "Epoch 1447 | train: Loss 0.005408 Accuracy 0.8017 | validation: Loss 0.043793 Accuracy -1.7512\n",
      "Epoch 1448 | train: Loss 0.003795 Accuracy 0.8608 | validation: Loss 0.040529 Accuracy -1.5462\n",
      "Epoch 1449 | train: Loss 0.004437 Accuracy 0.8373 | validation: Loss 0.039953 Accuracy -1.5100\n",
      "Epoch 1450 | train: Loss 0.004435 Accuracy 0.8374 | validation: Loss 0.040578 Accuracy -1.5493\n",
      "Epoch 1451 | train: Loss 0.003564 Accuracy 0.8693 | validation: Loss 0.041970 Accuracy -1.6367\n",
      "Epoch 1452 | train: Loss 0.004168 Accuracy 0.8471 | validation: Loss 0.041528 Accuracy -1.6089\n",
      "Epoch 1453 | train: Loss 0.003737 Accuracy 0.8630 | validation: Loss 0.039825 Accuracy -1.5020\n",
      "Epoch 1454 | train: Loss 0.004053 Accuracy 0.8514 | validation: Loss 0.037899 Accuracy -1.3809\n",
      "Epoch 1455 | train: Loss 0.003914 Accuracy 0.8565 | validation: Loss 0.038487 Accuracy -1.4179\n",
      "Epoch 1456 | train: Loss 0.003510 Accuracy 0.8713 | validation: Loss 0.040202 Accuracy -1.5256\n",
      "Epoch 1457 | train: Loss 0.003753 Accuracy 0.8624 | validation: Loss 0.041160 Accuracy -1.5858\n",
      "Epoch 1458 | train: Loss 0.003638 Accuracy 0.8666 | validation: Loss 0.040727 Accuracy -1.5586\n",
      "Epoch 1459 | train: Loss 0.003690 Accuracy 0.8647 | validation: Loss 0.038955 Accuracy -1.4473\n",
      "Epoch 1460 | train: Loss 0.003568 Accuracy 0.8691 | validation: Loss 0.037276 Accuracy -1.3418\n",
      "Epoch 1461 | train: Loss 0.003418 Accuracy 0.8747 | validation: Loss 0.037028 Accuracy -1.3262\n",
      "Epoch 1462 | train: Loss 0.003646 Accuracy 0.8663 | validation: Loss 0.038709 Accuracy -1.4318\n",
      "Epoch 1463 | train: Loss 0.003486 Accuracy 0.8722 | validation: Loss 0.040551 Accuracy -1.5475\n",
      "Epoch 1464 | train: Loss 0.003445 Accuracy 0.8736 | validation: Loss 0.041411 Accuracy -1.6015\n",
      "Epoch 1465 | train: Loss 0.003582 Accuracy 0.8686 | validation: Loss 0.040671 Accuracy -1.5550\n",
      "Epoch 1466 | train: Loss 0.003395 Accuracy 0.8755 | validation: Loss 0.039593 Accuracy -1.4873\n",
      "Epoch 1467 | train: Loss 0.003496 Accuracy 0.8718 | validation: Loss 0.038945 Accuracy -1.4467\n",
      "Epoch 1468 | train: Loss 0.003497 Accuracy 0.8718 | validation: Loss 0.039085 Accuracy -1.4555\n",
      "Epoch 1469 | train: Loss 0.003383 Accuracy 0.8759 | validation: Loss 0.039553 Accuracy -1.4849\n",
      "Epoch 1470 | train: Loss 0.003563 Accuracy 0.8693 | validation: Loss 0.039896 Accuracy -1.5064\n",
      "Epoch 1471 | train: Loss 0.003529 Accuracy 0.8706 | validation: Loss 0.039806 Accuracy -1.5007\n",
      "Epoch 1472 | train: Loss 0.003367 Accuracy 0.8765 | validation: Loss 0.039883 Accuracy -1.5055\n",
      "Epoch 1473 | train: Loss 0.003543 Accuracy 0.8701 | validation: Loss 0.040116 Accuracy -1.5202\n",
      "Epoch 1474 | train: Loss 0.003525 Accuracy 0.8707 | validation: Loss 0.040285 Accuracy -1.5308\n",
      "Epoch 1475 | train: Loss 0.003377 Accuracy 0.8761 | validation: Loss 0.040223 Accuracy -1.5269\n",
      "Epoch 1476 | train: Loss 0.003637 Accuracy 0.8666 | validation: Loss 0.039833 Accuracy -1.5024\n",
      "Epoch 1477 | train: Loss 0.003621 Accuracy 0.8672 | validation: Loss 0.039132 Accuracy -1.4584\n",
      "Epoch 1478 | train: Loss 0.003353 Accuracy 0.8770 | validation: Loss 0.038906 Accuracy -1.4442\n",
      "Epoch 1479 | train: Loss 0.003715 Accuracy 0.8637 | validation: Loss 0.039202 Accuracy -1.4628\n",
      "Epoch 1480 | train: Loss 0.003733 Accuracy 0.8631 | validation: Loss 0.039917 Accuracy -1.5077\n",
      "Epoch 1481 | train: Loss 0.003346 Accuracy 0.8773 | validation: Loss 0.040672 Accuracy -1.5551\n",
      "Epoch 1482 | train: Loss 0.003851 Accuracy 0.8588 | validation: Loss 0.040862 Accuracy -1.5671\n",
      "Epoch 1483 | train: Loss 0.003844 Accuracy 0.8590 | validation: Loss 0.040213 Accuracy -1.5263\n",
      "Epoch 1484 | train: Loss 0.003419 Accuracy 0.8746 | validation: Loss 0.039647 Accuracy -1.4907\n",
      "Epoch 1485 | train: Loss 0.004049 Accuracy 0.8515 | validation: Loss 0.039386 Accuracy -1.4743\n",
      "Epoch 1486 | train: Loss 0.003762 Accuracy 0.8620 | validation: Loss 0.039777 Accuracy -1.4989\n",
      "Epoch 1487 | train: Loss 0.003422 Accuracy 0.8745 | validation: Loss 0.040257 Accuracy -1.5291\n",
      "Epoch 1488 | train: Loss 0.003952 Accuracy 0.8551 | validation: Loss 0.040422 Accuracy -1.5394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-19 23:55:20,820] Trial 4 failed with parameters: {'hidden_units': 11, 'depth': 1, 'learning_rate': 0.0022170431245120644, 'weight_decay': 0.05906297894712533} because of the following error: The number of the values 1500 did not match the number of the objectives 1.\n",
      "[W 2024-05-19 23:55:20,820] Trial 4 failed with value [0.14928029477596283, 0.11359772086143494, 0.08473378419876099, 0.06197739392518997, 0.04529114067554474, 0.03311576321721077, 0.02487329952418804, 0.020083801820874214, 0.018273206427693367, 0.019262051209807396, 0.02217411994934082, 0.026198256760835648, 0.030574439093470573, 0.03465098515152931, 0.03793857619166374, 0.04013620689511299, 0.04112866520881653, 0.04096784070134163, 0.039826661348342896, 0.037950366735458374, 0.03547980636358261, 0.03264990448951721, 0.02976657636463642, 0.027134981006383896, 0.0249557476490736, 0.02328871376812458, 0.022098533809185028, 0.021283846348524094, 0.020862506702542305, 0.02072637900710106, 0.02078433893620968, 0.020887596532702446, 0.020976752042770386, 0.02100229449570179, 0.020949652418494225, 0.020829182118177414, 0.020672332495450974, 0.020520491525530815, 0.020409421995282173, 0.020420163869857788, 0.020511118695139885, 0.02069685235619545, 0.02101636305451393, 0.02145805023610592, 0.021989377215504646, 0.02257687784731388, 0.023185787722468376, 0.023778708651661873, 0.02432629093527794, 0.024802446365356445, 0.02519151382148266, 0.025482177734375, 0.02567838504910469, 0.025789227336645126, 0.025826595723628998, 0.025807050988078117, 0.025751108303666115, 0.025676660239696503, 0.025590823963284492, 0.025510646402835846, 0.025451501831412315, 0.025379987433552742, 0.02533397637307644, 0.025328228250145912, 0.0253754910081625, 0.02547432668507099, 0.02562667615711689, 0.025829963386058807, 0.026069341227412224, 0.026332935318350792, 0.026602935045957565, 0.026860708370804787, 0.02708866447210312, 0.027269860729575157, 0.027383657172322273, 0.0274245236068964, 0.027392836287617683, 0.027294868603348732, 0.027137137949466705, 0.02692601829767227, 0.026689354330301285, 0.026447858661413193, 0.026216359809041023, 0.0260072723031044, 0.025830013677477837, 0.0256862360984087, 0.025574319064617157, 0.02548898197710514, 0.025420187041163445, 0.025365259498357773, 0.02531358227133751, 0.02526063844561577, 0.025198057293891907, 0.02512708120048046, 0.02502799592912197, 0.024904029443860054, 0.024752579629421234, 0.02458150126039982, 0.02440021000802517, 0.024221517145633698, 0.024050548672676086, 0.02389380894601345, 0.023751534521579742, 0.023626381531357765, 0.023516520857810974, 0.023419121280312538, 0.02332773245871067, 0.023239675909280777, 0.023152781650424004, 0.02306552231311798, 0.0229779202491045, 0.022886265069246292, 0.022791968658566475, 0.022697169333696365, 0.022603997960686684, 0.02251407317817211, 0.022435856983065605, 0.022364217787981033, 0.022298434749245644, 0.022237608209252357, 0.02218038961291313, 0.022129656746983528, 0.02208174392580986, 0.022035852074623108, 0.021991966292262077, 0.021952377632260323, 0.021916603669524193, 0.021883759647607803, 0.021860644221305847, 0.021844763308763504, 0.02183554321527481, 0.02183062769472599, 0.021826287731528282, 0.02182331494987011, 0.021821944043040276, 0.02182527631521225, 0.021832896396517754, 0.021845495328307152, 0.021863054484128952, 0.021884962916374207, 0.021907784044742584, 0.021932968869805336, 0.021957552060484886, 0.021983107551932335, 0.022007524967193604, 0.0220316331833601, 0.022058634087443352, 0.022089464589953423, 0.02212759479880333, 0.02217065542936325, 0.02221718616783619, 0.02226838283240795, 0.022322921082377434, 0.02237785793840885, 0.02243148349225521, 0.022483427077531815, 0.02253337949514389, 0.022580130025744438, 0.022626712918281555, 0.022680915892124176, 0.022743836045265198, 0.022813742980360985, 0.022884290665388107, 0.022952750325202942, 0.02301291935145855, 0.02307259850203991, 0.023131370544433594, 0.0231939647346735, 0.02326635643839836, 0.023345982655882835, 0.023425985127687454, 0.023502247408032417, 0.023569636046886444, 0.023629948496818542, 0.02369004301726818, 0.023750731721520424, 0.023815862834453583, 0.023890923708677292, 0.023976026102900505, 0.024058448150753975, 0.024134358391165733, 0.024203823879361153, 0.02427435852587223, 0.024332966655492783, 0.02440050058066845, 0.0244675874710083, 0.024533867835998535, 0.02460319735109806, 0.024673689156770706, 0.024740342050790787, 0.024812839925289154, 0.024883728474378586, 0.02493225783109665, 0.024970117956399918, 0.025005005300045013, 0.025043925270438194, 0.025089632719755173, 0.025139885023236275, 0.025196192786097527, 0.025230785831809044, 0.02525904029607773, 0.025281298905611038, 0.02529689483344555, 0.02529662288725376, 0.02528584934771061, 0.02527002803981304, 0.025258902460336685, 0.0252530574798584, 0.025255396962165833, 0.02525450475513935, 0.02523800916969776, 0.02518642507493496, 0.025121362879872322, 0.02506808564066887, 0.025043649598956108, 0.025046400725841522, 0.025069333612918854, 0.02506849355995655, 0.02502366714179516, 0.024977922439575195, 0.0249667689204216, 0.025015471503138542, 0.025086473673582077, 0.025117255747318268, 0.025095608085393906, 0.025051424279808998, 0.025037260726094246, 0.025086048990488052, 0.02513919584453106, 0.02514052204787731, 0.025085102766752243, 0.025012260302901268, 0.02497280016541481, 0.024976208806037903, 0.02498198114335537, 0.024924062192440033, 0.02482840046286583, 0.024779634550213814, 0.02479522116482258, 0.024811234325170517, 0.02476930059492588, 0.024684222415089607, 0.02463277243077755, 0.024640262126922607, 0.024658488109707832, 0.024630723521113396, 0.02455751970410347, 0.0244999211281538, 0.024496907368302345, 0.02448883466422558, 0.02440367452800274, 0.02431228570640087, 0.024276336655020714, 0.024231357499957085, 0.024149611592292786, 0.024053236469626427, 0.02397751249372959, 0.023912152275443077, 0.023825060576200485, 0.023705193772912025, 0.023632314056158066, 0.02358192950487137, 0.02345588430762291, 0.023345090448856354, 0.02330920659005642, 0.02320437878370285, 0.023083528503775597, 0.02304048091173172, 0.0229509174823761, 0.0228137094527483, 0.022790662944316864, 0.02267342247068882, 0.022585660219192505, 0.022543372586369514, 0.022440241649746895, 0.022392695769667625, 0.022321661934256554, 0.02225056290626526, 0.022232070565223694, 0.022099630907177925, 0.02221701666712761, 0.021845169365406036, 0.022395899519324303, 0.021672792732715607, 0.022014092653989792, 0.022176895290613174, 0.021623237058520317, 0.02205120399594307, 0.022047869861125946, 0.021620534360408783, 0.022124065086245537, 0.021919643506407738, 0.021651243790984154, 0.022172264754772186, 0.021796246990561485, 0.0217728428542614, 0.022165559232234955, 0.021716777235269547, 0.02196034975349903, 0.022022366523742676, 0.021696019917726517, 0.022047579288482666, 0.021820973604917526, 0.02174251526594162, 0.02198161371052265, 0.021627359092235565, 0.021797530353069305, 0.02171415649354458, 0.021507488563656807, 0.021703513339161873, 0.02136124297976494, 0.021471256390213966, 0.02134792134165764, 0.02115289494395256, 0.021269772201776505, 0.02092754654586315, 0.021078376099467278, 0.02078353241086006, 0.020837023854255676, 0.020635370165109634, 0.020604440942406654, 0.020456964150071144, 0.02041281759738922, 0.020273642614483833, 0.020245423540472984, 0.020093273371458054, 0.020093994215130806, 0.01989789307117462, 0.020010849460959435, 0.019619112834334373, 0.020158467814326286, 0.019088618457317352, 0.020545700564980507, 0.01902022771537304, 0.019639067351818085, 0.020060617476701736, 0.01892007887363434, 0.019660400226712227, 0.01974102295935154, 0.018870888277888298, 0.019599560648202896, 0.019493339583277702, 0.01882220059633255, 0.019525615498423576, 0.01925748400390148, 0.0187736377120018, 0.019419049844145775, 0.019015999510884285, 0.01873639039695263, 0.019302550703287125, 0.01875344291329384, 0.018781237304210663, 0.019134055823087692, 0.0185282900929451, 0.01891987770795822, 0.018764756619930267, 0.01852278597652912, 0.018954847007989883, 0.018417444080114365, 0.018855750560760498, 0.018521465361118317, 0.018642213195562363, 0.018699802458286285, 0.018480323255062103, 0.01887725666165352, 0.018317412585020065, 0.01920764520764351, 0.018033789470791817, 0.019584493711590767, 0.01817147433757782, 0.0189104825258255, 0.019105933606624603, 0.01825854554772377, 0.019490310922265053, 0.018623383715748787, 0.018785987049341202, 0.019487157464027405, 0.018564485013484955, 0.019490517675876617, 0.019143229350447655, 0.019044652581214905, 0.019826514646410942, 0.019023727625608444, 0.020014114677906036, 0.019490765407681465, 0.0198343675583601, 0.020159535109996796, 0.019791703671216965, 0.020820515230298042, 0.01988447830080986, 0.02132939174771309, 0.020308000966906548, 0.021103141829371452, 0.02122233621776104, 0.020921582356095314, 0.022046785801649094, 0.02123282290995121, 0.02215735986828804, 0.021971073001623154, 0.02199847623705864, 0.02275838330388069, 0.022176723927259445, 0.023168884217739105, 0.022700177505612373, 0.023122401908040047, 0.023391494527459145, 0.023159567266702652, 0.024002745747566223, 0.023416345939040184, 0.024285821244120598, 0.023877477273344994, 0.02420063316822052, 0.024515025317668915, 0.024266382679343224, 0.025148482993245125, 0.024546658620238304, 0.025363141670823097, 0.024984998628497124, 0.025223780423402786, 0.02569659613072872, 0.025338448584079742, 0.026330774649977684, 0.02563438005745411, 0.026260098442435265, 0.02610057406127453, 0.026060860604047775, 0.02688530832529068, 0.026246005669236183, 0.02716189995408058, 0.02647838182747364, 0.02671961672604084, 0.027064023539423943, 0.026668470352888107, 0.027731211856007576, 0.02689964696764946, 0.0274745374917984, 0.027226973325014114, 0.027126934379339218, 0.02798030525445938, 0.027318531647324562, 0.028224609792232513, 0.027555208653211594, 0.02784857340157032, 0.028139328584074974, 0.027791276574134827, 0.028676720336079597, 0.0278742965310812, 0.028585568070411682, 0.028222745284438133, 0.028433088213205338, 0.028806127607822418, 0.02829574979841709, 0.02903948351740837, 0.028332388028502464, 0.029118603095412254, 0.028745656833052635, 0.029016239568591118, 0.02910979650914669, 0.028845736756920815, 0.029485754668712616, 0.02890871651470661, 0.029744422063231468, 0.028992094099521637, 0.029790032655000687, 0.02917279303073883, 0.029876593500375748, 0.029378972947597504, 0.02989736944437027, 0.0294627845287323, 0.029983222484588623, 0.02951452136039734, 0.030222158879041672, 0.029350854456424713, 0.03065323270857334, 0.02882862091064453, 0.03178143501281738, 0.027916288003325462, 0.03303308039903641, 0.028323333710432053, 0.030514758080244064, 0.03106829710304737, 0.02804318629205227, 0.032351408153772354, 0.02983897365629673, 0.029085801914334297, 0.03207677975296974, 0.028887232765555382, 0.03028017096221447, 0.031876418739557266, 0.029238471761345863, 0.030591577291488647, 0.03102191910147667, 0.029352813959121704, 0.03159172832965851, 0.030821165069937706, 0.029605252668261528, 0.031509701162576675, 0.03016558848321438, 0.030422426760196686, 0.03174354135990143, 0.03005639836192131, 0.030698049813508987, 0.03123452328145504, 0.030159013345837593, 0.03147507831454277, 0.03100213035941124, 0.030362075194716454, 0.03155430033802986, 0.030606377869844437, 0.031092118471860886, 0.03159809112548828, 0.030577288940548897, 0.03149765357375145, 0.03110862337052822, 0.031003175303339958, 0.03180279955267906, 0.03092660754919052, 0.03147931396961212, 0.03141399845480919, 0.03112264908850193, 0.031881000846624374, 0.031198689714074135, 0.031602367758750916, 0.03158697113394737, 0.03134612366557121, 0.03196094557642937, 0.03139815106987953, 0.03183341771364212, 0.03168134018778801, 0.03162795677781105, 0.03201199322938919, 0.031588323414325714, 0.032076023519039154, 0.031709883362054825, 0.031955964863300323, 0.031966209411621094, 0.03188208118081093, 0.03216514736413956, 0.03181532397866249, 0.03222915157675743, 0.03190825879573822, 0.0322926826775074, 0.03201204165816307, 0.032233819365501404, 0.03213014453649521, 0.03226340189576149, 0.03226490318775177, 0.03223632648587227, 0.03234439715743065, 0.03225046023726463, 0.03247315064072609, 0.03223014622926712, 0.03260236233472824, 0.03215080872178078, 0.03284398466348648, 0.03193175420165062, 0.03331845626235008, 0.03140753135085106, 0.034220706671476364, 0.03059331886470318, 0.03513120487332344, 0.030882900580763817, 0.03369957581162453, 0.03288920968770981, 0.031303152441978455, 0.034510288387537, 0.031581345945596695, 0.03334781154990196, 0.03335839509963989, 0.03131432086229324, 0.03411835804581642, 0.0323413610458374, 0.03277219459414482, 0.03376398980617523, 0.03161580115556717, 0.033621665090322495, 0.03298022970557213, 0.03245357796549797, 0.03387605771422386, 0.03216422349214554, 0.03319181874394417, 0.0333670936524868, 0.03244733065366745, 0.03380992263555527, 0.03267260268330574, 0.032964978367090225, 0.0335422046482563, 0.03261502459645271, 0.03371262550354004, 0.03299272060394287, 0.03296969085931778, 0.03360595181584358, 0.03280651196837425, 0.033690571784973145, 0.03314667567610741, 0.033153317868709564, 0.03361017256975174, 0.03297490254044533, 0.03375796973705292, 0.033206384629011154, 0.0334390252828598, 0.03353985399007797, 0.03319092467427254, 0.03382699191570282, 0.03325391560792923, 0.0337420292198658, 0.033406276255846024, 0.03355224430561066, 0.03373522311449051, 0.03346206992864609, 0.033869463950395584, 0.03339089825749397, 0.033932458609342575, 0.033507224172353745, 0.033939383924007416, 0.033605776727199554, 0.033880285918712616, 0.033723458647727966, 0.03387689217925072, 0.03384993225336075, 0.033869221806526184, 0.033919867128133774, 0.03385653346776962, 0.034035954624414444, 0.033870991319417953, 0.03415467217564583, 0.03378359600901604, 0.03435744717717171, 0.03362060710787773, 0.03478216007351875, 0.03312862291932106, 0.03561985492706299, 0.03219173103570938, 0.03687995672225952, 0.03194628283381462, 0.03618358075618744, 0.03394234925508499, 0.033087119460105896, 0.036126647144556046, 0.032710786908864975, 0.03564969822764397, 0.03474988788366318, 0.03272758424282074, 0.03595131263136864, 0.03360240161418915, 0.03457288071513176, 0.03548238053917885, 0.032958973199129105, 0.035321254283189774, 0.034525010734796524, 0.033778682351112366, 0.03573774918913841, 0.0338379368185997, 0.03449467197060585, 0.03511855751276016, 0.033726368099451065, 0.03549738973379135, 0.0346636101603508, 0.03397957980632782, 0.035327665507793427, 0.034169603139162064, 0.0350312665104866, 0.0350860096514225, 0.033987242728471756, 0.035308968275785446, 0.03458184748888016, 0.0347033366560936, 0.03526786342263222, 0.034286174923181534, 0.03523613139986992, 0.0347900427877903, 0.03464910015463829, 0.03538506478071213, 0.03456705063581467, 0.03518657386302948, 0.03490103408694267, 0.03479763865470886, 0.03543490171432495, 0.03471655771136284, 0.03523549064993858, 0.03497519716620445, 0.03503295034170151, 0.035375095903873444, 0.034837476909160614, 0.035382647067308426, 0.03500009700655937, 0.03528366982936859, 0.03524080663919449, 0.03508380427956581, 0.0354580357670784, 0.035049520432949066, 0.03548689931631088, 0.03512812405824661, 0.03547002375125885, 0.03528493270277977, 0.035345062613487244, 0.03542396053671837, 0.03532416373491287, 0.03556963428854942, 0.03525414317846298, 0.03563685715198517, 0.0352649949491024, 0.03576347976922989, 0.035216446965932846, 0.03585927188396454, 0.035149745643138885, 0.0360819436609745, 0.03496231883764267, 0.0364118367433548, 0.034615542739629745, 0.03698623552918434, 0.03409697487950325, 0.03760021552443504, 0.033917222172021866, 0.03745700418949127, 0.03471840173006058, 0.03609007969498634, 0.036088328808546066, 0.03490260988473892, 0.0371067114174366, 0.034579090774059296, 0.036937322467565536, 0.03521765395998955, 0.03583625331521034, 0.036262914538383484, 0.035091862082481384, 0.036907196044921875, 0.03496302664279938, 0.03657247871160507, 0.0355655774474144, 0.03590911254286766, 0.03632528334856033, 0.03535238280892372, 0.03672066703438759, 0.03530845418572426, 0.0365850031375885, 0.03568332642316818, 0.036137375980615616, 0.03617880493402481, 0.035721514374017715, 0.0365535169839859, 0.035547032952308655, 0.03668179735541344, 0.03556092828512192, 0.03659328073263168, 0.03577851131558418, 0.03643631562590599, 0.036014456301927567, 0.036229684948921204, 0.03625921905040741, 0.03609950467944145, 0.036456093192100525, 0.03596369922161102, 0.036636509001255035, 0.03584183007478714, 0.03684986010193825, 0.035661764442920685, 0.037139710038900375, 0.0353868342936039, 0.037527453154325485, 0.03507477417588234, 0.03789878264069557, 0.03496970981359482, 0.03782045841217041, 0.03541911393404007, 0.037162307649850845, 0.03624199330806732, 0.036325547844171524, 0.037033457309007645, 0.03573251888155937, 0.03750351071357727, 0.03551768884062767, 0.0374905951321125, 0.03577836602926254, 0.03709196671843529, 0.03631370887160301, 0.03656768426299095, 0.03685913234949112, 0.036169830709695816, 0.03725350648164749, 0.035972390323877335, 0.037408191710710526, 0.03600426763296127, 0.037345435470342636, 0.03622553125023842, 0.03713773563504219, 0.03651047497987747, 0.03690094128251076, 0.03678964078426361, 0.03670693188905716, 0.03701703995466232, 0.036543793976306915, 0.03723115101456642, 0.03639789670705795, 0.037473972886800766, 0.03620995953679085, 0.03778070956468582, 0.03597352281212807, 0.03813769668340683, 0.035770852118730545, 0.03835592046380043, 0.035845786333084106, 0.038171425461769104, 0.03632156178355217, 0.0376187302172184, 0.03696853667497635, 0.03703118488192558, 0.03753753751516342, 0.036567796021699905, 0.037936825305223465, 0.03629108890891075, 0.03814283013343811, 0.03627556934952736, 0.03809809684753418, 0.03652770444750786, 0.037829745560884476, 0.03694690391421318, 0.03750152885913849, 0.03736410662531853, 0.037199538201093674, 0.03770137578248978, 0.03696770220994949, 0.037965063005685806, 0.03677936643362045, 0.038179513067007065, 0.03663656488060951, 0.03835562989115715, 0.036572784185409546, 0.03843938931822777, 0.036647435277700424, 0.03837207332253456, 0.03687743842601776, 0.03817586600780487, 0.03718056157231331, 0.03793429210782051, 0.03746352344751358, 0.03772585093975067, 0.03768386319279671, 0.037554092705249786, 0.03786652535200119, 0.03739980235695839, 0.038073278963565826, 0.037203915417194366, 0.038374412804841995, 0.03690917789936066, 0.03882177174091339, 0.03654317930340767, 0.03925810009241104, 0.036466822028160095, 0.03917313739657402, 0.03701496496796608, 0.038409288972616196, 0.037850867956876755, 0.03765451908111572, 0.03854726254940033, 0.03708487004041672, 0.03890219330787659, 0.036942366510629654, 0.038782671093940735, 0.037267860025167465, 0.03830258175730705, 0.03779381886124611, 0.03781343251466751, 0.03830118849873543, 0.037487130612134933, 0.03863358497619629, 0.037317998707294464, 0.0387515053153038, 0.03740648552775383, 0.03865749388933182, 0.037608787417411804, 0.03841279819607735, 0.037878163158893585, 0.0382063090801239, 0.03808493912220001, 0.038004081696271896, 0.038259107619524, 0.03785783797502518, 0.03844398632645607, 0.03768591955304146, 0.03868785500526428, 0.03745275363326073, 0.03904375061392784, 0.0371897853910923, 0.03940219059586525, 0.037115488201379776, 0.039382241666316986, 0.03747738152742386, 0.038850583136081696, 0.03804410248994827, 0.0382690504193306, 0.038572654128074646, 0.037800345569849014, 0.038933657109737396, 0.03747228905558586, 0.03911580517888069, 0.037456054240465164, 0.0390256829559803, 0.03769140690565109, 0.03866582736372948, 0.03808404877781868, 0.038381703197956085, 0.03843984380364418, 0.03809504956007004, 0.0387021042406559, 0.03788890689611435, 0.03894880786538124, 0.03771152347326279, 0.03911174088716507, 0.03762431442737579, 0.03917348012328148, 0.03770700469613075, 0.039053983986377716, 0.03790219873189926, 0.03880973160266876, 0.03815152496099472, 0.03862546384334564, 0.038343239575624466, 0.038440026342868805, 0.038473889231681824, 0.038328953087329865, 0.038613904267549515, 0.03818536922335625, 0.03878036513924599, 0.03799089044332504, 0.03910478577017784, 0.03769460320472717, 0.039554059505462646, 0.037504397332668304, 0.0398145392537117, 0.03779315575957298, 0.03918828070163727, 0.03832021355628967, 0.03848668560385704, 0.03898347541689873, 0.03805745393037796, 0.03936655819416046, 0.03783005103468895, 0.039285335689783096, 0.0380496084690094, 0.038880426436662674, 0.0384240448474884, 0.038406308740377426, 0.03882274776697159, 0.03818744048476219, 0.039153117686510086, 0.03804846107959747, 0.039154648780822754, 0.038146767765283585, 0.03908708319067955, 0.03833172097802162, 0.03879858925938606, 0.03852098807692528, 0.03865634277462959, 0.03871862590312958, 0.0384957529604435, 0.03883158415555954, 0.03834318742156029, 0.03902995586395264, 0.03820908069610596, 0.03929593414068222, 0.0380459800362587, 0.0395280160009861, 0.038100406527519226, 0.03959145396947861, 0.03827429190278053, 0.0390629768371582, 0.03859886899590492, 0.0387997142970562, 0.038961831480264664, 0.03847667947411537, 0.03914283215999603, 0.03828039392828941, 0.03934263065457344, 0.03827793151140213, 0.03937477618455887, 0.0383438877761364, 0.03906996175646782, 0.03854779154062271, 0.03895587474107742, 0.038799382746219635, 0.038727182894945145, 0.03890518471598625, 0.03860898315906525, 0.03912684693932533, 0.03847099468111992, 0.03926723077893257, 0.03834187611937523, 0.03951220214366913, 0.03836461529135704, 0.03960194066166878, 0.03847109153866768, 0.03924176096916199, 0.03864907845854759, 0.038955170661211014, 0.038979507982730865, 0.03876248374581337, 0.039164524525403976, 0.0385558046400547, 0.03931877762079239, 0.03854476287961006, 0.039465710520744324, 0.038566526025533676, 0.03929222747683525, 0.03864719346165657, 0.039130888879299164, 0.03882555663585663, 0.03899463266134262, 0.03896059840917587, 0.03885733708739281, 0.03906703740358353, 0.038777004927396774, 0.03922220319509506, 0.03866543248295784, 0.039373550564050674, 0.038605161011219025, 0.03967591002583504, 0.03868971765041351, 0.039777785539627075, 0.03875577077269554, 0.03925995156168938, 0.03887004032731056, 0.03896031156182289, 0.039276011288166046, 0.03885035216808319, 0.03946691006422043, 0.03879751265048981, 0.03946046531200409, 0.03884490579366684, 0.03928201645612717, 0.038935497403144836, 0.039024002850055695, 0.03908374905586243, 0.038950227200984955, 0.0393076129257679, 0.038883525878190994, 0.03934786841273308, 0.038839440792798996, 0.039485856890678406, 0.0389072559773922, 0.039488568902015686, 0.03888199105858803, 0.03932351619005203, 0.03894519805908203, 0.03927409276366234, 0.03905058652162552, 0.0391695499420166, 0.03911423310637474, 0.03913084790110588, 0.039214372634887695, 0.0391123965382576, 0.039283350110054016, 0.039044495671987534, 0.03936712443828583, 0.03898957371711731, 0.039602313190698624, 0.03903872147202492, 0.04006894677877426, 0.039314743131399155, 0.04036066308617592, 0.03908547759056091, 0.03871123492717743, 0.03922167047858238, 0.039294470101594925, 0.040505118668079376, 0.03963256999850273, 0.039388276636600494, 0.03908848389983177, 0.03869481384754181, 0.039638832211494446, 0.03997588902711868, 0.04045696556568146, 0.03937816992402077, 0.038094133138656616, 0.03855649754405022, 0.039845965802669525, 0.041229747235774994, 0.039701689034700394, 0.03821518272161484, 0.0382707379758358, 0.03927217796444893, 0.04089641571044922, 0.03995281085371971, 0.039157986640930176, 0.038964007049798965, 0.038719840347766876, 0.03975360468029976, 0.04004514962434769, 0.040254320949316025, 0.0400245264172554, 0.038503993302583694, 0.03847089409828186, 0.03954978287220001, 0.040562570095062256, 0.04057180881500244, 0.03891110047698021, 0.03833840787410736, 0.039010342210531235, 0.03991149365901947, 0.0402238592505455, 0.039510730654001236, 0.03920363262295723, 0.039063893258571625, 0.03913770616054535, 0.03939032554626465, 0.039620086550712585, 0.03996143490076065, 0.0395231768488884, 0.03903423994779587, 0.03888861835002899, 0.03939090669155121, 0.03993998095393181, 0.03980019688606262, 0.03940233215689659, 0.03903289884328842, 0.039302609860897064, 0.03955376148223877, 0.03982163220643997, 0.03953428193926811, 0.039557378739118576, 0.03935705125331879, 0.039532724767923355, 0.03943061828613281, 0.03987017646431923, 0.039761681109666824, 0.04006793349981308, 0.03957855701446533, 0.0397113636136055, 0.03959968686103821, 0.039857786148786545, 0.039537206292152405, 0.039286740124225616, 0.03944675624370575, 0.039642658084630966, 0.040116287767887115, 0.03988921642303467, 0.03982503339648247, 0.03946808725595474, 0.03934701532125473, 0.0394577719271183, 0.03960295021533966, 0.039928603917360306, 0.03974705561995506, 0.03971032798290253, 0.03941851854324341, 0.0395217165350914, 0.039574868977069855, 0.039833661168813705, 0.0396447516977787, 0.03958194702863693, 0.039427656680345535, 0.03951737657189369, 0.03960675746202469, 0.03973931446671486, 0.039738692343235016, 0.03968322277069092, 0.03960154205560684, 0.039575908333063126, 0.039643362164497375, 0.039715517312288284, 0.03976353257894516, 0.03966628015041351, 0.03963750973343849, 0.03958233818411827, 0.03980433568358421, 0.03995989263057709, 0.04068903997540474, 0.040800463408231735, 0.043069738894701004, 0.040862295776605606, 0.038558751344680786, 0.04071451351046562, 0.04256465286016464, 0.04019901528954506, 0.04394456744194031, 0.04716707766056061, 0.04528241977095604, 0.04062514752149582, 0.0446665920317173, 0.0445093996822834, 0.04135974496603012, 0.04014812409877777, 0.044575441628694534, 0.04406571388244629, 0.042664535343647, 0.042169585824012756, 0.044520411640405655, 0.042638517916202545, 0.040346577763557434, 0.03951213136315346, 0.04068426415324211, 0.0415775328874588, 0.04184231162071228, 0.04130310192704201, 0.04223198816180229, 0.04147690162062645, 0.04025794193148613, 0.03912459313869476, 0.03918201103806496, 0.03974228352308273, 0.04053764045238495, 0.040799856185913086, 0.0414515845477581, 0.041599709540605545, 0.04064613953232765, 0.03936530649662018, 0.03885113447904587, 0.03912665694952011, 0.03984557092189789, 0.04053202643990517, 0.04135920852422714, 0.04190756753087044, 0.04136771708726883, 0.040214285254478455, 0.03946767747402191, 0.03932536393404007, 0.039674028754234314, 0.04031073674559593, 0.041152551770210266, 0.041634880006313324, 0.04129713401198387, 0.04062296077609062, 0.040139857679605484, 0.03981746360659599, 0.039711590856313705, 0.040001921355724335, 0.04056283086538315, 0.04089192673563957, 0.040848977863788605, 0.04071686044335365, 0.040485262870788574, 0.040092192590236664, 0.03984793275594711, 0.03998105973005295, 0.04028930515050888, 0.04052756726741791, 0.04071006923913956, 0.04078056663274765, 0.04057861119508743, 0.04027300700545311, 0.04013795033097267, 0.04015654698014259, 0.04023427516222, 0.04040390998125076, 0.04060249775648117, 0.04064013808965683, 0.04053434357047081, 0.040425971150398254, 0.040306206792593, 0.04019135981798172, 0.040207572281360626, 0.040341753512620926, 0.04044980928301811, 0.040500059723854065, 0.04051002115011215, 0.04042266681790352, 0.04028187319636345, 0.04021559655666351, 0.040226805955171585, 0.04026167839765549, 0.04032987728714943, 0.04039754718542099, 0.04039118066430092, 0.040339477360248566, 0.040293607860803604, 0.04023944213986397, 0.04020261764526367, 0.0402291975915432, 0.04028194397687912, 0.04031921178102493, 0.04034728556871414, 0.04034232720732689, 0.04029127210378647, 0.0402468778192997, 0.04023522883653641, 0.040240101516246796, 0.04026741534471512, 0.040306054055690765, 0.04031682014465332, 0.04030438885092735, 0.04028847813606262, 0.04026187211275101, 0.04023962840437889, 0.04024316370487213, 0.040257323533296585, 0.040273517370224, 0.04029379040002823, 0.04029759392142296, 0.040281228721141815, 0.04026538133621216, 0.04025374725461006, 0.04025012627243996, 0.040264274924993515, 0.04028221219778061, 0.04029061645269394, 0.040294237434864044, 0.04028954356908798, 0.04027775302529335, 0.04027263820171356, 0.04027359187602997, 0.04027779400348663, 0.0402894951403141, 0.04030023515224457, 0.04030335694551468, 0.04030311852693558, 0.040298253297805786, 0.04029201343655586, 0.04029352590441704, 0.040299899876117706, 0.04030759632587433, 0.04031621292233467, 0.04031945392489433, 0.040317803621292114, 0.04031636193394661, 0.040314532816410065, 0.040315091609954834, 0.04032016545534134, 0.0403253510594368, 0.040330588817596436, 0.04033486917614937, 0.04033519700169563, 0.04033441096544266, 0.040334250777959824, 0.04033500328660011, 0.04033917933702469, 0.04034462198615074, 0.04034864902496338, 0.04035177454352379, 0.040352486073970795, 0.0403522364795208, 0.040353305637836456, 0.0403548926115036, 0.04035775363445282, 0.04036152362823486, 0.04036429524421692, 0.04036654159426689, 0.040367696434259415, 0.040367744863033295, 0.040368448942899704, 0.040369704365730286, 0.04037186875939369, 0.04037485271692276, 0.04037686437368393, 0.04037820175290108, 0.04037873446941376, 0.040378790348768234, 0.04037953168153763, 0.040380604565143585, 0.0403820276260376, 0.0403837189078331, 0.04038476198911667, 0.04038553684949875, 0.04038579761981964, 0.040385566651821136, 0.040385786443948746, 0.0403861477971077, 0.04038695991039276, 0.04038771241903305, 0.0403878316283226, 0.04038775712251663, 0.04038708284497261, 0.040386561304330826, 0.040386054664850235, 0.04038558527827263, 0.04038531333208084, 0.04038471356034279, 0.040383920073509216, 0.04038282483816147, 0.040381427854299545, 0.040380023419857025, 0.04037853702902794, 0.0403771735727787, 0.04037567228078842, 0.04037391021847725, 0.04037196934223175, 0.04036971554160118, 0.04036742076277733, 0.040364865213632584, 0.040362320840358734, 0.040359582751989365, 0.04035674408078194, 0.04035377502441406, 0.0403505340218544, 0.04034709930419922, 0.040343306958675385, 0.04033944383263588, 0.0403355248272419, 0.04033144563436508, 0.04032713919878006, 0.04032265022397041, 0.04031785577535629, 0.04031287506222725, 0.0403076596558094, 0.04030226543545723, 0.04029674828052521, 0.04029101878404617, 0.0402851328253746, 0.04027891531586647, 0.040272459387779236, 0.04026570916175842, 0.04025888442993164, 0.04025173932313919, 0.04024457931518555, 0.04023696109652519, 0.040229298174381256, 0.04022117704153061, 0.040213070809841156, 0.0402044802904129, 0.04019589722156525, 0.040186807513237, 0.040177829563617706, 0.04016831889748573, 0.040158916264772415, 0.04014887660741806, 0.04013901576399803, 0.04012833908200264, 0.040118273347616196, 0.04010690003633499, 0.04009684547781944, 0.04008439928293228, 0.040074773132801056, 0.04006071388721466, 0.04005260020494461, 0.04003540426492691, 0.04003126919269562, 0.040007684379816055, 0.040014103055000305, 0.03997780755162239, 0.040013961493968964, 0.039958033710718155, 0.04009133204817772, 0.04008405655622482, 0.040591638535261154, 0.041696615517139435, 0.04258400574326515, 0.041465580463409424, 0.040091488510370255, 0.04054909199476242, 0.04317624866962433, 0.0470416434109211, 0.048790376633405685, 0.043224774301052094, 0.04080544784665108, 0.04048312455415726, 0.043969739228487015, 0.04570701718330383, 0.043792884796857834, 0.040529388934373856, 0.039953406900167465, 0.04057841747999191, 0.041969794780015945, 0.041528236120939255, 0.0398254357278347, 0.03789856284856796, 0.038486920297145844, 0.04020152986049652, 0.04115963354706764, 0.0407266803085804, 0.03895474225282669, 0.0372762531042099, 0.037027761340141296, 0.03870859742164612, 0.040550872683525085, 0.04141072928905487, 0.040670547634363174, 0.03959257900714874, 0.038945477455854416, 0.03908536210656166, 0.03955329209566116, 0.03989628702402115, 0.039806004613637924, 0.03988266736268997, 0.04011606052517891, 0.04028519243001938, 0.04022274166345596, 0.039832767099142075, 0.039132487028837204, 0.038906071335077286, 0.03920161351561546, 0.03991711884737015, 0.040671560913324356, 0.0408620722591877, 0.04021265357732773, 0.03964676335453987, 0.03938567265868187, 0.039776552468538284, 0.04025747627019882, 0.04042152687907219, 0.04017119109630585, 0.03992440178990364, 0.03986966237425804, 0.04002349078655243, 0.03993377089500427, 0.03979507088661194, 0.03967726603150368, 0.03959855064749718, 0.03979622572660446, 0.04008003696799278, 0.04005846753716469, 0.03979235142469406].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1489 | train: Loss 0.003573 Accuracy 0.8690 | validation: Loss 0.040171 Accuracy -1.5237\n",
      "Epoch 1490 | train: Loss 0.003548 Accuracy 0.8699 | validation: Loss 0.039924 Accuracy -1.5082\n",
      "Epoch 1491 | train: Loss 0.003753 Accuracy 0.8623 | validation: Loss 0.039870 Accuracy -1.5047\n",
      "Epoch 1492 | train: Loss 0.003367 Accuracy 0.8765 | validation: Loss 0.040023 Accuracy -1.5144\n",
      "Epoch 1493 | train: Loss 0.003586 Accuracy 0.8685 | validation: Loss 0.039934 Accuracy -1.5088\n",
      "Epoch 1494 | train: Loss 0.003489 Accuracy 0.8720 | validation: Loss 0.039795 Accuracy -1.5000\n",
      "Epoch 1495 | train: Loss 0.003369 Accuracy 0.8764 | validation: Loss 0.039677 Accuracy -1.4926\n",
      "Epoch 1496 | train: Loss 0.003524 Accuracy 0.8708 | validation: Loss 0.039599 Accuracy -1.4877\n",
      "Epoch 1497 | train: Loss 0.003331 Accuracy 0.8779 | validation: Loss 0.039796 Accuracy -1.5001\n",
      "Epoch 1498 | train: Loss 0.003446 Accuracy 0.8736 | validation: Loss 0.040080 Accuracy -1.5179\n",
      "Epoch 1499 | train: Loss 0.003442 Accuracy 0.8738 | validation: Loss 0.040058 Accuracy -1.5166\n",
      "Epoch 1500 | train: Loss 0.003343 Accuracy 0.8774 | validation: Loss 0.039792 Accuracy -1.4999\n",
      "Epoch 1500 | train: Loss 0.003343 Accuracy 0.8774 | validation: Loss 0.039792 Accuracy -1.4999\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m)\n",
      "File \u001b[1;32mc:\\All Python Projects\\Interpreters\\in_Machine_Learning_GPU118\\Lib\\site-packages\\optuna\\study\\study.py:114\u001b[0m, in \u001b[0;36mStudy.best_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[1;32mc:\\All Python Projects\\Interpreters\\in_Machine_Learning_GPU118\\Lib\\site-packages\\optuna\\study\\study.py:157\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\All Python Projects\\Interpreters\\in_Machine_Learning_GPU118\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:234\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[1;34m(self, study_id)\u001b[0m\n\u001b[0;32m    231\u001b[0m best_trial_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mbest_trial_id\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trials are completed yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mdirections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "import DataLoader, Machine_Model, Engine, Information\n",
    "import torch\n",
    "import optuna\n",
    "\n",
    "# seed = 45\n",
    "# pres = [0.0246, 0.025, 0.0254, 0.0271, 0.0279, 0.0396, 0.0404] # [0.0279, 0.0271, 0.0404, 0.0396, 0.025]\n",
    "train_percent = 0.7\n",
    "val_percent = 0.3\n",
    "test_percent = None\n",
    "batch_size = 100\n",
    "# torch.manual_seed(seed)\n",
    "data_path = \"data/data_complete.csv\"\n",
    "\n",
    "def objective(trial):\n",
    "    INPUT_SIZE = 1\n",
    "    HIDDEN_UNITS = trial.suggest_int(\"hidden_units\", 10, 50)\n",
    "    OUTPUT_SIZE = 1\n",
    "    DEPTH = trial.suggest_int('depth', 1, 5)\n",
    "\n",
    "    dataloader = DataLoader.MyDataloader(file_path=data_path, train_percent=train_percent, val_percent=val_percent,\n",
    "                                        test_percent=test_percent, batch_size=batch_size)\n",
    "    train, val, test = dataloader.fit()\n",
    "\n",
    "    try:\n",
    "        print(len(train), len(val), len(test))\n",
    "    except:\n",
    "        print(len(train), len(val))\n",
    "\n",
    "    # INPUT_SIZE = len(Information.features)\n",
    "\n",
    "    # torch.manual_seed(seed)\n",
    "    model0 = Machine_Model.LSTM_V0(input_size=INPUT_SIZE, hidden_size=HIDDEN_UNITS, output_size=3, depth_number=DEPTH)\n",
    "    # torch.manual_seed(seed)\n",
    "    model1 = Machine_Model.NN_V0(input_size=17, hidden_size=HIDDEN_UNITS, output_size=OUTPUT_SIZE)\n",
    "\n",
    "    # EPOCHS_list = [1382, 1423]\n",
    "    # EPOCHS = EPOCHS_list[1]\n",
    "    EPOCHS = 1500\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True)\n",
    "    optimizer = torch.optim.Adam(model0.parameters(), learning_rate)\n",
    "    optimizer1 = torch.optim.Adam(model1.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # torch.manual_seed(seed)\n",
    "    engine = Engine.Machine_Engine(model=model0, model1=model1, train_dataloader=train, val_dataloader=val,\n",
    "                                test_dataloader=test)\n",
    "    # torch.manual_seed(seed)\n",
    "    model0_results = engine.train(loss_fn=loss_fn, optimizer=optimizer, optimizer1=optimizer1, epochs_num=EPOCHS,\n",
    "                                early_stop_patience=None, writer=False, resolution=1)\n",
    "    \n",
    "    return model0_results[\"val_loss\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
